# ShareGPT4V: Improving Large Multi-Modal Models with Better Captions  

Lin Chen $^{*\S1,2}$ , Jinsong $\mathrm{Li^{*\S2}}$ , Xiaoyi Dong2, Pan Zhang2, Conghui $\mathrm{He^{2}}$ , Jiaqi Wang2, Feng Zhao†1, Dahua Lin†2 1University of Science and Technology of China 2Shanghai AI Laboratory chlin@mail.ustc.edu.cn, {lijingsong, dongxiaoyi, zhangpan, heconghui, wangjiaqi}.pjlab.org.cn fzhao956@ustc.edu.cn, dhlin@ie.cuhk.edu.hk  

![](https://cdn-mineru.openxlab.org.cn/extract/81d6ed4f-834d-4880-b1b8-f59bcf410ee6/502d1f6a432ed5f9c9dc167e969749ee8789935c797cae33a498bf407e1a7d55.jpg)  
(a) Comparison of Captions’ Quality  

Figure 1. (a) We showcase a comparison between the caption in our proposed ShareGPT4V dataset and those utilized by recent large multi-modal models (LMMs). Unlike COCO-Caption [7] involves brief human-made captions on the main subject. LLaVA-Instruct [31] combines human-made captions, bounding boxes, and GPT4 [39] to ‘imagine’ the image details, which leads to inevitable error/hallucination description (marked in red). Our approach involves feeding carefully designed prompts along with images directly into the advanced GPT4-Vision [40] and the descriptions are more detailed and accurate (marked in blue). (b) We highlight the remarkable performance of the proposed LMM, ShareGPT4V-7B, developed with the assistance of the ShareGPT4V dataset.  

# Abstract  

In the realm of large multi-modal models (LMMs), efficient modality alignment is crucial yet often constrained by the scarcity of high-quality image-text data. To address this bottleneck, we introduce the ShareGPT4V dataset, a pioneering large-scale resource featuring 1.2 million highly descriptive captions, which surpasses existing datasets in diversity and information content, covering world knowledge, object properties, spatial relationships, and aesthetic evaluations. Specifically, ShareGPT4V originates from a curated 100K high-quality captions collected from advanced GPT4-Vision and has been expanded to 1.2M with a superb caption model trained on this subset. ShareGPT4V first demonstrates its effectiveness for the Supervised Fine-Tuning (SFT) phase, by substituting an equivalent quantity of detailed captions in existing SFT datasets with a subset of our high-quality captions, significantly enhancing the LMMs like LLaVA-7B, LLaVA-1.5-13B, and QwenVL-Chat-7B on the MME and MMBench benchmarks, with respective gains of 222.8/22.0/22.3 and 2.7/1.3/1.5. We further incorporate ShareGPT4V data into both the pre-training and SFT phases, obtaining ShareGPT4V-7B, a superior LMM based on a simple architecture that has remarkable performance across a majority of the multi-modal benchmarks. This project is available at https://ShareGPT4V.github.io to serve as a pivotal resource for advancing the LMMs community.  

# 1. Introduction  

Recent breakthroughs in artificial intelligence have been driven notably by the development of large language models (LLMs) [2, 4, 8, 9, 12, 53, 56]. Following the evolution, modality unification via LLMs becomes the inevitable tendency, and visual-aligned multi-modal LLMs[3, 5, 10, 30, 31, 35, 57, 60–62] have witnessed ever-changing advances in recent days. Putting aside the diversity in model architecture and training data, most of the large multi-modal models (LMMs) adhere to a dual-phase paradigm encompassing a pre-training stage with large-scale image-text pairs for modality alignment, followed by a supervised fine-tuning (SFT) stage that enhances multi-modal capabilities through instruction-format data.  

Despite their efforts and achievements, we argue that the current LMMs still align the modalities in a sub-optimal manner, primarily due to the lack of sufficient high-quality image-text pairs. Vision, inherently rich in information and fine-grained semantics, is often reduced to simplistic captions in mainstream image-text datasets. These captions, typically brief and focused on salient objects, lead to a significant reduction in information content and sub-optimal modality alignment.  

To prove our argument, we conducted a straightforward experiment: we substituted the image-text pairs utilized in the SFT stage of several typical LMMs with equivalent comprehensive captions generated by the advanced GPT4-Vision model and re-benchmarked these LMMs. As shown in Figure 2, such equivalent substitution, despite its relatively minimal extent (only $3.5\%$ of the SFT data in the LLaVA-1.5 case), resulted in consistent performance gains across various LMMs and benchmarks. Encouraged by these promising results, we expanded our efforts to collect high-quality captions on a larger scale, involving two phases. In the initial phase, approximately 100K images from various data sources were gathered. We employed carefully designed data-specific prompts to effectively utilize GPT4-Vision to generate high-quality descriptions. The resulting captions, averaging 942 characters, encompass a comprehensive range of image information, such as world knowledge, object properties, spatial relation, aesthetic evaluation, etc. In the second phase, we utilize these captions to build a strong caption model, which gets rid of the data source specialized prompt and could generate comprehensive captions for given images.  

Based on the above endeavors, we introduce the ShareGPT4V dataset, the first highly descriptive image-text collection. It comprises two components: 100K GPT4- Vision generated captions with diverse image sources and  

1.2M captions crafted by our caption model, which is learned from the 100K high-quality captions. With the aid of this dataset, we have developed an eponymous stateof-the-art large multi-modal model, the ShareGPT4V-7B. To maintain clarity in our discourse, ‘dataset’ or ‘model’ will be distinctly specified when referring to ShareGPT4V. Figure 1(b) shows that ShareGPT4V-7B outperforms other advanced 7B-scale LMMs in all 11 benchmarks, showcasing its competitive performance. For instance, our ShareGPT4V-7B model achieves an impressive total score of 1943.8 on the MME benchmark, surpassing the secondranked Qwen-VL-Chat-7B model, which was trained on 1.4 billion samples, by 95.6 points.  

In a nutshell, our contributions are threefold:  

• We point out the fact that existing low-quality captions can impede the alignment between vision and language modalities of LMMs and we verify it with experimental results. This revelation highlights an urgent requirement within the LMM community for high-quality captions to effectively alleviate such a dilemma.  

• We introduce the ShareGPT4V dataset, a large-scale image-text collection featuring 100K highly descriptive captions generated by GPT4-Vision and 1.2M highquality captions generated by our caption model. The caption covers world knowledge, object attributes, spatial relations, aesthetic assessment, etc. Moreover, the general caption model trained on entire GPT4-Visiongenerated captions could further scale our dataset and will also be available for community usage.  

• Leveraging the proposed dataset, we have developed the ShareGPT4V-7B, an advanced large multimodal model. Despite without elaborate architecture design, this model consistently demonstrates impressive performance across various multi-modal benchmarks.  

# 2. Related Work  

Large Language Models. In recent years, with the surge in data and computational power, the development of large language models has experienced a boom. Early encoderdecoder models like BERT [11] and T5 [46], and decodercentric models such as GPT [44], leveraged the Transformer architecture [54] to excel in various NLP tasks. The success in GPT3 [4] has popularized the use of decoder-only architectures, which rely on auto-regressive decoding for generating predictions. Subsequent models like PaLM [9] extended the limits of model parameters and dataset scale, while others like InstructGPT [42] and ChatGPT [39] introduced fine-tuning and reinforcement learning techniques for improved conversational interaction. These developments, along with contributions from the open-source community [8, 52, 53, 53, 56], have set new benchmarks and opened avenues for future research in NLP area.  

![](https://cdn-mineru.openxlab.org.cn/extract/81d6ed4f-834d-4880-b1b8-f59bcf410ee6/85f0ba472254f4b66452d0ccc4778c7f882549476ce9931cc15f44bd83a53b25.jpg)  
Figure 2. Illustration of the benefits high-quality captions bring to the SFT stage. We compare the performance of various large multi-modal models before and after replacing a corresponding portion of their SFT captions with those generated by GPT4-Vision. The replacement ratio is only $3.5\%$ for LLaVA-1.5 [30] and Qwen-VL-Chat $[3]^{1}$ , and $14.5\%$ for LLaVA [31].  

Large Multi-modal Models. As LLMs rapidly evolve, a faction within the research community is increasingly concentrating on introducing visual knowledge into LLMs. Central to this area are the seminal works in modality alignment within the vision-language learning area [19, 45]. A notable instance is CLIP [45], which exemplifies the alignment of visual and textual modalities through contrastive learning on extensive image-text pairs. A series of works [26, 27] were improved upon CLIP by employing refined data strategies for more diverse data, they have been effective for basic visual tasks [28, 32, 59] but less so for complex tasks like visual question answering. MiniGPT-4 [5], leveraging an LLM [8] and a visual encoder [14], has shown proficiency in image-text dialogues through pre-training alignment and instruction fine-tuning. Subsequent research [3, 6, 10, 25, 31, 43, 57] has further enhanced LMMs by focusing on the quality and diversity of pretraining and finetuning data. For instance, LLaVA [31] and InstructBLIP [10], with improved instruction fine-tuning, have advanced the understanding of complex prompts. mPLUG-Owl [57], Shikra [6], and KOSMOS-2 [43] have introduced new data types and training techniques, like grounding data, to reduce hallucinations and improve LMMs’ grounding capability. Regrettably, it appears that the current LMMs have somewhat overlooked a crucial element: the quality of captions in image-text pairs.  

Image-text Data Enhancement. In the vision-language learning area, several initiatives [13, 16, 23, 38] have been undertaken to enhance the quality of captions within imagetext pairs. LaCLIP [13] leverages LLMs to rewrite raw captions, but its effectiveness is often hindered by hallucinations due to limited visual information and the low quality of original captions. Research [16, 38] explores methods to filter and blend raw and synthetic captions to enhance the CLIP model. A recent work, VeCLIP [23], proposes using LLMs to amalgamate information from both raw and synthetic captions. Nevertheless, the approach is constrained by the low quality of synthetic captions, resulting in only minimal incorporation of visual knowledge in the caption fusion process. To the best of our knowledge, in the LMM area, LLaVA [31] uniquely inputs human-annotated short captions and bounding boxes into the GPT4 language model. This approach lets the model ‘imagine’ viewing the image before producing detailed captions. However, this method relies heavily on extensive human-annotated data and does not allow the model to truly ‘see’ the images. Consequently, it tends to generate detailed descriptions primarily of main objects, often including those in obscure corners but annotated with bounding boxes, leading to potential hallucinations in the LMMs’ output. In contrast, we employ the most advanced LMM, GPT4-Vision, which is capable of directly producing highly descriptive captions from deliberated prompts and corresponding image inputs.  

# 3. ShareGPT4V Dataset  

# 3.1. Overview  

In this section, we provide a detailed exposition of the process involved in creating the ShareGPT4V dataset. Subsection 3.2 elaborates on how we utilized GPT4-Vision to generate 100K high-quality captions from various image sources and briefly validates their significant role in the SFT phase of LMMs. Subsection 3.3 describes our methodology for reasonably expanding the 100K high-quality captions in Sec.3.2 to 1.2M captions, matching the quality generated by GPT4-Vision with acceptable cost. Table 1 presents a comparison between our dataset and existing widely-used caption datasets in the LMM field. Our ShareGPT4V dataset stands out due to its more diverse range of image sources, the use of a more advanced caption producer, a larger number of samples, and the generation of longer captions.  

![](https://cdn-mineru.openxlab.org.cn/extract/81d6ed4f-834d-4880-b1b8-f59bcf410ee6/ebdaad533429d6235bbeab54d4a3aabb6dd75bc1a691eebbf60e0ab82afc8947.jpg)  
Figure 3. An overview for crafting the ShareGPT4V dataset. (a) We illustrate the procedure for collecting highly descriptive captions from GPT4-Vision [40] via various image sources and data-specific prompts, resulting in 100K high-quality captions that encapsulate a wide array of information conveyed by the images. (b) We delineate the process of utilizing the seed captions to train a general captioner and then employing this captioner to generate 1.2M high-quality captions for pre-training usage.   
Table 1. Comparison of widely-used caption datasets and ShareGPT4V. ‘LCS’ abbreviates the LAION [48], CC [50], and SBU [47] datasets. The ‘Visible’ column denotes the image visibility during captioning, and the last column shows the average character number of the caption.  

# 3.2. ShareGPT4V Data Collection  

The supervised fine-tuning captions were collected from GPT4-Vision, the latest and most advanced LMM. For each image selected from a specific data source $D$ , we employed a meticulously crafted, data-specific prompt $P_{D}$ . This prompt instructed GPT4-Vision to generate detailed descriptions, taking into account factors such as world knowledge, object attributes, spatial relationships, and aesthetic evaluations.  

<html><body><table><tr><td>Name</td><td>ImageSource</td><td>VisibleCaptionedby</td><td></td><td>Samples Avg.</td></tr><tr><td>COCO-Caption[7]c0CO[29]</td><td></td><td></td><td>Human</td><td>118K 52</td></tr><tr><td>BLIP-LCS [26]</td><td>LCS</td><td>√</td><td>BLIP [26]</td><td>558K 54</td></tr><tr><td>LLaVA-23K [31]</td><td>COC0[29]</td><td>X</td><td>GPT4[39]</td><td>23K 609</td></tr><tr><td>ShareGPT4V</td><td>LCS,C0CO[29],etc</td><td>√</td><td>GPT4-Vision [40]</td><td>100K 942</td></tr><tr><td>ShareGPT4V-PT</td><td>LCS,C0C0[29],etc</td><td></td><td>Share-Captioner</td><td>1,246K 826</td></tr></table></body></html>  

Data sources. To maximize the diversity and comprehensiveness of our data, we compiled around 100K images from various data sources, including images for detection [29] and segmentation [21], complex text-containing images[51], as well as various web images [41, 48, 50] containing artworks, landmarks, celebrities etc. More details could be found in the supplementary material.  

Prompt Design. Given the diversity of our image sources, we expect a highly content-related description for each image. That is, the captions should extend beyond mere appearance and attributes, incorporating knowledge-related information. For instance, the Eiffel Tower should not be simply described as a tall iron tower, and a picture of Einstein should not be concluded as an old man.  

For the description quality and stability, we designed a base prompt for a general description and added a specialized prompt for each data source. The base prompt asks the GPT4-Vision to describe the basic information of the image, including the object attributes, appearance, and spatial relationships. The specialized prompt focuses on some datarelated information, as shown in Figure 3, we emphasize that the GPT4-Vision should mention some corresponding knowledge, such as the name and geographical location of a landmark-related image. Additionally, we add an aestheticrelated prompt for part of the images, to further improve the comprehensiveness of the description.  

Quality Verification. We conducted a straightforward experiment to verify the quality of the collected data: we chose a range of advanced, publicly available LMMs, including LLaVA-7B [31], LLaVA-1.5-7B [30], LLaVA-1.5- 13B [30], and Qwen-VL-Chat-7B [3]. For a fair comparison, we replaced a corresponding portion of detailed captions in their Supervised Fine-Tuning (SFT) datasets with a selection from our 100K GPT4-Vision-generated captions, while maintaining image data sources as consistent as possible. As depicted in Figure 2, the integration of our highly descriptive captions significantly improved the SFT phase performance across these varied LMMs, reinforcing our pursuit to gather more high-quality captions for potential benefits in the pretraining stage.  

![](https://cdn-mineru.openxlab.org.cn/extract/81d6ed4f-834d-4880-b1b8-f59bcf410ee6/577bbf7f3d8bf4925be2dc8c91fe8d6c15e586908638b77d66b60f1bc799e91f.jpg)  
Figure 4. A qualitative comparison of caption quality from various sources. The COCO [7] captions were generated by humans and the remaining captions were produced by large multi-modal models using the same prompts and images. Mistakes within the captions are highlighted in red, whereas detailed and accurate parts are emphasized in blue. Notably, the image used for this comparison was not included in the training set of our Share-Captioner.  

# 3.3. ShareGPT4V-PT Data Generation  

Compared with the supervised fine-tuning stage, modality alignment in the pre-training phase is more crucial and demands an large-scale dataset. For building a pre-training dataset, we employed the 100K high-quality captions generated by GPT4-Vision to fine-tune an alternative caption model and we have named it as Share-Captioner. Thanks to its training on diverse and comprehensive data, the ShareCaptioner is capable of generating highly content-related descriptions with unified instruction. This approach allows the data scaling phase to proceed without the need for specialized prompt design.  

To amass a substantial volume of high-quality image-text pairs, we selected a subset of 1.2 million images from current public datasets (see supplementary material for more details) and employed our pre-trained Share-Captioner for the captioning process. The entire caption generation process required around 44 A100 GPU days and we name this part of data as ShareGPT4V-PT.  

<html><body><table><tr><td>Preference</td><td>GPT4-Vision</td><td>Share-Captioner</td><td>Comparable</td></tr><tr><td>Percentage</td><td>38.2%</td><td>35.3%</td><td>26.5%</td></tr></table></body></html>

Table 2. Human evaluation on Share-Captioner vs. GPT4-Vision over 100 validation samples and 10 volunteers.  

Qualitative Analysis. For qualitative analysis, Figure 4 presents caption results from human-made COCO-Captions [7], BLIP [26], LLaVA-1.5-7B [30], Share-Captioner, and GPT4-Vision. It is important to note that the images featured in this figure were not part of the training dataset for Share-Captioner. The results depicted in Figure 4 demonstrate that Share-Captioner produced results that are closely comparable to those generated by GPT4-Vision, aligning with our anticipated capabilities for the captioning process.  

Quantitative Analysis. As detailed in Table 2, we generate 100 captions with GPT4-Vision and our Share-Captioner, and invite 10 volunteers to select the better one. As anticipated, our Share-Captioner performs on par with the GPT4- Vision, confirming the quality of the ShareGPT4V dataset.  

<html><body><table><tr><td>Method</td><td>Language Model</td><td>LLaVAW</td><td>MMEP</td><td>MMEC</td><td>MMB</td><td>MMBCN</td><td>SEEDI</td><td>MM-Vet</td><td>QBench</td><td>SQA1</td><td>VQAV2</td><td>VizWiz</td></tr><tr><td>BLIP-2</td><td>FLAN-T5</td><td>38.1</td><td>1293.8</td><td>290.0</td><td></td><td></td><td>46.4</td><td>22.4</td><td></td><td>61.0</td><td>41.0</td><td>19.6</td></tr><tr><td>InstructBLIP</td><td>Vicuna-7B</td><td>60.9</td><td>-</td><td>-</td><td>36.0</td><td>23.7</td><td>53.4</td><td>26.2</td><td>56.7</td><td>60.5</td><td></td><td>34.5</td></tr><tr><td>InstructBLIP</td><td>FLAN-T5</td><td>58.2</td><td>1212.8</td><td>291.8</td><td>-</td><td></td><td></td><td>25.6</td><td></td><td>63.1</td><td></td><td>33.4</td></tr><tr><td>Shikra</td><td>Vicuna-13B</td><td>1</td><td></td><td></td><td>58.8</td><td>1</td><td></td><td></td><td>54.7</td><td></td><td>77.4</td><td></td></tr><tr><td>IDEFICS-80B</td><td>LLaMA-65B</td><td></td><td>-</td><td></td><td>54.5</td><td>38.1</td><td></td><td></td><td></td><td>-</td><td>60.0</td><td>36.0</td></tr><tr><td>Qwen-VL</td><td>Qwen-7B</td><td>1</td><td></td><td>-</td><td>38.2</td><td>7.4</td><td>56.3</td><td></td><td>59.4</td><td>67.1</td><td>78.8</td><td>35.2</td></tr><tr><td>Qwen-VL-Chat</td><td>Qwen-7B</td><td></td><td>1487.5</td><td>360.7</td><td>60.6</td><td>56.7</td><td>58.2</td><td>1</td><td></td><td>68.2</td><td>78.2</td><td>38.9</td></tr><tr><td>LLaVA</td><td>Vicuna-7B</td><td>63.0*</td><td>807.0*</td><td>247.9*</td><td>34.1*</td><td>14.1*</td><td>25.5*</td><td>26.7*</td><td>-</td><td>38.5*</td><td>79.0*</td><td>9.3*</td></tr><tr><td>LLaVA-1.5</td><td>Vicuna-7B</td><td>63.4</td><td>1510.7</td><td>316.1*</td><td>64.3</td><td>58.3</td><td>66.2*</td><td>30.5</td><td>58.7</td><td>66.8</td><td>78.5</td><td>50.0</td></tr><tr><td>LLaVA-1.5</td><td>Vicuna-13B</td><td>70.7</td><td>1531.3</td><td>295.4*</td><td>67.7</td><td>63.6</td><td>68.2</td><td>35.4</td><td>62.1</td><td>71.6</td><td>80.0</td><td>53.6</td></tr><tr><td>ShareGPT4V-7B</td><td>Vicuna-7B</td><td>72.6</td><td>1567.4</td><td>376.4</td><td>68.8</td><td>62.2</td><td>69.7</td><td>37.6</td><td>63.4</td><td>68.4</td><td>80.6</td><td>57.2</td></tr></table></body></html>

Table 3. Comparison with SoTA methods on 11 benchmarks. With 7B parameters, ShareGPT4V-7B outperforms competitors in 9 out of 11 benchmarks and ranks second on the others, despite these competitors using larger training datasets or more parameters. Benchmark names are abbreviated due to space limits. $\mathrm{LLaVA}^{W}$ : LLaVA-Bench (In-the-Wild) [31]; $\mathbf{M}\mathbf{M}\mathbf{E}^{P}$ : MME Perception [15]; $\mathbf{M}\mathbf{E}^{C}$ : MME Cognition [15]; MMB: MMBenchmark [33]; $\mathbf{M}\mathbf{M}\mathbf{B}^{C N}$ : MMBench-Chinese [33]; $\mathrm{SEED}^{I}$ : SEED-Bench (Image) [24]; MM-Vet [58]; QBench [55]; $\operatorname{SQA}^{I}$ : ScienceQA-IMG [34]; $\mathrm{VQA}^{V2}$ [17]; VizWiz [18]. \* indicates our re-implemented test results missed in benchmarks or origin papers. The best results are bold and the second-best results are underlined.  

# 4. ShareGPT4V-7B Model  

To ascertain the efficacy of the ShareGPT4V dataset, we conducted experiments within a fair and controlled setting. This led to the development of ShareGPT4V-7B, a streamlined yet superior baseline LMM leveraging the highquality data from the ShareGPT4V dataset in both the pretraining and SFT stages.  

# 4.1. Model Architecture  

The ShareGPT4V-7B model follows the design of LLaVA1.5 [30], including three integral components: (1) A vision encoder utilizing the CLIP-Large model [45], with a resolution of $336\!\times\!336$ and a patch size of 14, converting input images into 576 tokens. (2) A projector, which is a twolayer multi-layer perception (MLP), is introduced to connect the vision and language modalities. (3) A LLM, based on the open-source Vicuna-v1.5 [8], derived from LLaMA2 [53]. Currently, our focus is on the lightweight 7B model scale, and we have empirically validated that even with lightweight training data and model scale, it can significantly outperform many current LMMs that utilize extensive training datasets or larger model scales.  

# 4.2. Pre-Training  

In the pre-training stage, we utilize the pre-training subset of the ShareGPT4V dataset, i.e., ShareGPT4V-PT. Given these high-quality captions, solely fine-tuning the MLP does not suffice to exploit their full capabilities. In previous LMM research [5, 30, 31, 62], the vision encoder is generally not fine-tuned during pre-training, a rational approach considering the lower quality of previously used captions, where fine-tuned the vision encoder might degrade its visual knowledge extraction ability. We opted for simultaneous fine-tuning of the vision encoder, projector, and large language model. With this configuration, the large language model acquires a native understanding of visual embeddings, while also prompting the vision encoder to create relevant visual embeddings for elements in captions. This setup enables a comprehensive exploration and understanding of the knowledge embedded in visual embeddings, aligned with the intricate details of the captions. Specifically, we consistently applied a learning rate of $2e^{-5}$ across all components, with a batch size set at 256, and the comprehensive optimization process spanned roughly 4700 steps. Notably, we experimentally found that selectively fine-tuning only the latter half of the vision encoder’s layers achieves optimal results, coupled with a satisfactory level of training efficiency.  

# 4.3. Supervised Fine-Tuning.  

As we emphasized above, the goal of this paper is not to build a new SOTA model with some unique architecture designs but to investigate the effectiveness of highquality captions to realize better modality alignment of LMMs. So we utilize the $665\mathrm{k}$ supervised data organized by LLaVA-1.5 and only replace part of it with our ShareGPT4V dataset. In detail, the $665\mathrm{k}$ data is gathered from publicly available academic task-oriented data [1, 20, 22, 36, 37, 49, 51] and instruction-tuning data for conversational and complex reasoning tasks [31] involving natural images [29]. It contains $23\mathbf{k}$ detailed description data and we replaced it with randomly sampled 23K highquality captions from the 100K captions in ShareGPT4V. During the SFT stage, to enhance the training efficiency and compare fairly, we froze the vision encoder and instead focused on fine-tuning the projector and the large language model. The learning rate was established at $2e^{-5}$ , with a batch size of 128, and the total optimization process spanned around 5200 steps.  

![](https://cdn-mineru.openxlab.org.cn/extract/81d6ed4f-834d-4880-b1b8-f59bcf410ee6/e018eaf84df3c61702e578e9d51d1db1fdb51ec44ab644104404520ece418923.jpg)  
Figure 5. Examples of multi-modal dialogue with ShareGPT4V-7B model. High-quality content is highlighted in blue for clarity.  

# 5. Experiments  

# 5.1. Benchmarks  

To thoroughly assess our proposed ShareGPT4V-7B model, we evaluate it across 11 benchmarks, covering a range of academic Visual Question Answering (VQA) tasks and recent benchmarks designed specifically for large multimodal models (LMMs). The LLaVA (in the wild) benchmark [31] is composed of 60 questions, spanning three distinct tasks: conversation, complex reasoning, and detailed description. The MME Benchmark [15] evaluates LMMs’ perception and cognition capabilities through a series of carefully crafted questions across 14 sub-tasks. MMBench and MMBench-CN [33] benchmarks manually design questions to evaluate the model’s vision-related reasoning and perception abilities for English and Chinese, respectively. SEED [24], with the assistance of GPT4, generated a dataset comprising approximately 19K questions related to images and videos. MM-Vet [58] uses GPT4 for a six-dimensional LMM capability assessment. Q-Bench [55] assesses low-level perception, while VQA-v2 [17] and VisWiz [18] are benchmarks in the realm of traditional Visual Question Answering (VQA) tasks.  

strating superior capabilities in tasks such as detailed description and complex reasoning. On the MME Benchmark, it achieved the highest scores in both perception (P) and cognition (C) capabilities, surpassing LLaVA-1.5- 13B in perception by 36.1 points and exceeding QwenVL-Chat, which was trained on 1.4 billion data, by 15.7 points in cognition. Our model also achieved an optimal accuracy of $68.8\%$ on MMBench, leading the secondbest by $1.1\%$ . Furthermore, on the SEED (image) benchmark, which includes 9 assessment dimensions and 14K questions, ShareGPT4V-7B achieved the highest score of $69.7\%$ , $1.5\%$ higher than the second-ranked LLaVA-1.5- 13B. In the low-level image assessment QBench, our model’s top score of $63.4\%$ can be attributed to the diversity of our constructed dataset. Lastly, our model almost consistently performed best on traditional VQA benchmarks with the smallest model size.  

Our findings demonstrate to the community that even with a simple architecture, public data, and lighter parameters (7B), it is possible to outperform many competitors with massive training data and parameter sizes, thanks to the support of these high-quality captions.  

# 5.2. Quantitative Comparison  

We present a quantitative comparison between our proposed ShareGPT4V-7B model with existing state-of-theart LMMs. Notably, compared with previous LMMs, our ShareGPT4V-7B attained the most superior performance in 9 out of the total 11 benchmarks.  

Specifically, our ShareGPT4V-7B model outperformed the previously best-performing LLaVA-1.5-13B model by 1.9 points on the LLaVA (in the wild) benchmark, demon  

# 5.3. Multi-modal Dialogue  

In Figure 5, we present two representative examples within multi-modal dialogue scenarios. The figure demonstrates that our ShareGPT4V-7B exhibits satisfactory capabilities in understanding image details and performing aesthetic assessments. This further corroborates the significance of the high-quality captions we have collected.  

<html><body><table><tr><td>Pre-training with ShareGPT4V-PT</td><td>SFTwith ShareGPT4V</td><td>MMEP</td><td>MMB</td><td>SEED'</td></tr><tr><td></td><td></td><td>1510.7</td><td>64.3</td><td>66.2</td></tr><tr><td></td><td>√</td><td>1542.1</td><td>66.8</td><td>66.7</td></tr><tr><td>√</td><td>×</td><td>1557.2</td><td>67.4</td><td>68.5</td></tr><tr><td>√</td><td></td><td>1567.4</td><td>68.8</td><td>69.7</td></tr></table></body></html>  

Table 4. Ablation study of the training strategy. The ShareGPT4V dataset improves the model performance in both the pre-training and supervised fine-tuning stages.   


<html><body><table><tr><td>Method</td><td>MMEP</td><td>MMBench</td><td>SEED'</td></tr><tr><td>Basline</td><td>1516.9</td><td>65.3</td><td>66.8</td></tr><tr><td>+BLIP-558K</td><td>1521.6</td><td>66.2</td><td>66.9</td></tr><tr><td>+ShareGPT4V-PT-558K</td><td>1539.8</td><td>68.3</td><td>68.9</td></tr></table></body></html>

Table 5. Ablation on the pre-training caption quality. Based on the baseline, the second and third rows share the same end-to-end training strategy and images, but different captions from the BLIP captioner or our ShareGPT4V-PT dataset.  

# 5.4. Ablations  

Effectiveness of ShareGPT4V Dataset. As shown in Table 4, we conducted a thorough ablation study to assess the impact of the ShareGPT4V-PT and ShareGPT4V subsets. Our baseline is the LLaVA-1.5-7B model, without utilizing the ShareGPT4V dataset in either pretraining or SFT stages. Utilizing only our ShareGPT4V subset during the SFT stages resulted in a significant increase of 31.4 points in MME perception score, and improvements of $2.5\%$ and $0.5\%$ in accuracy on the MMBench and SEED benchmarks, respectively. Notably, ShareGPT4V used here was selected from various data sources, yielding more performance gains than those from solely the COCO dataset (see in Figure 2). When only the ShareGPT4V-PT subset was used during pretraining, we observed a remarkable gain of 46.5 points in MME perception, along with substantial accuracy improvements of $3.1\%$ and $2.3\%$ on the MMBench and SEED benchmarks, respectively. Moreover, employing the ShareGPT4V dataset in both pretraining and SFT phases led to further satisfactory enhancements in overall performance, effectively validating the necessity of incorporating high-quality captions in both training stages.  

Pre-training Caption Quality. Then we study how the caption quality influences the pre-training performance. For a fair comparison, we pre-train the model with the same setting and images, but the captions are generated by different models. In detail, we use the 558K LAION-CCSUB image-text pairs captioned by the BLIP as the baseline and replace the text with the high-quality one in our ShareGPT4V-PT.  

As results shown in Table 5, comparing with the baseline, the joint training strategy with the BLIP-558K data gets better results on all the benchmarks, while the gain is quite minor that only 4.7 in MME Perception and 0.1 on SEED Bench. When we replace the captions with our ShareGPT4V-PT-558K, the model gets significant gains. In detail, it gets 1549.8, 68.3, 68.9 on the three benchmarks, surpassing the BLIP-558K case with 18.2, 1.9 and 2.0 respectively. This proves the essential of high-quality captions for effective pre-training and modality alignment.  

![](https://cdn-mineru.openxlab.org.cn/extract/81d6ed4f-834d-4880-b1b8-f59bcf410ee6/40ad2f14f4b15261e9d166c7af6327bb2836b3cccd812ba9c54364b48fbaaf97.jpg)  
Figure 6. The pre-training data scaling performance on MMBench and SEED Bench. The model shows consistent gain with more pre-training data.  

Number of Captions in Pre-training. In Figure 6, we present our investigation into the required quantity of highquality captions for the pre-training stage. Here we randomly sample the data from the ShareGPT4V-PT and train the model with the subset, which varies from 100K to 1200K. The results show that with only 100K high-quality data, the model has a significant improvement on both benchmarks, this further proves the effectiveness of the high-quality data. Meanwhile, with the scaling of training data, the model performance tends to be saturated after more than 1000K data being used for pre-training. This may indicate that with high-quality captions, the modal alignment could be quite efficient and realized with a relatively lightweight data scale.  

Number of Learnable ViT Blocks in Pre-training. As detailed in Table 6, we extensively investigated the optimal approach for fine-tuning the vision encoder during the pretraining phase. Compared to freezing the vision encoder during the pretraining phase, we found that unlocking the latter half of its transformer blocks significantly enhances performance. Specifically, such an approach led to a 52.2 gain on the MME perception benchmark, and substantial accuracy improvements of $2.2\%$ and $1.6\%$ on the MMBench and SEED benchmarks, respectively. This suggests that for high-quality captions, unlocking the vision encoder facilitates more effective modality alignment.  

<html><body><table><tr><td>TunefromBlock</td><td>Memory Usage</td><td>MMEP</td><td>MMB</td><td>SEED'</td></tr><tr><td>24</td><td>49.6GB</td><td>1515.2</td><td>66.6</td><td>68.1</td></tr><tr><td>18</td><td>53.2GB</td><td>1556.0</td><td>67.2</td><td>69.3</td></tr><tr><td>12</td><td>56.7GB</td><td>1567.4</td><td>68.8</td><td>69.7</td></tr><tr><td>6</td><td>60.0GB</td><td>1529.5</td><td>67.7</td><td>69.6</td></tr><tr><td></td><td>63.6GB</td><td>1545.7</td><td>68.5</td><td>69.2</td></tr></table></body></html>

Table 6. Ablation study about the number of learnable blocks in the vision encoder.  

# 6. Conclusion  

In this study, we introduce ShareGPT4V, a groundbreaking large-scale image-text dataset with 1.2 million detailed and informative captions that surpass existing datasets in terms of richness and diversity, covering world knowledge, object attributes, spatial relationships, and aesthetic assessments. ShareGPT4V comprises 100K high-quality captions from GPT4-Vision for Supervised Fine-Tuning (SFT), expanded to 1.2 million for pre-training through a general caption model. We validated ShareGPT4V’s effectiveness through SFT results on recent LMMs and further demonstrated its capabilities with the superior performance of our ShareGPT4V-7B model, which incorporates the dataset in both pre-training and SFT stages. We are committed to making ShareGPT4V fully accessible to the public, with the aspiration that it becomes a foundational resource in advancing the field of LMMs.  

# A. Data Sources  

Data Source Composition for ShareGPT4V. To maximize the comprehensiveness of our captions, we compiled a total of 100K images from diverse sources. This includes 50K images from COCO [29], 30K images from ’LCS’ (which abbreviates LAION [48], CC-3M [50], and SBU [41]), 20K images from SAM [21], 500 images from TextCaps [51], 500 images from WikiArt [47], and 1K images from webcrawled data (split evenly between images of landmarks and images of celebrities).  

Data Source Composition for ShareGPT4V-PT. We utilized our pre-trained Share-Captioner to generate the pretraining dataset. This dataset is comprised of a subset of 1.2M images selected from existing public datasets. These include 118K images from COCO [29],570K images from SAM [21], and 558K images from LLaVA-1.5 pre-training data [30].  

# B. Caption Analysis  

Figure 7 provides a visualization of the root noun-verb pairs for the captions generated by both GPT4-Vision and ShareCaptioner. It’s clear to see that the diversity and linguistic expression of the captions produced by Share-Captioner are comparable to those of GPT4-Vision.  

We analyzed the lexical composition of the captions produced by GPT4-Vision and Share-Captioner, and the results are presented in Table 7. The analysis reveals that the captions generated by our Share-Captioner contain a comparable amount of information to those generated by GPT4- Vision.  

![](https://cdn-mineru.openxlab.org.cn/extract/81d6ed4f-834d-4880-b1b8-f59bcf410ee6/9e6e1527728484701d3f8bab0616da1b441f4143b740bff144c485e5e2a2b640.jpg)  
Figure 7. Analysis of captions generated by GPT4-Vision and Share-Captioner. Visualization of the root noun-verb pairs (occurring over $1\%$ ) of the captions.   
(b) Share-Captioner Generated Caption  

<html><body><table><tr><td>Lexical</td><td>n. adj. adv. V.</td><td>num.</td><td>prep.</td></tr><tr><td>GPT4-Vision</td><td>27.3% 9.5% 2.0% 12.3%</td><td>0.5%</td><td>11.4%</td></tr><tr><td>Share-Captioner</td><td>27.4% 8.8% 1.5%</td><td>12.5% 0.4%</td><td>11.5%</td></tr></table></body></html>

Table 7. Comparison of lexical composition of the captions generated by GPT4-Vision and Share-Captioner.  

# C. Prompts  

Given the diversity of our image sources, we expect a highly content-related description for each image. As shown in Figure 8, we designed a base prompt for a general description and added a specialized prompt for each data source.  

# D. Examples  

![](https://cdn-mineru.openxlab.org.cn/extract/81d6ed4f-834d-4880-b1b8-f59bcf410ee6/a27a34b8836a89bc2718c52e48242863550ad261bcc5e2fa4e729eb604aa8dda.jpg)  

Figure 8. Prompts for instructing GPT4-Vision to generate detailed descriptions. The Prompts are designed with base prompts at the beginning and end, with a data-specific prompt placed in between.  

![](https://cdn-mineru.openxlab.org.cn/extract/81d6ed4f-834d-4880-b1b8-f59bcf410ee6/8f1a87a6d26486869def9ecc6112b12def1387bc2480b5722a9043989d981373.jpg)  

![](https://cdn-mineru.openxlab.org.cn/extract/81d6ed4f-834d-4880-b1b8-f59bcf410ee6/a269a90ec32b60e23da68052551f02be0a321fefa4cf1d3165193a801b070d50.jpg)  

Share-Captioner: In the tranquil setting of a lush green lawn, three vintage suitcases, each a different size, are neatly arranged in a row. The smallest suitcase is positioned on the left, followed by the medium-sized one in the middle, and the largest suitcase on the right. Each suitcase is crafted from brown leather and features a sturdy brass handle on top, adding a touch of vintage charm to their appearance.  

The suitcases are not alone in this scene. Behind them, a brick wall stands tall, providing a rustic backdrop to the scene. Nestled against this wall is a small tree, its branches reaching out as if to touch the suitcases. Adding a splash of color to the scene is a blue tarp, spread out on the ground in front of the wall.  

The image captures a moment of stillness, with the suitcases waiting patiently for their next journey. The precise arrangement of the suitcases and their relative positions to each other and their surroundings create a harmonious composition that is pleasing to the eye.  

![](https://cdn-mineru.openxlab.org.cn/extract/81d6ed4f-834d-4880-b1b8-f59bcf410ee6/74b1ed523874c6b2581552b13a89044a211ccea95c109d0bc3932ac3636ffd10.jpg)  

Share-Captioner: The image captures a serene scene at a waterfront. Dominating the left side of the frame is a tall, brown clock tower with a pointed roof. The tower is adorned with a clock face on each of its four sides, allowing time to be read from any direction.  

Adjacent to the clock tower, on the right side of the image, is a green building with a curved roof. The building's green color contrasts with the brown of the clock tower, adding a splash of color to the scene.  

In the foreground, a body of water stretches out, reflecting the sky above and the buildings below. The water's surface is calm, mirroring the tranquility of the scene.In the background, beyond the clock tower and green building, construction cranes can be seen, hinting at ongoing development in the area. Despite this, the overall atmosphere of the image remains peaceful and serene.  

![](https://cdn-mineru.openxlab.org.cn/extract/81d6ed4f-834d-4880-b1b8-f59bcf410ee6/7c97801cb55847321a5f2b46e7bd329b1af5b5f273879fad2e6f8e631b422e9d.jpg)  

![](https://cdn-mineru.openxlab.org.cn/extract/81d6ed4f-834d-4880-b1b8-f59bcf410ee6/09f3d53f40fddf0b173eab38bb2579f2ed7fbbafc30468e45c35bcc633fe19bd.jpg)  

![](https://cdn-mineru.openxlab.org.cn/extract/81d6ed4f-834d-4880-b1b8-f59bcf410ee6/6e3223daca1c9bd0f1bc08f2e5be7652e3e3105304c87fde1ba03208477d6b8b.jpg)  

Share-Captioner: The image captures a vibrant scene of a train journey. Dominating the frame is a train, painted in a striking combination of yellow and blue. The train, composed of three carriages, is in motion, as suggested by the blurred background. The number 2 is prominently displayed on the side of the train, indicating its carriage number.  

The train is traveling on a railway track that cuts through a serene landscape. The track is nestled amidst lush greenery, adding a touch of nature to the industrial scene. Above, the sky is a clear blue, dotted with fluffy white clouds, suggesting a bright and sunny day. The perspective of the image is particularly interesting. It appears to be taken from the window of another train, as indicated by the blurred background. This gives the viewer the impression of being part of the journey, looking out at the world passing by.  

Overall, the image beautifully encapsulates a moment in a train journey, with its vivid colors and dynamic composition.  

Figure 9. Examples of captions generated by Share-Captioner. Detailed and accurate parts within the captions are emphasized in blue.  

![](https://cdn-mineru.openxlab.org.cn/extract/81d6ed4f-834d-4880-b1b8-f59bcf410ee6/37171b7959981c2f0dd07427d81cb08fadfd52f51ebf12e427faadcb86b6ce6a.jpg)  

![](https://cdn-mineru.openxlab.org.cn/extract/81d6ed4f-834d-4880-b1b8-f59bcf410ee6/84c729b34eac9795f1a30ec551bd816fd811cf49190a6196edb759c553f1eafe.jpg)  

ShareGPT4V: The image is a captivating oil painting titled "The Sower", created by the renowned artist Vincent Van Gogh. It's a beautiful representation of post-impressionism art. The painting vividly portrays a farmer in the act of sowing seeds in a field. The field, awash with hues of yellow and blue, forms a striking contrast against the farmer's figure. In the distance, a quaint village landscape unfolds, complete with houses and trees. The colors used are vibrant, bringing the scene to life with their intensity. The brushstrokes are clearly visible, adding texture and depth to the painting.  

This artwork is a testament to Van Gogh's fascination with the countryside and the simple, yet profound, peasant life.  

![](https://cdn-mineru.openxlab.org.cn/extract/81d6ed4f-834d-4880-b1b8-f59bcf410ee6/22da907c2b1ef04fc6dc54908837ea80ef0a3dde3f3bacf2aee311b9d3b95e94.jpg)  

ShareGPT4V: The image captures a charming Halloween scene. Dominating the frame is a large, glowing orange pumpkin. It's been skillfully carved to display a classic jack-o'-lantern face, with triangular eyes and a toothy grin. The word "FERGUSON" is also carved into the pumpkin, perhaps indicating the name of the family who owns this decoration. The pumpkin's interior is illuminated, casting a warm light that accentuates the carved features.  

Beside the orange pumpkin, there's a smaller white pumpkin. It's uncarved, providing a simple yet effective contrast to its larger, more elaborate neighbor. Adding a touch of whimsy to the scene is a silver owl figurine. Its metallic sheen stands out against the natural textures of the pumpkins.  

All three objects are arranged on a wooden surface, which provides a rustic backdrop to the scene. Behind them, a gray wall further emphasizes the vibrant orange of the carved pumpkin. The relative positions of the objects suggest a thoughtful arrangement, with the glowing orange pumpkin as the centerpiece, flanked by the white pumpkin and the silver owl.  

Despite the simplicity of the scene, it's clear that each object has been chosen and positioned with care, creating a Halloween decoration that's both festive and welcoming.  

![](https://cdn-mineru.openxlab.org.cn/extract/81d6ed4f-834d-4880-b1b8-f59bcf410ee6/da1a16ddde39e3c9f2d21cf0a4b94d5c243edf8bb52eae63e885da084c501050.jpg)  

![](https://cdn-mineru.openxlab.org.cn/extract/81d6ed4f-834d-4880-b1b8-f59bcf410ee6/253a83e8d6f13c81674369fb473ab28b343783c98429cc29b9addb8209e3e7f9.jpg)  

ShareGPT4V: The image captures a delightful scene of a meal, likely at a Chinese restaurant. The main focus is a white plate with a blue rim, presenting a dish of beef and broccoli. The beef, brown in color, and the broccoli, a vibrant green, are generously coated in a brown sauce, adding a glossy finish to the dish.  

![](https://cdn-mineru.openxlab.org.cn/extract/81d6ed4f-834d-4880-b1b8-f59bcf410ee6/1b78236020c28155b85454d9c8c4f6d6accfba5bd276a55e9e66c3adff79ce27.jpg)  

To the left of the plate, a spoon rests, ready to serve the delicious food. The plate itself is placed on a white tablecloth, providing a clean and simple backdrop that allows the colors of the food to stand out.  

In the background, there are hints of other dishes, one of which appears to have a red sauce. These dishes add depth to the scene, suggesting a variety of flavors waiting to be explored. The image as a whole paints a picture of a hearty and appetizing meal, inviting the viewer to partake in the culinary experience.  

Figure 10. Examples of captions generated by ShareGPT4V. Detailed and accurate parts within the captions are emphasized in blue.  

![](https://cdn-mineru.openxlab.org.cn/extract/81d6ed4f-834d-4880-b1b8-f59bcf410ee6/7985fd2820398f9f869a9acba738f2bc1920e6a8affc54c6c14ef5d945d01098.jpg)  
Figure 11. Example of Multi-Round Dialog of ShareGPT4V-7B. Detailed and accurate parts within the captions are emphasized in blue.  

# References  

[1] Sharegpt. https://sharegpt.com/, 2023. 6 [2] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. 2 [3] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 2,   
3, 4 [4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020. 2 [5] Jun Chen, Deyao Zhu1 Xiaoqian Shen1 Xiang Li, Zechun Liu2 Pengchuan Zhang, Raghuraman Krishnamoorthi2 Vikas Chandra2 Yunyang Xiong, and Mohamed Elhoseiny. Minigpt-v2: Large language model as a unified interface for vision-language multi-task learning. arXiv preprint arXiv:2310.09478, 2023. 2, 3, 6 [6] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llm’s referential dialogue magic. arXiv preprint arXiv:2306.15195, 2023. 3 [7] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dolla´r, and C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015. 1, 4, 5 [8] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with $90\%^{*}$ chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2023.   
2, 3, 6 [9] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. 2 [10] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards generalpurpose vision-language models with instruction tuning,   
2023. 2, 3 [11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. 2 [12] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive blank infilling. arXiv preprint arXiv:2103.10360, 2021. 2 [13] Lijie Fan, Dilip Krishnan, Phillip Isola, Dina Katabi, and Yonglong Tian. Improving clip training with language rewrites. arXiv preprint arXiv:2305.20088, 2023. 3   
[14] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19358–19369, 2023. 3   
[15] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Rongrong Ji. Mme: A comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. 6, 7   
[16] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. arXiv preprint arXiv:2304.14108, 2023. 3   
[17] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6904–6913, 2017. 6, 7   
[18] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3608–3617, 2018. 6, 7   
[19] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International conference on machine learning, pages 4904–4916. PMLR, 2021. 3   
[20] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 787–798, 2014. 6   
[21] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023. 4, 9   
[22] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123:32–73, 2017. 6   
[23] Zhengfeng Lai, Haotian Zhang, Wentao Wu, Haoping Bai, Aleksei Timofeev, Xianzhi Du, Zhe Gan, Jiulong Shan, Chen-Nee Chuah, Yinfei Yang, et al. From scarcity to efficiency: Improving clip training via visual-enriched captions. arXiv preprint arXiv:2310.07699, 2023. 3   
[24] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. 6, 7   
[25] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726, 2023. 3   
[26] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International Conference on Machine Learning, pages 12888– 12900. PMLR, 2022. 3, 4, 5   
[27] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. 3   
[28] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded language-image pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10965–10975, 2022. 3   
[29] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dolla´r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740–755. Springer, 2014. 4, 6, 9   
[30] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023. 2, 3, 4, 5, 6, 9   
[31] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023. 1, 2, 3, 4, 6, 7   
[32] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023. 3   
[33] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023. 6, 7   
[34] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:2507–2521, 2022. 6   
[35] Gen Luo, Yiyi Zhou, Tianhe Ren, Shengxin Chen, Xiaoshuai Sun, and Rongrong Ji. Cheap and quick: Efficient visionlanguage instruction tuning for large language models. arXiv preprint arXiv:2305.15023, 2023. 2   
[36] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual question answering benchmark requiring external knowledge. In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition, pages 3195–3204, 2019. 6   
[37] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question answering by reading text in images. In 2019 international conference on document analysis and recognition (ICDAR), pages 947– 952. IEEE, 2019. 6   
[38] Thao Nguyen, Samir Yitzhak Gadre, Gabriel Ilharco, Sewoong Oh, and Ludwig Schmidt. Improving multimodal datasets with image captioning. arXiv preprint arXiv:2307.10350, 2023. 3   
[39] OpenAI. Chatgpt. https://chat.openai.com/, 2023. 1, 2, 4   
[40] OpenAI. Gpt-4v(ision) system card. 2023. 1, 4   
[41] Vicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text: Describing images using 1 million captioned photographs. Advances in neural information processing systems, 24, 2011. 4, 9   
[42] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35: 27730–27744, 2022. 2   
[43] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023. 3   
[44] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018. 2   
[45] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748–8763. PMLR, 2021. 3, 6   
[46] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485–5551, 2020. 2   
[47] Babak Saleh and Ahmed Elgammal. Large-scale classification of fine-art paintings: Learning the right metric on the right feature. arXiv preprint arXiv:1505.00855, 2015. 4, 9   
[48] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion- ${\cdot400}\mathrm{m}$ : Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021. 4, 9   
[49] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-okvqa: A benchmark for visual question answering using world knowledge. In European Conference on Computer Vision, pages 146–162. Springer, 2022. 6   
[50] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556–2565, 2018. 4, 9   
[51] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: a dataset for image captioning with reading comprehension. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23– 28, 2020, Proceedings, Part II 16, pages 742–758. Springer, 2020. 4, 6, 9   
[52] InternLM Team. Internlm: A multilingual language model with progressively enhanced capabilities, 2023. 2   
[53] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 2, 6   
[54] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 2   
[55] Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Chunyi Li, Wenxiu Sun, Qiong Yan, Guangtao Zhai, et al. Q-bench: A benchmark for general-purpose foundation models on low-level vision. arXiv preprint arXiv:2309.14181, 2023. 6, 7   
[56] Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, Fan Yang, et al. Baichuan 2: Open large-scale language models. arXiv preprint arXiv:2309.10305, 2023. 2   
[57] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023. 2, 3   
[58] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. 6, 7   
[59] Haotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun Chen, Liunian Li, Xiyang Dai, Lijuan Wang, Lu Yuan, JenqNeng Hwang, and Jianfeng Gao. Glipv2: Unifying localization and vision-language understanding. Advances in Neural Information Processing Systems, 35:36067–36080, 2022. 3   
[60] Pan Zhang, Xiaoyi Dong Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang Zhang, Haodong Duan, Hang Yan, et al. Internlmxcomposer: A vision-language large model for advanced text-image comprehension and composition. arXiv preprint arXiv:2309.15112, 2023. 2   
[61] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199, 2023.   
[62] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 2, 6  