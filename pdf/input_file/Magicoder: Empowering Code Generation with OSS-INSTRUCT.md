# Magicoder: Empowering Code Generation with OSS-INSTRUCT  

Yuxiang Wei 1 Zhe Wang 2 † Jiawei Liu 1 Yifeng Ding 1 Lingming Zhang  

## Abstract  

We introduce Magicoder, a series of fully opensource (code, weights, and data) Large Language Models (LLMs) for code that signifcantly closes the gap with top code models while having no more than 7B parameters. Magicoder models are trained on 75K synthetic instruction data using OSS-INSTRUCT, a novel approach to enlightening LLMs with open-source code snippets to generate diverse instruction data for code. Our main motivation is to mitigate the inherent bias of the synthetic data generated by LLMs through the wealth of open-source references for the production of more realistic and controllable data. The orthogonality of OSS-INSTRUCT and other data generation methods like Evol-Instruct further enables us to build an enhanced MagicoderS. Both Magicoder and MagicoderS substantially outperform state-of-the-art code models with similar or even larger sizes on a wide range of coding benchmarks. Notably, Magicoder -CL-7B based on CODELLAMA even surpasses the prominent ChatGPT on HumanEval $^{+}$ (66.5 vs. 65.9 in pass $@1$ ). Overall, OSS-INSTRUCT opens a new direction for crafting diverse synthetic instruction data for code using abundant open-source references.  

ming by examples (Cambronero et al., 2023; Liu et al., 2023a) for domain-specifc tasks. Until recently, Large Language Models (LLMs) trained on code (Austin et al., 2021; Chen et al., 2021) has shown outstanding breakthroughs in generating code that accurately satisfes user intents, and they are widely deployed to assist real-world software development (Microsoft, 2023b; Services, 2023).  

Initially, closed-source models such as GPT-3.5 Turbo (OpenAI, 2022) (i.e., ChatGPT) and GPT-4 (OpenAI, 2023) massively dominated various coding benchmarks and leaderboards (Chen et al., 2021; Austin et al., 2021; Liu et al., 2023b; Lai et al., 2022; Xia & Zhang, 2023). To further push the boundaries of code generation with open source LLMs, SELF-INSTRUCT (Wang et al., 2023a) is adopted to bootstrap the instruction-following ability of LLMs. In the realm of code, practitioners commonly devise synthetic coding instructions using a stronger teacher model (e.g., ChatGPT and GPT-4) and then fnetune a weaker student model (e.g., CODELLAMA (Roziere et al., 2023)) with the generated data to distill the knowledge from the teacher (Taori et al., 2023; Chaudhary, 2023). For example, Code Alpaca (Chaudhary, 2023) consists of 20K automatically generated code instructions by applying SELF-INSTRUCT on ChatGPT using 21 seed tasks. To further enhance the coding abilities of LLMs, Luo et al. (2023b) proposes Code Evol-Instruct that employs various heuristics to increase the complexity of seed code instructions (Code Alpaca in this case), achieving state-ofthe-art (SOTA) results among open-source models.  

## 1. Introduction  

Code generation, also known as program synthesis (Gulwani et al., 2017), is a long-standing challenge in computer science. In the past few decades, a large body of research has been studying symbolic approaches, such as abstraction-based synthesis (Wang et al., 2017; Feng et al., 2018) for general-purpose synthesis problems and program  

While these data generation methods can effectively improve the instruction-following capability of an LLM, they rely on a narrow range of predefned tasks or heuristics under the hood. For example, on the one hand, Code Alpaca that adopts SELF-INSTRUCT only relies on 21 seed tasks to generate new code instructions using an identical prompt template. On the other hand, Code Evol-Instruct takes Code Alpaca as seeds and merely depends on 5 heuristics to evolve the dataset. As partly suggested by Yu et al. (2023) and Wang et al. (2023a), such approaches may signifcantly inherit the system bias inherent in the LLMs as well as the predefned tasks.  

Therefore, in this paper, we propose OSS-INSTRUCT to mitigate the inherent bias of LLMs and to unleash their potential to craft diverse and creative code instructions via direct learning from the open source. As shown in Figure 1, OSS-INSTRUCT leverages a powerful LLM to automatically generate new coding problems by drawing inspiration from any random code snippets collected from the open source. In this example, the LLM gets inspired by two incomplete code fragments from different functions and manages to relate them and craft a realistic machine learning problem. Thanks to the “infnite” real-world opensource code, OSS-INSTRUCT can directly produce diverse, realistic, and controllable code instructions by providing distinct seed code snippets. In the end, we generate 75K synthetic data to fnetune CODELLAMA-PYTHON-7B, resulting in Magicoder-CL. While being simple and effective, OSS-INSTRUCT is orthogonal to existing data generation methods, and they can be combined to further boost the models’ coding capabilities. Therefore, we continually fnetune Magicoder-CL on an open-source Evol-Instruct dataset with 110K entries, producing MagicoderS-CL.  

![](https://cdn-mineru.openxlab.org.cn/extract/46a178f3-c3ba-47d7-9d38-6ae7bbdbd1b1/bf78d8337e8879da556c4927d8c2e86e96598f2057bfb0871be58266144637f2.jpg)  

Figure 1: Overview of OSS-INSTRUCT and the pass $@1$ results of different LLMs on HumanEval (+)  

We evaluate Magicoder and MagicoderS on a wide range of coding tasks, including HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) for Python text-to-code generation, MultiPL-E (Cassano et al., 2022) for multilingual code completion, and DS-1000 (Lai et al., 2022) for solving data science problems. We further adopt EvalPlus (Liu et al., 2023b), which includes the augmented HumanEval+ and $\mathrm{MBPP+}$ datasets for more rigorous model evaluation. Both Magicoder-CL and MagicoderS-CL substantially boost the base CODELLAMA-PYTHON-7B. Additionally, MagicoderCL even outperforms WizardCoder-CL-7B, WizardCoderSC-15B, and all studied SOTA LLMs with less than or equal to 16B parameters on all the benchmarks we tested. Also, the pass $@1$ result of the enhanced MagicoderS-CL is on par with ChatGPT on HumanEval (70.7 vs. 72.6) and surpasses it on the more rigorous HumanEva $^{+}$ (66.5 vs. 65.9), indicating that MagicoderS-CL can generate more robust code. It also achieves SOTA results among all code models at the same scale.  

Additionally, we notice a very recent advancement in the development of the DeepSeek-Coder series (Guo et al., 2024) which has shown exceptional coding performance. However, due to the limited technical details disclosed, we only briefy discuss them in $\S3.4$ . Despite this, we applied OSS-INSTRUCT on DeepSeek-Coder-Base 6.7B, resulting in the creation of Magicoder-DS and MagicoderSDS. In addition to the consistent fndings on the previous results with CODELLAMA-PYTHON-7B as the base model, Magicoder-DS and MagicoderS-DS beneft from the more powerful DeepSeek-Coder-Base-6.7B. This advantage is demonstrated by Magicoder -DS, which achieves a remarkable 76.8 pass $@1$ on HumanEval. Magicoder -DS also outperforms DeepSeek-Coder-Instruct-6.7B on HumanEval $(+)$ and MBPP $(+)$ with $8\times$ less fnetuning tokens.  

To justify the design of OSS-INSTRUCT, i.e., generating instruction-tuning data from open-source references rather than using the references directly, we demonstrate that fnetuning the base models with semantically relevant commentfunction pairs extracted from open-source projects even negatively impacts the model performance (§4.2).  

In general, we make the following contributions:  

• We introduce OSS-INSTRUCT, a pioneering approach to enlightening LLMs with open-source code snippets to generate more diverse, realistic, and controllable coding instruction data, which can be leveraged to substantially boost the performance of various LLMs via instruction tuning. It opens a new dimension for creating low-bias and diverse instruction-tuning data from the abundance of open-source references.  

• We build the Magicoder series trained with OSSINSTRUCT and Magicoder series trained on a combination of OSS-INSTRUCT and Evol-Instruct. Our evaluation across 6 benchmarks shows that all Magicoders signifcantly improve the base LLMs. Notably, both MagicoderS-CL and Magicoder $\boldsymbol{S}$ -DS outperform ChatGPT on HumanEval $^+$ with only 7B parameters.  

• We fully open source the model weights, training data, and source code at https://github.com/ise-uiuc/ magicoder to facilitate future research.  

### 2. OSS-INSTRUCT: Instruction Tuning from Open Source  

In this section, we elaborate on our OSS-INSTRUCT approach. From a high level, as shown in Figure 1, OSSINSTRUCT works by prompting an LLM (e.g., ChatGPT) to generate a coding problem and its solution according to some seed code snippet collected from the wild (e.g., from GitHub). The seed snippet offers controllability of the generation and encourages the LLM to create diverse coding problems that can refect real-world programming scenarios.  

#### 2.1. Generating Coding Problems  

OSS-INSTRUCT is powered by seed code snippets that can be easily collected from open source. In this work, we directly adopt starcoderdata as our seed corpus, a fltered version of The Stack (Kocetkov et al., 2022) dataset that StarCoder is trained on, containing permissively licensed source code documents in various programming languages. We chose starcoderdata because it is widely adopted, includes massive high-quality code snippets, and is even post-processed for data decontamination (Li et al., 2023; Allal et al., 2023). For each code document from the corpus, we randomly extract 1–15 consecutive lines as the seed snippet for the model to gain inspiration from and produce coding problems. In total, we collected 80K initial seed snippets from 80K code documents, 40K from Python, and 5K from each of $C++$ , Java, TypeScript, Shell, C#, Rust, PHP, and Swift respectively. Then, each collected seed code snippet is applied to the prompt template shown in Appendix A.1, which a teacher model takes as input and outputs both a coding problem and its solution.  

#### 2.2. Data Cleaning and Decontamination  

We perform data cleaning by excluding samples that are identical or share the same seed code snippet. While there exist other sorts of noisiness (e.g., the solution is incomplete) in the generated data, inspired by Honovich et al. (2023), they are not removed as we believe they still contain valuable information for LLMs to learn. More experimental details can be found in Appendix C.3. Finally, we apply the same logic as StarCoder Li et al. (2023) to decontaminate our training data by removing coding problems that contain docstrings or solutions from HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021), docstrings from APPS (Hendrycks et al., 2021), prompts from DS1000 (Lai et al., 2022), or questions from GSM8K (Cobbe et al., 2021). As part of our analysis, the decontamination procedure only flters out 9 additional samples. Since the seed corpus starcoderdata has already gone through rigorous data decontamination, this observation suggests that OSS-INSTRUCT is unlikely to introduce additional data leakage beyond the seeds. The eventual OSS-INSTRUCT dataset contains about 75K entries. An overview of the dataset statistics can be found in Appendix A.3.  

#### 2.3. Qualitative Examples of OSS-INSTRUCT  

Figure 2 shows some qualitative examples of how OSSINSTRUCT can help LLM get inspiration from a seed code snippet to create new coding problems and solutions. For example, the shell script example shows how an LLM crafts a Python coding problem with just one line of shell script. The library imports example demonstrates how an LLM can create a realistic machine learning problem using just a few import statements. Meanwhile, the class signature instance illustrates the ability of LLM to draw inspiration from an incomplete class defnition featuring annotations like SpringBootApplication and keywords such as bank. From this, the LLM generates a problem that requires implementing a complete banking system based on Spring Boot. Overall, OSS-INSTRUCT can inspire an LLM with distinct code structures and semantics to create diverse coding tasks, including algorithmic challenges, realistic issues, single-function code generation, library-based program completion, whole-program development, and even whole-application construction.  

![](https://cdn-mineru.openxlab.org.cn/extract/46a178f3-c3ba-47d7-9d38-6ae7bbdbd1b1/6acd4039a35cbc97ea29ba924983c76e6e86bd964c84cff1735ca913ba5fb170.jpg)  
Figure 2: Examples showing how OSS-INSTRUCT generates problems and solutions from seed code snippets. Detailed problem requirements, implementations, and explanations are omitted for brevity. More examples can be found in Appendix A.2.  

![](https://cdn-mineru.openxlab.org.cn/extract/46a178f3-c3ba-47d7-9d38-6ae7bbdbd1b1/5fed28fe47d27db639be7ceff809c38473314ab8bed11f667d939fbbca68084b.jpg)  
Figure 3: Cosine similarities between HumanEval and synthetic data generated by different methods.  

Similarity with HumanEval To study whether our data generation process produces more HumanEval-like problems or solutions that contribute to high performance, we pair each sample from our 75K dataset with each of the 164 HumanEval (Chen et al., 2021) samples and compute their cosine similarity using TF-IDF (SPARCK JONES, 1972) embeddings. We then associate each OSS-INSTRUCT sample with a HumanEval sample with the highest similarity score. We also compare our dataset against Code Alpaca, a 20K dataset applying SELF-INSTRUCT to code, and evol-codealpaca-v1 (theblackcat102, 2023), an open-source reproduction of Evol-Instruct containing 110K coding instructions. We resort to the open-source implementation because the offcial Code Evol-Instruct (Luo et al., 2023b) dataset is not released. We decontaminate all the datasets beforehand using the same way discussed in $\S2.2$ Figure 3 shows that OSS-INSTRUCT exhibits the lowest average similarity among all the studied data generation techniques while SELF-INSTRUCT shows the highest average similarity. This result indicates that the improvements from OSS-INSTRUCT are not merely due to including data from the same distribution.  

### 3. Evaluation  

We choose CODELLAMA-PYTHON-7B and DeepSeekCoder-Base 6.7B as the base LLMs. To derive Magicoder series, we frst fnetune them on 75K synthetic data generated through OSS-INSTRUCT. We then obtain MagicoderS by continuing fnetuning Magicoder with the evol-codealpaca-v1 dataset, an open-source EvolInstruct implementation containing about 110K samples. More implementation details and additional evaluation results are listed in Appendices B and C. We also present interesting use cases that refect the effectiveness of instruction tuning in Appendix D and demonstrate Magicoder’s capability to generate complex programs in Appendix E.  

#### 3.1. Python Text-to-Code Generation  

HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) are two of the most widely used benchmarks for code generation. Each task in these benchmarks includes a task description (e.g., docstring) as the prompt, where LLMs generate corresponding code whose correctness is checked by a handful of test cases. Because tests in these benchmarks can be insuffcient, for more rigorous evaluation, we use HumanEval+ and $\mathrm{MBPP+}$ , both powered by the EvalPlus framework (Liu et al., 2023b) to obtain $80\!\times\!/35\!\times$ more tests. Following prior work (Liu et al., 2023b; Chen et al., 2023), for each task and LLM we use greedy decoding to generate one sample and focus on comparing the pass $@1$ metric.  

Table 1: Pass $@1$ $(\%)$ results of different LLMs on HumanEval $(+)$ and MBPP $(+)$ computed with greedy decoding. The abbreviations “CL” and “SC” refer to the base models CODELLAMA-PYTHON and StarCoder, respectively. We report the results consistently from the EvalPlus (Liu et al., 2023b) Leaderboard.   


<html><body><table><tr><td rowspan="2">Model</td><td rowspan="2">Release Date</td><td rowspan="2">Size</td><td colspan="2">Benchmark</td><td colspan="2">Open-Source</td></tr><tr><td>HumanEval (+)</td><td>MBPP (+)</td><td>Weight</td><td>Data</td></tr><tr><td>GPT-3.5 Turbo</td><td>Nov 2023</td><td></td><td>72.6 (65.9)</td><td>81.7 (69.4)</td><td>O</td><td>O</td></tr><tr><td>GPT-4 Turbo</td><td>Nov 2023</td><td></td><td>85.4 (81.7)</td><td>83.0 (70.7)</td><td>O</td><td>O</td></tr><tr><td>CODELLAMA-PYTHON</td><td>Aug 2023</td><td>34B</td><td>51.8 3 (42.7)</td><td>67.2 (52.9)</td><td>.</td><td>O</td></tr><tr><td>WizardCoder-CL</td><td>Sep 2023</td><td>34B</td><td>73.2 (64.6)</td><td>73.2 (59.9)</td><td>.</td><td>O</td></tr><tr><td>CodeT5+</td><td>May 2023</td><td>16B</td><td>31.7 (26.2)</td><td>54.6 (44.4)</td><td></td><td>.</td></tr><tr><td>CodeGen-Mono</td><td>Mar 2022</td><td>16B</td><td>32.9 (27.4)</td><td>52.6 (43.6)</td><td></td><td></td></tr><tr><td>StarCoder</td><td>May 2023</td><td>15B</td><td>34.1 (29.3)</td><td>55.1 (46.1)</td><td></td><td></td></tr><tr><td>CODELLAMA-PYTHON</td><td>Aug 2023</td><td>13B</td><td>42.7 (36.6)</td><td>61.2 (50.9)</td><td></td><td>O</td></tr><tr><td>WizardCoder-SC</td><td>Sep 2023</td><td>15B</td><td>51.9 (45.1)</td><td>61.9 (50.6)</td><td></td><td>O</td></tr><tr><td>StarCoder</td><td>May 2023</td><td>7B</td><td>24.4 (20.7)</td><td>33.1 (28.8)</td><td></td><td></td></tr><tr><td>Mistral</td><td>Oct 2023</td><td>7B</td><td>28.7 (23.2)</td><td>50.1 (40.9)</td><td></td><td>O</td></tr><tr><td>CodeT5+</td><td>May 2023</td><td>6B</td><td>29.3 (23.8)</td><td>51.9 (40.9)</td><td>.</td><td></td></tr><tr><td>CodeGen-Mono</td><td>Mar 2022</td><td>6B</td><td>29.3 (25.6)</td><td>49.9 (42.1)</td><td></td><td></td></tr><tr><td>CODELLAMA-PYTHON</td><td>Aug 2023</td><td>7B</td><td>37.8 8 (34.1)</td><td>57.6 (45.4)</td><td></td><td>O</td></tr><tr><td>WizardCoder-CL</td><td>Sep 2023</td><td>7B</td><td>48.2 (40.9)</td><td>56.6 (47.1)</td><td></td><td>O</td></tr><tr><td>Magicoder-CL</td><td>Dec 2023</td><td>7B</td><td>60.4 (55.5)</td><td>64.2 (52.6)</td><td></td><td></td></tr><tr><td>MagicoderS-CL</td><td>Dec 2023</td><td>7B</td><td>70.7 (66.5)</td><td>68.4 (56.6)</td><td></td><td></td></tr></table></body></html>  

We consider a wide range of baseline models, including CODELLAMA-PYTHON (Roziere et al., 2023), WizardCoder (Luo et al., 2023b), GPT-3.5 Turbo (OpenAI, 2022), GPT-4 Turbo (OpenAI, 2023), StarCoder (Li et al., 2023), CodeT $^{5+}$ (Wang et al., 2023b), CodeGen-Mono (Nijkamp et al., 2023), and Mistral (Jiang et al., 2023a). All the results are consistently reported from the EvalPlus (Liu et al., 2023b) leaderboard (EvalPlus hash: 1895d2f).  

Table 1 shows the pass $@1$ results of different LLMs on these benchmarks. From the results, we can frst observe that Magicoder-CL has a clear improvement over the base CODELLAMA-PYTHON-7B, and outperforms all studied open-source models except CODELLAMA-PYTHON-34B and WizardCoder-CL-34B. Notably, Magicoder-CL surpasses WizardCoder-SC-15B and has a substantial improvement on HumanEval and HumanEval+ over CODELLAMAPYTHON-34B. Magicoder -CL demonstrates further improvements by being trained with the orthogonal EvolInstruct method. MagicoderS-CL outperforms ChatGPT and all other open-source models on HumanEval+. Moreover, although it scores slightly lower than WizardCoderCL-34B and ChatGPT on HumanEval, it surpasses both of them on the more rigorous HumanEval+ dataset, indicating that MagicoderS-CL may produce more robust code.  

#### 3.2. Multilingual Code Generation  

In addition to Python, as shown in Table 2, we perform an extensive evaluation on 6 widely used programming languages, i.e., Java, JavaScript, $C++$ , PHP, Swift, and Rust, using the MultiPL-E benchmark (Cassano et al., 2022). We report available results from the WizardCoder paper (Luo et al., 2023b) and evaluate our models consistently through bigcode-evaluation-harness (Ben Allal et al., 2022). We skip proprietary models such as ChatGPT and GPT-4 as they are not supported by the framework. Due to a signifcant inference latency when running WizardCoder-CL-7B using the harness in our environment, we choose not to include it in our analysis.  

The results indicate that Magicoder-CL improves the base CODELLAMA-PYTHON-7B by a large margin among all the studied programming languages. Moreover, MagicoderCL also achieves better results than the SOTA 15B WizardCoder-SC among half of the programming languages. Additionally, Magicoder -CL demonstrates further improvement over Magicoder-CL on all programming languages, achieving comparable performance against WizardCoder-CL-34B with only 7B parameters. It is worth noting that Magicoder-CL is only trained with very limited multilingual data but still outperforms other LLMs with similar or even larger sizes. Also, although the harness evaluates models in completion formats which are for base models, Magicoders still show signifcant improvements despite being only instruction-tuned. This implies that LLMs can learn knowledge from the data beyond its format.  

#### 3.3. Code Generation for Data Science  

The DS-1000 dataset (Lai et al., 2022) contains 1K distinct data science coding issues ranging from 7 popular data science libraries in Python. It evaluates the realistic and practical use case of an LLM and offers unit tests for validating each problem. DS-1000 has both completion and insertion modes, but here we only evaluate completion because the base CODELLAMA-PYTHON does not support inflling. Table 3 shows the evaluation results where we include the recent INCODER (Fried et al., 2023), CodeGen (Nijkamp et al., 2023), Code-Cushman-001 (Microsoft, 2023a), StarCoder (Li et al., 2023), CoDELLAMA-PYTHON (Roziere et al., 2023), and WizardCoder (Luo et al., 2023b). We can see from the table that Magicoder-CL-7B already outperforms all the baselines we evaluate, including stateof-the-art WizardCoder-CL-7B and WizardCoder-SC-15B. MagicoderS-CL-7B further breaks the limit by introducing an 8.3 percentage point absolute improvement over WizardCoder-SC-15B.  

#### 3.4. Comparison with DeepSeek-Coder  

DeepSeek-Coder (Guo et al., 2024) is a series of models released concurrently to our work and they demonstrate superior coding performance. We only briefy discuss it in this section because its data and instruction tuning details are not publicly available at the time of writing. We apply the same fnetuning strategy on DeepSeek-Coder-Base-6.7B as we performed on CODELLAMA-PYTHON-7B, leading to Magicoder-DS and Magicoder -DS. Table 4 shows a similar trend as Table 1 that the base model can be signifcantly improved after applying OSS-INSTRUCT. Remarkably, the Magicoder -DS variant surpasses DeepSeekCoder-Instruct-6.7B on all the benchmarks with $\times8$ fewer training tokens, and it also closely matches DeepSeekCoder-Instruct-33B on these datasets.  

### 4. Ablations of Data Source  

#### 4.1. Impact of the Language Distribution  

To understand the correlation between the programming languages appearing in the training data and the downstream performance of different languages, we conduct an additional ablation study about the training data. We classify the 75K training data into approximately 43K Python-only, and 32K non-Python data according to whether ‘‘‘python is a substring of the generated data. We do not classify the data based on the seed code snippet because LLMs performing OSS-INSTRUCT may produce code in a different programming language than the seed.  

Table 5 shows the evaluation results, where we consistently fnetune the base CODELLAMA-PYTHON-7B for 2 epochs on different data partitions using the same training hyperparameters explained in Appendix B. From the table, we can see that, as can be imagined, training on Python or non-Python data can substantially boost the performance of the base model in Python or non-Python tasks, respectively. Interestingly, instruction tuning on different programming languages can still boost the overall coding performance that includes out-of-distribution languages. For example, when trained on only non-Python data, Magicoder-CL still achieves a 10.4 percentage point improvement over the base model in the Python-only evaluation. This implies LLMs can establish correlations between different programming languages and perform transfer learning of deeper code semantics. Finally, we observe a more signifcant boost in Python evaluation when combining data from both sources, with a slight decrease in multilingual performance compared with only fnetuning on multilingual data. We attribute this decrease to the dominant amount of Python data (around $57\%$ ) during instruction tuning.  

#### 4.2. OSS-INSTRUCT vs. Direct Finetuning  

The fact that OSS-INSTRUCT gets an LLM inspired from open-source code snippets may lead to a natural question: why not directly fnetuning on these open-source code? To answer this question, we follow CodeSearchNet (Husain et al., 2020) to mine semantically relevant comment-function pairs from the same seed document corpus we use to construct the 75K OSS-INSTRUCT dataset. We then train the model to predict the function bodies from the function signatures and comments. We prioritize comment-function pairs that overlap with our 75K seed snippets, resulting in about 11K data points. To align with our 75K samples, we collect the remaining 64K samples using the whole corpus of $75K$ seed documents. Eventually, we have the same number of comment-function pairs with OSS-INSTRUCT data.  

We fnetune the base CODELLAMA-PYTHON-7B for 2 epochs using the paired data, following the same training setup discussed in Appendix B. From Table 6, we observe that fnetuning on 75K paired comment-function data even worsens the base model, while OSS-INSTRUCT helps to introduce a substantial boost. We conjecture that the degradation is owing to the substantial noise and inconsistency that exists intrinsically in the data pairs, even though these paired data exhibit very similar format as HumanEval or MultiPL-E problems. This further shows that data factuality, rather than the format, is essential to code instruction tuning. It also indicates the superiority of OSS-INSTRUCT which can translate these loosely related code fragments into semantically-consistent instruction-tuning data.  

Table 2: $\mathrm{Pass}@1$ results of different LLMs on MultiPL-E (Cassano et al., 2022) following the same hyperparameter settings as the WizardCoder paper (Luo et al., 2023b): temperature $=0.2$ , $\mathtt{t o p\_p}=0.95$ , max length $=512$ , and num samp $1{\in}s=50$ . We evaluate all 7B models using bigcode-evaluation-harness (Ben Allal et al., 2022) and report other results from WizardCoder.   


<html><body><table><tr><td rowspan="2">Model</td><td rowspan="2">Size</td><td colspan="6">Programming Language</td></tr><tr><td>Java</td><td>JavaScript</td><td>++</td><td>PHP</td><td>Swift</td><td>Rust</td></tr><tr><td>CODELLAMA</td><td>34B</td><td>40.2</td><td>41.7</td><td>41.4</td><td>40.4</td><td>35.3</td><td>38.7</td></tr><tr><td>CODELLAMA-PYTHON</td><td>34B</td><td>39.5</td><td>44.7</td><td>39.1</td><td>39.8</td><td>34.3</td><td>39.7</td></tr><tr><td>CODELLAMA-INSTRUCT</td><td>34B</td><td>41.5</td><td>45.9</td><td>41.5</td><td>37.0</td><td>37.6</td><td>39.3</td></tr><tr><td>WizardCoder-CL</td><td>34B</td><td>44.9</td><td>55.3</td><td>47.2</td><td>47.2</td><td>44.3</td><td>46.2</td></tr><tr><td>StarCoderBase</td><td>15B</td><td>28.5</td><td>31.7</td><td>30.6</td><td>26.8</td><td>16.7</td><td>24.5</td></tr><tr><td>StarCoder</td><td>15B</td><td>30.2</td><td>30.8</td><td>31.6</td><td>26.1</td><td>22.7</td><td>21.8</td></tr><tr><td>WizardCoder-SC</td><td>15B</td><td>35.8</td><td>41.9</td><td>39.0</td><td>39.3</td><td>33.7</td><td>27.1</td></tr><tr><td>CODELLAMA</td><td>7B</td><td>29.3</td><td>31.7</td><td>27.0</td><td>25.1</td><td>25.6</td><td>25.5</td></tr><tr><td>CODELLAMA-PYTHON</td><td>7B</td><td>29.1</td><td>35.7</td><td>30.2</td><td>29.0</td><td>27.1</td><td>27.0</td></tr><tr><td>Magicoder-CL</td><td>7B</td><td>36.4</td><td>45.9</td><td>36.5</td><td>39.5</td><td>33.4</td><td>30.6</td></tr><tr><td>MagicoderS-CL</td><td>7B</td><td>42.9</td><td>57.5</td><td>44.4</td><td>47.6</td><td>44.1</td><td>40.3</td></tr></table></body></html>  

Table 3: Pass $@1$ results on DS-1000 (completion format) with temperature $=0.2$ , $\mathsf{t}\,\mathsf{o p\,.p}=0.5,$ max length $=$ 1024, and num samp $1\!\in\!\mathrm{s}=40$ , following the same hyperparameter setting used in WizardCoder (Luo et al., 2023b). We evaluate all the 7B models with their preferred prompt formats and report other results from WizardCoder.   


<html><body><table><tr><td>Model</td><td>Size</td><td>+ 155 Matplotlib</td><td>+ 220 NumPy</td><td>+ 291 Pandas</td><td>+ 68 PyTorch</td><td>+ 106 SciPy</td><td>+ 115 Sklearn</td><td>+ 45 TensorFlow</td><td>= 1000 Overall</td></tr><tr><td>INCODER</td><td>6.7B</td><td>28.3</td><td>4.4</td><td>3.1</td><td>4.4</td><td>2.8</td><td>2.8</td><td>3.8</td><td>7.4</td></tr><tr><td>CodeGen-Mono</td><td>16B</td><td>31.7</td><td>10.9</td><td>3.4</td><td>7.0</td><td>9.0</td><td>10.8</td><td>15.2</td><td>11.7</td></tr><tr><td>Code-Cushman-001</td><td></td><td>40.7</td><td>21.8</td><td>7.9</td><td>12.4</td><td>11.3</td><td>18.0</td><td>12.2</td><td>18.1</td></tr><tr><td>StarCoder</td><td>15B</td><td>51.7</td><td>29.7</td><td>11.4</td><td>21.4</td><td>20.2</td><td>29.5</td><td>24.5</td><td>26.0</td></tr><tr><td>WizardCoder-SC</td><td>15B</td><td>55.2</td><td>33.6</td><td>16.7</td><td>26.2</td><td>24.2</td><td>24.9</td><td>26.7</td><td>29.2</td></tr><tr><td>CODELLAMA-PYTHON</td><td>7B</td><td>55.3</td><td>34.5</td><td>16.4</td><td>19.9</td><td>22.3</td><td>17.6</td><td>28.5</td><td>28.0</td></tr><tr><td>WizardCoder-CL</td><td>7B</td><td>53.5</td><td>34.4</td><td>15.2</td><td>25.7</td><td>21.0</td><td>24.5</td><td>28.9</td><td>28.4</td></tr><tr><td>Magicoder-CL</td><td>7B</td><td>54.6</td><td>34.8</td><td>19.0</td><td>24.7</td><td>25.0</td><td>22.6</td><td>28.9</td><td>29.9</td></tr><tr><td>MagicoderS-CL</td><td>7B</td><td>55.9</td><td>40.6</td><td>28.4</td><td>40.4</td><td>28.8</td><td>35.8</td><td>37.6</td><td>37.5</td></tr></table></body></html>  

Table 4: Pass $@1$ (greedy decoding) comparison between Magicoder and DeepSeek-Coder (Guo et al., 2024) on HumanEval $(+)$ and MBPP $(+)$ . DeepSeek-Coder results are reported from EvalPlus (Liu et al., 2023b) Leaderboard.   


<html><body><table><tr><td rowspan="2">Model</td><td rowspan="2">Size</td><td rowspan="2">Training Tokens</td><td colspan="2">Benchmark</td><td colspan="2">Open-Source</td></tr><tr><td>HumanEval(+)</td><td>MBPP (+)</td><td>Weight</td><td>Data</td></tr><tr><td rowspan="3">DeepSeek-Coder-Base</td><td>1.3B</td><td>2T</td><td></td><td>55.4 (46.9)</td><td></td><td></td></tr><tr><td>6.7B</td><td>2T</td><td>47.6 (39.6)</td><td>70.2 (56.6)</td><td></td><td></td></tr><tr><td>33B</td><td>2T</td><td>51.2 (43.3)</td><td></td><td></td><td></td></tr><tr><td rowspan="3">DeepSeek-Coder Instruct</td><td>1.3B</td><td>+2B</td><td>64.6 (58.5)</td><td>63.7 (53.1)</td><td></td><td></td></tr><tr><td>6.7B</td><td>+2B</td><td>73.8 (70.1)</td><td>72.7 (63.4)</td><td></td><td></td></tr><tr><td>33B</td><td>+2B</td><td>78.7 (72.6)</td><td>78.7 (66.7)</td><td></td><td></td></tr><tr><td>Magicoder-DS</td><td>6.7B</td><td>+90M</td><td>66.5 (60.4)</td><td>75.4 (61.9)</td><td></td><td></td></tr><tr><td>MagicoderS-DS</td><td>6.7B</td><td>+240M</td><td>76.8 (70.7)</td><td>75.7 (64.4)</td><td></td><td></td></tr></table></body></html>  

Table 5: Ablation study of using different programming languages as training data. We show the pass $@1$ results on HumanEval+ (Liu et al., 2023b) for Python and the average pass $@1$ results on MultiPL-E (Cassano et al., 2022) for the same set of programming languages used in Table 2 (i.e., Java, JavaScript, $C++$ , PHP, Swift, and Rust). All the variants are fnetuned with 2 epochs and evaluated through greedy-decoding.   


<html><body><table><tr><td>Model (7B)</td><td>Finetuning Data</td><td>Python (HumanEval+)</td><td>Others (MultiPL-E)</td></tr><tr><td>CODELLAMA-PYTHON</td><td></td><td>34.1</td><td>29.6</td></tr><tr><td>Magicoder-CL</td><td>Python (43K)</td><td>47.6</td><td>32.7</td></tr><tr><td>Magicoder-CL</td><td>Others (32K)</td><td>44.5</td><td>38.3</td></tr><tr><td>Magicoder-CL</td><td>Both (75K)</td><td>55.5</td><td>37.8</td></tr></table></body></html>  

Table 6: Comparison between OSS-INSTRUCT and directly fnetuning on comment-function pairs with CODELLAMAPYTHON-7B as the base model.   


<html><body><table><tr><td>Finetuning Data</td><td>HumanEval+</td><td>MultiPL-E</td></tr><tr><td>Base model w/o finetuning</td><td>34.1</td><td>29.6</td></tr><tr><td>Comment-function pairs (75K)</td><td>34.1</td><td>24.1</td></tr><tr><td>OSS-INSTRUCT (75K)</td><td>55.5</td><td>37.8</td></tr></table></body></html>  

#### 4.3. OSS-INSTRUCT with A Less Powerful Teacher  

In this section, we explore the factors contributing to the effectiveness of OSS-INSTRUCT beyond just the distillation of the teacher model. We propose two potential key reasons. First, since the base model is pretrained with comprehensive code data, the distillation process likely activates the model’s internal capabilities, leading to improved performance in coding tasks. Second, OSS-INSTRUCT uses seed code snippets to generate problem-solution pairs in one shot. These seed snippets provide valuable context, enabling the model to create better solutions than a plain teacher model lacking such seed information. These enhanced solutions can then be used to train more effective student models. To verify these points, we conduct an additional experiment by generating a subset of 20K OSS-INSTRUCT data using Mixtral- $\mathbf{\nabla}{8\mathbf{x}7\mathbf{B}}$ -Instruct-v0.1 (Jiang et al., 2024), a state-ofthe-art, general-purpose, open-source LLM.  

Table 7: Pass $@1$ on HumanEval $^+$ and $\mathbf{M}\mathbf{B}\mathbf{P}\mathbf{P}+$ when fnetuning CODELLAMA-PYTHON-7B for 2 epochs on 20K OSS-INSTRUCT data generated by Mixtral- $\mathbf{\nabla}\cdot8\mathbf{x}7\mathbf{B}$ -Instructv0.1 (Jiang et al., 2024).   


<html><body><table><tr><td>Model</td><td>HumanEval+</td><td>MBPP+</td></tr><tr><td>Mixtral-8x7B-Instruct-v0.1</td><td>39.6</td><td>47.4</td></tr><tr><td>CODELLAMA-PYTHON-7B</td><td>34.1</td><td>45.4</td></tr><tr><td>Magicoder-CL-Mixtral-7B</td><td>55.5</td><td>50.4</td></tr></table></body></html>  

Table 7 indicates that Magicoder-CL-Mixtral-7B not only signifcantly improves over the base CODELLAMAPYTHON, but is also better than Mixtral- $\boldsymbol{\cdot}8\mathbf{x}7\mathbf{B}$ -Instruct-v0.1 (i.e., the teacher model) across HumanEval+ and $\mathrm{MBPP+}$ . These results suggest that OSS-INSTRUCT is not simply distilling a teacher model, but also triggering the base model’s own capability and effectively leveraging the information encapsulated in seed code snippets.  

### 5. Related Work  

Foundation models for code Trained over billions of lines of code, LLMs have demonstrated outstanding performance in a wide range of software engineering tasks, including code generation (Chen et al., 2021; Austin et al., 2021), program repair (Xia & Zhang, 2022; Wei et al., 2023; Xia et al., 2023b; Jiang et al., 2023b; Bouzenia et al., 2024), and software testing (Xia et al., 2023a; Deng et al., 2023; Yuan et al., 2023; Schafer et al., 2023; Lemieux et al. 2023). In particular, prominent base models, such as CodeGen (Nijkamp et al., 2023), CodeT5 (Wang et al., 2021), StarCoder (Li et al., 2023), and CoDELLAMA (Roziere et al., 2023), are pre-trained over a huge number of codebase from scratch, establishing the fundamental ability of general code generation and understanding. More recent code LLMs, such as DeepSeek-Coder (Guo et al., 2024) and StarCoder2 (Lozhkov et al., 2024), additionally organize the pretraining data at the repository level to enhance the model’s contextual understanding capabilities. Furthermore, these base models are also fnetuned (Luo et al., 2023b) or prompted (Chen et al., 2023) to unlock their true potential to specialize in solving domain-specifc coding tasks.  

Instruction tuning with synthetic data Instruction tuning aims to improve pretrained LLMs by fnetuning them with a mixture of instructions and corresponding responses (Wei et al., 2022). However, obtaining highquality instructional data is oftentimes laborious. Hence, researchers are increasingly focusing on the development of methods to generate synthetic instruction data. Wang et al. (2023a) introduces SELF-INSTRUCT, where a foundation LLM (GPT-3 (Brown et al., 2020)) is used to generate synthetic instruction-response pairs with carefully crafted prompts. The same LLM is then instruction-tuned on the synthetic data to distill such self-generated knowledge. This technique has been further extended to create synthetic data with different LLMs. For example, Alpaca (Taori et al., 2023) and Code Alpaca (Chaudhary, 2023) apply SELFINSTRUCT to fnetune LLAMA with ChatGPT-generated instructions. To improve SELF-INSTRUCT, WizardLM (Xu et al., 2023) and WizardCoder (Luo et al., 2023a) propose Evol-Instruct and Code Evol-Instruct by guiding ChatGPT with heuristic prompts to make the synthetic data more complex and diverse. More recently, Gunasekar et al. (2023) shows that textbook-quality synthetic data alone can help the model achieve remarkable coding and reasoning capabilities. Orthogonal to all existing methods, our proposed OSS-INSTRUCT allows LLMs to get inspired from realworld code snippets for better controllability, quality, and creativity in coding tasks.  

Evaluating LLMs for code Most code benchmarks evaluate LLMs on generating single-function programs from natural language descriptions. Such benchmarks include HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), APPS (Hendrycks et al., 2021), and CodeContests (Li et al., 2022). A handful of manual tests are used to assess the functional correctness of LLM-generated solutions. However, insuffcient tests can lead to false negatives. Consequently, the EvalPlus framework (Liu et al., 2023b) produces HumanEval $^+$ and $\mathrm{MBPP+}$ by extending $80\!\times\!/35\!\times$ more tests. To address dataset contamination issues, researchers propose LiveCodeBench (Jain et al., 2024), which compiles fresh coding problems not included in model training, and EvoEval (Xia et al., 2024), which strategically leverages LLMs to evolve existing benchmarks into new coding tasks. Meanwhile, there are comprehensive benchmarks evaluating code generation for data science (DS-1000 (Lai et al., 2022)), addressing open-source issues (SWE-bench (Jimenez et al., 2023)), and repository-level code generation (CROSSCODEEVAL (Ding et al., 2023) and RepoEval (Zhang et al., 2023)).  

### 6. Conclusion and Future Work  

We propose OSS-INSTRUCT, a novel data generation method using Large Language Models to generate diverse coding challenges from open-source code snippets. This approach enables Magicoder, which signifcantly improves the base LLM. Despite having less than 7B parameters, it can outperform all evaluate LLMs with less than or equal to 16B parameters, including the 15B WizardCoder. Combining OSS-INSTRUCT with Evol-Instruct allows us to build the enhanced MagicoderS models. They achieve remarkable results by rivaling leading models like ChatGPT in  

HumanEval benchmarks. We fully open source the model weights, training data, and source code, to enable future research in LLMs for code. In the near future, we will apply OSS-INSTRUCT to larger base models. We will also continue advancing OSS-INSTRUCT by generating higherquality data with a strategically designed distribution of the seed code snippets and with more advanced teacher LLMs such as GPT-4.  

### Acknowledgement  

We thank all the reviewers for their insightful comments and suggestions for our paper. This work was partially supported by NSF grant CCF-2131943, as well as Kwai Inc.  

### Impact Statement  

This work is motivated to boost large language models in terms of their code generation and understanding capabilities through instruction tuning. The proposed OSSINSTRUCT method leverages the abundance of open source to generate diverse and controllable instruction data. We expect this idea to also foster innovative software solutions tailored to domain-specifc needs, particularly in areas where real data is private and scarce, by generating extensive synthetic data. Additionally, our method reinforces the value of community-driven content and knowledge sharing by incorporating open-source code as references.  

However, it is essential to recognize the potential for misuse, such as the deliberate generation of vulnerable code that can be exploited for malicious purposes. Ultimately, adhering to ethical guidelines is crucial to ensure the responsible use of this technique.  

### References  

Allal, L. B., Li, R., Kocetkov, D., Mou, C., Akiki, C., Ferrandis, C. M., Muennighoff, N., Mishra, M., Gu, A., Dey, M., Umapathi, L. K., Anderson, C. J., Zi, Y., Poirier, J. L., Schoelkopf, H., Troshin, S., Abulkhanov, D., Romero, M., Lappert, M., Toni, F. D., del Rio, B. G., Liu, Q., Bose, S., Bhattacharyya, U., Zhuo, T. Y., Yu, I., Villegas, P., Zocca, M., Mangrulkar, S., Lansky, D., Nguyen, H., Contractor, D., Villa, L., Li, J., Bahdanau, D., Jernite, Y., Hughes, S., Fried, D., Guha, A., de Vries, H., and von Werra, L. Santacoder: don’t reach for the stars!, 2023.  

Austin, J., Odena, A., Nye, M. I., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C. J., Terry, M., Le, Q. V., and Sutton, C. Program synthesis with large language models. CoRR, abs/2108.07732, 2021. URL https: //arxiv.org/abs/2108.07732.  

Ben Allal, L., Muennighoff, N., Kumar Umapathi,  

L., Lipkin, B., and von Werra, L. A framework for the evaluation of code generation models. https://github.com/bigcode-project/ bigcode-evaluation-harness, 2022.   
Bouzenia, I., Devanbu, P., and Pradel, M. Repairagent: An autonomous, llm-based agent for program repair. arXiv preprint arXiv:2403.17134, 2024.   
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1877–1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips. cc/paper_files/paper/2020/file/ 1457c0d6bfcb4967418bfb8ac142f64a-Paper. pdf.   
Cambronero, J., Gulwani, S., Le, V., Perelman, D., Radhakrishna, A., Simon, C., and Tiwari, A. Flashfll++: Scaling programming by example by cutting to the chase. Proc. ACM Program. Lang., 7(POPL), jan 2023. doi: 10.1145/3571226. URL https://doi.org/10. 1145/3571226.   
Cassano, F., Gouwar, J., Nguyen, D., Nguyen, S., PhippsCostin, L., Pinckney, D., Yee, M.-H., Zi, Y., Anderson, C. J., Feldman, M. Q., Guha, A., Greenberg, M., and Jangda, A. Multipl-e: A scalable and extensible approach to benchmarking neural code generation, 2022.   
Chaudhary, S. Code alpaca: An instruction-following llama model for code generation. https://github.com/ sahil280114/codealpaca, 2023.   
Chen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H. P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such, F. P., Cummings, D., Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss, A., Guss, W. H., Nichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr, A. N., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., and Zaremba, W. Evaluating large language models trained on code, 2021.  

Chen, X., Lin, M., Scharli, N., and Zhou, D. Teaching large language models to self-debug, 2023.  

Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training verifers to solve math word problems, 2021.   
Deng, Y., Xia, C. S., Peng, H., Yang, C., and Zhang, L. Large language models are zero-shot fuzzers: Fuzzing deep-learning libraries via large language models, 2023.   
Ding, Y., Wang, Z., Ahmad, W. U., Ding, H., Tan, M., Jain, N., Ramanathan, M. K., Nallapati, R., Bhatia, P., Roth, D., and Xiang, B. Crosscodeeval: A diverse and multilingual benchmark for cross-fle code completion. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. URL https://openreview.net/forum? id $=$ wgDcbBMSfh.   
Feng, Y., Martins, R., Bastani, O., and Dillig, I. Program synthesis using confict-driven learning. SIGPLAN Not., 53(4):420–435, jun 2018. ISSN 0362-1340. doi: 10. 1145/3296979.3192382. URL https://doi.org/ 10.1145/3296979.3192382.   
Fried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E., Shi, F., Zhong, R., Yih, S., Zettlemoyer, L., and Lewis, M. Incoder: A generative model for code inflling and synthesis. In The Eleventh International Conference on Learning Representations, 2023. URL https: //openreview.net/forum?id $=$ hQwb-lbM6EL.   
Gulwani, S., Polozov, O., and Singh, R. Program synthesis. Foundations and Trends® in Programming Languages, 4(1-2):1–119, 2017. ISSN 2325-1107. doi: 10.1561/2500000010. URL http://dx.doi.org/ 10.1561/2500000010.   
Gunasekar, S., Zhang, Y., Aneja, J., Mendes, C. C. T., Giorno, A. D., Gopi, S., Javaheripi, M., Kauffmann, P., de Rosa, G., Saarikivi, O., Salim, A., Shah, S., Behl, H. S., Wang, X., Bubeck, S., Eldan, R., Kalai, A. T., Lee, Y. T., and Li, Y. Textbooks are all you need, 2023.   
Guo, D., Zhu, Q., Yang, D., Xie, Z., Dong, K., Zhang, W., Chen, G., Bi, X., Wu, Y., Li, Y. K., Luo, F., Xiong, Y., and Liang, W. Deepseek-coder: When the large language model meets programming – the rise of code intelligence, 2024.   
Hendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora, A., Guo, E., Burns, C., Puranik, S., He, H., Song, D., and Steinhardt, J. Measuring coding challenge competence with apps, 2021.   
Honovich, O., Scialom, T., Levy, O., and Schick, T. Unnatural instructions: Tuning language models with (almost) no human labor. In Rogers, A., Boyd-Graber, J., and Okazaki, N. (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 14409–14428, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.806. URL https: //aclanthology.org/2023.acl-long.806.   
Hugging Face. Hugging face: The ai community building the future. https://huggingface.co/, 2023. Accessed: 2023-12-01.   
Husain, H., Wu, H.-H., Gazit, T., Allamanis, M., and Brockschmidt, M. Codesearchnet challenge: Evaluating the state of semantic code search, 2020.   
Jain, N., Han, K., Gu, A., Li, W.-D., Yan, F., Zhang, T., Wang, S., Solar-Lezama, A., Sen, K., and Stoica, I. Livecodebench: Holistic and contamination free evaluation of large language models for code, 2024.   
Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., de las Casas, D., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., Lavaud, L. R., Lachaux, M.- A., Stock, P., Scao, T. L., Lavril, T., Wang, T., Lacroix, T., and Sayed, W. E. Mistral 7b, 2023a.   
Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S., de las Casas, D., Hanna, E. B., Bressand, F., Lengyel, G., Bour, G., Lample, G., Lavaud, L. R., Saulnier, L., Lachaux, M.-A., Stock, P., Subramanian, S., Yang, S., Antoniak, S., Scao, T. L., Gervet, T., Lavril, T., Wang, T., Lacroix, T., and Sayed, W. E. Mixtral of experts, 2024.   
Jiang, N., Liu, K., Lutellier, T., and Tan, L. Impact of code language models on automated program repair, 2023b.   
Jimenez, C. E., Yang, J., Wettig, A., Yao, S., Pei, K., Press, O., and Narasimhan, K. Swe-bench: Can language models resolve real-world github issues?, 2023.   
Kocetkov, D., Li, R., Allal, L. B., Li, J., Mou, C., Ferrandis, C. M., Jernite, Y., Mitchell, M., Hughes, S., Wolf, T., Bahdanau, D., von Werra, L., and de Vries, H. The stack: 3 tb of permissively licensed source code, 2022.   
Lai, Y., Li, C., Wang, Y., Zhang, T., Zhong, R., Zettlemoyer, L., tau Yih, S. W., Fried, D., Wang, S., and Yu, T. Ds1000: A natural and reliable benchmark for data science code generation, 2022.   
Lemieux, C., Inala, J. P., Lahiri, S. K., and Sen, S. Codamosa: Escaping coverage plateaus in test generation with pre-trained large language models. In 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE), pp. 919–931. IEEE, 2023.  

Li, R., Allal, L. B., Zi, Y., Muennighoff, N., Kocetkov, D., Mou, C., Marone, M., Akiki, C., Li, J., Chim, J., Liu, Q., Zheltonozhskii, E., Zhuo, T. Y., Wang, T., Dehaene, O., Davaadorj, M., Lamy-Poirier, J., Monteiro, J., Shliazhko, O., Gontier, N., Meade, N., Zebaze, A., Yee, M.-H., Umapathi, L. K., Zhu, J., Lipkin, B., Oblokulov, M., Wang, Z., Murthy, R., Stillerman, J., Patel, S. S., Abulkhanov, D., Zocca, M., Dey, M., Zhang, Z., Fahmy, N., Bhattacharyya, U., Yu, W., Singh, S., Luccioni, S., Villegas, P., Kunakov, M., Zhdanov, F., Romero, M., Lee, T., Timor, N., Ding, J., Schlesinger, C., Schoelkopf, H., Ebert, J., Dao, T., Mishra, M., Gu, A., Robinson, J., Anderson, C. J., Dolan-Gavitt, B., Contractor, D., Reddy, S., Fried, D., Bahdanau, D., Jernite, Y., Ferrandis, C. M., Hughes, S., Wolf, T., Guha, A., von Werra, L., and de Vries, H. Starcoder: may the source be with you!, 2023.  

Li, Y., Choi, D., Chung, J., Kushman, N., Schrittwieser, J., Leblond, R., Eccles, T., Keeling, J., Gimeno, F., Dal Lago, A., Hubert, T., Choy, P., de Masson d’Autume, C., Babuschkin, I., Chen, X., Huang, P.-S., Welbl, J., Gowal, S., Cherepanov, A., Molloy, J., Mankowitz, D. J., Sutherland Robson, E., Kohli, P., de Freitas, N., Kavukcuoglu, K., and Vinyals, O. Competitionlevel code generation with alphacode. Science, 378 (6624):1092–1097, December 2022. ISSN 1095-9203. doi: 10.1126/science.abq1158. URL http://dx.doi. org/10.1126/science.abq1158.  

Liu, J., Peng, J., Wang, Y., and Zhang, L. Neuri: Diversifying dnn generation via inductive rule inference. In Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE 2023, pp. 657–669, New York, NY, USA, 2023a. Association for Computing Machinery. ISBN 9798400703270. doi: 10.1145/3611643.3616337. URL https://doi. org/10.1145/3611643.3616337.  

Liu, J., Xia, C. S., Wang, Y., and Zhang, L. Is your code generated by chatGPT really correct? rigorous evaluation of large language models for code generation. In Thirtyseventh Conference on Neural Information Processing Systems, 2023b. URL https://openreview.net/ forum?id $=$ 1qvx610Cu7.  

Lozhkov, A., Li, R., Allal, L. B., Cassano, F., Lamy-Poirier, J., Tazi, N., Tang, A., Pykhtar, D., Liu, J., Wei, Y., Liu, T., Tian, M., Kocetkov, D., Zucker, A., Belkada, Y., Wang, Z., Liu, Q., Abulkhanov, D., Paul, I., Li, Z., Li, W.-D., Risdal, M., Li, J., Zhu, J., Zhuo, T. Y., Zheltonozhskii, E., Dade, N. O. O., Yu, W., Krauß, L., Jain, N., Su, Y., He, X., Dey, M., Abati, E., Chai, Y., Muennighoff, N., Tang, X., Oblokulov, M., Akiki, C., Marone, M., Mou, C., Mishra, M., Gu, A., Hui, B., Dao, T., Zebaze, A., Dehaene, O., Patry, N., Xu, C., McAuley, J., Hu, H.,  

Scholak, T., Paquet, S., Robinson, J., Anderson, C. J., Chapados, N., Patwary, M., Tajbakhsh, N., Jernite, Y., Ferrandis, C. M., Zhang, L., Hughes, S., Wolf, T., Guha, A., von Werra, L., and de Vries, H. Starcoder 2 and the stack v2: The next generation, 2024.   
Luo, Z., Xu, C., Zhao, P., Sun, Q., Geng, X., Hu, W., Tao, C., Ma, J., Lin, Q., and Jiang, D. Wizardcoder: Empowering code large language models with evol-instruct. arXiv preprint arXiv:2306.08568, 2023a.   
Luo, Z., Xu, C., Zhao, P., Sun, Q., Geng, X., Hu, W., Tao, C., Ma, J., Lin, Q., and Jiang, D. Wizardcoder: Empowering code large language models with evol-instruct, 2023b.   
Microsoft. Azure openai service models. https: //learn.microsoft.com/en-us/azure/ cognitive-services/openai/concepts/ models, 2023a.   
Microsoft. GitHub Copilot – Your AI pair programmer. https://github.com/features/ copilot, 2023b.   
Muennighoff, N., Liu, Q., Zebaze, A., Zheng, Q., Hui, B., Zhuo, T. Y., Singh, S., Tang, X., von Werra, L., and Longpre, S. Octopack: Instruction tuning code large language models, 2023.   
Nijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou, Y., Savarese, S., and Xiong, C. Codegen: An open large language model for code with multi-turn program synthesis. In The Eleventh International Conference on Learning Representations, 2023. URL https: //openreview.net/forum?id $=$ iaYcJKpY2B_.   
Olausson, T. X., Inala, J. P., Wang, C., Gao, J., and Solar-Lezama, A. Is self-repair a silver bullet for code generation? In The Twelfth International Conference on Learning Representations, 2024. URL https:// openreview.net/forum?id=y0GJXRungR.   
OpenAI. Chatgpt: Optimizing language models for dialogue. https://openai.com/blog/chatgpt/, 2022.   
OpenAI. Gpt-4 technical report, 2023.   
Roziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., Adi, Y., Liu, J., Remez, T., Rapin, J., Kozhevnikov, A., Evtimov, I., Bitton, J., Bhatt, M., Ferrer, C. C., Grattafiori, A., Xiong, W., Defossez, A., Copet, J., Azhar, F., Touvron, H., Martin, L., Usunier, N., Scialom, T., and Synnaeve, G. Code llama: Open foundation models for code, 2023.   
Schafer, M., Nadi, S., Eghbali, A., and Tip, F. An empirical evaluation of using large language models for automated unit test generation. IEEE Transactions on Software Engineering, 2023.  

Services, A. W. AI Code Generator - Amazon CodeWhisperer - AWS. https://aws.amazon.com/ codewhisperer/, 2023.  

Shazeer, N. and Stern, M. Adafactor: Adaptive learning rates with sublinear memory cost, 2018.  

SPARCK JONES, K. A statistical interpretation of term specifcity and its application in retrieval. 28(1):11–21, 2023/11/30 1972. doi: 10.1108/eb026526. URL https: //doi.org/10.1108/eb026526.  

Su, H., Shi, W., Kasai, J., Wang, Y., Hu, Y., Ostendorf, M., Yih, W.-t., Smith, N. A., Zettlemoyer, L., and Yu, T. One embedder, any task: Instruction-fnetuned text embeddings. 2022. URL https://arxiv.org/abs/ 2212.09741.  

Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., and Hashimoto, T. B. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/ stanford_alpaca, 2023.  

theblackcat102. The evolved code alpaca dataset. https://huggingface.co/datasets/ theblackcat102/evol-codealpaca-v1, 2023.  

Wang, X., Dillig, I., and Singh, R. Program synthesis using abstraction refnement. Proc. ACM Program. Lang., 2 (POPL), dec 2017. doi: 10.1145/3158151. URL https: //doi.org/10.1145/3158151.  

Wang, Y., Wang, W., Joty, S., and Hoi, S. C. CodeT5: Identifer-aware unifed pre-trained encoder-decoder models for code understanding and generation. In Moens, M.-F., Huang, X., Specia, L., and Yih, S. W.-t. (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 8696–8708, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.685. URL https:// aclanthology.org/2021.emnlp-main.685.  

Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H. Self-instruct: Aligning language models with self-generated instructions. In Rogers, A., Boyd-Graber, J., and Okazaki, N. (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 13484–13508, Toronto, Canada, July 2023a. Association for Computational Linguistics. doi: 10.18653/v1/ 2023.acl-long.754. URL https://aclanthology. org/2023.acl-long.754.  

Wang, Y., Le, H., Gotmare, A. D., Bui, N. D. Q., Li, J., and Hoi, S. C. H. Codet5+: Open code large language models for code understanding and generation, 2023b.   
Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V. Finetuned language models are zero-shot learners, 2022.   
Wei, Y., Xia, C. S., and Zhang, L. Copiloting the copilots: Fusing large language models with completion engines for automated program repair, 2023.   
Xia, C. S. and Zhang, L. Less training, more repairing please: Revisiting automated program repair via zeroshot learning, 2022.   
Xia, C. S. and Zhang, L. Keep the conversation going: Fixing 162 out of 337 bugs for $\mathbb{S}0.42$ each using chatgpt. arXiv preprint arXiv:2304.00385, 2023.   
Xia, C. S., Paltenghi, M., Tian, J. L., Pradel, M., and Zhang, L. Universal fuzzing via large language models, 2023a.   
Xia, C. S., Wei, Y., and Zhang, L. Automated program repair in the era of large pre-trained language models. In 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE), pp. 1482–1494, 2023b. doi: 10.1109/ICSE48619.2023.00129.   
Xia, C. S., Deng, Y., and Zhang, L. Top leaderboard ranking $=$ top coding profciency, always? evoeval: Evolving coding benchmarks via llm, 2024.   
Xu, C., Sun, Q., Zheng, K., Geng, X., Zhao, P., Feng, J., Tao, C., and Jiang, D. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023.   
Yu, Y., Zhuang, Y., Zhang, J., Meng, Y., Ratner, A., Krishna, R., Shen, J., and Zhang, C. Large language model as attributed training data generator: A tale of diversity and bias, 2023.   
Yuan, Z., Lou, Y., Liu, M., Ding, S., Wang, K., Chen, Y., and Peng, X. No more manual tests? evaluating and improving chatgpt for unit test generation. arXiv preprint arXiv:2305.04207, 2023.   
Zhang, F., Chen, B., Zhang, Y., Keung, J., Liu, J., Zan, D., Mao, Y., Lou, J.-G., and Chen, W. Repocoder: Repository-level code completion through iterative retrieval and generation, 2023.  

You are exceptionally skilled at crafting high-quality programming problems and offering precise solutions.  

Please gain inspiration from the following random code snippet to create a high-quality programming problem. Present your output in two distinct sections: [Problem Description] and [Solution].  

Code snippet for inspiration:  

#### {code}  

Guidelines for each section:  

1. [Problem Description]: This should be \*\*completely self-contained\*\*, providing all the contextual information one needs to understand and solve the problem. Assume common programming knowledge, but ensure that any specific context, variables, or code snippets pertinent to this problem are explicitly included.   
2. [Solution]: Offer a comprehensive, \*\*correct\*\* solution that accurately addresses the [Problem Description] you provided.  

Figure 4: The detailed prompt design for OSS-INSTRUCT  

### A. More Details of OSS-INSTRUCT  

#### A.1. Prompt Design  

Figure 4 illustrates the prompt template of OSS-INSTRUCT, where the frst section presents a high-level description of the task, the second section incorporates the code snippet, and the third section offers guidelines on the response.  

#### A.2. Qualitative Examples  

Figure 5 extends Figure 2 and shows more qualitative examples of OSS-INSTRUCT generation, including outputs that are inspired by method defnitions, shell scripts, library imports, class signatures, code statements, and code comments.  

#### A.3. Breakdown of OSS-INSTRUCT Dataset  

Categories To study the categories of OSS-INSTRUCT-generated data, we use INSTRUCTOR (Su et al., 2022), which is one of the SOTA embedding models and can generate different text embeddings according to a task instruction. Inspired by OctoPack (Muennighoff et al., 2023) and the topic tags on GitHub, we manually designed 10 categories specifc to coding. As shown in Figure 6, we calculate the cosine similarity between the embeddings of each sample in OSS-INSTRUCT and the 10 categories to obtain the category breakdown. Overall, OSS-INSTRUCT exhibits diversity and balance across different categories.  

Length distribution We depict the length distribution for both generated problems and solutions in Figure 7. The $\mathbf{X}$ -axis represents the number of tokens in each problem/solution, while the y-axis shows the correspondent number of samples.  

### B. Implementation Details  

#### B.1. Data Generation  

We use gpt-3.5-turbo-1106 as the foundation model to do OSS-INSTRUCT due to its high cost-effectiveness. We randomly extract 1–15 lines from each selected code document from starcoderdata and let gpt-3.5-turbo-1106 imagine a self-contained coding problem and a correct solution. Given the numerous seed code snippets, we perform greedy decoding to maximize the consistency between the generated problems and solutions.  

D Seed: method definition render() {  

#### D Seed: shell script  

python3 makeErrorFile.py data/test_dataset_14 14  

#### Problem  

Your task is to complete the \`render\` method to generate the rendered shape as a string...  

#### Problem  

Create a Python program that generates an error file based on a given dataset...  

#### Code  

class ShapeRenderer $\{$ constructor(vertices) { this.vertices $\equiv$ vertices; } render() { let renderedShape $\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad=\quad11\cdot11$ ; for (let $i\ =\ 0$ ; $_{\mathrm{~i~}}<$ this.vertices.length; $_{\mathrm{i++}}$ ) { const vertex $\underline{{\underline{{\mathbf{\delta\pi}}}}}$ this.vertices[i]; renderedShape $+=$ \`(\${vertex. $\boldsymbol{x}\}$ , \${vertex.y})\`; if $\langle_{\textup{i}}<$ this.vertices.length - 1) { renderedShape $+="-"$ ; } } return renderedShape; }   
}  

#### Code  

def generate_error_file(dataset_file, ...): error_lines $=\ [\,\mathrm{]}$ with open(dataset_file, 'r') as file: for line in file: with open(error_file_name, $\cdot_{{\sf w}}\cdot)$ as error_file: for error_line in error_lines: error_file.write(error_line $+\ ^{\prime}\cdot\mathsf{n}\,^{\prime}$ )   
if __name__ $==$ "__main__": if len(sys.argv) $:3$ : print("Usage: ...") else: dataset_file $\equiv$ sys.argv[1] dataset_number $\equiv$ sys.argv[2] generate_error_file(...)  

#### Seed: library imports  

import numpy as np import gym_electric_motor as gem import matplotlib.pyplot as plt  

#### Problem  

Create a reinforcement learning agent to control an electric motor using the OpenAI Gym environment...  

#### Code  

import numpy as np   
import gym_electric_motor as gem   
env $\underline{{\underline{{\mathbf{\delta\pi}}}}}$ gem.make("DcSeriesCont-v1")   
class DQNAgent: def __init__(self, state_dim, action_dim): ... def build_model(self): ... def act(self, state): ... def train(self, state, action, reward, ...): ...   
for episode in range(episodes): state $\equiv$ env.reset() state $\equiv$ np.reshape(state, [1, state_dim])  

#### D Seed: class signature  

@SpringBootApplication @Import({ AxonConfig.class }) public class AxonbankApplication { public static void main(String[] args) {  

#### Problem  

Create a simple Java Spring Boot application for a banking system...  

#### Code  

import org.axonframework.commandhandling.CommandBus;   
import org.axonframework.config.Configuration;  

@SpringBootApplication @Import({ AxonConfig.class }) public class AxonbankApplication {...} public class BankAccount {...} public class CreateAccountCommand $\left\lbrace\,.\,.\,\right\rbrace$ public class DepositFundsCommand $\left\{\,.\,.\,\right\}$ public class WithdrawFundsCommand {...} public class AccountCreatedEvent {...} public class FundsDepositedEvent {...} public class FundsWithdrawnEvent {...}  

#### Seed: code statements  

cutoff_range $\equiv$ np.ptp(cutoffs) if cutoff_range $==0$ : cutoff_range $\mathit{\Theta}=\;\;\uparrow$ cutoff_colors $\underline{{\underline{{\mathbf{\delta\pi}}}}}$ plt.get_cmap('plasma')( (cutoffs - np.min(cutoffs)) / cutoff_range )  

#### D Seed: comments  

# Set degrees  

#### Problem  

Implement a Python class that represents a temperature in degrees...  

#### Problem  

Implement a function that calculates the color values for a given set of cutof values based on a specified color map...  

#### Code  

import numpy as np import matplotlib.pyplot as plt def calculate_cutoff_colors(cutoffs, cmap_name): cutoff_range $\equiv$ np.ptp(cutoffs) if cutoff_range $\mathbf{\tau}=\mathbf{\tau}\theta$ : cutoff_range $\mathit{\Theta}=\mathit{\Theta}\;1$ cmap $\underline{{\underline{{\mathbf{\delta\pi}}}}}$ plt.get_cmap(cmap_name) normalized_cutoffs $\underline{{\underline{{\mathbf{\delta\pi}}}}}$ . cutoff_colors $\underline{{\underline{{\mathbf{\delta\pi}}}}}$ ... return cutoff_colors  

#### Code  

class TemperatureConverter: def __init__(self): .. def set_celsius(self, degrees): def set_fahrenheit(self, degrees): ... def set_kelvin(self, degrees): .. def get_celsius(self): ... def get_fahrenheit(self): .. def get_kelvin(self): .. def convert_to(self, unit): if unit $\mathbf{\omega}=\mathbf{\omega}^{\prime}\mathbf{C}^{\prime}$ : return self.get_celsius() elif unit $==\mathbf{\nabla}^{\prime}\cdot\mathsf{F}^{\prime}$ : return self.get_fahrenheit() elif unit $==\cdot\times\cdot$ : return self.get_kelvin()  

Figure 5: More examples showing how OSS-INSTRUCT generates problems and solutions from seed code snippets. Detailed problem requirements, implementations, and explanations are omitted for brevity.  

![](https://cdn-mineru.openxlab.org.cn/extract/46a178f3-c3ba-47d7-9d38-6ae7bbdbd1b1/bf6c37abd1c08dafdbc2b28f3081eb6d2a37f7b4f5db59fdb48cafc6331e85da.jpg)  
Figure 6: The category constitution of OSS-INSTRUCT  

![](https://cdn-mineru.openxlab.org.cn/extract/46a178f3-c3ba-47d7-9d38-6ae7bbdbd1b1/d05bf70f44033444df8e07f52e0caabd84f3ac9fa914b34fbd52fe4795130bb9.jpg)  
Figure 7: Token count distribution of OSS-INSTRUCT-generated problems and solutions  

#### B.2. Data Decontamination  

We apply data decontamination before training our Magicoder and MagicoderS models. Following Li et al. (2023), we decontaminate both our 75K OSS-INSTRUCT dataset and the evol-codealpaca-v1 (theblackcat102, 2023) dataset, an open-source reproduction of Evol-Instruct generated by GPT-4 (OpenAI, 2023), by removing exact matches from HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), DS-1000 (Lai et al., 2022), and GSM8K (Cobbe et al., 2021). Eventually, we fltered out 9 problems for OSS-INSTRUCT dataset and 89 for evol-codealpaca-v1.  

#### B.3. Training  

We employ CODELLAMA-PYTHON-7B and DeepSeek-Coder-Base 6.7B as the base LLMs. To obtain Magicoder series, we frst fnetune the base models on about 75K synthetic data generated through OSS-INSTRUCT using the transformers library from Hugging Face (Hugging Face, 2023). We fnetune the base models for 2 epochs using two NVIDIA A100-80GB GPUs through the Distributed Data Parallel (DDP) module from PyTorch. We set the initial learning rate at 5e-5 with 15 warmup steps and a linear scheduler. We use Adafactor (Shazeer & Stern, 2018) as our optimizer and choose a batch size of 512 with a sequence truncation length of 1216. To obtain MagicoderS, we continue to fnetune Magicoder models with the evol-codealpaca-v1 dataset, an open-source Evol-Instruct implementation containing about 110K samples. We use the same hyperparameters except for 15 warmup steps and a 1024 maximum sequence length.  

### C. More Evaluation Results  

#### C.1. Evaluation on APPS for Competitive Programming  

We additionally evaluate Magicoder on APPS (Hendrycks et al., 2021), a benchmark suite of competitive programming problems. Following Olausson et al. (2024), we select a subset of 300 problems from the APPS test set. From Table 8, we can observe that the CODELLAMA-PYTHON-based Magicoder-CL signifcantly outperforms the base model and WizardCoderCL. MagicoderS-CL-7B is even better than WizardCoder-SC-15B despite having less than half the number of parameters.  

Meanwhile, DeepSeek-Coder-based MagicoderS-DS achieves the best result among all the evaluated baselines, substantially outperforming the instruction-tuned DeepSeek-Coder-6.7B-Instruct.  

Table 8: Pass $@1$ results on APPS evaluated using greedy decoding in a zero-shot setting.   


<html><body><table><tr><td>Model</td><td>Introductory (60)</td><td>Interview (180)</td><td>Competition (60)</td><td>Overall (300)</td></tr><tr><td>WizardCoder-SC-15B</td><td>21.7</td><td>6.1</td><td>1.7</td><td>8.3</td></tr><tr><td>CODELLAMA-PYTHON-7B</td><td>3.3</td><td>2.8</td><td>0.0</td><td>2.3</td></tr><tr><td>WizardCoder-CL-7B</td><td>10.0</td><td>3.9</td><td>1.7</td><td>4.7</td></tr><tr><td>Magicoder-CL-7B</td><td>18.3</td><td>5.6</td><td>1.7</td><td>7.3</td></tr><tr><td>MagicoderS-CL-7B</td><td>23.3</td><td>6.1</td><td>1.7</td><td>8.7</td></tr><tr><td>DeepSeek-Coder-6.7B-Base</td><td>16.7</td><td>7.2</td><td>0.0</td><td>7.7</td></tr><tr><td>DeepSeek-Coder-6.7B-Instruct</td><td>23.3</td><td>9.4</td><td>0.0</td><td>10.3</td></tr><tr><td>Magicoder-DS-6.7B</td><td>20.0</td><td>8.9</td><td>1.7</td><td>9.7</td></tr><tr><td>MagicoderS-DS-6.7B</td><td>28.3</td><td>11.7</td><td>3.3</td><td>13.3</td></tr></table></body></html>  

#### C.2. Fill-in-the-Middle Evaluation on DS-1000  

Table 9 shows the evaluation results of Magicoder-DS and MagicoderS-DS on DS-1000 (Lai et al., 2022) (Insertion format), assessing a model’s fll-in-the-middle capability. In this experiment, we use DeepSeek-Coder as the base model and exclude CODELLAMA-PYTHON-based results, as CODELLAMA-PYTHON does not support the fll-in-the-middle format. The results highlight Magicoder’s superior performance in fll-in-the-middle tasks compared to all other evaluated baselines. This outstanding capability suggests that Magicoder can serve as a valuable copilot for developers.  

Table 9: Pass $@1$ results on DS-1000 (Insertion format) with temperature $=0.2$ , $\mathsf{t o p\_p=0.5}$ , max length $\l=1024$ , and num s ${\tt a m p l e s}=40$ .   


<html><body><table><tr><td>Model</td><td>NumPy</td><td>Pandas</td><td>PyTorch</td><td>SciPy</td><td>Sklearn</td><td>TensorFlow</td><td>Overall</td></tr><tr><td>WizardCoder-SC-15B</td><td>35.1</td><td>20.4</td><td>30.4</td><td>28.9</td><td>32.3</td><td>37.8</td><td>28.6</td></tr><tr><td>DeepSeek-Coder-6.7B-Base</td><td>36.3</td><td>28.6</td><td>15.8</td><td>19.3</td><td>32.8</td><td>35.1</td><td>29.3</td></tr><tr><td>DeepSeek-Coder-6.7B-Instruct</td><td>44.1</td><td>27.3</td><td>38.2</td><td>30.8</td><td>38.4</td><td>29.6</td><td>34.6</td></tr><tr><td>Magicoder-DS-6.7B</td><td>39.7</td><td>31.2</td><td>27.4</td><td>23.7</td><td>44.6</td><td>30.2</td><td>33.9</td></tr><tr><td>MagicoderS-DS-6.7B</td><td>43.3</td><td>29.5</td><td>39.2</td><td>26.2</td><td>44.5</td><td>36.2</td><td>35.9</td></tr></table></body></html>  

#### C.3. Impact of Removing Noisy Data  

In $\S2.2$ , we highlight the benefts of preserving certain types of noise in instruction-tuning data. To support this argument, we removed data samples with partially implemented code, such as those containing pass or NotImplemented tokens in Python problems. Table 10 shows the pass $@1$ performance of CODELLAMA-PYTHON-7B on HumanEval+ when fnetuned for 2 epochs on the raw OSS-INSTRUCT dataset compared to the same dataset with noisy samples removed. We can observe that retaining the noisy samples results in slightly better performance.  

Table 10: Pass $@1$ on HumanEval $^+$ when fnetuning CODELLAMA-PYTHON-7B with and without noisy samples.   


<html><body><table><tr><td>Setting</td><td>Dataset Size</td><td>HumanEval+</td></tr><tr><td>Withr noisy data</td><td>75K</td><td>55.5</td></tr><tr><td>Without noisy data</td><td>68K</td><td>54.9</td></tr></table></body></html>  

### D. Cases where OSS-INSTRUCT Improves Code Understanding  

In this section, we compare the base model, DeepSeek-Coder-6.7B-Base, and its OSS-INSTRUCT-tuned version, MagicoderDS-6.7B, in both in-domain and out-of-domain scenarios and demonstrate how OSS-INSTRUCT helps improve the base model’s instruction following capability. We provide the base model with an appropriate prompt prefx to make sure it can produce a plausible output format.  

#### D.1. More Precise Understanding of Requirements  

The following example shows that after instruction tuning, the model can understand the requirement “Check whether the input is valid” more precisely. DeepSeek-Coder-6.7B-Base only considers the integer case, while Magicoder-DS-6.7B effectively checks if the parameter is of the correct type.  

<html><body><table><tr><td colspan="2">Prompt: Write a function that takes an integer a and returns True if this integer is a cube of some integer number. Note: you should check whether the input is valid.</td></tr><tr><td>defis_cube（a）:</td><td>defis_cube（a）:</td></tr><tr><td>ifa<=O: returnFalse</td><td>ifnotisinstance（a，int）ora<O: return False#Invalidinput</td></tr><tr><td>x=int(a**(1/3）) return x**3 == a</td><td>#Checkifthenumberisaperfectcube</td></tr><tr><td>DeepSeek-Coder-6.7B-Base</td><td>x=int(a**(l/3))</td></tr><tr><td></td><td>return xx*3 == a Magicoder-DS-6.7B</td></tr></table></body></html>  

#### D.2. Handling Corner Cases  

The example below indicates that Magicoder-DS-6.7B can correctly handle the corner case, where the input string has less than or equal to 2 characters, even if this is not explicitly stated in the prompt. This is also an out-of-domain example since the Scala programming language is not included in OSS-INSTRUCT dataset.  

![](https://cdn-mineru.openxlab.org.cn/extract/46a178f3-c3ba-47d7-9d38-6ae7bbdbd1b1/5f76a0f3c01e4bd6b2de8cd3bb6e5bea17988013824a62a8214e5daae1f2ae3e.jpg)  

#### D.3. Generalizing to Unseen Tasks  

We also show that after instruction tuning, Magicoder-DS-6.7B can respond plausibly to unseen requests during training. This example asks the model to create an easy Python coding exercise, which the base model cannot interpret. Such kinds of tasks are also not incorporated in the OSS-INSTRUCT dataset.  

<html><body><table><tr><td colspan="2">Prompt:Create an easyPython coding exercise</td></tr><tr><td>def add_numbers(a, b) : return a + b</td><td># Given a list of integers,return the sum ofallthe positive integersin the list.</td></tr><tr><td rowspan="3">DeepSeek-Coder-6.7B-Base</td><td>def sum_positive_integers(nums):</td></tr><tr><td>return sum(num for num in nums if num 0) #Testthefunction</td></tr><tr><td>numbers = [1， -2，3，-4， 5，-6，7，-8， -10] print(sum_positive_integers(numbers)） #</td></tr><tr><td></td><td>Output: 25 Magicoder-DS-6.7B</td></tr></table></body></html>  

### E. Generating Complex Programs  

In this section, we present the cases where the best performing MagicoderS-DS-6.7B is capable of generating real-world, complex programs.  

#### E.1. Implementing Snake Game in Python  

Prompt: Write a snake game in Python using the turtle library (the game is created by Magicoder). Response: Here is a simple snake game in Python using the turtle library. This game is a basic version and does not include many features that a full-fedged snake game would have.  

import turtle import time import random delay = 0.1 # Score score $\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad=\quad0$ high_score $\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad=\quad0$  

# Set up the screen   
wn $=$ turtle.Screen()   
wn.title("Snake Game by Magicoder")   
wn.bgcolor("green")   
wn.setup(width $=600$ , height $=600$ )   
wn.tracer(0) # Turns off the screen updates   
# Snake head   
head $=$ turtle.Turtle()   
head.speed(0)   
head.shape("square")   
head.color("black")   
head.penup()   
head.goto(0, 0)   
head.direction $=$ "stop"   
# Snake food   
food $=$ turtle.Turtle()   
food.speed(0)   
food.shape("circle")   
food.color("red")   
food.penup()   
food.goto(0, 100)  

segments = []  

# Pen   
pen $=$ turtle.Turtle()   
pen.speed(0)   
pen.shape("square")   
pen.color("white")   
pen.penup()   
pen.hideturtle()   
pen.goto(0, 260)   
pen.write("Score: 0 High Score: 0", align $=$ "center", font $=$ ("Courier", 24, "normal"))  

# Functions  

def go_up(): if head.direction! $!=$ "down": head.direction $=$ "up"  

def go_down():  

if head.direction!= "up": head.direction $=$ "down"  

def go_left(): if head.direction! $=$ "right": head.direction $=$ "left"  

def go_right():  

if head.direction! $!=$ "left": head.direction $=$ "right"  

def move():  

if head.direction $==$ "up": y $=$ head.ycor() head.sety(y + 20)  

if head.direction $==$ "down": y $=$ head.ycor() head.sety(y - 20)  

if head.direction $==$ "left": $_{\mathrm{~x~}}=$ head.xcor() head.setx(x - 20)  

if head.direction $==$ "right": $_{\mathrm{~x~}}=$ head.xcor() head.setx $(\times+\ 20$ )  

# Keyboard bindings wn.listen()  

wn.onkeypress(go_up, "w")  

wn.onkeypress(go_down, "s"  

wn.onkeypress(go_left, "a")  

wn.onkeypress(go_right, "d")  

# Main game loop while True: wn.update()  

# Check for a collision with the border  

if head.xcor() $>~290$ or head.xcor() $<\ -2\,90$ or head.ycor() $>$ 290 or head.ycor() < -290: time.sleep(1) head.goto(0, 0) head.direction $=$ "stop"  

# Hide the segments for segment in segments: segment.goto(1000, 1000)  

# Clear the segments list segments.clear()  

# Reset the score score $\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad=\quad0$  

$\#$ Reset the delay delay $=~0\cdot1$  

# Update the score display   
pen.clear()   
pen.write("Score: {} High Score: {}".format(score, high_score), align="center", font $=$ ("Courier", 24, "  

normal"))  

$\#$ Check for a collision with the food   
if head.distance(food) $<~20$ : $\#$ Move the food to a random spot $_{\mathrm{~x~}}=$ random.randint(-280, 280) $\mathrm{~y~}=$ random.randint(-280, 280) food.goto(x, y)  

$\#$ Add a segment new_segment $=$ turtle.Turtle() new_segment.speed(0) new_segment.shape("square") new_segment.color("grey") new_segment.penup() segments.append(new_segment)  

# Shorten the delay delay $-=\mathrm{~0~.~}0\,0\,1$  

# Increase the score score $+=~10$  

if score $>$ high_score: high_score $=$ score  

pen.clear()   
pen.write("Score: {} High Score: {}".format(score, high_score), align $\cdot=$ "center", font $=$ ("Courier", 24, " normal"))  

$\#$ Move the end segments first in reverse order for index in range(len(segments) - 1, 0, -1): $_{\mathrm{~x~}}=$ segments[index - 1].xcor() $\mathrm{~y~}=$ segments[index - 1].ycor() segments[index].goto(x, y)  

# Move segment 0 to where the head is  

if len(segments) $>~0$ : $_{\mathrm{~x~}}=$ head.xcor() $\mathrm{~y~}=$ head.ycor() segments[0].goto(x, y)   
move()  

# Check for head collision with the body segments for segment in segments:  

if segment.distance(head) $<~20$ : time.sleep(1) head.goto(0, 0) head.direction $=$ "stop" # Hide the segments for segment in segments: segment.goto(1000, 1000) # Clear the segments list segments.clear() # Reset the score score $\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad=\quad0$ # Reset the delay delay $=~0\cdot1$ # Update the score display pen.clear() pen.write("Score: {} High Score: {}".format(score, high_score), align $=$ "center", font $=$ ("Courier", 24, " normal")) time.sleep(delay) .mainloop()  

This game is a simple version of the classic snake game. The snake starts with a single segment and grows in size when it eats food. The game ends when the snake hits the border or its own body. The score is increased by 10 points for each piece of food eaten. The game speed increases as the snake grows.  

#### E.2. Building Othello Game in Java  

Prompt: Build a console-based Othello game in Java with row and column numbers shown on the board. The game should end when there are no more valid moves for either player.   
Response: Here is a simple console-based Othello game in Java. It does not include the graphical interface, but it does include the row and column numbers on the board.  

import java.util.Scanner; ublic class Othello { private static final int BOARD_SIZE = 8; private static final char $\mathrm{EMP\,TY}~=~{\bf\nabla}^{\prime}\,-\prime$ ; private static final char BLACK $\begin{array}{r l}{\mathbf{\Phi}=}&{{}^{\prime}\mathbb{B}^{\prime}}\end{array}$ ; private static final char WHITE $\begin{array}{r l}{\mathbf{\Phi}=}&{{}^{\prime}\mathbf{\Phi}\mathbb{W}^{\prime}}\end{array}$ ; private static final char[][] board $=$ new char[BOARD_SIZE][BOARD_SIZE]; private static char currentPlayer $=$ BLACK; public static void main(String[] args) { initializeBoard(); printBoard(); while (true) { System.out.println("Player " $^+$ currentPlayer $^+$ ", enter your move (row column):"); Scanner scanner $=$ new Scanner(System.in); int row $=$ scanner.nextInt(); int col $=$ scanner.nextInt(); if (isValidMove(row, col)) { makeMove(row, col); printBoard(); if (!hasValidMove()) { System.out.println("Player " $^+$ currentPlayer $^+$ " has no valid moves. Game over!"); break; } currentPlayer $=$ (currentPlayer $==$ BLACK)? WHITE : BLACK; } else { System.out.println("Invalid move. Try again."); } } private static void initializeBoard() { for (int $\mathrm{~\small~{~i~\alpha~=~0~}~}$ ; $\mathrm{~\pm~}<$ BOARD_SIZE; $\dot{\ I}++$ ) { for (int $\dot{\ ]}\ =\ \ 0$ ; j $<$ BOARD_SIZE; $\dot{\b{7}}+\dot{\mathbf{\eta}}+$ ) { board[i][j] $=$ EMPTY; } } board[3][3] $=$ board[4][4] $=$ BLACK; board[3][4] $=$ board[4][3] $=$ WHITE; } private static void printBoard() { System.out.print $("_{\mathsf{c u}}")$ ; for (int $\mathrm{~\small~{~i~\alpha~=~0~}~}$ ; i $<$ BOARD_SIZE; $\dot{\ I}++$ ) { System.out.print $\left(\begin{array}{l l l}{\rule{0.5pt}{1}}&{+}&{\cdots}\\ {\rule{0.5pt}{1}}&{+}&{\cdots}\end{array}^{\mathfrak{m}}\right)$ ; } System.out.println(); for (int $\mathrm{~\small~{~i~\alpha~=~0~}~}$ ; i $<$ BOARD_SIZE; $\dot{\ I}++$ ) { System.out.print $^{(\mathrm{~i~}{}+{}^{\mathrm{~n~}}\!\!\!^{\dag})}$ ; for (int $\dot{\ ]}\ =\ \ 0$ ; j $<$ BOARD_SIZE; $\dot{\b{7}}+\dot{\mathbf{\eta}}+$ ) { System.out.print(board[i][j] + " "); } System.out.println(); } } private static boolean isValidMove(int row, int col) { if (board[row][col] $!=$ EMPTY) { return false; }  

for (int $\mathrm{~i~}=\mathrm{~-1~}$ ; $\mathrm{~i~}<=~1$ ; $\pm++$ ) { for $i\({\bf\dot{n}}{\epsilon}\dot{\mathrm{\boldmath~\lambda~}}\mathrm{\boldmath~\dot{~}{\phi}~}=\mathrm{\boldmath~-1~};\mathrm{\boldmath~\dot{~}{\phi}~}<=\mathrm{\boldmath~1~};\mathrm{\boldmath~\dot{~}{\phi}~}++\mathrm{\boldmath~\dot{~}{\phi}~}$ if $(\stackrel{.}{1}!=\;0\;\;\;\mid\;\;\;\dot{]}:=\;\;0\,)$ { if (isValidDirection $\because\mathtt{r o w}$ , col, i, j)) { return true;  

} return false; } private static boolean isValidDirection(int row, int col, int rowDir, int colDir) { int newRow $=$ row $^+$ rowDir; int newCol $=$ col $^+$ colDir; while (newRow $>=~0$ && newRow $<$ BOARD_SIZE && newCol $>=~0$ && newCol $<$ BOARD_SIZE) { if (board[newRow][newCol] $==$ EMPTY) { return false; } if (board[newRow][newCol] $==$ currentPlayer) { return true; } newRow $+=$ rowDir; newCol $+=$ colDir; } return false; } private static void makeMove(int row, int col) { board[row][col] $=$ currentPlayer; for (int $\mathrm{~i~}=\mathrm{~-1~}$ ; i $<=\ 1$ ; $\pm++$ ) { for (int $\dot{\mathrm{~\rightmoon~}}=\mathrm{~\it~-1~}$ ; j $<=~1$ ; $\dot{\jmath}++)$ ) { if $(\stackrel{.}{1}!=\;0\;\;\;\mid\;\;\;\dot{]}:=\;\;0\,)$ { flipDirection(row, col, i, j); } private static void flipDirection(int row, int col, int rowDir, int colDir) { int newRow $=$ row $^+$ rowDir; int newCol $=$ col $^+$ colDir; while (newRow $>=$ 0 && newRow $<$ BOARD_SIZE && newCol $>=~0$ && newCol $<$ BOARD_SIZE) { if (board[newRow][newCol] $==$ EMPTY) { return; } if (board[newRow][newCol] $==$ currentPlayer) { while (true) { newRow $-=$ rowDir; newCol $-=$ colDir; if (newRow $==$ row && newCol $==$ col) { break; } board[newRow][newCol] $=$ currentPlayer; } return; } newRow $+=$ rowDir; newCol $+=$ colDir; } } private static boolean hasValidMove() { for (int $\mathrm{~\small~{~i~\alpha~=~0~}~}$ ; i $<$ BOARD_SIZE; $\dot{\ I}++$ ) { for (int $\dot{\ ]}\ =\ \ 0$ ; j $<$ BOARD_SIZE; $\dot{\b{7}}+\dot{\mathbf{\eta}}+$ ) { if (isValidMove(i, j)) { return true; } } } return false; }   
This program starts with a 8x8 board with two pieces in the center of the board, one black and one white. The player who   
goes frst is black. The game ends when there are no more valid moves for either player. The player enters their move as  

two numbers, the row and the column. If the move is valid, the game makes the move and switches the player. If the move is not valid, the player is asked to enter a valid move.  

#### E.3. Writing Specifc Gradio Application  

Prompt: Write a gradio (3.48.0) application for the following use case: Take an input image and return a 45 degree clockwise rotated image. You should also add text description under the output showing the rotation degree. Response: Here is a simple snake game in Python using the turtle library. This game is a basic version and does not include many features that a full-fedged snake game would have.  

import gradio as gr import numpy as np from PIL import Image  

def rotate_image(img): img $=$ Image.fromarray(img) rotated_img $=$ img.rotate(-45) return np.array(rotated_img), "45 degree clockwise rotation"   
demo $=$ gr.Interface( fn=rotate_image, input $_{\mathbb{S}^{=}\mathbb{Q}^{\Sigma}}$ .inputs.Image(type $=$ "numpy"), output $z=$ [gr.outputs.Image(type $=$ "numpy"), "text"], title $=^{\prime}$ "Image Rotation", description $=$ "Upload an image and get a 45 degree clockwise rotated image",   
)  

demo.launch()  

This script defnes a function rotate_image that takes an image as input, rotates it 45 degrees clockwise using the PIL library, and returns the rotated image and a text description. The gr.Interface function is then used to create a Gradio interface for this function.  

The gr.inputs.Image(type $=$ "numpy") line specifes that the input to the function should be an image, and the gr.outputs.Image(type $=$ "numpy") line specifes that the output should be an image. The ”text” output type is used to display the text description.  

Finally, demo.launch() is used to launch the Gradio interface.  

#### E.4. Developing Machine Learning Pipeline with PyTorch  

Prompt: Build a simple neural network in Python using Pytorch to classify handwritten digits from the MNIST dataset. You should use CNN as the model structure, train the model for 5 epochs, draw a chart of the training loss, and show the fnal result.  

Response: Here is a simple implementation of a Convolutional Neural Network (CNN) using PyTorch to classify handwritten digits from the MNIST dataset.  

import torch   
import torch.nn as nn   
import torchvision   
import torchvision.transforms as transforms   
import matplotlib.pyplot as plt  

# Device configuration device $=$ torch.device(’cuda’ if torch.cuda.is_available() else ’cpu’)  

# Hyper-parameters num_epochs $=~5$ batch_size $=~100$ learning_rate $\begin{array}{r l}{=}&{{}0\cdot0\,0\,1}\end{array}$  

$\#$ MNIST dataset   
train_dataset $=$ torchvision.datasets.MNIST(root=’../../data’, train=True, transform=transforms.ToTensor() download=True)  

test_dataset $=$ torchvision.datasets.MNIST(root=’../../data’, train=False, transform=transforms.ToTensor())  

# Data loader train_loader $=$ torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size,  

test_loader $=$ torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle $,=$ False)  

# Convolutional neural network (two convolutional layers)   
class ConvNet(nn.Module): def __init__(self): super(ConvNet, self).__init__() self.layer1 $=$ nn.Sequential( nn.Conv2d(1, 16, kernel_size $=5$ , stride $^{-1}$ , padding $^{=2}$ ), nn.BatchNorm2d(16), nn.ReLU(), nn.MaxPool2d(kernel_size $^{=2}$ , stride $^{=2}$ )) self.layer2 $=$ nn.Sequential( nn.Conv2d(16, 32, kernel_size $=5$ , stride $\mathrel{\mathop:}$ , padding $^{=2}$ ), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(kernel_size $^{=2}$ , stride $^{=2}$ )) self.fc $=$ nn.Linear(7\*7\*32, 10) def forward(self, $\mathrm{~\boldmath~x~}_{\!\mathrm{~\!~-~}\!\!\mathrm{~\!~x~}_{\!}}^{\!\mathrm{~\!~\!~-~}}$ ): out $=$ self.layer1(x) out $=$ self.layer2(out) out $=$ out.reshape(out.size(0), -1) out $=$ self.fc(out) return out   
model $=$ ConvNet().to(device)   
$\#$ Loss and optimizer   
criterion $=$ nn.CrossEntropyLoss()   
optimizer $=$ torch.optim.Adam(model.parameters(), $\beth\beth\!=\!\beth$ earning_rate)   
# Train the model   
total_step $=$ len(train_loader)   
loss_list $=$ []   
for epoch in range(num_epochs): for i, (images, labels) in enumerate(train_loader): images $=$ images.to(device) labels $=$ labels.to(device) # Forward pass outputs $=$ model(images) loss $=$ criterion(outputs, labels) # Backward and optimize optimizer.zero_grad() loss.backward() optimizer.step() if $(\,\underline{{{1}}}+\underline{{{1}}}\,)$ ) % print (’Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}’ .format(epoch+1, num_epochs, i+1, total_step, loss.item())) loss_list.append(loss.item())   
# Test the model   
model.eval() # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)   
with torch.no_grad(): correct $\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad=\quad0$ total $\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad=\quad0$ for images, labels in test_loader: images $=$ images.to(device) labels $=$ labels.to(device) outputs $=$ model(images) _, predicted $=$ torch.max(outputs.data, 1) total $+=$ labels.size(0) correct $+=$ (predicted $==$ labels).sum().item() print(’Test Accuracy of the model on the 10000 test images: {} %   
# Draw a chart of the training loss   
plt.plot(loss_list)   
plt.title(’Training Loss’)   
plt.xlabel(’Epoch’)   
plt.ylabel(’Loss’)   
plt.show()  

This script frst loads the MNIST dataset and then defnes a Convolutional Neural Network (CNN) with two convolutional layers. The CNN is trained for 5 epochs using the Adam optimizer and the Cross Entropy Loss function. After training, the model is tested on the test dataset and the accuracy is printed. Finally, a chart of the training loss is drawn.  

### F. Limitations  

While Magicoders prove to be effective by being instruction-tuned with OSS-INSTRUCT and Evol-Instruct generated data, we apply our techniques to models with no more than 7B parameters due to resource constraints. This limitation hinders our ability to observe the method’s impact on larger models. Moreover, the OSS-INSTRUCT prompt is sophisticated and may require a relatively strong model to comprehend the correct intent. Weaker models may fail to produce plausible instruction data. Future research may explore applying OSS-INSTRUCT to models at different capacities in the context of self-training.  