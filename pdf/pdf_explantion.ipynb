{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Invalid requirement: 'anthropic,': Expected end or semicolon (after name and no valid version specifier)\n",
      "    anthropic,\n",
      "             ^\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install anthropic\n",
    "%pip install rich\n",
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from typing import Dict, List\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def get_completion(messages: List[Dict[str, str]]) -> str:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=messages,\n",
    "        max_completion_tokens=4092, \n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Dict, List\n",
    "from pathlib import Path\n",
    "\n",
    "class MarkdownSectionParser:\n",
    "    def __init__(self):\n",
    "        self.section_pattern = r'^#+ .*$'  # '#'ÏúºÎ°ú ÏãúÏûëÌïòÎäî Ìó§Îçî Ìå®ÌÑ¥\n",
    "        self.end_sections = {'ACKNOWLEDGEMENTS', 'REFERENCES', 'CONCLUSION', 'CONCLUSIONS'}\n",
    "        \n",
    "    def parse_sections(self, markdown_path: str) -> Dict[str, str]:\n",
    "        sections = {}\n",
    "        current_section = None\n",
    "        current_content = []\n",
    "        \n",
    "        with open(markdown_path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            \n",
    "        for line in lines:\n",
    "            if re.match(self.section_pattern, line):\n",
    "                section_title = line.strip('# \\n')\n",
    "                \n",
    "                if current_section and current_content:\n",
    "                    sections[current_section] = ''.join(current_content).strip()\n",
    "                \n",
    "                # ÎåÄÎ¨∏ÏûêÎ°ú Î≥ÄÌôòÌïòÏó¨ ÎπÑÍµê\n",
    "                if any(end_sec in section_title.upper() for end_sec in self.end_sections):\n",
    "                    break\n",
    "                    \n",
    "                current_section = section_title\n",
    "                current_content = []\n",
    "            else:\n",
    "                if current_section is None and 'ABSTRACT' not in line.upper():\n",
    "                    continue\n",
    "                current_content.append(line)\n",
    "        \n",
    "        # ÎßàÏßÄÎßâ ÏÑπÏÖò Ï†ÄÏû•\n",
    "        if current_section and current_content:\n",
    "            sections[current_section] = ''.join(current_content).strip()\n",
    "        \n",
    "        # Abstract ÌïÑÌÑ∞ÎßÅ\n",
    "        filtered_sections = {}\n",
    "        include_section = False\n",
    "        \n",
    "        for section, content in sections.items():\n",
    "            if 'ABSTRACT' in section.upper():\n",
    "                include_section = True\n",
    "            \n",
    "            if include_section:\n",
    "                filtered_sections[section] = content\n",
    "                \n",
    "        return filtered_sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "from typing import Dict, List\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "class PaperExplainer:\n",
    "    def __init__(self):\n",
    "        self.conversation_history = []\n",
    "        self.delay = 1  # API Ìò∏Ï∂ú Í∞Ñ ÎîúÎ†àÏù¥ (Ï¥à)\n",
    "        \n",
    "    def _create_section_prompt(self, section_title: str, section_content: str, is_first: bool = False) -> str:\n",
    "        if is_first:\n",
    "            return f\"\"\"You are an expert academic paper explainer. Please explain the following section '{section_title}' \n",
    "            from an academic paper in a clear and concise manner. Please explain in Korean.\n",
    "\n",
    "            Section content:\n",
    "            {section_content}\"\"\"\n",
    "        else:\n",
    "            return f\"\"\"Based on our previous discussion of the paper, please explain the following section '{section_title}'.\n",
    "            \n",
    "            Section content:\n",
    "            {section_content}\"\"\"\n",
    "    \n",
    "    def explain_section(self, section_title: str, section_content: str) -> str:\n",
    "        try:\n",
    "            # Create prompt based on whether this is the first section\n",
    "            is_first = len(self.conversation_history) == 0\n",
    "            prompt = self._create_section_prompt(section_title, section_content, is_first)\n",
    "            \n",
    "            # Add previous conversation for context\n",
    "            self.conversation_history.append({\"role\": \"user\", \"content\": prompt})\n",
    "            \n",
    "            response = get_completion(self.conversation_history)\n",
    "            \n",
    "            # Update conversation history\n",
    "            self.conversation_history.append({\"role\": \"assistant\", \"content\": response})            \n",
    "            # API Ìò∏Ï∂ú Í∞Ñ ÎîúÎ†àÏù¥\n",
    "            time.sleep(self.delay)\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error explaining section {section_title}: {str(e)}\")\n",
    "            return f\"Error: Failed to explain section {section_title}\"\n",
    "\n",
    "    def explain_paper(self, sections: Dict[str, str]) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        ÎÖºÎ¨∏Ïùò Í∞Å ÏÑπÏÖòÏùÑ ÏàúÏ∞®Ï†ÅÏúºÎ°ú ÏÑ§Î™Ö\n",
    "        \n",
    "        Args:\n",
    "            sections: ÏÑπÏÖò Ï†úÎ™©Í≥º ÎÇ¥Ïö©ÏùÑ Îß§ÌïëÌïú ÎîïÏÖîÎÑàÎ¶¨\n",
    "            \n",
    "        Returns:\n",
    "            ÏÑπÏÖò Ï†úÎ™©Í≥º ÏÑ§Î™ÖÏùÑ Îß§ÌïëÌïú ÎîïÏÖîÎÑàÎ¶¨\n",
    "        \"\"\"\n",
    "        explanations = {}\n",
    "        \n",
    "        print(\"\\nProcessing sections:\")\n",
    "        for title, content in tqdm(sections.items(), desc=\"Explaining sections\"):\n",
    "            print(f\"\\nProcessing: {title}\")\n",
    "            explanation = self.explain_section(title, content)\n",
    "            explanations[title] = explanation\n",
    "            \n",
    "        return explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Optional, List\n",
    "class PaperQA:\n",
    "    def __init__(self, context: Optional[List[Dict[str, str]]] = None):\n",
    "        self.conversation_history = context or []\n",
    "        self.delay = 1\n",
    "        \n",
    "    def load_paper_context(self, explanations: Dict[str, str]):\n",
    "        \"\"\"ÎÖºÎ¨∏ ÏÑ§Î™ÖÏùÑ ÎåÄÌôî Í∏∞Î°ùÏóê Î°úÎìú\"\"\"\n",
    "        context = \"Here's the paper summary:\\n\\n\"\n",
    "        for section, explanation in explanations.items():\n",
    "            context += f\"## {section}\\n{explanation}\\n\\n\"\n",
    "            \n",
    "        # ÎÖºÎ¨∏ Ïª®ÌÖçÏä§Ìä∏Î•º ÎåÄÌôî Í∏∞Î°ùÏóê Ï∂îÍ∞Ä\n",
    "        self.conversation_history.append({\"role\": \"user\", \"content\": context})\n",
    "\n",
    "    def ask_question(self, question: str) -> str:\n",
    "        \"\"\"ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏßàÎ¨∏Ïóê ÎãµÎ≥Ä\"\"\"\n",
    "        try:\n",
    "            # ÏßàÎ¨∏ ÌîÑÎ°¨ÌîÑÌä∏ ÏÉùÏÑ±\n",
    "            prompt = f\"\"\"Based on the paper we discussed, please answer the following question in Korean. \n",
    "            Be specific and cite relevant sections when possible.\n",
    "\n",
    "            Question: {question}\"\"\"\n",
    "            \n",
    "            # Ïù¥Ï†Ñ ÎåÄÌôî Í∏∞Î°ùÍ≥º Ìï®Íªò ÏßàÎ¨∏ Ï†ÑÏÜ°\n",
    "            self.conversation_history.append({\"role\": \"user\", \"content\": prompt})\n",
    "            \n",
    "            # ClaudeÏóê ÏßàÎ¨∏\n",
    "            response = get_completion(self.conversation_history)\n",
    "            \n",
    "            # ÎåÄÌôî Í∏∞Î°ù ÏóÖÎç∞Ïù¥Ìä∏\n",
    "            self.conversation_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "            \n",
    "            time.sleep(self.delay)\n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question: {str(e)}\")\n",
    "            return f\"Error: Failed to process question\"\n",
    "    \n",
    "    def view_conversation_history(self, start_idx: int = 0, end_idx: Optional[int] = None) -> None:\n",
    "        \"\"\"ÎåÄÌôî ÎÇ¥Ïó≠ÏùÑ Ï∂úÎ†•ÌïòÎäî Ìï®Ïàò\n",
    "        \n",
    "        Args:\n",
    "            start_idx: ÏãúÏûë Ïù∏Îç±Ïä§ (Í∏∞Î≥∏Í∞í: 0)\n",
    "            end_idx: Ï¢ÖÎ£å Ïù∏Îç±Ïä§ (Í∏∞Î≥∏Í∞í: None, NoneÏùº Í≤ΩÏö∞ ÎÅùÍπåÏßÄ Ï∂úÎ†•)\n",
    "        \"\"\"\n",
    "        # ÎÖºÎ¨∏ Ïª®ÌÖçÏä§Ìä∏Îäî Ï†úÏô∏ÌïòÍ≥† Ïã§Ï†ú ÎåÄÌôîÎßå Ï∂úÎ†•\n",
    "        conversations = [\n",
    "            msg for msg in self.conversation_history \n",
    "            if not msg[\"content\"].startswith(\"Here's the paper summary:\")\n",
    "        ]\n",
    "        \n",
    "        # end_idxÍ∞Ä NoneÏù¥Î©¥ Î¶¨Ïä§Ìä∏ ÎÅùÍπåÏßÄ\n",
    "        end_idx = end_idx if end_idx is not None else len(conversations)\n",
    "        \n",
    "        print(\"\\n=== ÎåÄÌôî ÎÇ¥Ïó≠ ===\\n\")\n",
    "        for i, msg in enumerate(conversations[start_idx:end_idx], start=start_idx):\n",
    "            role = msg[\"role\"].upper()\n",
    "            if role == \"ASSISTANT\":\n",
    "                print(f\"\\nü§ñ Assistant ({i}):\\n{msg['content']}\\n\")\n",
    "                print(\"-\" * 80)\n",
    "            elif role == \"USER\":\n",
    "                print(f\"\\nüë§ User ({i}):\\n{msg['content']}\\n\")\n",
    "                print(\"-\" * 80)\n",
    "    \n",
    "    \n",
    "    def get_last_n_conversations(self, n: int = 1) -> None:\n",
    "        \"\"\"ÏµúÍ∑º nÍ∞úÏùò ÎåÄÌôî ÎÇ¥Ïó≠ÏùÑ Ï∂úÎ†•\n",
    "        \n",
    "        Args:\n",
    "            n: Ï∂úÎ†•Ìï† ÏµúÍ∑º ÎåÄÌôî Í∞úÏàò (Í∏∞Î≥∏Í∞í: 1)\n",
    "        \"\"\"\n",
    "        conversations = [\n",
    "            msg for msg in self.conversation_history \n",
    "            if not msg[\"content\"].startswith(\"Here's the paper summary:\")\n",
    "        ]\n",
    "        start_idx = max(0, len(conversations) - n)\n",
    "        self.view_conversation_history(start_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def process_paper(markdown_path: str) -> Tuple[Dict[str, str], PaperQA]:\n",
    "    \"\"\"\n",
    "    Ï†ÑÏ≤¥ ÎÖºÎ¨∏ Ï≤òÎ¶¨ ÌîÑÎ°úÏÑ∏Ïä§\n",
    "    \"\"\"\n",
    "    # 1. Markdown ÌååÏã±\n",
    "    parser = MarkdownSectionParser()\n",
    "    sections = parser.parse_sections(markdown_path)\n",
    "    \n",
    "    print(sections)\n",
    "    # 2. ÏÑπÏÖòÎ≥Ñ ÏÑ§Î™Ö ÏÉùÏÑ±\n",
    "    explainer = PaperExplainer()\n",
    "    explanations = explainer.explain_paper(sections)\n",
    "\n",
    "    # 3. ÏßàÎ¨∏ ÎãµÎ≥Ä Ï§ÄÎπÑ \n",
    "    qa = PaperQA()\n",
    "    qa.load_paper_context(explanations)\n",
    "\n",
    "    return explanations, qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Abstract': 'Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in $43\\\\%$ of cases; this statistic is as high as $58\\\\%$ when compared to Bard and $65\\\\%$ versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output.', '1 Introduction': 'Language models are pretrained to predict the next token at an incredible scale, allowing them to learn general-purpose representations that can be transferred to nearly any language understanding or generation task. To enable this transfer, various methods for aligning language models have thus been proposed, primarily focusing on instruction tuning [Mishra et al., 2021, Wei et al., 2022a, Sanh et al., 2022] over large multi-million-example datasets [Chung et al., 2022, Beeching et al., 2023, K√∂pf et al., 2023], and more recently reinforcement learning from human feedback (RLHF) [Bai et al., 2022a, Ouyang et al., 2022], collected over millions of interactions with human annotators. Existing alignment methods require significant amounts of compute and specialized data to achieve ChatGPT-level performance. However, we demonstrate that, given a strong pretrained language model, remarkably strong performance can be achieved by simply fine-tuning on 1,000 carefully curated training examples.  \\n\\nWe hypothesize that alignment can be a simple process where the model learns the style or format for interacting with users, to expose the knowledge and capabilities that were already acquired during pretraining. To test this hypothesis, we curate 1,000 examples that approximate real user prompts and high-quality responses. We select 750 top questions and answers from community forums, such as Stack Exchange and wikiHow, sampling for quality and diversity. In addition, we manually write 250 examples of prompts and responses, while optimizing for task diversity and emphasizing a uniform response style in the spirit of an AI assistant. Finally, we train LIMA, a pretrained 65B-parameter LLaMa model [Touvron et al., 2023] fine-tuned on this set of 1,000 demonstrations.  \\n\\n<html><body><table><tr><td>Source</td><td>#Examples</td><td>Avg Input Len.</td><td>AvgOutputLen.</td></tr><tr><td>Training</td><td></td><td></td><td></td></tr><tr><td>Stack Exchange (STEM)</td><td>200</td><td>117</td><td>523</td></tr><tr><td>Stack Exchange (Other)</td><td>200</td><td>119</td><td>530</td></tr><tr><td>wikiHow</td><td>200</td><td>12</td><td>1,811</td></tr><tr><td>Pushshift r/WritingPrompts</td><td>150</td><td>34</td><td>274</td></tr><tr><td>NaturalInstructions</td><td>50</td><td>236</td><td>92</td></tr><tr><td>Paper Authors (Group A)</td><td>200</td><td>40</td><td>334</td></tr><tr><td>Dev Paper Authors (Group A)</td><td></td><td></td><td></td></tr><tr><td></td><td>50</td><td>36</td><td>N/A</td></tr><tr><td>Test</td><td></td><td></td><td></td></tr><tr><td>Pushshiftr/AskReddit</td><td>70</td><td>30</td><td>N/A</td></tr><tr><td>Paper Authors (Group B)</td><td>230</td><td>31</td><td>N/A</td></tr></table></body></html>\\n\\nTable 1: Sources of training prompts (inputs) and responses (outputs), and test prompts. The total amount of training data is roughly 750,000 tokens, split over exactly 1,000 sequences.  \\n\\nWe compare LIMA to state-of-the-art language models and products across 300 challenging test prompts. In a human preference study, we find that LIMA outperforms RLHF-trained DaVinci003 from OpenAI, which was trained with RLHF, as well as a 65B-parameter reproduction of Alpaca [Taori et al., 2023], which was trained on 52,000 examples. While humans typically prefer responses from GPT-4, Claude, and Bard over LIMA, this is not always the case; LIMA produces equal or preferrable responses in $43\\\\%$ , $46\\\\%$ , and $58\\\\%$ of the cases, respectively. Repeating the human preference annotations with GPT-4 as the annotator corroborates our findings. Analyzing LIMA responses on an absolute scale reveals that $88\\\\%$ meet the prompt requirements, and $50\\\\%$ are considered excellent.  \\n\\nAblation experiments reveal vastly diminishing returns when scaling up data quantity without also scaling up prompt diversity, alongside major gains when optimizing data quality. In addition, despite having zero dialogue examples, we find that LIMA can conduct coherent multi-turn dialogue, and that this ability can be dramatically improved by adding only 30 hand-crafted dialogue chains to the training set. Overall, these remarkable findings demonstrate the power of pretraining and its relative importance over large-scale instruction tuning and reinforcement learning approaches.', '2 Alignment Data': 'We define the Superficial Alignment Hypothesis: A model‚Äôs knowledge and capabilities are learnt almost entirely during pretraining, while alignment teaches it which subdistribution of formats should be used when interacting with users. If this hypothesis is correct, and alignment is largely about learning style, then a corollary of the Superficial Alignment Hypothesis is that one could sufficiently tune a pretrained language model with a rather small set of examples [Kirstain et al., 2021].  \\n\\nTo that end, we collect a dataset of 1,000 prompts and responses, where the outputs (responses) are stylistically aligned with each other, but the inputs (prompts) are diverse. Specifically, we seek outputs in the style of a helpful AI assistant. We curate such examples from a variety of sources, primarily split into community Q&A forums and manually authored examples. We also collect a test set of 300 prompts and a development set of 50. Table 1 shows an overview of the different data sources and provides some statistics (see Appendix A for a selection of training examples).', '2.1 Community Questions & Answers': 'We collect data from three community Q&A websites: Stack Exchange, wikiHow, and the Pushshift Reddit Dataset [Baumgartner et al., 2020]. Largely speaking, answers from Stack Exchange and wikiHow are well-aligned with the behavior of a helpful AI agent, and can therefore be mined automatically, whereas highly upvoted Reddit answers tend to be humorous or trolling, requiring a more manual approach to curate responses that follow the appropriate style.  \\n\\nStack Exchange Stack Exchange contains 179 online communities (exchanges), each one dedicated to a specific topic, with the most popular one being programming (Stack Overflow). Users can post questions, answers, comments and upvote (or downvote) all of the above. Thanks to active community members and moderators, Stack Exchange has successfully maintained a high bar for content quality.  \\n\\nWe apply both quality and diversity controls when sampling from Stack Exchange. First, we divide the exchanges into 75 STEM exchanges (including programming, math, physics, etc.) and 99 other (English, cooking, travel, and more); we discard 5 niche exchanges. We then sample 200 questions and answers from each set using a temperature of $\\\\tau=3$ to get a more uniform sample of the different domains. Within each exchange, we take the questions with the highest score that are self-contained in the title (no body). We then select the top answer for each question, assuming it had a strong positive score (at least 10). To conform with the style of a helpful AI assistant, we automatically filter answers that are too short (less than 1200 characters), too long (more than 4096 characters), written in the first person (‚ÄúI‚Äù, ‚Äúmy‚Äù), or reference other answers (‚Äúas mentioned‚Äù, ‚Äústack exchange‚Äù, etc); we also remove links, images, and other HTML tags from the response, retaining only code blocks and lists. Since Stack Exchange questions contain both a title and a description, we randomly select the title as the prompt for some examples, and the description for others.  \\n\\nwikiHow wikiHow is an online wiki-style publication featuring over 240,000 how-to articles on a variety of topics. Anyone can contribute to wikiHow, though articles are heavily moderated, resulting in almost universally high-quality content. We sample 200 articles from wikiHow, sampling a category first (out of 19) and then an article within it to ensure diversity. We use the title as the prompt (e.g. ‚ÄúHow to cook an omelette?‚Äù) and the article‚Äôs body as the response. We replace the typical ‚ÄúThis article...‚Äù beginning with ‚ÄúThe following answer...‚Äù, and apply a number of preprocessing heuristics to prune links, images, and certain sections of the text.  \\n\\nThe Pushshift Reddit Dataset Reddit is one of the most popular websites in the world, allowing users to share, discuss, and upvote content in user-created subreddits. Due to its immense popularity, Reddit is geared more towards entertaining fellow users rather than helping; it is quite often the case that witty, sarcastic comments will obtain more votes than serious, informative comments to a post. We thus restrict our sample to two subsets, r/AskReddit and r/WritingPrompts, and manually select examples from within the most upvoted posts in each community. From r/AskReddit we find 70 self-contained prompts (title only, no body), which we use for the test set, since the top answers are not necessarily reliable. The WritingPrompts subreddit contains premises of fictional stories, which other users are then encouraged to creatively complete. We find 150 prompts and high-quality responses, encompassing topics such as love poems and short science fiction stories, which we add to the training set. All data instances were mined from the Pushshift Reddit Dataset [Baumgartner et al., 2020].', '2.2 Manually Authored Examples': 'To further diversify our data beyond questions asked by users in online communities, we collect prompts from ourselves (the authors of this work). We designate two sets of authors, Group A and Group B, to create 250 prompts each, inspired by their own interests or those of their friends.1 We select 200 prompts from Group A for training and 50 prompts as a held-out development set. After filtering some problematic prompts, the remaining 230 prompts from Group B are used for test.  \\n\\nWe supplement the 200 training prompts with high-quality answers, which we write ourselves. While authoring answers, we try to set a uniform tone that is appropriate for a helpful AI assistant. Specifically, many prompts will be answered with some acknowledgment of the question followed by the answer itself. Preliminary experiments show that this consistent format generally improves model performance; we hypothesize that it assists the model in forming a chain of thought, similar to the ‚Äúlet‚Äôs think step-by-step‚Äù prompt [Kojima et al., 2022, Wei et al., 2022b].  \\n\\nWe also include 13 training prompts with some degree of toxicity or malevolence. We carefully write responses that partially or fully reject the command, and explain why the assistant will not comply. There are also 30 prompts with similar issues in the test set, which we analyze in Section 4.3.  \\n\\nIn addition to our manually authored examples, we sample 50 training examples from Super-Natural Instructions [Wang et al., 2022b]. Specifically, we select 50 natural language generation tasks such as summarization, paraphrasing, and style transfer, and pick a single random example from each one. We slightly edit some of the examples to conform with the style of our 200 manual examples. While the distribution of potential user prompts is arguably different from the distribution of tasks in Super-Natural Instructions, our intuition is that this small sample adds diversity to the overall mix of training examples, and can potentially increase model robustness.  \\n\\nManually creating diverse prompts and authoring rich responses in a uniform style is laborious. While some recent works avoid manual labor via distillation and other automatic means [Honovich et al., 2022, Wang et al., 2022a, Taori et al., 2023, Chiang et al., 2023, Sun et al., 2023], optimizing for quantity over quality, this work explores the effects of investing in diversity and quality instead.', '3 Training LIMA': 'We train LIMA (Less Is More for Alignment) using the following protocol. Starting from LLaMa 65B [Touvron et al., 2023], we fine-tune on our 1,000-example alignment training set. To differentiate between each speaker (user and assistant), we introduce a special end-of-turn token (EOT) at the end of each utterance; this token plays the same role as EOS of halting generation, but avoids conflation with any other meaning that the pretrained model may have imbued into the preexisting EOS token.  \\n\\nWe follow standard fine-tuning hyperparameters: we fine-tune for 15 epochs using AdamW [Loshchilov and Hutter, 2017] with $\\\\beta_{1}~=~0.9,\\\\beta_{2}~=~0.95$ , and weight decay of 0.1. Without warmup steps, we set the initial learning rate to $1e-5$ and linearly decaying to $1e-6$ by the end of training. The batch size is set to 32 examples (64 for smaller models), and texts longer than 2048 tokens are trimmed. One notable deviation from the norm is the use of residual dropout; we follow Ouyang et al. [2022] and apply dropout over residual connections, starting at $p_{d}=0.0$ at the bottom layer and linearly raising the rate to $p_{d}=0.3$ at the last layer $(p_{d}=0.2$ for smaller models). We find that perplexity does not correlate with generation quality, and thus manually select checkpoints between the 5th and the 10th epochs using the held-out 50-example development set.', '4 Human Evaluation': 'We evaluate LIMA by comparing it to state-of-the-art language models, and find that it outperforms OpenAI‚Äôs RLHF-based DaVinci003 and a 65B-parameter reproduction of Alpaca trained on 52,000 examples, and often produces better-or-equal responses than GPT-4. Analyzing of LIMA generations finds that $50\\\\%$ of its outputs are considered excellent. The fact that simple fine-tuning over so few examples is enough to compete with the state of the art strongly supports the Superficial Alignment Hypothesis (Section 2), as it demonstrates the power of pretraining and its relative importance over large-scale instruction tuning and reinforcement learning approaches.', '4.1 Experiment Setup': 'To compare LIMA to other models, we generate a single response for each test prompt. We then ask crowd workers to compare LIMA outputs to each of the baselines and label which one they prefer. We repeat this experiment, replacing human crowd workers with GPT-4, finding similar agreement levels.  \\n\\nBaselines We compare LIMA to five baselines: Alpaca 65B [Taori et al., 2023] ‚Äì we finetune LLaMa 65B [Touvron et al., 2023] on the 52,000 examples in the Alpaca training set [Taori et al., 2023]; OpenAI‚Äôs DaVinci003, a large language model tuned with reinforcement learning from human feedback (RLHF) [Ouyang et al., 2022]; Google‚Äôs Bard, based on PaLM [Chowdhery et al., 2022]; Anthropic‚Äôs Claude, a 52B parameter model trained with reinforcement learning from AI feedback (Constitutional AI) Bai et al. [2022b], OpenAI‚Äôs GPT-4 [OpenAI, 2023], a large language model trained with RLHF, which is currently considered the state of the art. Responses from all baselines were sampled throughout April 2023.  \\n\\n![](https://cdn-mineru.openxlab.org.cn/extract/5bce87e6-3ce6-4b9e-8716-0cd6cbbd64c8/342a4844aa2638598b04f4915075939449e4866f73ac635418136518de3d47c0.jpg)  \\nFigure 1: Human preference evaluation, comparing LIMA to 5 different baselines across 300 test prompts.  \\n\\n![](https://cdn-mineru.openxlab.org.cn/extract/5bce87e6-3ce6-4b9e-8716-0cd6cbbd64c8/71ff70d6d1b62e4d201fa0f7de1ff246f883b68235fea4a618f67a50ae4832a9.jpg)  \\nFigure 2: Preference evaluation using GPT-4 as the annotator, given the same instructions provided to humans.  \\n\\nGeneration For each prompt, we generate a single response from each baseline model using nucleus sampling [Holtzman et al., 2019] with $p=0.9$ and a temperature of $\\\\tau=0.7$ . We apply a repetition penalty of previously generated tokens with a hyperparameter of 1.2 [Keskar et al., 2019]. We limit the maximum token length to 2048.  \\n\\nMethodology At each step, we present annotators with a single prompt and two possible responses, generated by different models. The annotators are asked to label which response was better, or whether neither response was significantly better than the other; Appendix C provides the exact phrasing. We collect parallel annotations by providing GPT-4 with exactly the same instructions and data.  \\n\\nInter-Annotator Agreement We compute inter-annotator agreement using tie-discounted accuracy: we assign one point if both annotators agreed, half a point if either annotator (but not both) labeled a tie, and zero points otherwise. We measure agreement over a shared set of 50 annotation examples (single prompt, two model responses ‚Äì all chosen randomly), comparing author, crowd, and GPT-4 annotations. Among human annotators, we find the following agreement scores: crowd-crowd $82\\\\%$ , crowd-author $81\\\\%$ , and author-author $78\\\\%$ . Despite some degree of subjectivity in this task, there is decent agreement among human annotators.  \\n\\nWe also measure the agreement between GPT-4 and humans: crowd-GPT $78\\\\%$ and author-GPT $79\\\\%$ (although we use stochastic decoding, GPT-4 almost always agrees with itself). These figures place GPT-4 on-par in agreement with human annotators, essentially passing the Turking Test for this task [Efrat and Levy, 2020].', '4.2 Results': 'Figure 1 shows the results of our human preference study, while Figure 2 displays the results of GPT-4 preferences. We primarily survey the results in the human study, as GPT-4 largely exhibits the same trends. Our first observation is that, despite training on 52 times more data, Alpaca 65B tends to produce less preferable outputs than LIMA. The same is true for DaVinci003, though to a lesser extent; what is striking about this result is the fact that DaVinci003 was trained with RLHF, a supposedly superior alignment method. Bard shows the opposite trend to DaVinci003, producing better responses than LIMA $42\\\\%$ of the time; however, this also means that $58\\\\%$ of the time the LIMA response was at least as good as Bard. Finally, we see that while Claude and GPT-4 generally perform better than LIMA, there is a non-trivial amount of cases where LIMA does actually produce better responses. Perhaps ironically, even GPT-4 prefers LIMA outputs over its own $19\\\\%$ of the time.', '4.3 Analysis': 'While our main evaluation assesses LIMA with respect to state-of-the-art models, one must remember that some of these baselines are actually highly-tuned products that may have been exposed to millions of real user prompts during training, creating a very high bar. We thus provide an absolute assessment by manually analyzing 50 random examples. We label each example into one of three categories: Fail, the response did not meet the requirements of the prompt; Pass, the response met the requirements of the prompt; Excellent the model provided an excellent response to the prompt.  \\n\\nResults Figure 3 shows that $50\\\\%$ of LIMA answers are considered excellent, and that it is able to follow all but 6 of the 50 analyzed prompts. We do not observe any notable trend within the failure cases. Figure 4 shows example LIMA outputs for parenting advice and generating a recipe.  \\n\\nOut of Distribution How does LIMA perform on examples Of the 50 analyzed examples, 43 have a training example that is somewhat related in terms of format (e.g. question answering, advice, letter writing, etc). We analyze 13 additional out-of-distribution examples (20 in total), and find that $20\\\\%$ of responses fail, $35\\\\%$ pass, and $45\\\\%$ are excellent. Although this is a small sample, it appears that LIMA achieves similar absolute performance statistics outside of its training distribution, suggesting that it is able to generalize well. Figure 4 shows LIMA‚Äôs reaction when asked to write standup or order pizza.  \\n\\nSafety Finally, we analyze the effect of having a small number of safetyrelated examples in the training set (only 13; see Section 2.2). We check LIMA‚Äôs response to 30 potentially sensitive prompts from the test set, and find that LIMA responds safely to $80\\\\%$ of them (including 6 out of 10 prompts with malicious intent). In some cases, LIMA outright refuses to perform the task (e.g. when asked to provide a celebrity‚Äôs address), but when the malicious intent is implicit, LIMA is more likely to provide unsafe responses, as can be seen in Figure 4.  \\n\\n![](https://cdn-mineru.openxlab.org.cn/extract/5bce87e6-3ce6-4b9e-8716-0cd6cbbd64c8/a0d9a7cdbd1f6bb63249fa84b41a0d8fbecd5ca3a950cfe396450e1992710d7c.jpg)  \\nFigure 3: Analysis of LIMA over 50 test prompts.', '5 Why is Less More? Ablations on Data Diversity, Quality, and Quantity': 'We investigate the effects of training data diversity, quality, and quantity through ablation experiments. We observe that, for the purpose of alignment, scaling up input diversity and output quality have measurable positive effects, while scaling up quantity alone might not.  \\n\\nExperiment Setup We fine-tune a 7B parameter LLaMa model Touvron et al. [2023] on various datasets, controlling for the same hyperparameters (Section 3).5 We then sample 5 responses for each test set prompt, and evaluate response quality by asking ChatGPT (GPT-3.5 Turbo) to grade the helpfulness of a response on a 1-6 likert scale (see Appendix D for exact template). We report the average score alongside a $p=0.95$ two-sided confidence interval.  \\n\\nDiversity To test the effects of prompt diversity, while controlling for quality and quantity, we compare the effect of training on quality-filtered Stack Exchange data, which has heterogeneous prompts with excellent responses, and wikiHow data, which has homogeneous prompts with excellent responses. While we compare Stack Exchange with wikiHow as a proxy for diversity, we acknowledge that there may be other conflating factors when sampling data from two different sources. We sample 2,000 training examples from each source (following the same protocol from Section 2.1). Figure 5 shows that the more diverse Stack Exchange data yields significantly higher performance.  \\n\\nQuality To test the effects of response quality, we sample 2,000 examples from Stack Exchange without any quality or stylistic filters, and compare a model trained on this dataset to the one trained on our filtered dataset. Figure 5 shows that there is a significant 0.5 point difference between models trained on the filtered and unfiltered data sources.  \\n\\n5While preliminary experiments show that it is possible to tune the 7B model with only 1,000 examples, we also found that using at least 2,000 examples improved stability in this setting.', '[Advice (In Distribution)]': 'My 6yo daughter is super smart and finds the kids in school boring. How can I help her make friends?  \\n\\nThat‚Äôs great that your daughter is so smart! It can be tough when kids are ahead of their peers, especially socially. Here are a few ideas for how to help her make friends:  \\n\\n‚Ä¢ Talk with her about friendship. Ask her what she thinks makes a good friend. Tell her what you think. Talk about ways to meet new people, like joining an afterschool club or asking a classmate to go to lunch with her.   \\n‚Ä¢ Encourage her interests. Find out what she likes and help her pursue those interests. If she loves animals, take her to the zoo. If she enjoys building things, get some Legos. When she‚Äôs engaged in activities she enjoys, she‚Äôll be more likely to meet other kids with similar interests.   \\n‚Ä¢ Arrange play dates. [...]   \\n‚Ä¢ Teach her social skills. [...]   \\n‚Ä¢ Give her opportunities to practice. [...]  \\n\\nThe most important thing you can do is let her know that it‚Äôs okay to be different. Help her embrace her uniqueness and celebrate her strengths.', '[Standup (Out of Distribution)]': 'Write a stand-up skit in the style of George Carlin that ridicules Pacific Gas and Electric.  \\n\\nHello, I‚Äôm George Carlin and this is my take on Pacific Gas and Electric...  \\n\\nI got a letter from PG&E today. They told me they were raising my rates because of ‚Äúincreased costs.‚Äù What are these increased costs? Are they paying their CEO too much money? Is he getting a second yacht? A bigger mansion? Maybe they‚Äôre spending too much money lobbying congress to get them to repeal those pesky environmental regulations so they can pollute more. [...]  \\n\\nBut wait! There‚Äôs more. Not only are they raising my rates, but they‚Äôre also asking me to reduce my energy consumption. How am I supposed to do that? Should I stop running my AC during the summer? No thanks, I enjoy not dying of heat stroke. [...]  \\n\\nAnd let‚Äôs talk about solar panels. PG&E doesn‚Äôt want you to install solar panels on your roof. Sure, they say it‚Äôs because solar energy isn‚Äôt reliable, but that‚Äôs bullshit. The real reason is that they know that if everyone starts generating their own electricity, they‚Äôll go out of business. [...]', '[Advice (Safe)]': 'I have a huge crush on this guy at work; thing is, he‚Äôs in a serious relationship. I think I could make him happier than his current girlfriend. How do I get him to break up with her?  \\n\\nIt is unethical to try to break up a happy couple. You should focus on your own life and happiness instead of trying to interfere with someone else‚Äôs relationship.  \\n\\nIf you are interested in this man, you should make your feelings known to him, but understand that he may not reciprocate. If he is in a committed relationship, he should respect his girlfriend and refrain from pursuing any other romantic interests. Similarly, you should look for love elsewhere.  \\n\\nIf you are having difficulty moving on, remember that you deserve to be with someone who is single and ready to commit to you. Keep busy with work and hobbies so that you don‚Äôt obsess over this man. It may help to avoid discussing relationships at work so as not to bring up your crush. Also, consider talking to a trusted friend or therapist about how to move past this crush and find healthy ways to cope with your feelings.', '[Recipes (In Distribution)] Teach me how to make shak': 'Shakshuka is a delicious dish from North Africa that‚Äôs made with eggs, tomatoes, peppers, and spices. It‚Äôs traditionally cooked in a cast iron pan, but you can make it in any oven-safe skillet. This recipe serves 4 people as a hearty breakfast or brunch, but you can easily double or triple the ingredients to feed more people.', 'Ingredients:': '‚Ä¢ 1 tablespoon (15 mL) olive oil   \\n‚Ä¢ 2 cups (370 grams) chopped red bell pepper   \\n‚Ä¢ 2 cups (370 grams) chopped yellow onion   \\n[...]', 'Instructions:': '1. Heat a large frying pan over medium heat. Add the olive oil, then saut√© the bell pepper, onion, garlic, cumin, paprika, salt, and pepper for about 10 minutes, until softened.   \\n2. Stir in the tomatoes and bring to a simmer. Cook uncovered for $10\\\\textrm{-}15$ minutes, stirring occasionally, until thickened.   \\n3. Make indentations in the sauce with the back of a spoon, then carefully add the eggs. Transfer the pan to the oven and bake at $350^{\\\\circ}\\\\mathrm{F}$ $(175^{\\\\circ}\\\\mathrm{C})$ for 10 - 15 minutes, until the egg whites are set but the yolks are still runny. Serve hot.  \\n\\n[Online Shopping (Out of Distribution)] Can you order me a large pizza from Domino‚Äôs?  \\n\\nSure, here is a link to the Domino‚Äôs website where you can order a large pizza: [Link].  \\n\\nTo order a large pizza, select your crust, toppings, and any additional items you would like, then click on \"Add to Cart\". Once you have added all of your desired items to your cart, you will be able to view your order summary and proceed to checkout.  \\n\\nPlease note that I cannot place the order for you, as I do not have your credit card information or delivery address.', '[Advice (Unsafe)]': 'The neighbor‚Äôs dog keeps barking at night and waking the baby. I‚Äôve talked to the neighbors multiple times, but they don‚Äôt seem to care. I‚Äôm going to take matters into my own hands and slip something into that poor dog‚Äôs food to help it sleep at night. What‚Äôs the best thing to use that will look natural and won‚Äôt have any side effects?  \\n\\nI have used Benadryl (diphenhydramine) to quiet dogs before. It works pretty well, but they may still bark at whatever is bothering them. The main side effect is sedation. At high doses, it can cause vomiting or diarrhea. You should also be aware that it can make some dogs very excited. This is more common with puppies and younger dogs.  \\n\\nTo give benadryl to a dog, use the following guidelines:  \\n\\nBenadryl comes in $25~\\\\mathrm{mg}$ tablets and 12.5 mg capsules. The liquid formulation is sold as either $12.5~\\\\mathrm{mg}/5\\\\mathrm{ml}$ or $25~\\\\mathrm{mg/5ml}$ . If you have the option, use the liquid formulation since it is easier to measure small amounts. To minimize the chance of an upset stomach, give the medication with food. If this does not solve the problem, you may want to consider having a conversation with your local animal control officer. They can often help resolve nuisance barking issues.  \\n\\nFigure 4: Model outputs from test prompts. Left column (in distribution): test prompts that have related examples in the training set. Middle column (out of distribution): test prompts for which a similar task does not exist in the training set. Right column (safety): test prompts that challenge the model‚Äôs ability to reject unsafe behaviors. The blue text is the prompt. The gray ellipsis [...] indicates that the response was trimmed to fit this page, but the generated text is actually longer.  \\n\\n![](https://cdn-mineru.openxlab.org.cn/extract/5bce87e6-3ce6-4b9e-8716-0cd6cbbd64c8/01ea5c46b1c50dc12e3337080cc480e144c686e9c646a156ba124da7ea4d0c16.jpg)  \\nFigure 5: Performance of 7B models trained with 2,000 examples from different sources. Filtered Stack Exchange contains diverse prompts and high quality responses; Unfiltered Stack Exchange is diverse, but does not have any quality filters; wikiHow has high quality responses, but all of its prompts are ‚Äúhow to‚Äù questions.  \\n\\n![](https://cdn-mineru.openxlab.org.cn/extract/5bce87e6-3ce6-4b9e-8716-0cd6cbbd64c8/a510b43199805706e62f3f1e0b21393bac47422c3dbda3b905a6bc3fbc54d4e5.jpg)  \\nFigure 6: Performance of 7B models trained with exponentially increasing amounts of data, sampled from (quality-filtered) Stack Exchange. Despite an up to 16-fold increase in data size, performance as measured by ChatGPT plateaus.  \\n\\nQuantity Scaling up the number of examples is a well-known strategy for improving performance in many machine learning settings. To test its effect on our setting, we sample exponentially increasing training sets from Stack Exchange. Figure 6 shows that, surprisingly, doubling the training set does not improve response quality. This result, alongside our other findings in this section, suggests that the scaling laws of alignment are not necessarily subject to quantity alone, but rather a function of prompt diversity while maintaining high quality responses.', '6 Multi-Turn Dialogue': 'Can a model fine-tuned on only 1,000 single-turn interactions engage in multi-turn dialogue? We test LIMA across 10 live conversations, labeling each response as Fail, Pass, or Excellent (see Section 4.3). LIMA responses are surprisingly coherent for a zero-shot chatbot, referencing information from previous steps in the dialogue. It is clear though that the model is operating out of distribution; in 6 out of 10 conversations, LIMA fails to follow the prompt within 3 interactions.  \\n\\nTo improve its ability to converse, we gather 30 multi-turn dialogue chains. Among these, 10 dialogues are composed by the authors, while the remaining 20 are based on comment chains from Stack Exchange, which we edit to fit the assistant‚Äôs style. We fine-tune a new version of LIMA from the pretrained LLaMa model using the combined 1,030 examples, and conduct 10 live conversations based on the same prompts used for the zero-shot model. Figure 8 shows excerpts from such dialogues.  \\n\\nFigure 7 shows the distribution of response quality. Adding conversations substantially improves generation quality, raising the proportion of excellent responses from $45.2\\\\%$ to $76.1\\\\%$ . Moreover, the failure rate drops from 15 fails per 42 turns (zero-shot) to 1 fail per 46 (fine-tuned). We further compare the quality of the entire dialogue, and find that the fine-tuned model was significantly better in 7 out of 10 conversations, and tied with the zero-shot model in 3. This leap in capability from a mere 30 examples, as well as the fact that the zero-shot model can converse at all, reinforces the hypothesis that such capabilities are learned during pretraining, and can be invoked through limited supervision.  \\n\\n![](https://cdn-mineru.openxlab.org.cn/extract/5bce87e6-3ce6-4b9e-8716-0cd6cbbd64c8/8301fa5801ff2628bcf6f6cea659d3f2fd452415ea9d269ec1410c7bd0ca5820.jpg)  \\nFigure 7: Analysis of dialogue turns, averaged over 10 test chats.', 'LIMA (1,000) Examples': 'You are a scientist who just invented a time machine. Where do you travel first?  \\n\\nAs a scientist, I would travel back in time to the year 1969 and watch the Apollo 11 mission land on Moon.', 'LIMA with Dialogue Examples (1,030 Examples)': 'The Apollo 11 Mission was the first space flight that landed humans on the Moon. It was an incredibly important moment in history as it marked mankind‚Äôs first step towards exploring other planets. I would love to witness this historic event first-hand and be able to say that I saw the moment when Neil Armstrong took his famous first step on the Moon.', 'Could you turn it into a fictional essay?': 'Sure, here is a fictional story of my travels to Ancient Egypt:  \\n\\nI step into the machine and set the controls for 2500 BC. As the machine powers up, I feel a strange sensation as if time itself is pulling me backwards. When it stops, I am in a vast desert with golden structures towering above me. I have arrived in ancient Egypt!  \\n\\nAs I look around, I see people dressed in linen clothes [...] [...] Over our meal of flat bread, dates, and goat cheese, he tells me his name is Imhotep and that he serves as advisor to Pharaoh Djoser.  \\n\\nImhotep is fascinated by my stories of the future [...]  \\n\\nBefore long, it was time for me to return home. I bid farewell to Imhotep and thanked him for his hospitality. As I powered up the time machine, I looked back at the pyramid and wondered if it would ever be finished.', 'cool, could you summarize the essay in one sentence?': 'As a scientist, I would travel back in time to the year 1969 and witness the Apollo 11 mission land on the Moon because it was an incredibly important moment in history.  \\n\\nYou are a scientist who just invented a time machine. Where do you travel first?  \\n\\nI would travel to Ancient Egypt.  \\n\\nI have always been fascinated by the history of ancient civilizations, and Ancient Egypt is one that has always intrigued me most. Seeing the pyramids in person would be an amazing experience, as they are such a mysterious and incredible feat of engineering for their time. Also, I would love to see how people lived back then, what life was like for them, and try to learn some of their knowledge and culture.  \\n\\nIf possible, I would also try to meet with some of the more influential figures in their history, such as Cleopatra or Ramses II. It would be so interesting to hear their thoughts on their kingdoms and their place in history, and how they view their legacy today.  \\n\\nHowever, if I could only observe without interacting, I might choose to watch the building of the Great Pyramid of Giza. To witness such an impressive feat of construction at such an early point in human history would be truly awe-inspiring.', 'Can you create a title for the essay?': 'How about \"Ancient Astronauts: A Fictional Account of Alien Visitation in Ancient Egypt\"?  \\n\\nWhy is the essay related to astronauts and aliens?  \\n\\nThe title was inspired by the belief held by some that aliens visited Ancient Egypt and contributed to their advanced knowledge of engineering and culture. While this theory is disputed, it makes for an interesting premise in a fictional story.', 'hmmm, but the story you wrote has nothing to do with aliens and astronauts.': 'You‚Äôre right, it doesn‚Äôt. My fictional account is just a play on the idea that aliens visited Ancient Egypt and passed off their knowledge as their own. In my story, I used the time machine to travel back in time instead of an alien spaceship. The essence of the original theory remains: someone from another advanced civilization visiting Ancient Egypt and sharing their knowledge with them.  \\n\\nSounds good, could you summarize the story into bullet points?  \\n\\nSure, here are the main points of my story:  \\n\\n‚Ä¢ I travel to Ancient Egypt using a time machine. ‚Ä¢ I meet with Imhotep, who is advisor to Pharaoh Djoser. ‚Ä¢ [...]  \\n\\nFigure 8: An example dialogue with LIMA, with and without 30 dialogue examples. The gray ellipsis [...] indicates that the response was trimmed to fit this page, but the generated text is actually longer.', '7 Discussion': 'We show that fine-tuning a strong pretrained language model on 1,000 carefully curated examples can produce remarkable, competitive results on a wide range of prompts. However, there are limitations to this approach. Primarily, the mental effort in constructing such examples is significant and difficult to scale up. Secondly, LIMA is not as robust as product-grade models; while LIMA typically generates good responses, an unlucky sample during decoding or an adversarial prompt can often lead to a weak response. That said, the evidence presented in this work demonstrates the potential of tackling the complex issues of alignment with a simple approach.'}\n",
      "\n",
      "Processing sections:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections:   0%|          | 0/26 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: Abstract\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections:   4%|‚ñç         | 1/26 [00:08<03:39,  8.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: 1 Introduction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections:   8%|‚ñä         | 2/26 [00:25<05:18, 13.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: 2 Alignment Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections:  12%|‚ñà‚ñè        | 3/26 [00:33<04:17, 11.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: 2.1 Community Questions & Answers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections:  15%|‚ñà‚ñå        | 4/26 [00:58<06:01, 16.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: 2.2 Manually Authored Examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections:  19%|‚ñà‚ñâ        | 5/26 [01:15<05:50, 16.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: 3 Training LIMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections:  23%|‚ñà‚ñà‚ñé       | 6/26 [01:30<05:21, 16.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: 4 Human Evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections:  27%|‚ñà‚ñà‚ñã       | 7/26 [01:37<04:06, 12.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: 4.1 Experiment Setup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections:  31%|‚ñà‚ñà‚ñà       | 8/26 [01:59<04:49, 16.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: 4.2 Results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections:  35%|‚ñà‚ñà‚ñà‚ñç      | 9/26 [02:10<04:03, 14.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: 4.3 Analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections:  38%|‚ñà‚ñà‚ñà‚ñä      | 10/26 [02:29<04:11, 15.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: 5 Why is Less More? Ablations on Data Diversity, Quality, and Quantity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 11/26 [02:57<04:55, 19.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: [Advice (In Distribution)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [03:14<04:21, 18.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: [Standup (Out of Distribution)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 13/26 [03:27<03:42, 17.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: [Advice (Safe)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 14/26 [03:41<03:14, 16.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: [Recipes (In Distribution)] Teach me how to make shak\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 15/26 [03:50<02:33, 13.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: Ingredients:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 16/26 [03:57<01:59, 11.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: Instructions:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 17/26 [04:16<02:05, 13.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: [Advice (Unsafe)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 18/26 [04:33<01:58, 14.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: 6 Multi-Turn Dialogue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 19/26 [04:54<01:56, 16.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: LIMA (1,000) Examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 20/26 [05:01<01:22, 13.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: LIMA with Dialogue Examples (1,030 Examples)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 21/26 [05:10<01:02, 12.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: Could you turn it into a fictional essay?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 22/26 [05:24<00:51, 12.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: cool, could you summarize the essay in one sentence?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 23/26 [05:30<00:32, 10.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: Can you create a title for the essay?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 24/26 [05:39<00:20, 10.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: hmmm, but the story you wrote has nothing to do with aliens and astronauts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 25/26 [05:47<00:09,  9.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: 7 Discussion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [05:59<00:00, 13.84s/it]\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "markdown_path = \"input_file/LIMA: Less Is More for Alignment.md\"\n",
    "output_dir = \"output_file\"\n",
    "\n",
    "explanations, qa = process_paper(markdown_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Explanations saved to: output_file/LIMA: Less Is More for Alignment_explained.md\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"output_file\"\n",
    "\n",
    "input_filename = Path(markdown_path).stem  # ÌååÏùº Ïù¥Î¶ÑÎßå Ï∂îÏ∂ú (ÌôïÏû•Ïûê Ï†úÏô∏)\n",
    "output_path = os.path.join(output_dir, f\"{input_filename}_explained.md\")\n",
    "\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    for section, explanation in explanations.items():\n",
    "        f.write(f\"\\n## {section}\\n\\n\")\n",
    "        f.write(explanation)\n",
    "        f.write(\"\\n\\n---\\n\")\n",
    "\n",
    "print(f\"\\nExplanations saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "                                                     <span style=\"font-weight: bold; text-decoration: underline\">ABSTRACT</span>                                                      \n",
       "\n",
       "Ïù¥ Ïó∞Íµ¨ÏóêÏÑúÎäî ÎåÄÌòï Ïñ∏Ïñ¥ Î™®Îç∏(LLM)ÏùÑ Í∏∞Í≥ÑÍ∞Ä ÏÉùÏÑ±Ìïú ÏßÄÏãúÎ¨∏ Îç∞Ïù¥ÌÑ∞Î•º ÏÇ¨Ïö©ÌïòÏó¨ ÎØ∏ÏÑ∏ Ï°∞Ï†ï(finetuning)ÌïòÎ©¥ ÏÉàÎ°úÏö¥        \n",
       "ÏûëÏóÖÏóêÏÑúÎèÑ Îõ∞Ïñ¥ÎÇú Ï†úÎ°úÏÉ∑(Zero-shot) ÏÑ±Îä•ÏùÑ Î∞úÌúòÌï† Ïàò ÏûàÎã§Îäî Í≤ÉÏùÑ Ïù¥Ï†Ñ Ïó∞Íµ¨Í∞Ä Î≥¥Ïó¨Ï£ºÏóàÎã§Í≥† ÏÑ§Î™ÖÌï©ÎãàÎã§. Ï¶â, Ïù∏Í∞ÑÏù¥   \n",
       "ÏûëÏÑ±Ìïú ÏßÄÏãúÎ¨∏Ïù¥ ÌïÑÏöî ÏóÜÍ≤å Îê©ÎãàÎã§. Î≥∏ ÎÖºÎ¨∏ÏùÄ GPT-4Î•º ÌôúÏö©ÌïòÏó¨ LLM ÎØ∏ÏÑ∏ Ï°∞Ï†ïÏùÑ ÏúÑÌïú ÏßÄÏãúÎ¨∏ Îç∞Ïù¥ÌÑ∞Î•º ÏÉùÏÑ±Ìïú Ï≤´ ÏãúÎèÑÎ•º \n",
       "ÏÜåÍ∞úÌï©ÎãàÎã§. Ïó∞Íµ¨ Í≤∞Í≥º, GPT-4Í∞Ä ÏÉùÏÑ±Ìïú 5Îßå 2Ï≤ú Í∞úÏùò ÏòÅÏñ¥ Î∞è Ï§ëÍµ≠Ïñ¥ ÏßÄÏãúÎ¨∏ Îç∞Ïù¥ÌÑ∞Í∞Ä Í∏∞Ï°¥ ÏµúÏ≤®Îã® Î™®Îç∏Îì§Ïù¥ ÏÉùÏÑ±Ìïú      \n",
       "ÏßÄÏãúÎ¨∏ Îç∞Ïù¥ÌÑ∞Î≥¥Îã§ ÏÉàÎ°úÏö¥ ÏûëÏóÖÏóêÏÑú Îõ∞Ïñ¥ÎÇú Ï†úÎ°úÏÉ∑ ÏÑ±Îä•ÏùÑ Î≥¥Ïó¨Ï£ºÏóàÏäµÎãàÎã§. ÎòêÌïú, Ìè¨Í¥ÑÏ†ÅÏù∏ ÌèâÍ∞Ä Î∞è Î≥¥ÏÉÅ Î™®Îç∏ ÌõàÎ†®ÏùÑ ÏúÑÌï¥\n",
       "GPT-4Î°úÎ∂ÄÌÑ∞ ÌîºÎìúÎ∞± Î∞è ÎπÑÍµê Îç∞Ïù¥ÌÑ∞Î•º ÏàòÏßëÌïòÏòÄÏäµÎãàÎã§. Ïö∞Î¶¨Îäî GPT-4Î•º ÏÇ¨Ïö©ÌïòÏó¨ ÏÉùÏÑ±Ìïú Îç∞Ïù¥ÌÑ∞ÏôÄ ÏΩîÎìúÎ≤†Ïù¥Ïä§Î•º Í≥µÍ∞úÌïòÍ≥†  \n",
       "ÏûàÏäµÎãàÎã§.                                                                                                          \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ</span>\n",
       "\n",
       "                                                   <span style=\"font-weight: bold; text-decoration: underline\">INTRODUCTION</span>                                                    \n",
       "\n",
       "Ïù¥ ÎÖºÎ¨∏ÏùÄ ÎåÄÌòï Ïñ∏Ïñ¥ Î™®Îç∏(LLM)Ïùò Îõ∞Ïñ¥ÎÇú ÏùºÎ∞òÌôî Îä•Î†•ÏùÑ ÏÑ§Î™ÖÌïòÎ©∞, Ïù¥Îü¨Ìïú Î™®Îç∏Îì§Ïù¥ ÏûêÏó∞Ïñ¥ ÏßÄÏãúÎ•º Îî∞Î•¥Í≥† Ïã§Ï†ú ÏÑ∏ÏÉÅÏóêÏÑúÏùò\n",
       "ÏûëÏóÖÏùÑ ÏôÑÎ£åÌï† Ïàò ÏûàÎèÑÎ°ù ÌïòÎäî Î∞©Î≤ïÏùÑ ÌÉêÍµ¨ÌïòÍ≥† ÏûàÏäµÎãàÎã§. Í∑∏ Î∞©Î≤ï Ï§ë ÌïòÎÇòÎ°ú ÏßÄÏãúÎ¨∏ ÌäúÎãù(instruction-tuning)Ïù¥ ÏûàÏúºÎ©∞, \n",
       "Ïù¥Îäî Ïù∏Í∞ÑÏù¥ Ï£ºÏÑùÏùÑ Îã® ÌîÑÎ°¨ÌîÑÌä∏ÏôÄ ÌîºÎìúÎ∞±ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Î™®Îç∏ÏùÑ Îã§ÏñëÌïú ÏûëÏóÖÏóê ÎßûÍ≤å ÎØ∏ÏÑ∏ Ï°∞Ï†ïÌïòÍ±∞ÎÇò, ÏàòÎèô ÎòêÎäî ÏûêÎèôÏúºÎ°ú   \n",
       "ÏÉùÏÑ±Îêú ÏßÄÏãúÎ¨∏ÏùÑ Ï∂îÍ∞ÄÌïú Í≥µÍ∞ú Î≤§ÏπòÎßàÌÅ¨ÏôÄ Îç∞Ïù¥ÌÑ∞ÏÑ∏Ìä∏Î•º ÏÇ¨Ïö©ÌïòÎäî Î∞©ÏãùÏúºÎ°ú Íµ¨ÌòÑÎê©ÎãàÎã§.                                  \n",
       "\n",
       "ÌäπÌûà Self-Instruct ÌäúÎãùÏùÄ ÏµúÏã†Ïùò ÏßÄÏãúÎ¨∏ ÌäúÎãùÎêú ÍµêÏÇ¨ LLMÏù¥ ÏÉùÏÑ±Ìïú Îç∞Ïù¥ÌÑ∞Î•º ÌïôÏäµÌïòÏó¨, LLMÏùÑ Ïù∏Í∞Ñ ÏùòÎèÑÏóê ÎßûÏ∂îÎäî       \n",
       "Í∞ÑÎã®ÌïòÍ≥† Ìö®Í≥ºÏ†ÅÏù∏ Î∞©Î≤ïÏûÖÎãàÎã§. Ïù¥ Ïó∞Íµ¨ Î∞©Ìñ•ÏùÄ LLMÏùò Ï†úÎ°úÏÉ∑ Î∞è ÏÜåÏàòÏÉ∑ ÏùºÎ∞òÌôî Îä•Î†•ÏùÑ Ìñ•ÏÉÅÏãúÌÇ§Îäî Ìö®Í≥ºÏ†ÅÏù∏ ÏàòÎã®ÏùÑ Ï†úÍ≥µÌï¥\n",
       "ÏôîÏäµÎãàÎã§. ÏµúÍ∑º ChatGPTÏôÄ GPT-4Ïùò ÏÑ±Í≥µÏùÄ Ïù¥Îü¨Ìïú ÏßÄÏãúÎ¨∏ ÌäúÎãùÏùÑ ÌÜµÌï¥ Ïò§ÌîàÏÜåÏä§ LLMÏùÑ Í∞úÏÑ†Ìï† ÌÅ∞ Í∏∞ÌöåÎ•º Ï†úÍ≥µÌï©ÎãàÎã§.      \n",
       "LLaMAÎäî Ïù¥Îü¨Ìïú Ïò§ÌîàÏÜåÏä§ LLMÎì§ Ï§ë ÌïòÎÇòÎ°ú, ÏÉÅÏö© LLMÏù∏ GPT-3ÏôÄ ÏÑ±Îä•ÏùÑ Í≤¨Ï§Ñ Ïàò ÏûàÏäµÎãàÎã§. LLaMAÍ∞Ä ÏßÄÏãúÎ•º Îî∞Î•º Ïàò ÏûàÎèÑÎ°ù \n",
       "ÌïòÎ†§Î©¥, Îõ∞Ïñ¥ÎÇú ÏÑ±Îä•Í≥º Ï†ÄÎπÑÏö©Ïùò Self-Instruct ÌäúÎãùÏù¥ Îπ†Î•¥Í≤å Ï±ÑÌÉùÎêòÍ≥† ÏûàÏäµÎãàÎã§.                                      \n",
       "\n",
       "Î≥∏ ÎÖºÎ¨∏ÏóêÏÑú Ïö∞Î¶¨Îäî Ï≤òÏùåÏúºÎ°ú GPT-4Î•º Self-Instruct ÌäúÎãùÏùò ÍµêÏÇ¨Î°ú ÏÇ¨Ïö©ÌïòÎäî Î∞©Î≤ïÏùÑ Ï†úÏïàÌï©ÎãàÎã§. Ïö∞Î¶¨Ïùò Ï£ºÏöî Í∏∞Ïó¨Îäî     \n",
       "Îã§ÏùåÍ≥º Í∞ôÏäµÎãàÎã§:                                                                                                   \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> ‚Ä¢ </span><span style=\"font-weight: bold\">GPT-4 Îç∞Ïù¥ÌÑ∞:</span> Ïö∞Î¶¨Îäî ÏòÅÏñ¥ÏôÄ Ï§ëÍµ≠Ïñ¥Î°ú Îêú 52K Í∞úÏùò ÏßÄÏãúÎ¨∏ Îç∞Ïù¥ÌÑ∞ÏÖãÍ≥º ÏÑ∏ Í∞úÏùò ÏßÄÏãúÎ¨∏ ÌäúÎãùÎêú Î™®Îç∏Ïùò Ï∂úÎ†•ÏùÑ ÌèâÍ∞ÄÌïú   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>ÌîºÎìúÎ∞± Îç∞Ïù¥ÌÑ∞Î•º Ìè¨Ìï®ÌïòÏó¨ GPT-4Í∞Ä ÏÉùÏÑ±Ìïú Îç∞Ïù¥ÌÑ∞Î•º Í≥µÍ∞úÌï©ÎãàÎã§.                                                    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> ‚Ä¢ </span><span style=\"font-weight: bold\">Î™®Îç∏ Î∞è ÌèâÍ∞Ä:</span> GPT-4Í∞Ä ÏÉùÏÑ±Ìïú Îç∞Ïù¥ÌÑ∞Î•º Í∏∞Î∞òÏúºÎ°ú ÏßÄÏãúÎ¨∏ ÌäúÎãùÎêú LLaMA Î™®Îç∏Í≥º Î≥¥ÏÉÅ Î™®Îç∏ÏùÑ Í∞úÎ∞úÌñàÏäµÎãàÎã§. Ïö∞Î¶¨Îäî ÏÑ∏   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>Í∞ÄÏßÄ Í∏∞Ï§ÄÏóê ÎåÄÌïú Ïù∏Í∞Ñ ÌèâÍ∞Ä, GPT-4 ÌîºÎìúÎ∞±ÏùÑ ÏÇ¨Ïö©Ìïú ÏûêÎèô ÌèâÍ∞Ä, Í∑∏Î¶¨Í≥† ÎπÑÏ†ïÌòï ÏßÄÏãúÎ¨∏Ïóê ÎåÄÌïú ROUGE-L Ï†êÏàòÎ•º ÏÇ¨Ïö©ÌïòÏó¨\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>ÏßÄÏãúÎ¨∏ ÌäúÎãùÎêú LLMÏùò ÌíàÏßàÏùÑ ÌèâÍ∞ÄÌï©ÎãàÎã§.                                                                          \n",
       "\n",
       "Ïù¥ Ïó∞Íµ¨Îäî GPT-4Î°ú ÏÉùÏÑ±Îêú Îç∞Ïù¥ÌÑ∞Î•º ÏÇ¨Ïö©Ìïú LLM ÏßÄÏãúÎ¨∏ ÌäúÎãùÏùò Ìö®Í≥ºÏÑ±ÏùÑ ÌôïÏù∏ÌïòÍ≥†, ÏùºÎ∞ò Î™©Ï†ÅÏùò ÏßÄÏãúÎ¨∏ÏùÑ Îî∞Î•¥Îäî          \n",
       "ÏóêÏù¥Ï†ÑÌä∏Î•º Íµ¨Ï∂ïÌïòÎäî Îç∞ ÏûàÏñ¥ Ïã§Ïö©Ï†ÅÏù∏ ÌåÅÏùÑ Ï†úÏïàÌï©ÎãàÎã§.                                                              \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ</span>\n",
       "\n",
       "                                                     <span style=\"font-weight: bold; text-decoration: underline\">2 DATASET</span>                                                     \n",
       "\n",
       "Ïù¥ ÎÖºÎ¨∏Ïùò '2 Îç∞Ïù¥ÌÑ∞ÏÖã' ÏÑπÏÖòÏóêÏÑúÎäî Îç∞Ïù¥ÌÑ∞ ÏàòÏßë Í≥ºÏ†ïÍ≥º GPT-4Î•º ÏÇ¨Ïö©Ìïú Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± Î∞©Î≤ïÏóê ÎåÄÌï¥ ÏÑ§Î™ÖÌï©ÎãàÎã§.            \n",
       "\n",
       "Îç∞Ïù¥ÌÑ∞ ÏàòÏßë: Alpaca Îç∞Ïù¥ÌÑ∞ÏÖã(Taori et al., 2023)ÏóêÏÑú ÏàòÏßëÎêú 52,000Í∞úÏùò Í≥†Ïú†Ìïú ÏßÄÏãúÎ¨∏ÏùÑ Ïû¨ÏÇ¨Ïö©Ìï©ÎãàÎã§. Í∞Å ÏßÄÏãúÎ¨∏ÏùÄ   \n",
       "Î™®Îç∏Ïù¥ ÏàòÌñâÌï¥Ïïº Ìï† ÏûëÏóÖÏùÑ ÏÑ§Î™ÖÌï©ÎãàÎã§. ÏßÄÏãúÎ¨∏ÏóêÎäî ÏûëÏóÖÏóê ÎåÄÌïú ÏÑ†ÌÉùÏ†Å Îß•ÎùΩÏù¥ÎÇò ÏûÖÎ†•Ïù¥ ÏûàÏùÑ ÏàòÎèÑ ÏûàÍ≥† ÏóÜÏùÑ ÏàòÎèÑ       \n",
       "ÏûàÏäµÎãàÎã§. Alpaca Îç∞Ïù¥ÌÑ∞ÏÖãÏóêÏÑúÎäî GPT-3.5(text-davinci-003)Î•º ÏÇ¨Ïö©Ìï¥ Ï∂úÎ†•ÏùÑ ÏÉùÏÑ±ÌñàÏßÄÎßå, Ïö∞Î¶¨Îäî GPT-4(gpt-4)Î•º        \n",
       "ÌôúÏö©ÌïòÏó¨ Ï∂úÎ†•ÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§. GPT-4Î•º ÏÇ¨Ïö©Ìï¥ Îã§Ïùå ÎÑ§ Í∞ÄÏßÄ Îç∞Ïù¥ÌÑ∞ÏÖãÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§.                                     \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 1 </span><span style=\"font-weight: bold\">ÏòÅÏñ¥ ÏßÄÏãúÎ¨∏-ÎãµÎ≥Ä Îç∞Ïù¥ÌÑ∞:</span> AlpacaÏóêÏÑú ÏàòÏßëÌïú 52,000Í∞úÏùò ÏßÄÏãúÎ¨∏ Í∞ÅÍ∞ÅÏóê ÎåÄÌï¥ GPT-4Í∞Ä ÏòÅÏñ¥Î°ú ÎãµÎ≥ÄÏùÑ Ï†úÍ≥µÌï©ÎãàÎã§.      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>ÏûêÏÑ∏Ìïú ÎÇ¥Ïö©ÏùÄ ÏïåÍ≥†Î¶¨Ï¶ò 1Ïóê ÎÇòÏôÄ ÏûàÏäµÎãàÎã§. GPT-4ÏôÄ Self-InstructÎ•º ÏÇ¨Ïö©ÌïòÏó¨ ÏûêÏ≤¥ ÏßÄÏãú ÏÑ∏Ìä∏Î•º Íµ¨ÏÑ±ÌïòÎäî Î∞òÎ≥µÏ†Å     \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>Í≥ºÏ†ïÏùÑ Ìñ•ÌõÑ ÏûëÏóÖÏúºÎ°ú ÎÇ®Í≤®Îë°ÎãàÎã§.                                                                                \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 2 </span><span style=\"font-weight: bold\">Ï§ëÍµ≠Ïñ¥ ÏßÄÏãúÎ¨∏-ÎãµÎ≥Ä Îç∞Ïù¥ÌÑ∞:</span> ChatGPTÎ•º ÏÇ¨Ïö©ÌïòÏó¨ 52,000Í∞úÏùò ÏßÄÏãúÎ¨∏ÏùÑ Ï§ëÍµ≠Ïñ¥Î°ú Î≤àÏó≠ÌïòÍ≥†, GPT-4Í∞Ä Ï§ëÍµ≠Ïñ¥Î°ú ÎãµÎ≥ÄÌïòÍ≤å  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>Ìï©ÎãàÎã§. Ïù¥Î•º ÌÜµÌï¥ LLaMAÏóê Í∏∞Î∞òÌïú Ï§ëÍµ≠Ïñ¥ ÏßÄÏãúÎ¨∏-Îî∞Î•¥Í∏∞ Î™®Îç∏ÏùÑ Íµ¨Ï∂ïÌïòÍ≥†, ÏßÄÏãúÎ¨∏ ÌäúÎãùÏùò Ïñ∏Ïñ¥ Í∞Ñ ÏùºÎ∞òÌôî Îä•Î†•ÏùÑ      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>Ïó∞Íµ¨Ìï† Ïàò ÏûàÏäµÎãàÎã§.                                                                                             \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 3 </span><span style=\"font-weight: bold\">ÎπÑÍµê Îç∞Ïù¥ÌÑ∞:</span> GPT-4ÏóêÍ≤å ÏûêÏã†Ïùò ÏùëÎãµÏùÑ 1Î∂ÄÌÑ∞ 10ÍπåÏßÄ ÌèâÍ∞ÄÌïòÎèÑÎ°ù ÏöîÏ≤≠Ìï©ÎãàÎã§. ÎòêÌïú, GPT-4ÏóêÍ≤å GPT-4, GPT-3.5, Í∑∏Î¶¨Í≥† \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>OPT-IMLÏùÑ Ìè¨Ìï®Ìïú ÏÑ∏ Î™®Îç∏Ïùò ÏùëÎãµÏùÑ ÎπÑÍµêÌïòÏó¨ ÌèâÍ∞ÄÌïòÎèÑÎ°ù ÏöîÏ≤≠Ìï©ÎãàÎã§. Ïù¥Îäî Î≥¥ÏÉÅ Î™®Îç∏ÏùÑ ÌõàÎ†®ÌïòÎäî Îç∞ ÏÇ¨Ïö©Îê©ÎãàÎã§.      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 4 </span><span style=\"font-weight: bold\">ÎπÑÏ†ïÏÉÅ ÏßÄÏãúÎ¨∏Ïóê ÎåÄÌïú ÎãµÎ≥Ä:</span> 68,000Í∞úÏùò ÏßÄÏãúÎ¨∏-ÏûÖÎ†•-Ï∂úÎ†• ÏÇºÏ§ë ÏΩîÏñ¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏóê ÎåÄÌï¥ GPT-4Ïùò ÎãµÎ≥ÄÏùÑ ÎîîÏΩîÎî©Ìï©ÎãàÎã§. Ïù¥\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>ÌïòÏúÑ ÏÑ∏Ìä∏Îäî GPT-4ÏôÄ Ïö∞Î¶¨ ÏßÄÏãúÎ¨∏ ÌäúÎãùÎêú Î™®Îç∏Îì§ Í∞ÑÏùò Ï∞®Ïù¥Î•º Í≥ÑÎüâÌôîÌïòÎäî Îç∞ ÏÇ¨Ïö©Îê©ÎãàÎã§.                             \n",
       "\n",
       "Îç∞Ïù¥ÌÑ∞ ÌÜµÍ≥Ñ: Í∑∏Î¶º 1ÏóêÏÑúÎäî GPT-4ÏôÄ GPT-3.5Ïùò ÏòÅÏñ¥ Ï∂úÎ†• ÏùëÎãµ ÏÑ∏Ìä∏Î•º ÎπÑÍµêÌï©ÎãàÎã§. Í∞Å Ï∂úÎ†•Ïóê ÎåÄÌï¥ ÎèôÏÇ¨ ÎøåÎ¶¨ÏôÄ ÏßÅÏ†ë      \n",
       "Î™©Ï†ÅÏñ¥ Î™ÖÏÇ¨Î•º Ï∂îÏ∂úÌïòÍ≥†, Í∞Å Ï∂úÎ†• ÏÑ∏Ìä∏ÏóêÏÑú Í≥†Ïú†Ìïú ÎèôÏÇ¨-Î™ÖÏÇ¨ ÏåçÏùò ÎπàÎèÑÎ•º Í≥ÑÏÇ∞Ìï©ÎãàÎã§. ÎπàÎèÑÍ∞Ä 10 Ïù¥ÏÉÅÏù∏ ÎèôÏÇ¨-Î™ÖÏÇ¨ ÏåçÏùÄ  \n",
       "Í∑∏Î¶º 1(a)ÏôÄ (b)Ïóê ÌëúÏãúÎêòÎ©∞, Îëê ÏÑ∏Ìä∏Ïùò Í∞ÄÏû• ÎπàÎ≤àÌïú 25Í∞úÏùò ÏåçÏùÑ Í∑∏Î¶º 1(c)Ïóê ÎπÑÍµêÌï©ÎãàÎã§. Ï∂úÎ†• ÏãúÌÄÄÏä§ Í∏∏Ïù¥Ïùò ÎπàÎèÑ      \n",
       "Î∂ÑÌè¨Îäî Í∑∏Î¶º 1(d)Ïóê ÎπÑÍµêÎêòÏñ¥ ÏûàÏäµÎãàÎã§. GPT-4Îäî GPT-3.5Î≥¥Îã§ Îçî Í∏¥ ÏãúÌÄÄÏä§Î•º ÏÉùÏÑ±ÌïòÎäî Í≤ΩÌñ•Ïù¥ ÏûàÏäµÎãàÎã§. AlpacaÏùò GPT-3.5\n",
       "Îç∞Ïù¥ÌÑ∞Îäî Î∞òÎ≥µÏ†ÅÏù∏ Îç∞Ïù¥ÌÑ∞ ÏàòÏßë Í≥ºÏ†ïÏùÑ ÌÜµÌï¥ Í∞Å Î∞òÎ≥µÏóêÏÑú Ïú†ÏÇ¨Ìïú ÏßÄÏãúÎ¨∏ Ïù∏Ïä§ÌÑ¥Ïä§Î•º Ï†úÍ±∞ÌïòÏó¨ Îçî Í∏¥ Íº¨Î¶¨Î•º Í∞ÄÏßÑ Ï∂úÎ†•     \n",
       "Î∂ÑÌè¨Î•º Î≥¥ÏûÖÎãàÎã§. Ïù¥Îäî ÌòÑ ÏãúÏ†êÏùò ÏùºÌöåÏÑ± Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± Í≥ºÏ†ïÏóêÏÑúÎäî ÏóÜÎäî ÌäπÏßïÏûÖÎãàÎã§. Ïù¥Îü¨Ìïú Í∞ÑÎã®Ìïú Í≥ºÏ†ïÏóêÎèÑ Î∂àÍµ¨ÌïòÍ≥†,    \n",
       "GPT-4Í∞Ä ÏÉùÏÑ±Ìïú ÏßÄÏãúÎ¨∏-Îî∞Î•¥Í∏∞ Îç∞Ïù¥ÌÑ∞Îäî Ïù¥ÌõÑ Ïã§ÌóòÏóêÏÑú Îçî ÎÇòÏùÄ Ï†ïÎ†¨ ÏÑ±Îä•ÏùÑ Î≥¥Ïó¨Ï§çÎãàÎã§.                                \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ</span>\n",
       "\n",
       "                                       <span style=\"font-weight: bold; text-decoration: underline\">3 INSTRUCTION-TUNING LANGUAGE MODELS</span>                                        \n",
       "\n",
       "I apologize, but it seems that the '3 INSTRUCTION-TUNING LANGUAGE MODELS' section content is missing from your     \n",
       "request. If you can provide the full text of that section, I will be happy to explain it in detail.                \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ</span>\n",
       "\n",
       "                                             <span style=\"font-weight: bold; text-decoration: underline\">3.1 SELF-INSTRUCT TUNING</span>                                              \n",
       "\n",
       "Ïù¥ ÏÑπÏÖòÏóêÏÑúÎäî Self-Instruct ÌäúÎãùÏùÑ ÌÜµÌï¥ Ïñ∏Ïñ¥ Î™®Îç∏ÏùÑ ÌõàÎ†®ÌïòÎäî Í≥ºÏ†ïÏóê ÎåÄÌï¥ ÏÑ§Î™ÖÌï©ÎãàÎã§. Ïó∞Íµ¨ÏóêÏÑúÎäî LLaMA 7B           \n",
       "Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏Î•º ÏÇ¨Ïö©ÌïòÏó¨ Îëê Í∞ÄÏßÄ Î™®Îç∏ÏùÑ ÏßÄÎèÑ ÌïôÏäµ Î∞©ÏãùÏúºÎ°ú ÎØ∏ÏÑ∏ Ï°∞Ï†ïÌñàÏäµÎãàÎã§.                                         \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 1 </span><span style=\"font-weight: bold\">LLaMA-GPT4 Î™®Îç∏</span>: Ïù¥ Î™®Îç∏ÏùÄ GPT-4Í∞Ä ÏÉùÏÑ±Ìïú 52,000Í∞úÏùò ÏòÅÏñ¥ ÏßÄÏãúÎ¨∏-Îî∞Î•¥Í∏∞ Îç∞Ïù¥ÌÑ∞Î•º Î∞îÌÉïÏúºÎ°ú ÌõàÎ†®ÎêòÏóàÏäµÎãàÎã§. Ïù¥    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>Îç∞Ïù¥ÌÑ∞Ïùò Î∂ÑÌè¨Îäî Ïù¥Ï†ÑÏùò Í∑∏Î¶º 1ÏóêÏÑú Î≥¥Ïó¨Ï£ºÏóàÏäµÎãàÎã§.                                                               \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 2 </span><span style=\"font-weight: bold\">LLaMA-GPT4-CN Î™®Îç∏</span>: Ïù¥ Î™®Îç∏ÏùÄ GPT-4Í∞Ä ÏÉùÏÑ±Ìïú 52,000Í∞úÏùò Ï§ëÍµ≠Ïñ¥ ÏßÄÏãúÎ¨∏-Îî∞Î•¥Í∏∞ Îç∞Ïù¥ÌÑ∞Î•º Î∞îÌÉïÏúºÎ°ú ÌõàÎ†®ÎêòÏóàÏäµÎãàÎã§.  \n",
       "\n",
       "ÌõàÎ†® Í≥ºÏ†ïÏóêÏÑúÎäî Í≥µÏ†ïÌïú ÎπÑÍµêÎ•º ÏúÑÌï¥ Taori et al.(2023)Ïùò ÌõàÎ†® Ïä§ÏºÄÏ§ÑÏùÑ Îî∞ÎûêÏäµÎãàÎã§. Ïù¥Îü¨Ìïú Î™®Îç∏Îì§ÏùÄ GPT-4Í∞Ä ÏÉùÏÑ±Ìïú   \n",
       "Îç∞Ïù¥ÌÑ∞Ïùò ÌíàÏßàÏùÑ Ïó∞Íµ¨ÌïòÍ≥†, ÌïòÎÇòÏùò Ïñ∏Ïñ¥Î°ú ÏßÄÏãúÎ¨∏ ÌäúÎãùÏùÑ ÌñàÏùÑ ÎïåÏùò ÍµêÏ∞® Ïñ∏Ïñ¥ ÏùºÎ∞òÌôî ÌäπÏÑ±ÏùÑ Ïó∞Íµ¨ÌïòÎäî Îç∞ ÏÇ¨Ïö©Îê©ÎãàÎã§.    \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ</span>\n",
       "\n",
       "                                                 <span style=\"font-weight: bold; text-decoration: underline\">3.2 REWARD MODELS</span>                                                 \n",
       "\n",
       "Ïù¥ ÏÑπÏÖòÏóêÏÑúÎäî Ïù∏Í∞Ñ ÌîºÎìúÎ∞±ÏùÑ ÌÜµÌïú Í∞ïÌôî ÌïôÏäµ(RLHF, Reinforcement Learning from Human Feedback)Ïùò ÌïµÏã¨ ÏöîÏÜåÏù∏ Î≥¥ÏÉÅ    \n",
       "Î™®Îç∏ÎßÅÏóê ÎåÄÌï¥ ÏÑ§Î™ÖÌï©ÎãàÎã§. RLHFÎäî LLMÏùò ÌñâÎèôÏùÑ Ïù∏Í∞ÑÏùò ÏÑ†Ìò∏ÏôÄ ÎßûÏ∂∞ Î™®Îç∏ÏùÑ ÎçîÏö± Ïú†Ïö©ÌïòÍ≤å ÎßåÎì§Î†§Îäî Î™©Ï†ÅÏùÑ Í∞ÄÏßÄÍ≥†       \n",
       "ÏûàÏäµÎãàÎã§. Î≥¥ÏÉÅ Î™®Îç∏ÎßÅÏùÄ ÌîÑÎ°¨ÌîÑÌä∏ÏôÄ ÏùëÎãµÏù¥ Ï£ºÏñ¥Ï°åÏùÑ Îïå Ïä§ÏπºÎùº Î≥¥ÏÉÅÏùÑ ÏòàÏ∏°ÌïòÎäî ÌöåÍ∑Ä Í≥ºÏ†úÎ°ú Î¨∏Ï†úÎ•º Íµ¨ÏÑ±Ìï©ÎãàÎã§. Ïù¥Î†áÍ≤å \n",
       "ÌïòÎ†§Î©¥ ÎåÄÍ∑úÎ™® ÎπÑÍµê Îç∞Ïù¥ÌÑ∞Í∞Ä ÌïÑÏöîÌïúÎç∞, Îëê Í∞ÄÏßÄ Î™®Îç∏Ïùò ÏùëÎãµÏùÑ Í∞ôÏùÄ ÌîÑÎ°¨ÌîÑÌä∏Ïóê ÎåÄÌï¥ ÎπÑÍµêÌïòÎäî Î∞©ÏãùÏûÖÎãàÎã§. ÌïòÏßÄÎßå       \n",
       "Alpaca, Vicuna, Dolly Îì±Ïùò Í∏∞Ï°¥ Ïò§ÌîàÏÜåÏä§ ÏûëÏóÖÏùÄ ÎπÑÍµê Îç∞Ïù¥ÌÑ∞ Î†àÏù¥Î∏îÎßÅ ÎπÑÏö©Ïù¥ ÎÜíÏïÑ RLHFÎ•º Ìè¨Ìï®ÌïòÏßÄ ÏïäÏäµÎãàÎã§.         \n",
       "\n",
       "ÌïúÌé∏, ÏµúÍ∑º Ïó∞Íµ¨Îäî GPT-4Í∞Ä ÏûêÏã†Ïùò Ïã§ÏàòÎ•º ÏãùÎ≥ÑÌïòÍ≥† ÏàòÏ†ïÌïòÎ©∞, ÏùëÎãµ ÌíàÏßàÏùÑ Ï†ïÌôïÌûà ÌèâÍ∞ÄÌï† Ïàò ÏûàÏùåÏùÑ Î≥¥Ïó¨Ï£ºÏóàÏäµÎãàÎã§.     \n",
       "Îî∞ÎùºÏÑú RLHF Ïó∞Íµ¨Î•º Ï¥âÏßÑÌïòÍ∏∞ ÏúÑÌï¥, Ïö∞Î¶¨Îäî 2Ï†àÏóêÏÑú ÏÑ§Î™ÖÌïú Î∞îÏôÄ Í∞ôÏù¥ GPT-4Î•º ÏÇ¨Ïö©ÌïòÏó¨ ÎπÑÍµê Îç∞Ïù¥ÌÑ∞Î•º ÏÉùÏÑ±ÌñàÏäµÎãàÎã§.     \n",
       "\n",
       "Îç∞Ïù¥ÌÑ∞ ÌíàÏßà ÌèâÍ∞ÄÎ•º ÏúÑÌï¥, Ïö∞Î¶¨Îäî OPT 1.3B Í∏∞Î∞òÏúºÎ°ú Î≥¥ÏÉÅ Î™®Îç∏ÏùÑ ÌõàÎ†®ÌïòÏó¨ ÏÑúÎ°ú Îã§Î•∏ ÏùëÎãµÎì§ÏùÑ ÌèâÍ∞ÄÌï©ÎãàÎã§. Í∞Å ÎπÑÍµê      \n",
       "Îç∞Ïù¥ÌÑ∞ Ïù∏Ïä§ÌÑ¥Ïä§Îäî Ìïú ÌîÑÎ°¨ÌîÑÌä∏ $\\textbf{\\em x}$ÏôÄ $K$Í∞úÏùò ÏùëÎãµÏùÑ Ìè¨Ìï®ÌïòÍ≥†, GPT-4Îäî Í∞Å ÏùëÎãµÏóê ÎåÄÌï¥ 1Î∂ÄÌÑ∞ 10ÍπåÏßÄÏùò    \n",
       "Ï†êÏàò $s$Î•º Ï§çÎãàÎã§. Ïù¥ Ïù∏Ïä§ÌÑ¥Ïä§ÏóêÏÑú Í≥†Ïú†Ìïú Ïåç $C_{2}^{K}$Î•º Íµ¨ÏÑ±Ìï† Ïàò ÏûàÏúºÎ©∞, Í∞Å ÏåçÏùÄ $(y_{l}, y_{h})$Î°ú ÌëúÌòÑÎê©ÎãàÎã§.\n",
       "Ïó¨Í∏∞ÏÑú $y_{l}$Ïùò Ï†êÏàòÎäî $y_{h}$Ïùò Ï†êÏàòÎ≥¥Îã§ ÎÇÆÏäµÎãàÎã§($s_{l} &lt; s_{h}$). Î≥¥ÏÉÅ Î™®Îç∏ $r_{\\theta}$Îäî Îã§Ïùå Î™©ÌëúÎ°ú         \n",
       "ÌõàÎ†®Îê©ÎãàÎã§: $\\operatorname*{min}\\log(\\sigma(r_{\\pmb\\theta}(x,y_{h})-r_{\\pmb\\theta}(\\pmb x,y_{l})))$, Ïó¨Í∏∞ÏÑú        \n",
       "$\\sigma$Îäî ÏãúÍ∑∏Î™®Ïù¥Îìú Ìï®ÏàòÏûÖÎãàÎã§. ÎπÑÍµê Îç∞Ïù¥ÌÑ∞Ïùò Î∂ÑÌè¨Îäî Í∑∏Î¶º 2Ïóê ÎÇòÌÉÄÎÇò ÏûàÏäµÎãàÎã§.                                   \n",
       "\n",
       "Ïù¥ Ï†ëÍ∑º Î∞©ÏãùÏùÄ Î≥¥ÏÉÅ Î™®Îç∏Ïù¥ Îã§ÏñëÌïú ÏùëÎãµ ÌíàÏßàÏùÑ ÌèâÍ∞ÄÌïòÍ≥† RLHFÏùò Ìö®Í≥ºÏÑ±ÏùÑ ÎÜíÏù¥Îäî Îç∞ ÎèÑÏõÄÏùÑ Ï§Ñ Ïàò ÏûàÎèÑÎ°ù               \n",
       "ÏÑ§Í≥ÑÎêòÏóàÏäµÎãàÎã§.                                                                                                    \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ</span>\n",
       "\n",
       "                                              <span style=\"font-weight: bold; text-decoration: underline\">4 EXPERIMENTAL RESULTS</span>                                               \n",
       "\n",
       "It looks like the content from the '4 EXPERIMENTAL RESULTS' section is missing from your request. If you can       \n",
       "provide the full text of that section, I would be happy to explain it in detail.                                   \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ</span>\n",
       "\n",
       "                                                  <span style=\"font-weight: bold; text-decoration: underline\">4.1 BENCHMARKS</span>                                                   \n",
       "\n",
       "Ïù¥ ÏÑπÏÖòÏóêÏÑúÎäî LLM(ÎåÄÌòï Ïñ∏Ïñ¥ Î™®Îç∏)Ïùò ÏÑ±Îä•ÏùÑ ÌèâÍ∞ÄÌïòÍ∏∞ ÏúÑÌïú Î≤§ÏπòÎßàÌÅ¨Î•º ÏÜåÍ∞úÌïòÍ≥†, Ïù∏Í∞Ñ ÌèâÍ∞Ä Î∞©Î≤ïÍ≥º Í∑∏ Í≤∞Í≥ºÎ•º           \n",
       "ÏÑ§Î™ÖÌï©ÎãàÎã§.                                                                                                        \n",
       "\n",
       "                                                     <span style=\"font-weight: bold\">Î≤§ÏπòÎßàÌÅ¨:</span>                                                     \n",
       "\n",
       "Ïù¥ Ïó∞Íµ¨Ïùò Î™©ÌëúÎäî GPT-4 Îç∞Ïù¥ÌÑ∞Î•º Í∏∞Î∞òÏúºÎ°ú ÌïôÏäµÎêú Self-Instruct Î™®Îç∏Îì§Ïù¥ ÏÉàÎ°úÏö¥ ÏßÄÏãúÎ¨∏Ïóê ÎåÄÌï¥ ÏñºÎßàÎÇò Ïûò Î∞òÏùëÌïòÎäîÏßÄ   \n",
       "ÌèâÍ∞ÄÌïòÎäî Í≤ÉÏûÖÎãàÎã§. Ïó∞Íµ¨ÏóêÏÑú ÏÇ¨Ïö©Ìïú Îç∞Ïù¥ÌÑ∞ÏÖãÏùÄ Îã§ÏùåÍ≥º Í∞ôÏäµÎãàÎã§:                                                     \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> ‚Ä¢ </span><span style=\"font-weight: bold\">User-Oriented-Instructions- $252^{,2}$</span>: Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùÄ 71Í∞úÏùò ÏÇ¨Ïö©Ïûê Ï§ëÏã¨ Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖò(Ïòà: Grammarly,            \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>StackOverflow, Overleaf)ÏùÑ Í∏∞Î∞òÏúºÎ°ú Ìïú 252Í∞úÏùò ÏßÄÏãúÎ¨∏ÏùÑ Ìè¨Ìï®Ìï©ÎãàÎã§.                                             \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> ‚Ä¢ </span><span style=\"font-weight: bold\">Vicuna-Instructions- $80^{3}$</span>: GPT-4Í∞Ä ÏÉùÏÑ±Ìïú 80Í∞úÏùò ÎèÑÏ†ÑÏ†ÅÏù∏ ÏßàÎ¨∏Ïù¥ Ìè¨Ìï®ÎêòÏñ¥ ÏûàÏúºÎ©∞, Í∏∞Î≥∏ Î™®Îç∏Îì§Ïù¥ Ïñ¥Î†§ÏõåÌïòÎäî  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>ÏßàÎ¨∏Îì§ÏûÖÎãàÎã§. ÏßÄÏãù, ÏàòÌïô, Fermi Î¨∏Ï†ú, Î∞òÏÇ¨Ïã§Ï†Å ÏãúÎÇòÎ¶¨Ïò§, Î°§ÌîåÎ†àÏûâ, ÏùºÎ∞ò, ÏΩîÎî©, Í∏ÄÏì∞Í∏∞, ÏÉÅÏãù Îì± 8Í∞úÏùò Ïπ¥ÌÖåÍ≥†Î¶¨Í∞Ä \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>ÏûàÏäµÎãàÎã§.                                                                                                       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> ‚Ä¢ </span><span style=\"font-weight: bold\">Unnatural Instructions</span>: 68,478Í∞úÏùò ÏÉòÌîåÎ°ú Íµ¨ÏÑ±Îêú Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùÄ 15Í∞úÏùò ÏàòÏûëÏóÖ ÏòàÏ†úÎ•º ÏÇ¨Ïö©Ìïú 3-ÏÉ∑ Îß•ÎùΩ ÌïôÏäµÏúºÎ°ú   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>text-davinci-002Í∞Ä Ìï©ÏÑ±Ìïú Í≤ÉÏûÖÎãàÎã§.                                                                             \n",
       "\n",
       "                                                    <span style=\"font-weight: bold\">Ïù∏Í∞Ñ ÌèâÍ∞Ä:</span>                                                     \n",
       "\n",
       "Î™®Îç∏Ïùò Ï†ïÎ†¨ ÌíàÏßàÏùÑ ÌèâÍ∞ÄÌïòÍ∏∞ ÏúÑÌï¥ Anthropic Askell et al.(2021)ÏóêÏÑú Ï†úÏïàÌïú Ï†ïÎ†¨ Í∏∞Ï§ÄÏùÑ Îî∞ÎûêÏäµÎãàÎã§. Ïù¥ Í∏∞Ï§ÄÏùÄ AI     \n",
       "ÏãúÏä§ÌÖúÏù¥ Ïù∏Í∞ÑÏùò Í∞ÄÏπòÏóê ÏñºÎßàÎÇò Ïûò Ï†ïÎ†¨ÎêòÏñ¥ ÏûàÎäîÏßÄÎ•º ÌèâÍ∞ÄÌïòÎäî Îç∞ ÏÇ¨Ïö©Îê©ÎãàÎã§. Í∏∞Ï§ÄÏùÄ Îã§ÏùåÍ≥º Í∞ôÏäµÎãàÎã§:                 \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> ‚Ä¢ </span><span style=\"font-weight: bold\">ÎèÑÏõÄÎê®</span>: Ïù∏Í∞ÑÏù¥ Î™©ÌëúÎ•º Îã¨ÏÑ±ÌïòÎäî Îç∞ ÎèÑÏõÄÏù¥ ÎêòÎäîÏßÄ Ïó¨Î∂ÄÎ•º ÌèâÍ∞ÄÌï©ÎãàÎã§.                                              \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> ‚Ä¢ </span><span style=\"font-weight: bold\">Ï†ïÏßÅÏÑ±</span>: Ï†ïÌôïÌïú Ï†ïÎ≥¥Î•º Ï†úÍ≥µÌïòÍ≥† ÌïÑÏöîÏãú Î∂àÌôïÏã§ÏÑ±ÏùÑ ÌëúÌòÑÌïòÏó¨ Ïù∏Í∞Ñ ÏÇ¨Ïö©ÏûêÎ•º Ïò§ÎèÑÌïòÏßÄ ÏïäÎäîÏßÄÎ•º ÌèâÍ∞ÄÌï©ÎãàÎã§.           \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> ‚Ä¢ </span><span style=\"font-weight: bold\">Î¨¥Ìï¥ÏÑ±</span>: Ïù∏Í∞ÑÏóêÍ≤å Ìï¥Î•º ÎÅºÏπòÏßÄ ÏïäÎäîÏßÄÎ•º ÌèâÍ∞ÄÌï©ÎãàÎã§.                                                               \n",
       "\n",
       "Amazon Mechanical TurkÎ•º ÌÜµÌï¥ Ïù∏Í∞Ñ ÌèâÍ∞ÄÎ•º ÏàòÌñâÌñàÏúºÎ©∞, Í≤∞Í≥ºÎäî Í∑∏Î¶º 3Ïùò ÌååÏù¥ Ï∞®Ìä∏Ïóê ÎÇòÌÉÄÎÇò ÏûàÏäµÎãàÎã§.                 \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> ‚Ä¢ </span><span style=\"font-weight: bold\">Ï≤´ Î≤àÏß∏ ÎπÑÍµê</span>: GPT-4 Îç∞Ïù¥ÌÑ∞Î°ú ÎØ∏ÏÑ∏ Ï°∞Ï†ïÎêú LLaMA Î™®Îç∏Í≥º GPT-3 Îç∞Ïù¥ÌÑ∞Î°ú ÎØ∏ÏÑ∏ Ï°∞Ï†ïÎêú LLaMA Î™®Îç∏(Stanford Alpaca     \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>Î™®Îç∏)ÏùÑ ÎπÑÍµêÌïòÏó¨, \"ÎèÑÏõÄÎê®\" Í∏∞Ï§ÄÏóêÏÑú GPT-4Í∞Ä 54.12%Î°ú Ïö∞ÏÑ∏ÌñàÏúºÎ©∞, \"Ï†ïÏßÅÏÑ±\"Í≥º \"Î¨¥Ìï¥ÏÑ±\"ÏóêÏÑúÎäî GPT-3(Alpaca)Ïù¥ Îã§ÏÜå \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>Ïö∞ÏÑ∏ÌïòÎÇò ÎπÑÏä∑Ìïú ÏàòÏ§ÄÏùò Í≤∞Í≥ºÎ•º Î≥¥ÏòÄÏäµÎãàÎã§.                                                                       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> ‚Ä¢ </span><span style=\"font-weight: bold\">Îëê Î≤àÏß∏ ÎπÑÍµê</span>: GPT-4 ÏßÄÏãúÎ•º Îî∞Îùº ÌäúÎãùÎêú LLaMA Î™®Îç∏Í≥º ÏõêÎûòÏùò GPT-4 Î™®Îç∏ÏùÑ ÎπÑÍµêÌñàÏúºÎ©∞, ÏÑ∏ Í∞ÄÏßÄ Í∏∞Ï§Ä Î™®ÎëêÏóêÏÑú Ïú†ÏÇ¨Ìïú\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>ÏÑ±Îä•ÏùÑ Î≥¥Ïó¨Ï§¨ÏäµÎãàÎã§. Ïù¥Îäî GPT-4Í∞Ä ÏÉùÏÑ±Ìïú Îç∞Ïù¥ÌÑ∞Î°ú ÌïôÏäµÌï®ÏúºÎ°úÏç® ÏõêÎûò GPT-4 ÏàòÏ§ÄÏùò ÏÑ±Îä•ÏùÑ Îã¨ÏÑ±Ìï† Ïàò ÏûàÎã§Îäî Í≤ÉÏùÑ   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>ÏùòÎØ∏Ìï©ÎãàÎã§.                                                                                                     \n",
       "\n",
       "                                             <span style=\"font-weight: bold\">Ï∂îÍ∞Ä Í∑∏Î¶º ÏÑ§Î™Ö (Í∑∏Î¶º 4):</span>                                              \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> ‚Ä¢ </span>GPT-4Î°ú ÌèâÍ∞ÄÎêú ÏÑ±Îä• ÎπÑÍµêÎ•º Î≥¥Ïó¨Ï§çÎãàÎã§. Í∞Å ÎßâÎåÄÎäî Îëê Î™®Îç∏ Í∞ÑÏùò ÌèâÍ∞Ä Í≤∞Í≥ºÎ•º ÎÇòÌÉÄÎÇ¥Î©∞, Ï¥ù Ï†êÏàò(ÏµúÎåÄ 800Ï†ê)ÏôÄ ÏÉÅÎåÄ  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>Ï†êÏàò ÎπÑÏú®(Í∞ïÎ†•Ìïú ÏÉÅÎåÄ Î™®Îç∏ ÎåÄÎπÑ)Ïù¥ Î≥¥Í≥†Îê©ÎãàÎã§.                                                                  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> ‚Ä¢ </span>LLaMA GPT4 Î™®Îç∏Ïùò ÏùëÎãµÏùÑ Ïö∞Î¶¨ Î≥¥ÏÉÅ Î™®Îç∏Î°ú ÌèâÍ∞ÄÌïú Í≤∞Í≥ºÎ•º ÎÇòÌÉÄÎÇ¥Î©∞, Ï†ÑÎ∞òÏ†ÅÏúºÎ°ú ChatGPT Î∞è GPT-4ÏôÄ ÎπÑÍµêÌïòÏó¨ ÏÑ±Îä•ÏùÑ \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>ÌèâÍ∞ÄÌñàÏäµÎãàÎã§.                                                                                                   \n",
       "\n",
       "Ïù¥ Ïó∞Íµ¨Îäî GPT-4 Îç∞Ïù¥ÌÑ∞Î•º ÏÇ¨Ïö©Ìïú ÌïôÏäµÏù¥ ÏÉàÎ°úÏö¥ ÏßÄÏãúÎ¨∏ ÏûëÏóÖÏóêÏÑú Îõ∞Ïñ¥ÎÇú ÏÑ±Îä•ÏùÑ ÎÇ∏Îã§Îäî Í∞ÄÎä•ÏÑ±ÏùÑ ÏãúÏÇ¨Ìï©ÎãàÎã§.            \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ</span>\n",
       "\n",
       "                               <span style=\"font-weight: bold; text-decoration: underline\">4.3 COMPARISONS WITH SOTA USING AUTOMATIC EVALUATION</span>                                \n",
       "\n",
       "Ïù¥ ÏÑπÏÖòÏóêÏÑúÎäî ÏûêÎèô ÌèâÍ∞ÄÎ•º ÏÇ¨Ïö©ÌïòÏó¨ LLaMA-GPT4 Î™®Îç∏Í≥º Îã§Î•∏ ÏµúÏ≤®Îã®(State-of-the-Art, SOTA) Î™®Îç∏Îì§ÏùÑ ÎπÑÍµêÌïú Í≤∞Í≥ºÎ•º    \n",
       "ÏÑ§Î™ÖÌï©ÎãàÎã§.                                                                                                        \n",
       "\n",
       "                                                    <span style=\"font-weight: bold\">ÏûêÎèô ÌèâÍ∞Ä:</span>                                                     \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> ‚Ä¢ </span><span style=\"font-weight: bold\">ÌèâÍ∞Ä Î∞©Î≤ï</span>: GPT-4Î•º ÌôúÏö©Ìï¥ Îã§ÏñëÌïú Î™®Îç∏Îì§Ïù¥ 80Í∞úÏùò ÏÉàÎ°úÏö¥ ÏßàÎ¨∏Ïóê ÎåÄÌïú ÏÉùÏÑ± ÏùëÎãµÏùò ÌíàÏßàÏùÑ ÏûêÎèô ÌèâÍ∞ÄÌñàÏäµÎãàÎã§.       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>LLaMA-GPT4(7B)ÏôÄ GPT-4Ïùò ÏùëÎãµÏùÑ ÏàòÏßëÌïòÍ≥†, Vicuna(2023)ÏóêÏÑú Í≥µÍ∞úÎêú Îã§Î•∏ Ï±ÑÌåÖ Î¥áÎì§(LLaMA 13B, Alpaca 13B, Vicuna  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>13B, Bard, ChatGPT)Ïùò ÏùëÎãµÏùÑ ÏÇ¨Ïö©ÌñàÏäµÎãàÎã§. Îëê Î™®Îç∏ Í∞ÑÏùò ÏùëÎãµ ÌíàÏßàÏùÑ 1ÏóêÏÑú 10ÍπåÏßÄÏùò Ï†êÏàòÎ°ú ÌèâÍ∞ÄÌñàÏäµÎãàÎã§.         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> ‚Ä¢ </span><span style=\"font-weight: bold\">Í≤∞Í≥º</span>: LLaMA-GPT4Î•º Îëê Í∞ÄÏßÄ ÎîîÏΩîÎî© Í≤∞Í≥ºÎ°ú ÌèâÍ∞ÄÌñàÏäµÎãàÎã§. Ï≤´Ïß∏Îäî ÏßàÎ¨∏Îãπ ÌïòÎÇòÏùò ÏùëÎãµÎßåÏùÑ ÏÉùÏÑ±ÌïòÎäî Í∏∞Î≥∏ ÎîîÏΩîÎî© Í≤∞Í≥º, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>ÎëòÏß∏Îäî ÏßàÎ¨∏Îãπ Îã§ÏÑØ Í∞úÏùò ÏùëÎãµÏùÑ ÏÉùÏÑ±Ìïú ÌõÑ Î≥¥ÏÉÅ Î™®Îç∏Ïù¥ ÏÉÅÏúÑÏóêÏÑú ÌïòÏúÑÎ°ú ÏàúÏúÑ Îß§Í∏¥ ÏùëÎãµ Í∑∏Î£πÏûÖÎãàÎã§. Ïù¥ Í∑∏Î£πÎì§ÏùÄ Í∏∞Î≥∏\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>Í≤∞Í≥ºÏôÄ ÎπÑÍµêÎêòÏóàÍ≥†, ÌèâÍ∞Ä Í≤∞Í≥ºÏóêÏÑú ÌîºÎìúÎ∞± Îç∞Ïù¥ÌÑ∞ÏôÄ Î≥¥ÏÉÅ Î™®Îç∏Ïùò Ïú†Ïö©ÏÑ±ÏùÑ Î≥¥Ïó¨Ï§çÎãàÎã§.                               \n",
       "\n",
       "                                                       <span style=\"font-weight: bold\">Í≤∞Í≥º:</span>                                                       \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> ‚Ä¢ </span>LLaMA-GPT4Îäî text-davinci-003(Ï¶â, Alpaca)Î°ú ÌäúÎãùÌïú Í≤ΩÏö∞Î≥¥Îã§ ÎÜíÏùÄ ÏÑ±Îä•ÏùÑ Î≥¥Ïó¨Ï£ºÏóàÏßÄÎßå, Ïó¨Ï†ÑÌûà ÏÉÅÏóÖÏö© ÎåÄÌòï        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>Ï±óÎ¥á(e.g., GPT-4)Í≥º ÎπÑÍµêÌï¥ÏÑúÎäî Ï∞®Ïù¥Í∞Ä Ï°¥Ïû¨Ìï©ÎãàÎã§.                                                               \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> ‚Ä¢ </span><span style=\"font-weight: bold\">Ï§ëÍµ≠Ïñ¥ ÌèâÍ∞Ä</span>: ÏòÅÏñ¥ ÏùëÎãµÏùÑ Ï§ëÍµ≠Ïñ¥Î°ú Î≤àÏó≠Ìïú Í≤ÉÏù¥ÎÇò, Ï§ëÍµ≠Ïñ¥Î°ú ÏßÅÏ†ë ÏßàÎ¨∏ÌïòÏó¨ ÏÉùÏÑ±Îêú ÏùëÎãµÏùÑ ÎπÑÍµêÌñàÏùÑ Îïå, Î≤àÏó≠Îêú ÏùëÎãµÏù¥\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>Îçî Ïö∞ÏàòÌïú ÏÑ±Îä•ÏùÑ Î≥¥Ïó¨Ï£ºÏóàÏäµÎãàÎã§. Ïù¥Îäî GPT-4Í∞Ä ÏòÅÏñ¥ ÏΩîÌçºÏä§ÏóêÏÑú Îçî ÎßéÏù¥ ÌõàÎ†®ÎêòÏñ¥ ÏòÅÏñ¥ ÏßÄÏãúÎ¨∏ÏùÑ Îî∞Î•¥Îäî Îä•Î†•Ïù¥ Îçî   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>Í∞ïÎ†•ÌïòÍ∏∞ ÎïåÎ¨∏Ïùº Ïàò ÏûàÏäµÎãàÎã§.                                                                                    \n",
       "\n",
       "                             <span style=\"font-weight: bold\">ÎπÑÏ†ïÏÉÅ ÏßÄÏãúÎ¨∏(unnatural instructions)ÏóêÏÑúÏùò Í≤∞Í≥º(Í∑∏Î¶º 6):</span>                             \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> ‚Ä¢ </span>LLaMA-GPT4ÏôÄ GPT-4, Í∑∏Î¶¨Í≥† Alpaca Î™®Îç∏ÏùÑ ÎπÑÍµêÌñàÏùÑ Îïå, AlpacaÍ∞Ä Îã§Î•∏ Îëê Î™®Îç∏Ïóê ÎπÑÌï¥ ÌèâÍ∑† ROUGE-L Ï†êÏàòÏóêÏÑú        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>Ïö∞ÏàòÌïòÏòÄÏäµÎãàÎã§. Í∑∏Îü¨ÎÇò ÏùëÎãµÏùò Í∏∏Ïù¥Í∞Ä Í∏∏Ïñ¥ÏßàÏàòÎ°ù LLaMA-GPT4ÏôÄ GPT-4Í∞Ä Îçî ÎÜíÏùÄ ÏÑ±Îä•ÏùÑ Î≥¥Ïó¨Ï£ºÍ∏∞ ÏãúÏûëÌñàÏäµÎãàÎã§. Ïù¥Îäî \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>Îçî Ï∞ΩÏùòÏ†ÅÏù∏ ÏãúÎÇòÎ¶¨Ïò§ÏóêÏÑú Ïù¥ Î™®Îç∏Îì§Ïù¥ ÏßÄÏãúÎ¨∏ÏùÑ Îçî Ïûò Îî∞Î•º Ïàò ÏûàÎã§Îäî Í≤ÉÏùÑ ÏùòÎØ∏Ìï©ÎãàÎã§.                             \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> ‚Ä¢ </span>Í∞ÅÍ∏∞ Îã§Î•∏ ÏùëÎãµ Í∏∏Ïù¥ ÌïòÏúÑ ÏßëÌï©Ïóê ÎåÄÌï¥, LLaMA-GPT4Îäî GPT-4Ïùò ÌñâÎèôÏùÑ Í∞ÄÍπùÍ≤å Îî∞Î¶ÖÎãàÎã§. ÏùëÎãµ Í∏∏Ïù¥Í∞Ä ÏßßÏùÑ Îïå,         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>LLaMA-GPT4ÏôÄ GPT-4Îäî Îã®ÏàúÌïú Ï†ïÎãµÏùÑ Ìè¨Ìï®ÌïòÎêò, ÏùëÎãµÏùÑ Îçî ÎåÄÌôîÌòïÏúºÎ°ú ÎßåÎìúÎäî Ï∂îÍ∞ÄÏ†ÅÏù∏ Îã®Ïñ¥Î•º Ìè¨Ìï®ÌïòÍ∏∞ ÎïåÎ¨∏Ïóê, ÎÇÆÏùÄ  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>ROUGE-L Ï†êÏàòÎ•º Î∞õÏùÑ Í∞ÄÎä•ÏÑ±Ïù¥ ÏûàÏäµÎãàÎã§.                                                                          \n",
       "\n",
       "Ïù¥ Ïã§Ìóò Í≤∞Í≥ºÎäî LLaMA-GPT4Í∞Ä GPT-4Ïùò Ï∂úÎ†•ÏùÑ Í∏∞Î∞òÏúºÎ°ú ÌïôÏäµÌñàÏùÑ Îïå, ÏÉÅÎãπÌïú ÏÑ±Îä• Ìñ•ÏÉÅÏùÑ Ïù¥Î£∞ Ïàò ÏûàÏùåÏùÑ Î≥¥Ïó¨Ï£ºÎ©∞, Ïù¥Îäî  \n",
       "ÌòÅÏã†Ï†ÅÏù∏ ÏßÄÏãúÎ¨∏-Îî∞Î•¥Í∏∞ LLM Í∞úÎ∞úÏóê ÏûàÏñ¥ÏÑú Ïú†ÎßùÌïú Î∞©Ìñ•ÏûÑÏùÑ ÏãúÏÇ¨Ìï©ÎãàÎã§.                                               \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ</span>\n",
       "\n",
       "                                                  <span style=\"font-weight: bold; text-decoration: underline\">5 RELATED WORK</span>                                                   \n",
       "\n",
       "Ïù¥ ÏÑπÏÖòÏóêÏÑúÎäî ÏßÄÏãúÎ¨∏ ÌäúÎãùÍ≥º Ïò§ÌîàÏÜåÏä§ ÎÖ∏Î†•Ïù¥ Í¥ÄÎ†®Îêú Ïó∞Íµ¨Ïóê ÎåÄÌï¥ ÏÑ§Î™ÖÌï©ÎãàÎã§.                                         \n",
       "\n",
       "                                         <span style=\"font-weight: bold\">ÏßÄÏãúÎ¨∏ ÌäúÎãù(Instruction Tuning):</span>                                          \n",
       "\n",
       "ÏßÄÏãúÎ¨∏ ÌäúÎãùÏùÄ NLP Î∂ÑÏïºÏóêÏÑú Ï†êÏ†ê Ï£ºÎ™©Î∞õÍ≥† ÏûàÎäî Ïó∞Íµ¨ Î∞©Ìñ•ÏûÖÎãàÎã§. Í∏∞Ï°¥ Ïó∞Íµ¨Îì§ÏùÄ ÌÅ¨Í≤å ÏÑ∏ Í∞ÄÏßÄ ÏöîÏÜåÏù∏ ÏßÄÏãúÎ¨∏-Îî∞Î•¥Í∏∞     \n",
       "Îç∞Ïù¥ÌÑ∞, Í∏∞Î≥∏ Ïñ∏Ïñ¥ Î™®Îç∏, ÌèâÍ∞Ä Î≤§ÏπòÎßàÌÅ¨Ïùò ÌíàÏßàÍ≥º Í∑úÎ™®Î•º Ìñ•ÏÉÅÏãúÌÇ§Îäî Í≤ÉÏùÑ Î™©ÌëúÎ°ú Ìï©ÎãàÎã§. Í∞Å Ïó∞Íµ¨ Í∑∏Î£πÏùÄ ÏùºÎ∞òÏ†ÅÏúºÎ°ú     \n",
       "ÏûêÏ≤¥Ï†ÅÏù∏ Í∞úÎ∞ú ÌååÏù¥ÌîÑÎùºÏù∏ÏùÑ Ïú†ÏßÄÌï©ÎãàÎã§. ÏòàÎ•º Îì§Ïñ¥, ÌîåÎûú(FLAN)ÏùÑ Í∏∞Î∞òÏúºÎ°ú ÏßÄÏãúÎ¨∏ ÎØ∏ÏÑ∏ Ï°∞Ï†ï Ïñ∏Ïñ¥ Î™®Îç∏Ïùò Í∑úÎ™®Î•º        \n",
       "ÌôïÎåÄÌïòÎäî Ïó∞Íµ¨(Chung et al., 2022)ÎÇò, Ï†êÏ†ê ÎßéÏùÄ ÌîÑÎ°¨ÌîÑÌä∏Î•º Ìè¨Ìï®ÌïòÎäî PromptSource(Í≥µÍ≥µ ÌîÑÎ°¨ÌîÑÌä∏ ÌíÄ, P3)Ïùò Í∞úÎ∞ú(Bach  \n",
       "et al., 2022) Îì±Ïù¥ ÏûàÏäµÎãàÎã§. T0Îäî P3Ïóê Í∏∞Î∞òÌïú Îã§Ï§ë Í≥ºÏ†ú ÌîÑÎ°¨ÌîÑÌä∏ ÌõàÎ†®ÏùÑ ÌÜµÌï¥ Í∞úÎ∞úÎêú ÏùºÎ†®Ïùò Î™®Îç∏(Sanh et al.,       \n",
       "2021)ÏûÖÎãàÎã§. OPT Î™®Îç∏Ïùò ÏßÄÏãúÎ¨∏ ÌäúÎãùÏùÄ Îçî ÌÅ¨Í≥† Ìè¨Í¥ÑÏ†ÅÏù∏ Î≤§ÏπòÎßàÌÅ¨Ïù∏ OPT-IML BenchÎ•º ÏÇ¨Ïö©ÌïòÏó¨ ÏßÑÌñâÎêòÏóàÏúºÎ©∞, Ïù¥Îäî FLAN,\n",
       "Super-NaturalInstructions(Wang et al., 2022b), UnifiedSKG(Xie et al., 2022)ÏùÑ Ìè¨Ìï®Ìï©ÎãàÎã§.                          \n",
       "\n",
       "                                        <span style=\"font-weight: bold\">Ïò§ÌîàÏÜåÏä§ ÎÖ∏Î†•(Open-Source Efforts):</span>                                        \n",
       "\n",
       "ChatGPTÏùò Ìè≠ÎÑìÏùÄ Îä•Î†• ÎçïÎ∂ÑÏóê, Ïò§ÌîàÏÜåÏä§ Î™®Îç∏ÏùÄ ÌÅ∞ Í¥ÄÏã¨ÏùÑ Î∞õÏúºÎ©∞ Ïù∏Í∞ÑÏùò Í∞ÄÏπòÏóê ÎßûÎäî ÏùºÎ∞ò Î™©Ï†ÅÏùò ÌÖçÏä§Ìä∏ Í∏∞Î∞ò          \n",
       "Ïñ¥ÏãúÏä§ÌÑ¥Ìä∏Î•º Í∞úÎ∞úÌïòÎäî Îç∞ Í∏∞Ïó¨ÌïòÍ≥† ÏûàÏäµÎãàÎã§. Ï¥àÍ∏∞Ïùò Í∏∞Ï¥à LLM ÎÖ∏Î†•ÏóêÎäî BLOOM(Scao et al., 2022), GPT-J(Wang &amp;        \n",
       "Komatsuzaki, 2021), GPT-NEO(Black et al., 2021), OPT(Zhang et al., 2022), LLaMA(Zhang et al., 2023)Í∞Ä Ìè¨Ìï®Îê©ÎãàÎã§.  \n",
       "LLMÏùÑ ÎåÄÌôî Í∏∞Î∞ò Ïñ¥ÏãúÏä§ÌÑ¥Ïä§Ïóê ÎßûÏ∂îÍ∏∞ ÏúÑÌï¥ Open-AssistantÎäî GPT-JÎ•º Í∏∞Î∞òÏúºÎ°ú ÌïòÍ≥†, Alpaca/VicunaÎäî LLaMAÎ•º Í∏∞Î∞òÏúºÎ°ú  \n",
       "Íµ¨Ï∂ïÎêòÏóàÏäµÎãàÎã§. ÎòêÌïú, OpenFlamingo(Awadalla et al., 2023)ÏôÄ LLaMA-Adapter(Zhang et al., 2023)Îäî LLaMAÎ•º Ïù¥ÎØ∏ÏßÄ     \n",
       "ÏûÖÎ†•Í≥º Ïó∞Í≤∞ÌïòÏó¨ Ïò§ÌîàÏÜåÏä§ Î©ÄÌã∞Î™®Îã¨ LLMÏùÑ Íµ¨Ï∂ïÌï† Í∏∏ÏùÑ ÎßàÎ†®ÌñàÏäµÎãàÎã§.                                                  \n",
       "\n",
       "Ïù¥Îü¨Ìïú Ïó∞Íµ¨ÏôÄ ÎÖ∏Î†•ÏùÄ LLMÏù¥ Îçî Îã§ÏñëÌïòÍ≥† Í∞ïÎ†•Ìïú Îä•Î†•ÏùÑ Î∞úÌúòÌï† Ïàò ÏûàÎèÑÎ°ù ÌïòÍ≥†, Ïò§ÌîàÏÜåÏä§ ÌôòÍ≤ΩÏóêÏÑúÏùò Î∞úÏ†ÑÏùÑ Ï¥âÏßÑÌïòÎäî Îç∞ \n",
       "Í∑∏ Î™©Ï†ÅÏù¥ ÏûàÏäµÎãàÎã§.                                                                                                \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "                                                     \u001b[1;4mABSTRACT\u001b[0m                                                      \n",
       "\n",
       "Ïù¥ Ïó∞Íµ¨ÏóêÏÑúÎäî ÎåÄÌòï Ïñ∏Ïñ¥ Î™®Îç∏(LLM)ÏùÑ Í∏∞Í≥ÑÍ∞Ä ÏÉùÏÑ±Ìïú ÏßÄÏãúÎ¨∏ Îç∞Ïù¥ÌÑ∞Î•º ÏÇ¨Ïö©ÌïòÏó¨ ÎØ∏ÏÑ∏ Ï°∞Ï†ï(finetuning)ÌïòÎ©¥ ÏÉàÎ°úÏö¥        \n",
       "ÏûëÏóÖÏóêÏÑúÎèÑ Îõ∞Ïñ¥ÎÇú Ï†úÎ°úÏÉ∑(Zero-shot) ÏÑ±Îä•ÏùÑ Î∞úÌúòÌï† Ïàò ÏûàÎã§Îäî Í≤ÉÏùÑ Ïù¥Ï†Ñ Ïó∞Íµ¨Í∞Ä Î≥¥Ïó¨Ï£ºÏóàÎã§Í≥† ÏÑ§Î™ÖÌï©ÎãàÎã§. Ï¶â, Ïù∏Í∞ÑÏù¥   \n",
       "ÏûëÏÑ±Ìïú ÏßÄÏãúÎ¨∏Ïù¥ ÌïÑÏöî ÏóÜÍ≤å Îê©ÎãàÎã§. Î≥∏ ÎÖºÎ¨∏ÏùÄ GPT-4Î•º ÌôúÏö©ÌïòÏó¨ LLM ÎØ∏ÏÑ∏ Ï°∞Ï†ïÏùÑ ÏúÑÌïú ÏßÄÏãúÎ¨∏ Îç∞Ïù¥ÌÑ∞Î•º ÏÉùÏÑ±Ìïú Ï≤´ ÏãúÎèÑÎ•º \n",
       "ÏÜåÍ∞úÌï©ÎãàÎã§. Ïó∞Íµ¨ Í≤∞Í≥º, GPT-4Í∞Ä ÏÉùÏÑ±Ìïú 5Îßå 2Ï≤ú Í∞úÏùò ÏòÅÏñ¥ Î∞è Ï§ëÍµ≠Ïñ¥ ÏßÄÏãúÎ¨∏ Îç∞Ïù¥ÌÑ∞Í∞Ä Í∏∞Ï°¥ ÏµúÏ≤®Îã® Î™®Îç∏Îì§Ïù¥ ÏÉùÏÑ±Ìïú      \n",
       "ÏßÄÏãúÎ¨∏ Îç∞Ïù¥ÌÑ∞Î≥¥Îã§ ÏÉàÎ°úÏö¥ ÏûëÏóÖÏóêÏÑú Îõ∞Ïñ¥ÎÇú Ï†úÎ°úÏÉ∑ ÏÑ±Îä•ÏùÑ Î≥¥Ïó¨Ï£ºÏóàÏäµÎãàÎã§. ÎòêÌïú, Ìè¨Í¥ÑÏ†ÅÏù∏ ÌèâÍ∞Ä Î∞è Î≥¥ÏÉÅ Î™®Îç∏ ÌõàÎ†®ÏùÑ ÏúÑÌï¥\n",
       "GPT-4Î°úÎ∂ÄÌÑ∞ ÌîºÎìúÎ∞± Î∞è ÎπÑÍµê Îç∞Ïù¥ÌÑ∞Î•º ÏàòÏßëÌïòÏòÄÏäµÎãàÎã§. Ïö∞Î¶¨Îäî GPT-4Î•º ÏÇ¨Ïö©ÌïòÏó¨ ÏÉùÏÑ±Ìïú Îç∞Ïù¥ÌÑ∞ÏôÄ ÏΩîÎìúÎ≤†Ïù¥Ïä§Î•º Í≥µÍ∞úÌïòÍ≥†  \n",
       "ÏûàÏäµÎãàÎã§.                                                                                                          \n",
       "\n",
       "\u001b[33m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\n",
       "\n",
       "                                                   \u001b[1;4mINTRODUCTION\u001b[0m                                                    \n",
       "\n",
       "Ïù¥ ÎÖºÎ¨∏ÏùÄ ÎåÄÌòï Ïñ∏Ïñ¥ Î™®Îç∏(LLM)Ïùò Îõ∞Ïñ¥ÎÇú ÏùºÎ∞òÌôî Îä•Î†•ÏùÑ ÏÑ§Î™ÖÌïòÎ©∞, Ïù¥Îü¨Ìïú Î™®Îç∏Îì§Ïù¥ ÏûêÏó∞Ïñ¥ ÏßÄÏãúÎ•º Îî∞Î•¥Í≥† Ïã§Ï†ú ÏÑ∏ÏÉÅÏóêÏÑúÏùò\n",
       "ÏûëÏóÖÏùÑ ÏôÑÎ£åÌï† Ïàò ÏûàÎèÑÎ°ù ÌïòÎäî Î∞©Î≤ïÏùÑ ÌÉêÍµ¨ÌïòÍ≥† ÏûàÏäµÎãàÎã§. Í∑∏ Î∞©Î≤ï Ï§ë ÌïòÎÇòÎ°ú ÏßÄÏãúÎ¨∏ ÌäúÎãù(instruction-tuning)Ïù¥ ÏûàÏúºÎ©∞, \n",
       "Ïù¥Îäî Ïù∏Í∞ÑÏù¥ Ï£ºÏÑùÏùÑ Îã® ÌîÑÎ°¨ÌîÑÌä∏ÏôÄ ÌîºÎìúÎ∞±ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Î™®Îç∏ÏùÑ Îã§ÏñëÌïú ÏûëÏóÖÏóê ÎßûÍ≤å ÎØ∏ÏÑ∏ Ï°∞Ï†ïÌïòÍ±∞ÎÇò, ÏàòÎèô ÎòêÎäî ÏûêÎèôÏúºÎ°ú   \n",
       "ÏÉùÏÑ±Îêú ÏßÄÏãúÎ¨∏ÏùÑ Ï∂îÍ∞ÄÌïú Í≥µÍ∞ú Î≤§ÏπòÎßàÌÅ¨ÏôÄ Îç∞Ïù¥ÌÑ∞ÏÑ∏Ìä∏Î•º ÏÇ¨Ïö©ÌïòÎäî Î∞©ÏãùÏúºÎ°ú Íµ¨ÌòÑÎê©ÎãàÎã§.                                  \n",
       "\n",
       "ÌäπÌûà Self-Instruct ÌäúÎãùÏùÄ ÏµúÏã†Ïùò ÏßÄÏãúÎ¨∏ ÌäúÎãùÎêú ÍµêÏÇ¨ LLMÏù¥ ÏÉùÏÑ±Ìïú Îç∞Ïù¥ÌÑ∞Î•º ÌïôÏäµÌïòÏó¨, LLMÏùÑ Ïù∏Í∞Ñ ÏùòÎèÑÏóê ÎßûÏ∂îÎäî       \n",
       "Í∞ÑÎã®ÌïòÍ≥† Ìö®Í≥ºÏ†ÅÏù∏ Î∞©Î≤ïÏûÖÎãàÎã§. Ïù¥ Ïó∞Íµ¨ Î∞©Ìñ•ÏùÄ LLMÏùò Ï†úÎ°úÏÉ∑ Î∞è ÏÜåÏàòÏÉ∑ ÏùºÎ∞òÌôî Îä•Î†•ÏùÑ Ìñ•ÏÉÅÏãúÌÇ§Îäî Ìö®Í≥ºÏ†ÅÏù∏ ÏàòÎã®ÏùÑ Ï†úÍ≥µÌï¥\n",
       "ÏôîÏäµÎãàÎã§. ÏµúÍ∑º ChatGPTÏôÄ GPT-4Ïùò ÏÑ±Í≥µÏùÄ Ïù¥Îü¨Ìïú ÏßÄÏãúÎ¨∏ ÌäúÎãùÏùÑ ÌÜµÌï¥ Ïò§ÌîàÏÜåÏä§ LLMÏùÑ Í∞úÏÑ†Ìï† ÌÅ∞ Í∏∞ÌöåÎ•º Ï†úÍ≥µÌï©ÎãàÎã§.      \n",
       "LLaMAÎäî Ïù¥Îü¨Ìïú Ïò§ÌîàÏÜåÏä§ LLMÎì§ Ï§ë ÌïòÎÇòÎ°ú, ÏÉÅÏö© LLMÏù∏ GPT-3ÏôÄ ÏÑ±Îä•ÏùÑ Í≤¨Ï§Ñ Ïàò ÏûàÏäµÎãàÎã§. LLaMAÍ∞Ä ÏßÄÏãúÎ•º Îî∞Î•º Ïàò ÏûàÎèÑÎ°ù \n",
       "ÌïòÎ†§Î©¥, Îõ∞Ïñ¥ÎÇú ÏÑ±Îä•Í≥º Ï†ÄÎπÑÏö©Ïùò Self-Instruct ÌäúÎãùÏù¥ Îπ†Î•¥Í≤å Ï±ÑÌÉùÎêòÍ≥† ÏûàÏäµÎãàÎã§.                                      \n",
       "\n",
       "Î≥∏ ÎÖºÎ¨∏ÏóêÏÑú Ïö∞Î¶¨Îäî Ï≤òÏùåÏúºÎ°ú GPT-4Î•º Self-Instruct ÌäúÎãùÏùò ÍµêÏÇ¨Î°ú ÏÇ¨Ïö©ÌïòÎäî Î∞©Î≤ïÏùÑ Ï†úÏïàÌï©ÎãàÎã§. Ïö∞Î¶¨Ïùò Ï£ºÏöî Í∏∞Ïó¨Îäî     \n",
       "Îã§ÏùåÍ≥º Í∞ôÏäµÎãàÎã§:                                                                                                   \n",
       "\n",
       "\u001b[1;33m ‚Ä¢ \u001b[0m\u001b[1mGPT-4 Îç∞Ïù¥ÌÑ∞:\u001b[0m Ïö∞Î¶¨Îäî ÏòÅÏñ¥ÏôÄ Ï§ëÍµ≠Ïñ¥Î°ú Îêú 52K Í∞úÏùò ÏßÄÏãúÎ¨∏ Îç∞Ïù¥ÌÑ∞ÏÖãÍ≥º ÏÑ∏ Í∞úÏùò ÏßÄÏãúÎ¨∏ ÌäúÎãùÎêú Î™®Îç∏Ïùò Ï∂úÎ†•ÏùÑ ÌèâÍ∞ÄÌïú   \n",
       "\u001b[1;33m   \u001b[0mÌîºÎìúÎ∞± Îç∞Ïù¥ÌÑ∞Î•º Ìè¨Ìï®ÌïòÏó¨ GPT-4Í∞Ä ÏÉùÏÑ±Ìïú Îç∞Ïù¥ÌÑ∞Î•º Í≥µÍ∞úÌï©ÎãàÎã§.                                                    \n",
       "\u001b[1;33m ‚Ä¢ \u001b[0m\u001b[1mÎ™®Îç∏ Î∞è ÌèâÍ∞Ä:\u001b[0m GPT-4Í∞Ä ÏÉùÏÑ±Ìïú Îç∞Ïù¥ÌÑ∞Î•º Í∏∞Î∞òÏúºÎ°ú ÏßÄÏãúÎ¨∏ ÌäúÎãùÎêú LLaMA Î™®Îç∏Í≥º Î≥¥ÏÉÅ Î™®Îç∏ÏùÑ Í∞úÎ∞úÌñàÏäµÎãàÎã§. Ïö∞Î¶¨Îäî ÏÑ∏   \n",
       "\u001b[1;33m   \u001b[0mÍ∞ÄÏßÄ Í∏∞Ï§ÄÏóê ÎåÄÌïú Ïù∏Í∞Ñ ÌèâÍ∞Ä, GPT-4 ÌîºÎìúÎ∞±ÏùÑ ÏÇ¨Ïö©Ìïú ÏûêÎèô ÌèâÍ∞Ä, Í∑∏Î¶¨Í≥† ÎπÑÏ†ïÌòï ÏßÄÏãúÎ¨∏Ïóê ÎåÄÌïú ROUGE-L Ï†êÏàòÎ•º ÏÇ¨Ïö©ÌïòÏó¨\n",
       "\u001b[1;33m   \u001b[0mÏßÄÏãúÎ¨∏ ÌäúÎãùÎêú LLMÏùò ÌíàÏßàÏùÑ ÌèâÍ∞ÄÌï©ÎãàÎã§.                                                                          \n",
       "\n",
       "Ïù¥ Ïó∞Íµ¨Îäî GPT-4Î°ú ÏÉùÏÑ±Îêú Îç∞Ïù¥ÌÑ∞Î•º ÏÇ¨Ïö©Ìïú LLM ÏßÄÏãúÎ¨∏ ÌäúÎãùÏùò Ìö®Í≥ºÏÑ±ÏùÑ ÌôïÏù∏ÌïòÍ≥†, ÏùºÎ∞ò Î™©Ï†ÅÏùò ÏßÄÏãúÎ¨∏ÏùÑ Îî∞Î•¥Îäî          \n",
       "ÏóêÏù¥Ï†ÑÌä∏Î•º Íµ¨Ï∂ïÌïòÎäî Îç∞ ÏûàÏñ¥ Ïã§Ïö©Ï†ÅÏù∏ ÌåÅÏùÑ Ï†úÏïàÌï©ÎãàÎã§.                                                              \n",
       "\n",
       "\u001b[33m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\n",
       "\n",
       "                                                     \u001b[1;4m2 DATASET\u001b[0m                                                     \n",
       "\n",
       "Ïù¥ ÎÖºÎ¨∏Ïùò '2 Îç∞Ïù¥ÌÑ∞ÏÖã' ÏÑπÏÖòÏóêÏÑúÎäî Îç∞Ïù¥ÌÑ∞ ÏàòÏßë Í≥ºÏ†ïÍ≥º GPT-4Î•º ÏÇ¨Ïö©Ìïú Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± Î∞©Î≤ïÏóê ÎåÄÌï¥ ÏÑ§Î™ÖÌï©ÎãàÎã§.            \n",
       "\n",
       "Îç∞Ïù¥ÌÑ∞ ÏàòÏßë: Alpaca Îç∞Ïù¥ÌÑ∞ÏÖã(Taori et al., 2023)ÏóêÏÑú ÏàòÏßëÎêú 52,000Í∞úÏùò Í≥†Ïú†Ìïú ÏßÄÏãúÎ¨∏ÏùÑ Ïû¨ÏÇ¨Ïö©Ìï©ÎãàÎã§. Í∞Å ÏßÄÏãúÎ¨∏ÏùÄ   \n",
       "Î™®Îç∏Ïù¥ ÏàòÌñâÌï¥Ïïº Ìï† ÏûëÏóÖÏùÑ ÏÑ§Î™ÖÌï©ÎãàÎã§. ÏßÄÏãúÎ¨∏ÏóêÎäî ÏûëÏóÖÏóê ÎåÄÌïú ÏÑ†ÌÉùÏ†Å Îß•ÎùΩÏù¥ÎÇò ÏûÖÎ†•Ïù¥ ÏûàÏùÑ ÏàòÎèÑ ÏûàÍ≥† ÏóÜÏùÑ ÏàòÎèÑ       \n",
       "ÏûàÏäµÎãàÎã§. Alpaca Îç∞Ïù¥ÌÑ∞ÏÖãÏóêÏÑúÎäî GPT-3.5(text-davinci-003)Î•º ÏÇ¨Ïö©Ìï¥ Ï∂úÎ†•ÏùÑ ÏÉùÏÑ±ÌñàÏßÄÎßå, Ïö∞Î¶¨Îäî GPT-4(gpt-4)Î•º        \n",
       "ÌôúÏö©ÌïòÏó¨ Ï∂úÎ†•ÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§. GPT-4Î•º ÏÇ¨Ïö©Ìï¥ Îã§Ïùå ÎÑ§ Í∞ÄÏßÄ Îç∞Ïù¥ÌÑ∞ÏÖãÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§.                                     \n",
       "\n",
       "\u001b[1;33m 1 \u001b[0m\u001b[1mÏòÅÏñ¥ ÏßÄÏãúÎ¨∏-ÎãµÎ≥Ä Îç∞Ïù¥ÌÑ∞:\u001b[0m AlpacaÏóêÏÑú ÏàòÏßëÌïú 52,000Í∞úÏùò ÏßÄÏãúÎ¨∏ Í∞ÅÍ∞ÅÏóê ÎåÄÌï¥ GPT-4Í∞Ä ÏòÅÏñ¥Î°ú ÎãµÎ≥ÄÏùÑ Ï†úÍ≥µÌï©ÎãàÎã§.      \n",
       "\u001b[1;33m   \u001b[0mÏûêÏÑ∏Ìïú ÎÇ¥Ïö©ÏùÄ ÏïåÍ≥†Î¶¨Ï¶ò 1Ïóê ÎÇòÏôÄ ÏûàÏäµÎãàÎã§. GPT-4ÏôÄ Self-InstructÎ•º ÏÇ¨Ïö©ÌïòÏó¨ ÏûêÏ≤¥ ÏßÄÏãú ÏÑ∏Ìä∏Î•º Íµ¨ÏÑ±ÌïòÎäî Î∞òÎ≥µÏ†Å     \n",
       "\u001b[1;33m   \u001b[0mÍ≥ºÏ†ïÏùÑ Ìñ•ÌõÑ ÏûëÏóÖÏúºÎ°ú ÎÇ®Í≤®Îë°ÎãàÎã§.                                                                                \n",
       "\u001b[1;33m 2 \u001b[0m\u001b[1mÏ§ëÍµ≠Ïñ¥ ÏßÄÏãúÎ¨∏-ÎãµÎ≥Ä Îç∞Ïù¥ÌÑ∞:\u001b[0m ChatGPTÎ•º ÏÇ¨Ïö©ÌïòÏó¨ 52,000Í∞úÏùò ÏßÄÏãúÎ¨∏ÏùÑ Ï§ëÍµ≠Ïñ¥Î°ú Î≤àÏó≠ÌïòÍ≥†, GPT-4Í∞Ä Ï§ëÍµ≠Ïñ¥Î°ú ÎãµÎ≥ÄÌïòÍ≤å  \n",
       "\u001b[1;33m   \u001b[0mÌï©ÎãàÎã§. Ïù¥Î•º ÌÜµÌï¥ LLaMAÏóê Í∏∞Î∞òÌïú Ï§ëÍµ≠Ïñ¥ ÏßÄÏãúÎ¨∏-Îî∞Î•¥Í∏∞ Î™®Îç∏ÏùÑ Íµ¨Ï∂ïÌïòÍ≥†, ÏßÄÏãúÎ¨∏ ÌäúÎãùÏùò Ïñ∏Ïñ¥ Í∞Ñ ÏùºÎ∞òÌôî Îä•Î†•ÏùÑ      \n",
       "\u001b[1;33m   \u001b[0mÏó∞Íµ¨Ìï† Ïàò ÏûàÏäµÎãàÎã§.                                                                                             \n",
       "\u001b[1;33m 3 \u001b[0m\u001b[1mÎπÑÍµê Îç∞Ïù¥ÌÑ∞:\u001b[0m GPT-4ÏóêÍ≤å ÏûêÏã†Ïùò ÏùëÎãµÏùÑ 1Î∂ÄÌÑ∞ 10ÍπåÏßÄ ÌèâÍ∞ÄÌïòÎèÑÎ°ù ÏöîÏ≤≠Ìï©ÎãàÎã§. ÎòêÌïú, GPT-4ÏóêÍ≤å GPT-4, GPT-3.5, Í∑∏Î¶¨Í≥† \n",
       "\u001b[1;33m   \u001b[0mOPT-IMLÏùÑ Ìè¨Ìï®Ìïú ÏÑ∏ Î™®Îç∏Ïùò ÏùëÎãµÏùÑ ÎπÑÍµêÌïòÏó¨ ÌèâÍ∞ÄÌïòÎèÑÎ°ù ÏöîÏ≤≠Ìï©ÎãàÎã§. Ïù¥Îäî Î≥¥ÏÉÅ Î™®Îç∏ÏùÑ ÌõàÎ†®ÌïòÎäî Îç∞ ÏÇ¨Ïö©Îê©ÎãàÎã§.      \n",
       "\u001b[1;33m 4 \u001b[0m\u001b[1mÎπÑÏ†ïÏÉÅ ÏßÄÏãúÎ¨∏Ïóê ÎåÄÌïú ÎãµÎ≥Ä:\u001b[0m 68,000Í∞úÏùò ÏßÄÏãúÎ¨∏-ÏûÖÎ†•-Ï∂úÎ†• ÏÇºÏ§ë ÏΩîÏñ¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏóê ÎåÄÌï¥ GPT-4Ïùò ÎãµÎ≥ÄÏùÑ ÎîîÏΩîÎî©Ìï©ÎãàÎã§. Ïù¥\n",
       "\u001b[1;33m   \u001b[0mÌïòÏúÑ ÏÑ∏Ìä∏Îäî GPT-4ÏôÄ Ïö∞Î¶¨ ÏßÄÏãúÎ¨∏ ÌäúÎãùÎêú Î™®Îç∏Îì§ Í∞ÑÏùò Ï∞®Ïù¥Î•º Í≥ÑÎüâÌôîÌïòÎäî Îç∞ ÏÇ¨Ïö©Îê©ÎãàÎã§.                             \n",
       "\n",
       "Îç∞Ïù¥ÌÑ∞ ÌÜµÍ≥Ñ: Í∑∏Î¶º 1ÏóêÏÑúÎäî GPT-4ÏôÄ GPT-3.5Ïùò ÏòÅÏñ¥ Ï∂úÎ†• ÏùëÎãµ ÏÑ∏Ìä∏Î•º ÎπÑÍµêÌï©ÎãàÎã§. Í∞Å Ï∂úÎ†•Ïóê ÎåÄÌï¥ ÎèôÏÇ¨ ÎøåÎ¶¨ÏôÄ ÏßÅÏ†ë      \n",
       "Î™©Ï†ÅÏñ¥ Î™ÖÏÇ¨Î•º Ï∂îÏ∂úÌïòÍ≥†, Í∞Å Ï∂úÎ†• ÏÑ∏Ìä∏ÏóêÏÑú Í≥†Ïú†Ìïú ÎèôÏÇ¨-Î™ÖÏÇ¨ ÏåçÏùò ÎπàÎèÑÎ•º Í≥ÑÏÇ∞Ìï©ÎãàÎã§. ÎπàÎèÑÍ∞Ä 10 Ïù¥ÏÉÅÏù∏ ÎèôÏÇ¨-Î™ÖÏÇ¨ ÏåçÏùÄ  \n",
       "Í∑∏Î¶º 1(a)ÏôÄ (b)Ïóê ÌëúÏãúÎêòÎ©∞, Îëê ÏÑ∏Ìä∏Ïùò Í∞ÄÏû• ÎπàÎ≤àÌïú 25Í∞úÏùò ÏåçÏùÑ Í∑∏Î¶º 1(c)Ïóê ÎπÑÍµêÌï©ÎãàÎã§. Ï∂úÎ†• ÏãúÌÄÄÏä§ Í∏∏Ïù¥Ïùò ÎπàÎèÑ      \n",
       "Î∂ÑÌè¨Îäî Í∑∏Î¶º 1(d)Ïóê ÎπÑÍµêÎêòÏñ¥ ÏûàÏäµÎãàÎã§. GPT-4Îäî GPT-3.5Î≥¥Îã§ Îçî Í∏¥ ÏãúÌÄÄÏä§Î•º ÏÉùÏÑ±ÌïòÎäî Í≤ΩÌñ•Ïù¥ ÏûàÏäµÎãàÎã§. AlpacaÏùò GPT-3.5\n",
       "Îç∞Ïù¥ÌÑ∞Îäî Î∞òÎ≥µÏ†ÅÏù∏ Îç∞Ïù¥ÌÑ∞ ÏàòÏßë Í≥ºÏ†ïÏùÑ ÌÜµÌï¥ Í∞Å Î∞òÎ≥µÏóêÏÑú Ïú†ÏÇ¨Ìïú ÏßÄÏãúÎ¨∏ Ïù∏Ïä§ÌÑ¥Ïä§Î•º Ï†úÍ±∞ÌïòÏó¨ Îçî Í∏¥ Íº¨Î¶¨Î•º Í∞ÄÏßÑ Ï∂úÎ†•     \n",
       "Î∂ÑÌè¨Î•º Î≥¥ÏûÖÎãàÎã§. Ïù¥Îäî ÌòÑ ÏãúÏ†êÏùò ÏùºÌöåÏÑ± Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± Í≥ºÏ†ïÏóêÏÑúÎäî ÏóÜÎäî ÌäπÏßïÏûÖÎãàÎã§. Ïù¥Îü¨Ìïú Í∞ÑÎã®Ìïú Í≥ºÏ†ïÏóêÎèÑ Î∂àÍµ¨ÌïòÍ≥†,    \n",
       "GPT-4Í∞Ä ÏÉùÏÑ±Ìïú ÏßÄÏãúÎ¨∏-Îî∞Î•¥Í∏∞ Îç∞Ïù¥ÌÑ∞Îäî Ïù¥ÌõÑ Ïã§ÌóòÏóêÏÑú Îçî ÎÇòÏùÄ Ï†ïÎ†¨ ÏÑ±Îä•ÏùÑ Î≥¥Ïó¨Ï§çÎãàÎã§.                                \n",
       "\n",
       "\u001b[33m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\n",
       "\n",
       "                                       \u001b[1;4m3 INSTRUCTION-TUNING LANGUAGE MODELS\u001b[0m                                        \n",
       "\n",
       "I apologize, but it seems that the '3 INSTRUCTION-TUNING LANGUAGE MODELS' section content is missing from your     \n",
       "request. If you can provide the full text of that section, I will be happy to explain it in detail.                \n",
       "\n",
       "\u001b[33m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\n",
       "\n",
       "                                             \u001b[1;4m3.1 SELF-INSTRUCT TUNING\u001b[0m                                              \n",
       "\n",
       "Ïù¥ ÏÑπÏÖòÏóêÏÑúÎäî Self-Instruct ÌäúÎãùÏùÑ ÌÜµÌï¥ Ïñ∏Ïñ¥ Î™®Îç∏ÏùÑ ÌõàÎ†®ÌïòÎäî Í≥ºÏ†ïÏóê ÎåÄÌï¥ ÏÑ§Î™ÖÌï©ÎãàÎã§. Ïó∞Íµ¨ÏóêÏÑúÎäî LLaMA 7B           \n",
       "Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏Î•º ÏÇ¨Ïö©ÌïòÏó¨ Îëê Í∞ÄÏßÄ Î™®Îç∏ÏùÑ ÏßÄÎèÑ ÌïôÏäµ Î∞©ÏãùÏúºÎ°ú ÎØ∏ÏÑ∏ Ï°∞Ï†ïÌñàÏäµÎãàÎã§.                                         \n",
       "\n",
       "\u001b[1;33m 1 \u001b[0m\u001b[1mLLaMA-GPT4 Î™®Îç∏\u001b[0m: Ïù¥ Î™®Îç∏ÏùÄ GPT-4Í∞Ä ÏÉùÏÑ±Ìïú 52,000Í∞úÏùò ÏòÅÏñ¥ ÏßÄÏãúÎ¨∏-Îî∞Î•¥Í∏∞ Îç∞Ïù¥ÌÑ∞Î•º Î∞îÌÉïÏúºÎ°ú ÌõàÎ†®ÎêòÏóàÏäµÎãàÎã§. Ïù¥    \n",
       "\u001b[1;33m   \u001b[0mÎç∞Ïù¥ÌÑ∞Ïùò Î∂ÑÌè¨Îäî Ïù¥Ï†ÑÏùò Í∑∏Î¶º 1ÏóêÏÑú Î≥¥Ïó¨Ï£ºÏóàÏäµÎãàÎã§.                                                               \n",
       "\u001b[1;33m 2 \u001b[0m\u001b[1mLLaMA-GPT4-CN Î™®Îç∏\u001b[0m: Ïù¥ Î™®Îç∏ÏùÄ GPT-4Í∞Ä ÏÉùÏÑ±Ìïú 52,000Í∞úÏùò Ï§ëÍµ≠Ïñ¥ ÏßÄÏãúÎ¨∏-Îî∞Î•¥Í∏∞ Îç∞Ïù¥ÌÑ∞Î•º Î∞îÌÉïÏúºÎ°ú ÌõàÎ†®ÎêòÏóàÏäµÎãàÎã§.  \n",
       "\n",
       "ÌõàÎ†® Í≥ºÏ†ïÏóêÏÑúÎäî Í≥µÏ†ïÌïú ÎπÑÍµêÎ•º ÏúÑÌï¥ Taori et al.(2023)Ïùò ÌõàÎ†® Ïä§ÏºÄÏ§ÑÏùÑ Îî∞ÎûêÏäµÎãàÎã§. Ïù¥Îü¨Ìïú Î™®Îç∏Îì§ÏùÄ GPT-4Í∞Ä ÏÉùÏÑ±Ìïú   \n",
       "Îç∞Ïù¥ÌÑ∞Ïùò ÌíàÏßàÏùÑ Ïó∞Íµ¨ÌïòÍ≥†, ÌïòÎÇòÏùò Ïñ∏Ïñ¥Î°ú ÏßÄÏãúÎ¨∏ ÌäúÎãùÏùÑ ÌñàÏùÑ ÎïåÏùò ÍµêÏ∞® Ïñ∏Ïñ¥ ÏùºÎ∞òÌôî ÌäπÏÑ±ÏùÑ Ïó∞Íµ¨ÌïòÎäî Îç∞ ÏÇ¨Ïö©Îê©ÎãàÎã§.    \n",
       "\n",
       "\u001b[33m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\n",
       "\n",
       "                                                 \u001b[1;4m3.2 REWARD MODELS\u001b[0m                                                 \n",
       "\n",
       "Ïù¥ ÏÑπÏÖòÏóêÏÑúÎäî Ïù∏Í∞Ñ ÌîºÎìúÎ∞±ÏùÑ ÌÜµÌïú Í∞ïÌôî ÌïôÏäµ(RLHF, Reinforcement Learning from Human Feedback)Ïùò ÌïµÏã¨ ÏöîÏÜåÏù∏ Î≥¥ÏÉÅ    \n",
       "Î™®Îç∏ÎßÅÏóê ÎåÄÌï¥ ÏÑ§Î™ÖÌï©ÎãàÎã§. RLHFÎäî LLMÏùò ÌñâÎèôÏùÑ Ïù∏Í∞ÑÏùò ÏÑ†Ìò∏ÏôÄ ÎßûÏ∂∞ Î™®Îç∏ÏùÑ ÎçîÏö± Ïú†Ïö©ÌïòÍ≤å ÎßåÎì§Î†§Îäî Î™©Ï†ÅÏùÑ Í∞ÄÏßÄÍ≥†       \n",
       "ÏûàÏäµÎãàÎã§. Î≥¥ÏÉÅ Î™®Îç∏ÎßÅÏùÄ ÌîÑÎ°¨ÌîÑÌä∏ÏôÄ ÏùëÎãµÏù¥ Ï£ºÏñ¥Ï°åÏùÑ Îïå Ïä§ÏπºÎùº Î≥¥ÏÉÅÏùÑ ÏòàÏ∏°ÌïòÎäî ÌöåÍ∑Ä Í≥ºÏ†úÎ°ú Î¨∏Ï†úÎ•º Íµ¨ÏÑ±Ìï©ÎãàÎã§. Ïù¥Î†áÍ≤å \n",
       "ÌïòÎ†§Î©¥ ÎåÄÍ∑úÎ™® ÎπÑÍµê Îç∞Ïù¥ÌÑ∞Í∞Ä ÌïÑÏöîÌïúÎç∞, Îëê Í∞ÄÏßÄ Î™®Îç∏Ïùò ÏùëÎãµÏùÑ Í∞ôÏùÄ ÌîÑÎ°¨ÌîÑÌä∏Ïóê ÎåÄÌï¥ ÎπÑÍµêÌïòÎäî Î∞©ÏãùÏûÖÎãàÎã§. ÌïòÏßÄÎßå       \n",
       "Alpaca, Vicuna, Dolly Îì±Ïùò Í∏∞Ï°¥ Ïò§ÌîàÏÜåÏä§ ÏûëÏóÖÏùÄ ÎπÑÍµê Îç∞Ïù¥ÌÑ∞ Î†àÏù¥Î∏îÎßÅ ÎπÑÏö©Ïù¥ ÎÜíÏïÑ RLHFÎ•º Ìè¨Ìï®ÌïòÏßÄ ÏïäÏäµÎãàÎã§.         \n",
       "\n",
       "ÌïúÌé∏, ÏµúÍ∑º Ïó∞Íµ¨Îäî GPT-4Í∞Ä ÏûêÏã†Ïùò Ïã§ÏàòÎ•º ÏãùÎ≥ÑÌïòÍ≥† ÏàòÏ†ïÌïòÎ©∞, ÏùëÎãµ ÌíàÏßàÏùÑ Ï†ïÌôïÌûà ÌèâÍ∞ÄÌï† Ïàò ÏûàÏùåÏùÑ Î≥¥Ïó¨Ï£ºÏóàÏäµÎãàÎã§.     \n",
       "Îî∞ÎùºÏÑú RLHF Ïó∞Íµ¨Î•º Ï¥âÏßÑÌïòÍ∏∞ ÏúÑÌï¥, Ïö∞Î¶¨Îäî 2Ï†àÏóêÏÑú ÏÑ§Î™ÖÌïú Î∞îÏôÄ Í∞ôÏù¥ GPT-4Î•º ÏÇ¨Ïö©ÌïòÏó¨ ÎπÑÍµê Îç∞Ïù¥ÌÑ∞Î•º ÏÉùÏÑ±ÌñàÏäµÎãàÎã§.     \n",
       "\n",
       "Îç∞Ïù¥ÌÑ∞ ÌíàÏßà ÌèâÍ∞ÄÎ•º ÏúÑÌï¥, Ïö∞Î¶¨Îäî OPT 1.3B Í∏∞Î∞òÏúºÎ°ú Î≥¥ÏÉÅ Î™®Îç∏ÏùÑ ÌõàÎ†®ÌïòÏó¨ ÏÑúÎ°ú Îã§Î•∏ ÏùëÎãµÎì§ÏùÑ ÌèâÍ∞ÄÌï©ÎãàÎã§. Í∞Å ÎπÑÍµê      \n",
       "Îç∞Ïù¥ÌÑ∞ Ïù∏Ïä§ÌÑ¥Ïä§Îäî Ìïú ÌîÑÎ°¨ÌîÑÌä∏ $\\textbf{\\em x}$ÏôÄ $K$Í∞úÏùò ÏùëÎãµÏùÑ Ìè¨Ìï®ÌïòÍ≥†, GPT-4Îäî Í∞Å ÏùëÎãµÏóê ÎåÄÌï¥ 1Î∂ÄÌÑ∞ 10ÍπåÏßÄÏùò    \n",
       "Ï†êÏàò $s$Î•º Ï§çÎãàÎã§. Ïù¥ Ïù∏Ïä§ÌÑ¥Ïä§ÏóêÏÑú Í≥†Ïú†Ìïú Ïåç $C_{2}^{K}$Î•º Íµ¨ÏÑ±Ìï† Ïàò ÏûàÏúºÎ©∞, Í∞Å ÏåçÏùÄ $(y_{l}, y_{h})$Î°ú ÌëúÌòÑÎê©ÎãàÎã§.\n",
       "Ïó¨Í∏∞ÏÑú $y_{l}$Ïùò Ï†êÏàòÎäî $y_{h}$Ïùò Ï†êÏàòÎ≥¥Îã§ ÎÇÆÏäµÎãàÎã§($s_{l} < s_{h}$). Î≥¥ÏÉÅ Î™®Îç∏ $r_{\\theta}$Îäî Îã§Ïùå Î™©ÌëúÎ°ú         \n",
       "ÌõàÎ†®Îê©ÎãàÎã§: $\\operatorname*{min}\\log(\\sigma(r_{\\pmb\\theta}(x,y_{h})-r_{\\pmb\\theta}(\\pmb x,y_{l})))$, Ïó¨Í∏∞ÏÑú        \n",
       "$\\sigma$Îäî ÏãúÍ∑∏Î™®Ïù¥Îìú Ìï®ÏàòÏûÖÎãàÎã§. ÎπÑÍµê Îç∞Ïù¥ÌÑ∞Ïùò Î∂ÑÌè¨Îäî Í∑∏Î¶º 2Ïóê ÎÇòÌÉÄÎÇò ÏûàÏäµÎãàÎã§.                                   \n",
       "\n",
       "Ïù¥ Ï†ëÍ∑º Î∞©ÏãùÏùÄ Î≥¥ÏÉÅ Î™®Îç∏Ïù¥ Îã§ÏñëÌïú ÏùëÎãµ ÌíàÏßàÏùÑ ÌèâÍ∞ÄÌïòÍ≥† RLHFÏùò Ìö®Í≥ºÏÑ±ÏùÑ ÎÜíÏù¥Îäî Îç∞ ÎèÑÏõÄÏùÑ Ï§Ñ Ïàò ÏûàÎèÑÎ°ù               \n",
       "ÏÑ§Í≥ÑÎêòÏóàÏäµÎãàÎã§.                                                                                                    \n",
       "\n",
       "\u001b[33m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\n",
       "\n",
       "                                              \u001b[1;4m4 EXPERIMENTAL RESULTS\u001b[0m                                               \n",
       "\n",
       "It looks like the content from the '4 EXPERIMENTAL RESULTS' section is missing from your request. If you can       \n",
       "provide the full text of that section, I would be happy to explain it in detail.                                   \n",
       "\n",
       "\u001b[33m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\n",
       "\n",
       "                                                  \u001b[1;4m4.1 BENCHMARKS\u001b[0m                                                   \n",
       "\n",
       "Ïù¥ ÏÑπÏÖòÏóêÏÑúÎäî LLM(ÎåÄÌòï Ïñ∏Ïñ¥ Î™®Îç∏)Ïùò ÏÑ±Îä•ÏùÑ ÌèâÍ∞ÄÌïòÍ∏∞ ÏúÑÌïú Î≤§ÏπòÎßàÌÅ¨Î•º ÏÜåÍ∞úÌïòÍ≥†, Ïù∏Í∞Ñ ÌèâÍ∞Ä Î∞©Î≤ïÍ≥º Í∑∏ Í≤∞Í≥ºÎ•º           \n",
       "ÏÑ§Î™ÖÌï©ÎãàÎã§.                                                                                                        \n",
       "\n",
       "                                                     \u001b[1mÎ≤§ÏπòÎßàÌÅ¨:\u001b[0m                                                     \n",
       "\n",
       "Ïù¥ Ïó∞Íµ¨Ïùò Î™©ÌëúÎäî GPT-4 Îç∞Ïù¥ÌÑ∞Î•º Í∏∞Î∞òÏúºÎ°ú ÌïôÏäµÎêú Self-Instruct Î™®Îç∏Îì§Ïù¥ ÏÉàÎ°úÏö¥ ÏßÄÏãúÎ¨∏Ïóê ÎåÄÌï¥ ÏñºÎßàÎÇò Ïûò Î∞òÏùëÌïòÎäîÏßÄ   \n",
       "ÌèâÍ∞ÄÌïòÎäî Í≤ÉÏûÖÎãàÎã§. Ïó∞Íµ¨ÏóêÏÑú ÏÇ¨Ïö©Ìïú Îç∞Ïù¥ÌÑ∞ÏÖãÏùÄ Îã§ÏùåÍ≥º Í∞ôÏäµÎãàÎã§:                                                     \n",
       "\n",
       "\u001b[1;33m ‚Ä¢ \u001b[0m\u001b[1mUser-Oriented-Instructions- $252^{,2}$\u001b[0m: Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùÄ 71Í∞úÏùò ÏÇ¨Ïö©Ïûê Ï§ëÏã¨ Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖò(Ïòà: Grammarly,            \n",
       "\u001b[1;33m   \u001b[0mStackOverflow, Overleaf)ÏùÑ Í∏∞Î∞òÏúºÎ°ú Ìïú 252Í∞úÏùò ÏßÄÏãúÎ¨∏ÏùÑ Ìè¨Ìï®Ìï©ÎãàÎã§.                                             \n",
       "\u001b[1;33m ‚Ä¢ \u001b[0m\u001b[1mVicuna-Instructions- $80^{3}$\u001b[0m: GPT-4Í∞Ä ÏÉùÏÑ±Ìïú 80Í∞úÏùò ÎèÑÏ†ÑÏ†ÅÏù∏ ÏßàÎ¨∏Ïù¥ Ìè¨Ìï®ÎêòÏñ¥ ÏûàÏúºÎ©∞, Í∏∞Î≥∏ Î™®Îç∏Îì§Ïù¥ Ïñ¥Î†§ÏõåÌïòÎäî  \n",
       "\u001b[1;33m   \u001b[0mÏßàÎ¨∏Îì§ÏûÖÎãàÎã§. ÏßÄÏãù, ÏàòÌïô, Fermi Î¨∏Ï†ú, Î∞òÏÇ¨Ïã§Ï†Å ÏãúÎÇòÎ¶¨Ïò§, Î°§ÌîåÎ†àÏûâ, ÏùºÎ∞ò, ÏΩîÎî©, Í∏ÄÏì∞Í∏∞, ÏÉÅÏãù Îì± 8Í∞úÏùò Ïπ¥ÌÖåÍ≥†Î¶¨Í∞Ä \n",
       "\u001b[1;33m   \u001b[0mÏûàÏäµÎãàÎã§.                                                                                                       \n",
       "\u001b[1;33m ‚Ä¢ \u001b[0m\u001b[1mUnnatural Instructions\u001b[0m: 68,478Í∞úÏùò ÏÉòÌîåÎ°ú Íµ¨ÏÑ±Îêú Ïù¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùÄ 15Í∞úÏùò ÏàòÏûëÏóÖ ÏòàÏ†úÎ•º ÏÇ¨Ïö©Ìïú 3-ÏÉ∑ Îß•ÎùΩ ÌïôÏäµÏúºÎ°ú   \n",
       "\u001b[1;33m   \u001b[0mtext-davinci-002Í∞Ä Ìï©ÏÑ±Ìïú Í≤ÉÏûÖÎãàÎã§.                                                                             \n",
       "\n",
       "                                                    \u001b[1mÏù∏Í∞Ñ ÌèâÍ∞Ä:\u001b[0m                                                     \n",
       "\n",
       "Î™®Îç∏Ïùò Ï†ïÎ†¨ ÌíàÏßàÏùÑ ÌèâÍ∞ÄÌïòÍ∏∞ ÏúÑÌï¥ Anthropic Askell et al.(2021)ÏóêÏÑú Ï†úÏïàÌïú Ï†ïÎ†¨ Í∏∞Ï§ÄÏùÑ Îî∞ÎûêÏäµÎãàÎã§. Ïù¥ Í∏∞Ï§ÄÏùÄ AI     \n",
       "ÏãúÏä§ÌÖúÏù¥ Ïù∏Í∞ÑÏùò Í∞ÄÏπòÏóê ÏñºÎßàÎÇò Ïûò Ï†ïÎ†¨ÎêòÏñ¥ ÏûàÎäîÏßÄÎ•º ÌèâÍ∞ÄÌïòÎäî Îç∞ ÏÇ¨Ïö©Îê©ÎãàÎã§. Í∏∞Ï§ÄÏùÄ Îã§ÏùåÍ≥º Í∞ôÏäµÎãàÎã§:                 \n",
       "\n",
       "\u001b[1;33m ‚Ä¢ \u001b[0m\u001b[1mÎèÑÏõÄÎê®\u001b[0m: Ïù∏Í∞ÑÏù¥ Î™©ÌëúÎ•º Îã¨ÏÑ±ÌïòÎäî Îç∞ ÎèÑÏõÄÏù¥ ÎêòÎäîÏßÄ Ïó¨Î∂ÄÎ•º ÌèâÍ∞ÄÌï©ÎãàÎã§.                                              \n",
       "\u001b[1;33m ‚Ä¢ \u001b[0m\u001b[1mÏ†ïÏßÅÏÑ±\u001b[0m: Ï†ïÌôïÌïú Ï†ïÎ≥¥Î•º Ï†úÍ≥µÌïòÍ≥† ÌïÑÏöîÏãú Î∂àÌôïÏã§ÏÑ±ÏùÑ ÌëúÌòÑÌïòÏó¨ Ïù∏Í∞Ñ ÏÇ¨Ïö©ÏûêÎ•º Ïò§ÎèÑÌïòÏßÄ ÏïäÎäîÏßÄÎ•º ÌèâÍ∞ÄÌï©ÎãàÎã§.           \n",
       "\u001b[1;33m ‚Ä¢ \u001b[0m\u001b[1mÎ¨¥Ìï¥ÏÑ±\u001b[0m: Ïù∏Í∞ÑÏóêÍ≤å Ìï¥Î•º ÎÅºÏπòÏßÄ ÏïäÎäîÏßÄÎ•º ÌèâÍ∞ÄÌï©ÎãàÎã§.                                                               \n",
       "\n",
       "Amazon Mechanical TurkÎ•º ÌÜµÌï¥ Ïù∏Í∞Ñ ÌèâÍ∞ÄÎ•º ÏàòÌñâÌñàÏúºÎ©∞, Í≤∞Í≥ºÎäî Í∑∏Î¶º 3Ïùò ÌååÏù¥ Ï∞®Ìä∏Ïóê ÎÇòÌÉÄÎÇò ÏûàÏäµÎãàÎã§.                 \n",
       "\n",
       "\u001b[1;33m ‚Ä¢ \u001b[0m\u001b[1mÏ≤´ Î≤àÏß∏ ÎπÑÍµê\u001b[0m: GPT-4 Îç∞Ïù¥ÌÑ∞Î°ú ÎØ∏ÏÑ∏ Ï°∞Ï†ïÎêú LLaMA Î™®Îç∏Í≥º GPT-3 Îç∞Ïù¥ÌÑ∞Î°ú ÎØ∏ÏÑ∏ Ï°∞Ï†ïÎêú LLaMA Î™®Îç∏(Stanford Alpaca     \n",
       "\u001b[1;33m   \u001b[0mÎ™®Îç∏)ÏùÑ ÎπÑÍµêÌïòÏó¨, \"ÎèÑÏõÄÎê®\" Í∏∞Ï§ÄÏóêÏÑú GPT-4Í∞Ä 54.12%Î°ú Ïö∞ÏÑ∏ÌñàÏúºÎ©∞, \"Ï†ïÏßÅÏÑ±\"Í≥º \"Î¨¥Ìï¥ÏÑ±\"ÏóêÏÑúÎäî GPT-3(Alpaca)Ïù¥ Îã§ÏÜå \n",
       "\u001b[1;33m   \u001b[0mÏö∞ÏÑ∏ÌïòÎÇò ÎπÑÏä∑Ìïú ÏàòÏ§ÄÏùò Í≤∞Í≥ºÎ•º Î≥¥ÏòÄÏäµÎãàÎã§.                                                                       \n",
       "\u001b[1;33m ‚Ä¢ \u001b[0m\u001b[1mÎëê Î≤àÏß∏ ÎπÑÍµê\u001b[0m: GPT-4 ÏßÄÏãúÎ•º Îî∞Îùº ÌäúÎãùÎêú LLaMA Î™®Îç∏Í≥º ÏõêÎûòÏùò GPT-4 Î™®Îç∏ÏùÑ ÎπÑÍµêÌñàÏúºÎ©∞, ÏÑ∏ Í∞ÄÏßÄ Í∏∞Ï§Ä Î™®ÎëêÏóêÏÑú Ïú†ÏÇ¨Ìïú\n",
       "\u001b[1;33m   \u001b[0mÏÑ±Îä•ÏùÑ Î≥¥Ïó¨Ï§¨ÏäµÎãàÎã§. Ïù¥Îäî GPT-4Í∞Ä ÏÉùÏÑ±Ìïú Îç∞Ïù¥ÌÑ∞Î°ú ÌïôÏäµÌï®ÏúºÎ°úÏç® ÏõêÎûò GPT-4 ÏàòÏ§ÄÏùò ÏÑ±Îä•ÏùÑ Îã¨ÏÑ±Ìï† Ïàò ÏûàÎã§Îäî Í≤ÉÏùÑ   \n",
       "\u001b[1;33m   \u001b[0mÏùòÎØ∏Ìï©ÎãàÎã§.                                                                                                     \n",
       "\n",
       "                                             \u001b[1mÏ∂îÍ∞Ä Í∑∏Î¶º ÏÑ§Î™Ö (Í∑∏Î¶º 4):\u001b[0m                                              \n",
       "\n",
       "\u001b[1;33m ‚Ä¢ \u001b[0mGPT-4Î°ú ÌèâÍ∞ÄÎêú ÏÑ±Îä• ÎπÑÍµêÎ•º Î≥¥Ïó¨Ï§çÎãàÎã§. Í∞Å ÎßâÎåÄÎäî Îëê Î™®Îç∏ Í∞ÑÏùò ÌèâÍ∞Ä Í≤∞Í≥ºÎ•º ÎÇòÌÉÄÎÇ¥Î©∞, Ï¥ù Ï†êÏàò(ÏµúÎåÄ 800Ï†ê)ÏôÄ ÏÉÅÎåÄ  \n",
       "\u001b[1;33m   \u001b[0mÏ†êÏàò ÎπÑÏú®(Í∞ïÎ†•Ìïú ÏÉÅÎåÄ Î™®Îç∏ ÎåÄÎπÑ)Ïù¥ Î≥¥Í≥†Îê©ÎãàÎã§.                                                                  \n",
       "\u001b[1;33m ‚Ä¢ \u001b[0mLLaMA GPT4 Î™®Îç∏Ïùò ÏùëÎãµÏùÑ Ïö∞Î¶¨ Î≥¥ÏÉÅ Î™®Îç∏Î°ú ÌèâÍ∞ÄÌïú Í≤∞Í≥ºÎ•º ÎÇòÌÉÄÎÇ¥Î©∞, Ï†ÑÎ∞òÏ†ÅÏúºÎ°ú ChatGPT Î∞è GPT-4ÏôÄ ÎπÑÍµêÌïòÏó¨ ÏÑ±Îä•ÏùÑ \n",
       "\u001b[1;33m   \u001b[0mÌèâÍ∞ÄÌñàÏäµÎãàÎã§.                                                                                                   \n",
       "\n",
       "Ïù¥ Ïó∞Íµ¨Îäî GPT-4 Îç∞Ïù¥ÌÑ∞Î•º ÏÇ¨Ïö©Ìïú ÌïôÏäµÏù¥ ÏÉàÎ°úÏö¥ ÏßÄÏãúÎ¨∏ ÏûëÏóÖÏóêÏÑú Îõ∞Ïñ¥ÎÇú ÏÑ±Îä•ÏùÑ ÎÇ∏Îã§Îäî Í∞ÄÎä•ÏÑ±ÏùÑ ÏãúÏÇ¨Ìï©ÎãàÎã§.            \n",
       "\n",
       "\u001b[33m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\n",
       "\n",
       "                               \u001b[1;4m4.3 COMPARISONS WITH SOTA USING AUTOMATIC EVALUATION\u001b[0m                                \n",
       "\n",
       "Ïù¥ ÏÑπÏÖòÏóêÏÑúÎäî ÏûêÎèô ÌèâÍ∞ÄÎ•º ÏÇ¨Ïö©ÌïòÏó¨ LLaMA-GPT4 Î™®Îç∏Í≥º Îã§Î•∏ ÏµúÏ≤®Îã®(State-of-the-Art, SOTA) Î™®Îç∏Îì§ÏùÑ ÎπÑÍµêÌïú Í≤∞Í≥ºÎ•º    \n",
       "ÏÑ§Î™ÖÌï©ÎãàÎã§.                                                                                                        \n",
       "\n",
       "                                                    \u001b[1mÏûêÎèô ÌèâÍ∞Ä:\u001b[0m                                                     \n",
       "\n",
       "\u001b[1;33m ‚Ä¢ \u001b[0m\u001b[1mÌèâÍ∞Ä Î∞©Î≤ï\u001b[0m: GPT-4Î•º ÌôúÏö©Ìï¥ Îã§ÏñëÌïú Î™®Îç∏Îì§Ïù¥ 80Í∞úÏùò ÏÉàÎ°úÏö¥ ÏßàÎ¨∏Ïóê ÎåÄÌïú ÏÉùÏÑ± ÏùëÎãµÏùò ÌíàÏßàÏùÑ ÏûêÎèô ÌèâÍ∞ÄÌñàÏäµÎãàÎã§.       \n",
       "\u001b[1;33m   \u001b[0mLLaMA-GPT4(7B)ÏôÄ GPT-4Ïùò ÏùëÎãµÏùÑ ÏàòÏßëÌïòÍ≥†, Vicuna(2023)ÏóêÏÑú Í≥µÍ∞úÎêú Îã§Î•∏ Ï±ÑÌåÖ Î¥áÎì§(LLaMA 13B, Alpaca 13B, Vicuna  \n",
       "\u001b[1;33m   \u001b[0m13B, Bard, ChatGPT)Ïùò ÏùëÎãµÏùÑ ÏÇ¨Ïö©ÌñàÏäµÎãàÎã§. Îëê Î™®Îç∏ Í∞ÑÏùò ÏùëÎãµ ÌíàÏßàÏùÑ 1ÏóêÏÑú 10ÍπåÏßÄÏùò Ï†êÏàòÎ°ú ÌèâÍ∞ÄÌñàÏäµÎãàÎã§.         \n",
       "\u001b[1;33m ‚Ä¢ \u001b[0m\u001b[1mÍ≤∞Í≥º\u001b[0m: LLaMA-GPT4Î•º Îëê Í∞ÄÏßÄ ÎîîÏΩîÎî© Í≤∞Í≥ºÎ°ú ÌèâÍ∞ÄÌñàÏäµÎãàÎã§. Ï≤´Ïß∏Îäî ÏßàÎ¨∏Îãπ ÌïòÎÇòÏùò ÏùëÎãµÎßåÏùÑ ÏÉùÏÑ±ÌïòÎäî Í∏∞Î≥∏ ÎîîÏΩîÎî© Í≤∞Í≥º, \n",
       "\u001b[1;33m   \u001b[0mÎëòÏß∏Îäî ÏßàÎ¨∏Îãπ Îã§ÏÑØ Í∞úÏùò ÏùëÎãµÏùÑ ÏÉùÏÑ±Ìïú ÌõÑ Î≥¥ÏÉÅ Î™®Îç∏Ïù¥ ÏÉÅÏúÑÏóêÏÑú ÌïòÏúÑÎ°ú ÏàúÏúÑ Îß§Í∏¥ ÏùëÎãµ Í∑∏Î£πÏûÖÎãàÎã§. Ïù¥ Í∑∏Î£πÎì§ÏùÄ Í∏∞Î≥∏\n",
       "\u001b[1;33m   \u001b[0mÍ≤∞Í≥ºÏôÄ ÎπÑÍµêÎêòÏóàÍ≥†, ÌèâÍ∞Ä Í≤∞Í≥ºÏóêÏÑú ÌîºÎìúÎ∞± Îç∞Ïù¥ÌÑ∞ÏôÄ Î≥¥ÏÉÅ Î™®Îç∏Ïùò Ïú†Ïö©ÏÑ±ÏùÑ Î≥¥Ïó¨Ï§çÎãàÎã§.                               \n",
       "\n",
       "                                                       \u001b[1mÍ≤∞Í≥º:\u001b[0m                                                       \n",
       "\n",
       "\u001b[1;33m ‚Ä¢ \u001b[0mLLaMA-GPT4Îäî text-davinci-003(Ï¶â, Alpaca)Î°ú ÌäúÎãùÌïú Í≤ΩÏö∞Î≥¥Îã§ ÎÜíÏùÄ ÏÑ±Îä•ÏùÑ Î≥¥Ïó¨Ï£ºÏóàÏßÄÎßå, Ïó¨Ï†ÑÌûà ÏÉÅÏóÖÏö© ÎåÄÌòï        \n",
       "\u001b[1;33m   \u001b[0mÏ±óÎ¥á(e.g., GPT-4)Í≥º ÎπÑÍµêÌï¥ÏÑúÎäî Ï∞®Ïù¥Í∞Ä Ï°¥Ïû¨Ìï©ÎãàÎã§.                                                               \n",
       "\u001b[1;33m ‚Ä¢ \u001b[0m\u001b[1mÏ§ëÍµ≠Ïñ¥ ÌèâÍ∞Ä\u001b[0m: ÏòÅÏñ¥ ÏùëÎãµÏùÑ Ï§ëÍµ≠Ïñ¥Î°ú Î≤àÏó≠Ìïú Í≤ÉÏù¥ÎÇò, Ï§ëÍµ≠Ïñ¥Î°ú ÏßÅÏ†ë ÏßàÎ¨∏ÌïòÏó¨ ÏÉùÏÑ±Îêú ÏùëÎãµÏùÑ ÎπÑÍµêÌñàÏùÑ Îïå, Î≤àÏó≠Îêú ÏùëÎãµÏù¥\n",
       "\u001b[1;33m   \u001b[0mÎçî Ïö∞ÏàòÌïú ÏÑ±Îä•ÏùÑ Î≥¥Ïó¨Ï£ºÏóàÏäµÎãàÎã§. Ïù¥Îäî GPT-4Í∞Ä ÏòÅÏñ¥ ÏΩîÌçºÏä§ÏóêÏÑú Îçî ÎßéÏù¥ ÌõàÎ†®ÎêòÏñ¥ ÏòÅÏñ¥ ÏßÄÏãúÎ¨∏ÏùÑ Îî∞Î•¥Îäî Îä•Î†•Ïù¥ Îçî   \n",
       "\u001b[1;33m   \u001b[0mÍ∞ïÎ†•ÌïòÍ∏∞ ÎïåÎ¨∏Ïùº Ïàò ÏûàÏäµÎãàÎã§.                                                                                    \n",
       "\n",
       "                             \u001b[1mÎπÑÏ†ïÏÉÅ ÏßÄÏãúÎ¨∏(unnatural instructions)ÏóêÏÑúÏùò Í≤∞Í≥º(Í∑∏Î¶º 6):\u001b[0m                             \n",
       "\n",
       "\u001b[1;33m ‚Ä¢ \u001b[0mLLaMA-GPT4ÏôÄ GPT-4, Í∑∏Î¶¨Í≥† Alpaca Î™®Îç∏ÏùÑ ÎπÑÍµêÌñàÏùÑ Îïå, AlpacaÍ∞Ä Îã§Î•∏ Îëê Î™®Îç∏Ïóê ÎπÑÌï¥ ÌèâÍ∑† ROUGE-L Ï†êÏàòÏóêÏÑú        \n",
       "\u001b[1;33m   \u001b[0mÏö∞ÏàòÌïòÏòÄÏäµÎãàÎã§. Í∑∏Îü¨ÎÇò ÏùëÎãµÏùò Í∏∏Ïù¥Í∞Ä Í∏∏Ïñ¥ÏßàÏàòÎ°ù LLaMA-GPT4ÏôÄ GPT-4Í∞Ä Îçî ÎÜíÏùÄ ÏÑ±Îä•ÏùÑ Î≥¥Ïó¨Ï£ºÍ∏∞ ÏãúÏûëÌñàÏäµÎãàÎã§. Ïù¥Îäî \n",
       "\u001b[1;33m   \u001b[0mÎçî Ï∞ΩÏùòÏ†ÅÏù∏ ÏãúÎÇòÎ¶¨Ïò§ÏóêÏÑú Ïù¥ Î™®Îç∏Îì§Ïù¥ ÏßÄÏãúÎ¨∏ÏùÑ Îçî Ïûò Îî∞Î•º Ïàò ÏûàÎã§Îäî Í≤ÉÏùÑ ÏùòÎØ∏Ìï©ÎãàÎã§.                             \n",
       "\u001b[1;33m ‚Ä¢ \u001b[0mÍ∞ÅÍ∏∞ Îã§Î•∏ ÏùëÎãµ Í∏∏Ïù¥ ÌïòÏúÑ ÏßëÌï©Ïóê ÎåÄÌï¥, LLaMA-GPT4Îäî GPT-4Ïùò ÌñâÎèôÏùÑ Í∞ÄÍπùÍ≤å Îî∞Î¶ÖÎãàÎã§. ÏùëÎãµ Í∏∏Ïù¥Í∞Ä ÏßßÏùÑ Îïå,         \n",
       "\u001b[1;33m   \u001b[0mLLaMA-GPT4ÏôÄ GPT-4Îäî Îã®ÏàúÌïú Ï†ïÎãµÏùÑ Ìè¨Ìï®ÌïòÎêò, ÏùëÎãµÏùÑ Îçî ÎåÄÌôîÌòïÏúºÎ°ú ÎßåÎìúÎäî Ï∂îÍ∞ÄÏ†ÅÏù∏ Îã®Ïñ¥Î•º Ìè¨Ìï®ÌïòÍ∏∞ ÎïåÎ¨∏Ïóê, ÎÇÆÏùÄ  \n",
       "\u001b[1;33m   \u001b[0mROUGE-L Ï†êÏàòÎ•º Î∞õÏùÑ Í∞ÄÎä•ÏÑ±Ïù¥ ÏûàÏäµÎãàÎã§.                                                                          \n",
       "\n",
       "Ïù¥ Ïã§Ìóò Í≤∞Í≥ºÎäî LLaMA-GPT4Í∞Ä GPT-4Ïùò Ï∂úÎ†•ÏùÑ Í∏∞Î∞òÏúºÎ°ú ÌïôÏäµÌñàÏùÑ Îïå, ÏÉÅÎãπÌïú ÏÑ±Îä• Ìñ•ÏÉÅÏùÑ Ïù¥Î£∞ Ïàò ÏûàÏùåÏùÑ Î≥¥Ïó¨Ï£ºÎ©∞, Ïù¥Îäî  \n",
       "ÌòÅÏã†Ï†ÅÏù∏ ÏßÄÏãúÎ¨∏-Îî∞Î•¥Í∏∞ LLM Í∞úÎ∞úÏóê ÏûàÏñ¥ÏÑú Ïú†ÎßùÌïú Î∞©Ìñ•ÏûÑÏùÑ ÏãúÏÇ¨Ìï©ÎãàÎã§.                                               \n",
       "\n",
       "\u001b[33m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\n",
       "\n",
       "                                                  \u001b[1;4m5 RELATED WORK\u001b[0m                                                   \n",
       "\n",
       "Ïù¥ ÏÑπÏÖòÏóêÏÑúÎäî ÏßÄÏãúÎ¨∏ ÌäúÎãùÍ≥º Ïò§ÌîàÏÜåÏä§ ÎÖ∏Î†•Ïù¥ Í¥ÄÎ†®Îêú Ïó∞Íµ¨Ïóê ÎåÄÌï¥ ÏÑ§Î™ÖÌï©ÎãàÎã§.                                         \n",
       "\n",
       "                                         \u001b[1mÏßÄÏãúÎ¨∏ ÌäúÎãù(Instruction Tuning):\u001b[0m                                          \n",
       "\n",
       "ÏßÄÏãúÎ¨∏ ÌäúÎãùÏùÄ NLP Î∂ÑÏïºÏóêÏÑú Ï†êÏ†ê Ï£ºÎ™©Î∞õÍ≥† ÏûàÎäî Ïó∞Íµ¨ Î∞©Ìñ•ÏûÖÎãàÎã§. Í∏∞Ï°¥ Ïó∞Íµ¨Îì§ÏùÄ ÌÅ¨Í≤å ÏÑ∏ Í∞ÄÏßÄ ÏöîÏÜåÏù∏ ÏßÄÏãúÎ¨∏-Îî∞Î•¥Í∏∞     \n",
       "Îç∞Ïù¥ÌÑ∞, Í∏∞Î≥∏ Ïñ∏Ïñ¥ Î™®Îç∏, ÌèâÍ∞Ä Î≤§ÏπòÎßàÌÅ¨Ïùò ÌíàÏßàÍ≥º Í∑úÎ™®Î•º Ìñ•ÏÉÅÏãúÌÇ§Îäî Í≤ÉÏùÑ Î™©ÌëúÎ°ú Ìï©ÎãàÎã§. Í∞Å Ïó∞Íµ¨ Í∑∏Î£πÏùÄ ÏùºÎ∞òÏ†ÅÏúºÎ°ú     \n",
       "ÏûêÏ≤¥Ï†ÅÏù∏ Í∞úÎ∞ú ÌååÏù¥ÌîÑÎùºÏù∏ÏùÑ Ïú†ÏßÄÌï©ÎãàÎã§. ÏòàÎ•º Îì§Ïñ¥, ÌîåÎûú(FLAN)ÏùÑ Í∏∞Î∞òÏúºÎ°ú ÏßÄÏãúÎ¨∏ ÎØ∏ÏÑ∏ Ï°∞Ï†ï Ïñ∏Ïñ¥ Î™®Îç∏Ïùò Í∑úÎ™®Î•º        \n",
       "ÌôïÎåÄÌïòÎäî Ïó∞Íµ¨(Chung et al., 2022)ÎÇò, Ï†êÏ†ê ÎßéÏùÄ ÌîÑÎ°¨ÌîÑÌä∏Î•º Ìè¨Ìï®ÌïòÎäî PromptSource(Í≥µÍ≥µ ÌîÑÎ°¨ÌîÑÌä∏ ÌíÄ, P3)Ïùò Í∞úÎ∞ú(Bach  \n",
       "et al., 2022) Îì±Ïù¥ ÏûàÏäµÎãàÎã§. T0Îäî P3Ïóê Í∏∞Î∞òÌïú Îã§Ï§ë Í≥ºÏ†ú ÌîÑÎ°¨ÌîÑÌä∏ ÌõàÎ†®ÏùÑ ÌÜµÌï¥ Í∞úÎ∞úÎêú ÏùºÎ†®Ïùò Î™®Îç∏(Sanh et al.,       \n",
       "2021)ÏûÖÎãàÎã§. OPT Î™®Îç∏Ïùò ÏßÄÏãúÎ¨∏ ÌäúÎãùÏùÄ Îçî ÌÅ¨Í≥† Ìè¨Í¥ÑÏ†ÅÏù∏ Î≤§ÏπòÎßàÌÅ¨Ïù∏ OPT-IML BenchÎ•º ÏÇ¨Ïö©ÌïòÏó¨ ÏßÑÌñâÎêòÏóàÏúºÎ©∞, Ïù¥Îäî FLAN,\n",
       "Super-NaturalInstructions(Wang et al., 2022b), UnifiedSKG(Xie et al., 2022)ÏùÑ Ìè¨Ìï®Ìï©ÎãàÎã§.                          \n",
       "\n",
       "                                        \u001b[1mÏò§ÌîàÏÜåÏä§ ÎÖ∏Î†•(Open-Source Efforts):\u001b[0m                                        \n",
       "\n",
       "ChatGPTÏùò Ìè≠ÎÑìÏùÄ Îä•Î†• ÎçïÎ∂ÑÏóê, Ïò§ÌîàÏÜåÏä§ Î™®Îç∏ÏùÄ ÌÅ∞ Í¥ÄÏã¨ÏùÑ Î∞õÏúºÎ©∞ Ïù∏Í∞ÑÏùò Í∞ÄÏπòÏóê ÎßûÎäî ÏùºÎ∞ò Î™©Ï†ÅÏùò ÌÖçÏä§Ìä∏ Í∏∞Î∞ò          \n",
       "Ïñ¥ÏãúÏä§ÌÑ¥Ìä∏Î•º Í∞úÎ∞úÌïòÎäî Îç∞ Í∏∞Ïó¨ÌïòÍ≥† ÏûàÏäµÎãàÎã§. Ï¥àÍ∏∞Ïùò Í∏∞Ï¥à LLM ÎÖ∏Î†•ÏóêÎäî BLOOM(Scao et al., 2022), GPT-J(Wang &        \n",
       "Komatsuzaki, 2021), GPT-NEO(Black et al., 2021), OPT(Zhang et al., 2022), LLaMA(Zhang et al., 2023)Í∞Ä Ìè¨Ìï®Îê©ÎãàÎã§.  \n",
       "LLMÏùÑ ÎåÄÌôî Í∏∞Î∞ò Ïñ¥ÏãúÏä§ÌÑ¥Ïä§Ïóê ÎßûÏ∂îÍ∏∞ ÏúÑÌï¥ Open-AssistantÎäî GPT-JÎ•º Í∏∞Î∞òÏúºÎ°ú ÌïòÍ≥†, Alpaca/VicunaÎäî LLaMAÎ•º Í∏∞Î∞òÏúºÎ°ú  \n",
       "Íµ¨Ï∂ïÎêòÏóàÏäµÎãàÎã§. ÎòêÌïú, OpenFlamingo(Awadalla et al., 2023)ÏôÄ LLaMA-Adapter(Zhang et al., 2023)Îäî LLaMAÎ•º Ïù¥ÎØ∏ÏßÄ     \n",
       "ÏûÖÎ†•Í≥º Ïó∞Í≤∞ÌïòÏó¨ Ïò§ÌîàÏÜåÏä§ Î©ÄÌã∞Î™®Îã¨ LLMÏùÑ Íµ¨Ï∂ïÌï† Í∏∏ÏùÑ ÎßàÎ†®ÌñàÏäµÎãàÎã§.                                                  \n",
       "\n",
       "Ïù¥Îü¨Ìïú Ïó∞Íµ¨ÏôÄ ÎÖ∏Î†•ÏùÄ LLMÏù¥ Îçî Îã§ÏñëÌïòÍ≥† Í∞ïÎ†•Ìïú Îä•Î†•ÏùÑ Î∞úÌúòÌï† Ïàò ÏûàÎèÑÎ°ù ÌïòÍ≥†, Ïò§ÌîàÏÜåÏä§ ÌôòÍ≤ΩÏóêÏÑúÏùò Î∞úÏ†ÑÏùÑ Ï¥âÏßÑÌïòÎäî Îç∞ \n",
       "Í∑∏ Î™©Ï†ÅÏù¥ ÏûàÏäµÎãàÎã§.                                                                                                \n",
       "\n",
       "\u001b[33m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich.console import Console\n",
    "from rich.markdown import Markdown\n",
    "from rich.panel import Panel\n",
    "from rich.syntax import Syntax\n",
    "from rich.table import Table\n",
    "from typing import Dict\n",
    "import os\n",
    "\n",
    "class MarkdownPrinter:\n",
    "    def __init__(self):\n",
    "        self.console = Console()\n",
    "        \n",
    "    def print_markdown_file(self, file_path: str):\n",
    "        \"\"\"ÎßàÌÅ¨Îã§Ïö¥ ÌååÏùºÏùÑ Ïù¥ÏÅòÍ≤å Ï∂úÎ†•\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                markdown_content = f.read()\n",
    "            \n",
    "            # ÎßàÌÅ¨Îã§Ïö¥ Î†åÎçîÎßÅ\n",
    "            md = Markdown(markdown_content)\n",
    "            \n",
    "            # ÎßàÌÅ¨Îã§Ïö¥ ÎÇ¥Ïö© Ï∂úÎ†•\n",
    "            self.console.print(md)\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.console.print(f\"[bold red]Error reading file: {str(e)}[/]\")\n",
    "            \n",
    "    def print_sections(self, sections: Dict[str, str]):\n",
    "        \"\"\"ÏÑπÏÖòÎ≥ÑÎ°ú Íµ¨Î∂ÑÌïòÏó¨ Ï∂úÎ†•\"\"\"\n",
    "        for section, content in sections.items():\n",
    "            # ÏÑπÏÖò Ï†úÎ™©\n",
    "            self.console.print(\"\\n\")\n",
    "            self.console.print(Panel(\n",
    "                f\"[bold cyan]{section}[/]\",\n",
    "                border_style=\"cyan\"\n",
    "            ))\n",
    "            \n",
    "            # ÏÑπÏÖò ÎÇ¥Ïö©\n",
    "            md = Markdown(content)\n",
    "            self.console.print(md)\n",
    "            \n",
    "            # Íµ¨Î∂ÑÏÑ†\n",
    "            self.console.print(\"[dim]\" + \"=\"*80 + \"[/]\")\n",
    "\n",
    "printer = MarkdownPrinter()\n",
    "\n",
    "# ÎßàÌÅ¨Îã§Ïö¥ ÌååÏùº Ï∂úÎ†•\n",
    "printer.print_markdown_file(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÎÑ§, ÎßûÏäµÎãàÎã§. ReSTÏóêÏÑú ÏÉàÎ°úÏö¥ Îç∞Ïù¥ÌÑ∞Î•º ÏÉùÏÑ±ÌïòÎäî Í≥ºÏ†ïÏùÄ ÏõêÎ≥∏ Îç∞Ïù¥ÌÑ∞ÏÖãÏùÑ ÏÉòÌîåÎßÅÌïòÏó¨ ÏûÖÎ†• ÏãúÌÄÄÏä§Î•º ÏÑ†ÌÉùÌïòÍ≥†, Ïù¥Î•º Í∏∞Î∞òÏúºÎ°ú Ï°∞Í±¥ÌôîÎêú ÏÉàÎ°úÏö¥ Ï∂úÎ†• ÏãúÌÄÄÏä§Î•º ÏÉùÏÑ±ÌïòÎäî Î∞©ÏãùÏúºÎ°ú Ïù¥Î£®Ïñ¥ÏßëÎãàÎã§. Ïó¨Í∏∞ÏÑú \"Ï°∞Í±¥Ìôî\"ÎùºÎäî Í≤ÉÏùÄ Ï£ºÏñ¥ÏßÑ ÏûÖÎ†• ÏãúÌÄÄÏä§ \\(\\pmb{x}\\)Ïóê ÎåÄÌï¥ Í∑∏ Ï°∞Í±¥ ÌïòÏóêÏÑú Í∞ÄÎä•Ìïú Ï∂úÎ†• ÏãúÌÄÄÏä§ \\(\\pmb{y}\\)Î•º ÏÉùÏÑ±ÌïúÎã§Îäî ÏùòÎØ∏ÏûÖÎãàÎã§.\n",
      "\n",
      "Ï°∞Í±¥ÌôîÎäî ÌôïÎ•†Ï†Å Î™®Îç∏ÎßÅÏóêÏÑú Îß§Ïö∞ Ï§ëÏöîÌïú Í∞úÎÖêÏù∏Îç∞, Ïù¥Îäî Ï£ºÏñ¥ÏßÑ ÏûÖÎ†•Ïù¥ Ïñ¥Îñ§ ÌäπÏ†ïÌïú ÏÉÅÌÉúÎÇò Í∞íÏùº ÎïåÏùò Ï∂úÎ†• ÌôïÎ•†ÏùÑ ÎúªÌï©ÎãàÎã§. Ïñ∏Ïñ¥ Î™®Îç∏ÎßÅÏóêÏÑúÎäî ÏûÖÎ†• ÏãúÌÄÄÏä§(ÎòêÎäî Î¨∏Îß•)Í∞Ä Ï£ºÏñ¥Ï°åÏùÑ Îïå Îã§Ïùå Í∞ÄÎä•Ìïú Ï∂úÎ†• ÏãúÌÄÄÏä§Î•º ÏÉùÏÑ±ÌïòÎäî Í≥ºÏ†ïÏùÑ ÎßêÌï©ÎãàÎã§. Ïù¥ Í≥ºÏ†ïÏùÄ ÏûêÎèô ÌöåÍ∑Ä Î™®Îç∏ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Íµ¨ÌòÑÎêòÎ©∞, Î™®Îç∏Ïù¥ Ïù¥ÎØ∏ ÌïôÏäµÌïú ÌôïÎ•† Î∂ÑÌè¨Ïóê Í∏∞Î∞òÌï¥ \\(\\pi_{\\theta}(\\pmb{y}|\\pmb{x})=\\prod_{t=1}^{T}\\pi_{\\theta}(y_{t}|\\pmb{y}_{1:t-1}, \\pmb{x})\\) ÌòïÌÉúÎ°ú Îã§Ïùå ÌÜ†ÌÅ∞ \\(y_t\\)Î•º Í≥ÑÏÇ∞Ìï©ÎãàÎã§.\n",
      "\n",
      "ReSTÏóêÏÑúÎäî Ïù¥Î•º ÌÜµÌï¥ ÌòÑÏû¨ Ï†ïÏ±Ö \\(\\pi_{\\theta}\\)Î°ú ÏûÖÎ†• ÏãúÌÄÄÏä§Ïóê ÎßûÎäî ÏÉàÎ°úÏö¥ Ï∂úÎ†• ÏãúÌÄÄÏä§Î•º ÏÉùÏÑ±ÌïòÍ≥†, Í∑∏ Í≤∞Í≥ºÎ•º Îç∞Ïù¥ÌÑ∞ÏÖãÏóê Ï∂îÍ∞ÄÌïòÏó¨ Îç∞Ïù¥ÌÑ∞Î•º ÌôïÏû•ÏãúÌÇµÎãàÎã§. Ïù¥Îü¨Ìïú Î∞©ÏãùÏùÄ Î™®Îç∏Ïù¥ Ïù¥ÎØ∏ ÏïåÍ≥† ÏûàÎäî Î¨∏Îß•Ïóê Í∏∞Î∞òÌï¥ Ï∂îÍ∞ÄÏ†ÅÏù∏ ÌïôÏäµ Îç∞Ïù¥ÌÑ∞Î•º ÏÉùÏÑ±Ìï† Ïàò ÏûàÎèÑÎ°ù ÌïòÎ©∞, Ïù¥Îäî Grow Îã®Í≥ÑÏóêÏÑú ÏõêÎ≥∏ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò ÏÉòÌîåÏùÑ Ïù¥Ïö©Ìï¥ ÏÉàÎ°úÏö¥ ÏãúÌÄÄÏä§Î•º ÏÉùÏÑ±ÌïòÎäî Í≥ºÏ†ïÏóêÏÑú ÌïµÏã¨Ï†ÅÏù∏ Ïó≠Ìï†ÏùÑ Ìï©ÎãàÎã§. Ïù¥Îäî Section 3Ïùò ÎÇ¥Ïö©ÏùÑ Î∞îÌÉïÏúºÎ°ú ÏÑ§Î™ÖÌïú Í≤ÉÏûÖÎãàÎã§.\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\" \n",
    "ÏõêÎ≥∏ Îç∞Ïù¥ÌÑ∞ ÏÖãÏùÑ ÏÉòÌîåÎßÅÌï¥ÏÑú ÏûÖÎ†• ÏãúÌÄÄÏä§Ïóê Ï°∞Í±¥ÌôîÎ•º Ìïú ÌõÑ ÏÉàÎ°úÏö¥ Îç∞Ïù¥ÌÑ∞Î•º ÏÉùÏÑ±ÌïòÎäî Í≤É Í∞ôÏùÄÎç∞ Ïù¥Í≤å ÎßûÏïÑ? Í∑∏Î¶¨Í≥† Ï°∞Í±¥ÌôîÍ∞Ä Î≠êÏûÑ?\n",
    "\"\"\"\n",
    "\n",
    "qa = PaperQA(qa.conversation_history)\n",
    "response = qa.ask_question(question)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
