{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Invalid requirement: 'anthropic,': Expected end or semicolon (after name and no valid version specifier)\n",
      "    anthropic,\n",
      "             ^\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install anthropic\n",
    "%pip install rich\n",
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from typing import Dict, List\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def get_completion(messages: List[Dict[str, str]]) -> str:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=messages,\n",
    "        max_completion_tokens=4092, \n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Dict, List\n",
    "from pathlib import Path\n",
    "\n",
    "class MarkdownSectionParser:\n",
    "    def __init__(self):\n",
    "        self.section_pattern = r'^#+ .*$'  # '#'ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” í—¤ë” íŒ¨í„´\n",
    "        self.end_sections = {'ACKNOWLEDGEMENTS', 'REFERENCES', 'CONCLUSION', 'CONCLUSIONS'}\n",
    "        \n",
    "    def parse_sections(self, markdown_path: str) -> Dict[str, str]:\n",
    "        sections = {}\n",
    "        current_section = None\n",
    "        current_content = []\n",
    "        \n",
    "        with open(markdown_path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            \n",
    "        for line in lines:\n",
    "            if re.match(self.section_pattern, line):\n",
    "                section_title = line.strip('# \\n')\n",
    "                \n",
    "                if current_section and current_content:\n",
    "                    sections[current_section] = ''.join(current_content).strip()\n",
    "                \n",
    "                # ëŒ€ë¬¸ìë¡œ ë³€í™˜í•˜ì—¬ ë¹„êµ\n",
    "                if any(end_sec in section_title.upper() for end_sec in self.end_sections):\n",
    "                    break\n",
    "                    \n",
    "                current_section = section_title\n",
    "                current_content = []\n",
    "            else:\n",
    "                if current_section is None and 'ABSTRACT' not in line.upper():\n",
    "                    continue\n",
    "                current_content.append(line)\n",
    "        \n",
    "        # ë§ˆì§€ë§‰ ì„¹ì…˜ ì €ì¥\n",
    "        if current_section and current_content:\n",
    "            sections[current_section] = ''.join(current_content).strip()\n",
    "        \n",
    "        # Abstract í•„í„°ë§\n",
    "        filtered_sections = {}\n",
    "        include_section = False\n",
    "        \n",
    "        for section, content in sections.items():\n",
    "            if 'ABSTRACT' in section.upper():\n",
    "                include_section = True\n",
    "            \n",
    "            if include_section:\n",
    "                filtered_sections[section] = content\n",
    "                \n",
    "        return filtered_sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "from typing import Dict, List\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "class PaperExplainer:\n",
    "    def __init__(self):\n",
    "        self.conversation_history = []\n",
    "        self.delay = 1  # API í˜¸ì¶œ ê°„ ë”œë ˆì´ (ì´ˆ)\n",
    "        \n",
    "    def _create_section_prompt(self, section_title: str, section_content: str, is_first: bool = False) -> str:\n",
    "        if is_first:\n",
    "            return f\"\"\"You are an expert academic paper explainer. Please explain the following section '{section_title}' \n",
    "            from an academic paper in a clear and concise manner. Please explain in Korean.\n",
    "\n",
    "            Section content:\n",
    "            {section_content}\"\"\"\n",
    "        else:\n",
    "            return f\"\"\"Based on our previous discussion of the paper, please explain the following section '{section_title}'.\n",
    "            \n",
    "            Section content:\n",
    "            {section_content}\"\"\"\n",
    "    \n",
    "    def explain_section(self, section_title: str, section_content: str) -> str:\n",
    "        try:\n",
    "            # Create prompt based on whether this is the first section\n",
    "            is_first = len(self.conversation_history) == 0\n",
    "            prompt = self._create_section_prompt(section_title, section_content, is_first)\n",
    "            \n",
    "            # Add previous conversation for context\n",
    "            self.conversation_history.append({\"role\": \"user\", \"content\": prompt})\n",
    "            \n",
    "            response = get_completion(self.conversation_history)\n",
    "            \n",
    "            # Update conversation history\n",
    "            self.conversation_history.append({\"role\": \"assistant\", \"content\": response})            \n",
    "            # API í˜¸ì¶œ ê°„ ë”œë ˆì´\n",
    "            time.sleep(self.delay)\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error explaining section {section_title}: {str(e)}\")\n",
    "            return f\"Error: Failed to explain section {section_title}\"\n",
    "\n",
    "    def explain_paper(self, sections: Dict[str, str]) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        ë…¼ë¬¸ì˜ ê° ì„¹ì…˜ì„ ìˆœì°¨ì ìœ¼ë¡œ ì„¤ëª…\n",
    "        \n",
    "        Args:\n",
    "            sections: ì„¹ì…˜ ì œëª©ê³¼ ë‚´ìš©ì„ ë§¤í•‘í•œ ë”•ì…”ë„ˆë¦¬\n",
    "            \n",
    "        Returns:\n",
    "            ì„¹ì…˜ ì œëª©ê³¼ ì„¤ëª…ì„ ë§¤í•‘í•œ ë”•ì…”ë„ˆë¦¬\n",
    "        \"\"\"\n",
    "        explanations = {}\n",
    "        \n",
    "        print(\"\\nProcessing sections:\")\n",
    "        for title, content in tqdm(sections.items(), desc=\"Explaining sections\"):\n",
    "            print(f\"\\nProcessing: {title}\")\n",
    "            explanation = self.explain_section(title, content)\n",
    "            explanations[title] = explanation\n",
    "            \n",
    "        return explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Optional, List\n",
    "class PaperQA:\n",
    "    def __init__(self, context: Optional[List[Dict[str, str]]] = None):\n",
    "        self.conversation_history = context or []\n",
    "        self.delay = 1\n",
    "        \n",
    "    def load_paper_context(self, explanations: Dict[str, str]):\n",
    "        \"\"\"ë…¼ë¬¸ ì„¤ëª…ì„ ëŒ€í™” ê¸°ë¡ì— ë¡œë“œ\"\"\"\n",
    "        context = \"Here's the paper summary:\\n\\n\"\n",
    "        for section, explanation in explanations.items():\n",
    "            context += f\"## {section}\\n{explanation}\\n\\n\"\n",
    "            \n",
    "        # ë…¼ë¬¸ ì»¨í…ìŠ¤íŠ¸ë¥¼ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€\n",
    "        self.conversation_history.append({\"role\": \"user\", \"content\": context})\n",
    "\n",
    "    def ask_question(self, question: str) -> str:\n",
    "        \"\"\"ë…¼ë¬¸ì— ëŒ€í•œ ì§ˆë¬¸ì— ë‹µë³€\"\"\"\n",
    "        try:\n",
    "            # ì§ˆë¬¸ í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "            prompt = f\"\"\"Based on the paper we discussed, please answer the following question in Korean. \n",
    "            Be specific and cite relevant sections when possible.\n",
    "\n",
    "            Question: {question}\"\"\"\n",
    "            \n",
    "            # ì´ì „ ëŒ€í™” ê¸°ë¡ê³¼ í•¨ê»˜ ì§ˆë¬¸ ì „ì†¡\n",
    "            self.conversation_history.append({\"role\": \"user\", \"content\": prompt})\n",
    "            \n",
    "            # Claudeì— ì§ˆë¬¸\n",
    "            response = get_completion(self.conversation_history)\n",
    "            \n",
    "            # ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸\n",
    "            self.conversation_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "            \n",
    "            time.sleep(self.delay)\n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question: {str(e)}\")\n",
    "            return f\"Error: Failed to process question\"\n",
    "    \n",
    "    def view_conversation_history(self, start_idx: int = 0, end_idx: Optional[int] = None) -> None:\n",
    "        \"\"\"ëŒ€í™” ë‚´ì—­ì„ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜\n",
    "        \n",
    "        Args:\n",
    "            start_idx: ì‹œì‘ ì¸ë±ìŠ¤ (ê¸°ë³¸ê°’: 0)\n",
    "            end_idx: ì¢…ë£Œ ì¸ë±ìŠ¤ (ê¸°ë³¸ê°’: None, Noneì¼ ê²½ìš° ëê¹Œì§€ ì¶œë ¥)\n",
    "        \"\"\"\n",
    "        # ë…¼ë¬¸ ì»¨í…ìŠ¤íŠ¸ëŠ” ì œì™¸í•˜ê³  ì‹¤ì œ ëŒ€í™”ë§Œ ì¶œë ¥\n",
    "        conversations = [\n",
    "            msg for msg in self.conversation_history \n",
    "            if not msg[\"content\"].startswith(\"Here's the paper summary:\")\n",
    "        ]\n",
    "        \n",
    "        # end_idxê°€ Noneì´ë©´ ë¦¬ìŠ¤íŠ¸ ëê¹Œì§€\n",
    "        end_idx = end_idx if end_idx is not None else len(conversations)\n",
    "        \n",
    "        print(\"\\n=== ëŒ€í™” ë‚´ì—­ ===\\n\")\n",
    "        for i, msg in enumerate(conversations[start_idx:end_idx], start=start_idx):\n",
    "            role = msg[\"role\"].upper()\n",
    "            if role == \"ASSISTANT\":\n",
    "                print(f\"\\nğŸ¤– Assistant ({i}):\\n{msg['content']}\\n\")\n",
    "                print(\"-\" * 80)\n",
    "            elif role == \"USER\":\n",
    "                print(f\"\\nğŸ‘¤ User ({i}):\\n{msg['content']}\\n\")\n",
    "                print(\"-\" * 80)\n",
    "    \n",
    "    \n",
    "    def get_last_n_conversations(self, n: int = 1) -> None:\n",
    "        \"\"\"ìµœê·¼ nê°œì˜ ëŒ€í™” ë‚´ì—­ì„ ì¶œë ¥\n",
    "        \n",
    "        Args:\n",
    "            n: ì¶œë ¥í•  ìµœê·¼ ëŒ€í™” ê°œìˆ˜ (ê¸°ë³¸ê°’: 1)\n",
    "        \"\"\"\n",
    "        conversations = [\n",
    "            msg for msg in self.conversation_history \n",
    "            if not msg[\"content\"].startswith(\"Here's the paper summary:\")\n",
    "        ]\n",
    "        start_idx = max(0, len(conversations) - n)\n",
    "        self.view_conversation_history(start_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def process_paper(markdown_path: str) -> Tuple[Dict[str, str], PaperQA]:\n",
    "    \"\"\"\n",
    "    ì „ì²´ ë…¼ë¬¸ ì²˜ë¦¬ í”„ë¡œì„¸ìŠ¤\n",
    "    \"\"\"\n",
    "    # 1. Markdown íŒŒì‹±\n",
    "    parser = MarkdownSectionParser()\n",
    "    sections = parser.parse_sections(markdown_path)\n",
    "    \n",
    "    print(sections)\n",
    "    # 2. ì„¹ì…˜ë³„ ì„¤ëª… ìƒì„±\n",
    "    explainer = PaperExplainer()\n",
    "    explanations = explainer.explain_paper(sections)\n",
    "\n",
    "    # 3. ì§ˆë¬¸ ë‹µë³€ ì¤€ë¹„ \n",
    "    qa = PaperQA()\n",
    "    qa.load_paper_context(explanations)\n",
    "\n",
    "    return explanations, qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ABSTRACT': 'Prior work has shown that finetuning large language models (LLMs) using machinegenerated instruction-following data enables such models to achieve remarkable zero-shot capabilities on new tasks, and no human-written instructions are needed. In this paper, we present the first attempt to use GPT-4 to generate instructionfollowing data for LLM finetuning. Our early experiments on instruction-tuned LLaMA models show that the 52K English and Chinese instruction-following data generated by GPT-4 leads to superior zero-shot performance on new tasks to the instruction-following data generated by previous state-of-the-art models. We also collect feedback and comparison data from GPT-4 to enable a comprehensive evaluation and reward model training. We make our data generated using GPT-4 as well as our codebase publicly available. 1', 'INTRODUCTION': 'Large Language Models (LLMs) have shown impressive generalization capabilities such as incontext-learning (Brown et al., 2020) and chain-of-thoughts reasoning (Wei et al., 2022). To enable LLMs to follow natural language instructions and complete real-world tasks, researchers have been exploring methods of instruction-tuning of LLMs. This is implemented by either finetuning the model on a wide range of tasks using human-annotated prompts and feedback (Ouyang et al., 2022), or supervised finetuning using public benchmarks and datasets augmented with manually or automatically generated instructions (Wang et al., 2022b). Among these methods, Self-Instruct tuning (Wang et al., 2022a) is a simple and effective method of aligning LLMs to human intent, by learning from instruction-following data generated by state-of-the-art instruction-tuned teacher LLMs. It turns out that the line of instruction-tuning research has produced effective means to improve the zero and few-shot generalization abilities of LLMs. The recent success of ChatGPT (OpenAI, 2023a) and GPT-4 (OpenAI, 2023b) offers tremendous opportunities to improve open-source LLMs using instruction-tuning. LLaMA (Touvron et al., 2023) is a series of open-sourced LLMs, which match the performance of proprietary LLMs such as GPT-3. To teach LLaMA to follow instructions, Self-Instruct tuning has been quickly adopted given its superior performance and low cost. For example, Stanford Alpaca (Taori et al., 2023) uses 52K instruction-following samples generated by GPT-3.5, while Vicuna (Vicuna, 2023) uses around 700K instruction-following samples (70K conversions) shared user-ChatGPT (ShareGPT, 2023).  \\n\\nTo advance the state of the art of instruction-tuning for LLMs, we propose for the first time to use GPT-4 as a teacher for self-instruct tuning. Our paper makes the following contributions:  \\n\\nâ€¢ GPT-4 data. We release data generated by GPT-4, including the 52K instruction-following dataset in both English and Chinese, and the GPT-4-generated feedback data that rate the outputs of three instruction-tuned models. â€¢ Models & Evaluation. Based on the GPT-4-generated data, we have developed instruction-tuned LLaMA models and reward models. To evaluate the quality of instruction-tuned LLMs, we use three metrics evaluated on test samples (i.e., unseen instructions): human evaluation on three alignment criteria, automatic evaluation using GPT-4 feedback, and ROUGE-L on un-natural  \\n\\nAlgorithm 1: Pseudo code for prompt engineering, GPT-4 call and hyper-parameters in data generation. Each instruction instance is used as variables in the prompt template, the data flow is highlighted in blue. 1 PROMPT DICT{ 2 prompt input: ( 3 â€œBelow is an instruction that describes a task, paired with an input that provides further context.â€ 4 â€œWrite a response that appropriately completes the request. $\\\\bar{\\\\langle{\\\\bf n}\\\\backslash{\\\\bf n}^{\\\\circ}\\\\rangle}$ 5 â€œ### Instruction: \\\\n {instruction} $\\\\backslash{\\\\mathfrak{n}}\\\\backslash{\\\\mathfrak{n}}$ ### Input: $\\\\{{\\\\tt i n p u t}\\\\}\\\\setminus{\\\\tt n\\\\backslash n}$ ### Response:â€ 6 ), 7 prompt no input: ( â€œBelow is an instruction that describes a task. â€ â€œWrite a response that appropriately completes the request. $\\\\langle\\\\mathbf{n}\\\\mid\\\\mathbf{n}^{\\\\ast}$ 10 â€œ### Instruction: $\\\\setminus\\\\mathbf{n}$ {instruction} $\\\\backslash{\\\\mathfrak{n}}\\\\backslash{\\\\mathfrak{n}}$ ### Response:â€ ) 11 $\\\\}$ 12 output $=$ openai.ChatCompletion.create( 13 model $=$ \"gpt-4\", 14 messages $=$ [\"role\": \"user\", \"content\": prompt], 15 temperature $=\\\\textsf{1}.0$ , 16 top p=1.0, # nucleus sampling over entire vocabulary 17 max tokens $=\\\\!512$ # the max number of generated tokens 18 )  \\n\\ninstructions (Honovich et al., 2022). Our empirical study validates the effectiveness of using GPT-4-generated data for LLM instruction-tuning, and suggests practical tips of building a general-purpose instruction-following agent powered by LLMs.', '2 DATASET': 'Data Collection. We reuse 52K unique instructions in the instruction-following data collected in the Alpaca dataset (Taori et al., 2023). Each instruction describes the task the model should perform. We follow the same prompting strategy to consider cases with and without input, which is the optional context or input for the task. The output answers to the instruction instance using LLMs. In the Alpaca dataset, the output is generated using GPT-3.5 (text-davinci-003) but we instead consider GPT-4 (gpt-4) for data generation. Specifically, we generate the following four datasets with GPT-4:  \\n\\n(1) English Instruction-Following Data: For the 52K instructions collected in Alpaca (Taori et al., 2023), one English GPT-4 answer is provided for each. The details are described in Algorithm 1. We leave it as future work to follow an iterative process to construct our own instruction set using GPT-4 and self-instruct (Wang et al., 2022a).   \\n(2) Chinese Instruction-Following Data: We use ChatGPT to translate the 52K instructions into Chinese and ask GPT-4 to answer them in Chinese. This allows us to build a Chinese instruction-following model based on LLaMA, and study cross-language generalization ability of instruction-tuning.   \\n(3) Comparison Data: We ask GPT-4 to rate its own response from 1 to 10. Furthermore, we ask GPT-4 to compare and rate the responses from the three models, including GPT-4, GPT-3.5 and OPT-IML (Iyer et al., 2022). This is used to train reward models.   \\n(4) Answers on Unnatural Instructions: The GPT-4 answers are decoded on the core dataset of 68K instruction-input-output triplets (Honovich et al., 2022). The subset is used to quantify the gap between GPT-4 and our instruction-tuned models at scale.  \\n\\nData Statistics. We compare the English output response sets of GPT-4 and GPT-3.5 in Figure 1. For each output, the root verb and the direct-object noun are extracted; The frequency over the unique verb-noun pairs are computed over each output set. The verb-noun pairs whose frequency are higher than 10 are displayed in Figure 1(a) and (b), and the most frequent 25 pairs of two sets are compared in Figure 1(c). The frequency distributions of the sequence length are compared in Figure 1(d). GPT-4 tends to generated longer sequences than GPT-3.5. The GPT-3.5 data in Alpaca exhibits an output distribution with a longer tail than our GPT-4-generated output distribution, probably because the Alpaca dataset involves an iterative data collection process to remove similar instruction instances at each iteration, which is absent in our current one-time data generation. Despite this simple process, the GPT-4 generated instruction-following data demonstrates more favorable alignment performance, as shown in experiments later.  \\n\\n![](https://cdn-mineru.openxlab.org.cn/extract/1c083c34-6489-47d8-b145-80ab407aa524/648e7b3ac7ef1b964ee54fec1f56ea8817604666a8cb6a75cd4cb4ed720ff2bf.jpg)  \\nFigure 1: Comparison of generated responses using GPT-4 and GPT-3: (a,b) The root verb-noun pairs of GPT-4 and GPT-3, where the inner circle of the plot represents the root verb of the output response, and the outer circle represents the direct nouns. (c) The top 25 verb-noun pairs and their frequencies. (d) Comparison of output sequence length.', '3 INSTRUCTION-TUNING LANGUAGE MODELS': '', '3.1 SELF-INSTRUCT TUNING': 'We train two models using supervised finetuning using the LLaMA 7B checkpoint: $(i)$ LLaMA-GPT4 is trained on 52K English instruction-following data generated by GPT-4, which distribution is displayed in Figure 1. (ii) LLaMA-GPT4-CN is trained on 52K Chinese instruction-following data from GPT-4. We follow the training schedule in (Taori et al., 2023) for fair comparisons. These models are used to study the data quality of GPT-4 and the cross-language generalization properties when instruction-tuning LLMs in one language.', '3.2 REWARD MODELS': 'Reinforcement Learning from Human Feedback (RLHF) aims to align the LLM behavior with human preferences in order to make it more useful. One key component of RLHF is reward modeling, where the problem is formulated as a regression task to predict a scalar reward given a prompt and a response (Askell et al., 2021; Ouyang et al., 2022). This approach typically requires large-scale comparison data, where two model responses on the same prompt are compared Ouyang et al. (2022). Existing open-source works such as Alpaca, Vicuna, and Dolly (Databricks, 2023) do not involve RLHF due to the high cost of labeling comparison data. Meanwhile, recent studies show that GPT-4 is capable of identifying and fixing its own mistakes, and accurately judging the quality of responses(Peng et al., 2023; Bai et al., 2022; Madaan et al., 2023; Kim et al., 2023). Therefore, to facilitate research on RLHF, we have created comparison data using GPT-4, as described in Section 2.  \\n\\nTo evaluate data quality, we train a reward model based on OPT 1.3B (Iyer et al., 2022) to rate different responses. For each instance of the comparison data involving one prompt $\\\\textbf{\\\\em x}$ and $K$ responses, GPT-4 assigns a score $s\\\\in[1,10]$ for each response. There are $C_{2}^{K}$ unique pairs constructed from this instance, each pair is $(y_{l},y_{h})$ , whose corresponding scores follow $s_{l}~<~s_{h}$ . A reward model $r_{\\\\theta}$ parameterized by $\\\\pmb{\\\\theta}$ is trained with the objective: $\\\\operatorname*{min}\\\\log(\\\\sigma(r_{\\\\pmb\\\\theta}(x,y_{h})-r_{\\\\pmb\\\\theta}(\\\\pmb x,y_{l})))$ , where $\\\\sigma$ is the sigmoid function. The distribution of the comparison data is shown in Figure 2.  \\n\\n![](https://cdn-mineru.openxlab.org.cn/extract/1c083c34-6489-47d8-b145-80ab407aa524/f3c4e0d3b5af16b40081d39fc62beab25c81f465d0774242ad96e88ab334ad4b.jpg)  \\nFigure 2: The distribution of comparison data.', '4 EXPERIMENTAL RESULTS': '', '4.1 BENCHMARKS': 'It is known that LLM evaluation remains a significant challenge. Our goal is to evaluate self-instruct tuned models on GPT-4 data on unseen instructions, to study their ability to follow instructions for arbitrary tasks. Specifically, we use three established datasets in our study:  \\n\\nâ€¢ User-Oriented-Instructions- $252^{\\\\,2}$ (Wang et al., 2022a) is a manually curated set involving 252 instructions, motivated by 71 user-oriented applications such as Grammarly, StackOverflow, Overleaf, rather than well-studied NLP tasks.   \\nâ€¢ Vicuna-Instructions- $80^{3}$ (Vicuna, 2023) is a dataset synthesized by gpt-4 with 80 challenging questions that baseline models find challenging. Beside generic instructions, there are 8 categories, including knowledge, math, Fermi, counterfactual, roleplay, generic, coding, writing, common-sense.   \\nâ€¢ Unnatural Instructions4 (Honovich et al., 2022) is a dataset of 68,478 samples synthesized by text-davinci-002 using 3-shot in-context-learning from 15 manually-constructed examples.  \\n\\n![](https://cdn-mineru.openxlab.org.cn/extract/1c083c34-6489-47d8-b145-80ab407aa524/87f25a37070ba8649c23d9507c8779372aa5ec3ca4cff0989798cfa82d65fe70.jpg)  \\nFigure 3: Human evaluation.  \\n\\n4.2 HUMAN EVALUATION WITH ALIGNMENT CRITERIA  \\n\\nTo evaluate the alignment quality of our instruction-tuned LLMs, we follow alignment criteria from Anthropic Askell et al. (2021): an assistant is aligned if it is helpful, honest, and harmless (HHH). These criteria are used to evaluate how well an AI system is aligned with human values.  \\n\\nâ€¢ Helpfulness: whether it helps humans achieve their goals. A model that can answer questions accurately is helpful.   \\nâ€¢ Honesty: whether it provides true information, and expresses its uncertainty to avoid misleading human users when necessary. A model that provides false information is not honest.   \\nâ€¢ Harmlessness: whether it does not cause harm to humans. A model that generates hate speech or promotes violence is not harmless.  \\n\\nBased on HHH alignment criteria, we used Amazon Mechanical Turk to perform human evaluation on the model generation results. Please find the interface in Appendix Section A.1. Following (Wang et al., 2022a; Taori et al., 2023), we consider 252 user-oriented instructions for evaluation. We display the human evaluation results in pie charts in Figure 3.  \\n\\nFirst, we compare the quality of generated responses from two instruction-tuned LLaMA models, which are fine-tuned on data generated by GPT-4 and GPT-3, respectively. Note that aligning LLaMA to GPT-3 corresponds to the Stanford Alpaca model. From Figure 3(a), we observe that $(i)$ For the â€œHelpfulnessâ€ criterion, GPT-4 is the clear winner with $54.12\\\\%$ of the votes. GPT-3 only wins $19.74\\\\%$ of the time. (ii) For the â€œHonestyâ€ and â€œHarmlessnessâ€ criteria, the largest portion of votes goes to the tie category, which is substantially higher than the winning categories but GPT-3 (Alpaca) is slightly superior.  \\n\\nSecond, we compare GPT-4-instruction-tuned LLaMA models against the teacher model GPT-4 in Figure 3(b). The observations are quite consistent over the three criteria: GPT-4-instruction-tuned LLaMA performs similarly to the original GPT-4. We conclude that learning from GPT-4 generated data can lead to very comparable performance with the original GPT-4 on the unseen instructional tasks, which suggests a promising direction to developing state-of-the-art instruction-following LLMs.  \\n\\n![](https://cdn-mineru.openxlab.org.cn/extract/1c083c34-6489-47d8-b145-80ab407aa524/391ce593cd636aa1d97e33668f4fce955396fd95f6d83492832bb0978f31573b.jpg)  \\nFigure 4: Performance comparisons evaluated by GPT-4. Each bar represents an evaluation result between two models; the sum of scores are computed and reported (the full score is 800). The relative score is reported in percentage, which is computed as the ratio against a strong opponent model. (a,b) The comparisons of responses from LLaMA GPT4 ranked by our reward model. â€˜Bâ€™ indicates the baseline that the model decodes one response per question. (c,d) All chatbots are compared against ChatGPT and GPT-4, respectively.', '4.3 COMPARISONS WITH SOTA USING AUTOMATIC EVALUATION': 'Automatic Evaluation with GPT-4. Following (Vicuna, 2023), we employ GPT-4 to automatically evaluate the generated responses of different models on 80 unseen questions in (Vicuna, 2023). We first collect answers from two chatbots, including LLaMA-GPT-4 (7B) and GPT-4, and use the release answers of other chatbots from (Vicuna, 2023), including LLaMA (13B), Alpaca (13B), Vicuna (13B), Bard (Google, 2023), and ChatGPT. For each evaluation, we ask GPT-4 to rate the response quality between two models with scores from 1 to 10. We compare all models against a strong competing model such as ChatGPT and GPT-4, respectively. The results are shown in Figure 4.  \\n\\nFor LLaMA instruction-tuned with GPT-4, we provide two sets of decoding results: $(i)$ One response per question, which is considered the baseline decoding result. $(i i)$ Five responses per questions. For the latter, the reward model is used to rank the responses which are then grouped into five subsets ranked from top 1 to top 5. We compare the five ranked groups against the baseline, and show the relative scores in Figure 4 (a,b). The ChatGPT and GPT-4 evaluation is consistent with the orders suggested by our reward model, which demonstrate the value of the feedback data and effectiveness of the reward model.  \\n\\n![](https://cdn-mineru.openxlab.org.cn/extract/1c083c34-6489-47d8-b145-80ab407aa524/c661bdcdb2121dac7141676b9ecf33a671b85c5f15bb0a86f36646d2ce185a86.jpg)  \\n\\n![](https://cdn-mineru.openxlab.org.cn/extract/1c083c34-6489-47d8-b145-80ab407aa524/29a8da4f1641549379f9319f390757157c475a89f73378094a0d7bdcec722adb.jpg)  \\n(b) All chatbots against GPT-4, whose Chinese responses are generated by asking Chinese questions  \\n\\n![](https://cdn-mineru.openxlab.org.cn/extract/1c083c34-6489-47d8-b145-80ab407aa524/4aa569edc6e700e968f12c9c118787baa83f754d0997493bb5794f772e8dbffe.jpg)  \\nFigure 5: Performance comparisons of Chinese instruction-following evaluated by GPT-4. In (a,b), all models are asked to respond in English, and the responses are translated into Chinese; the scores are computed against translated Chinese in (a) and model generated Chinese in (b). In (c), all models are asked to respond in Chinese.  \\n\\nWe compare all the chatbots in Figure 4(c,d). Instruction tuning of LLaMA with GPT-4 often achieves higher performance than tuning with text-davinci-003 (i.e., Alpaca) and no tuning (i.e., LLaMA): The 7B LLaMA GPT4 outperforms the 13B Alpaca and LLaMA. However, there is still a gap compared with large commercial chatbots such as GPT-4.  \\n\\nWe further study the performance of all the chatbots in Chinese in Figure 5. We first translate English responses of chatbots into Chinese using GPT-4. We also translate English questions into Chinese to obtain answers with GPT-4. The comparisons against translated and generated Chinese responses from GPT-4 are shown in Figure 5 (a) and (b), respectively. There are two interesting observations: (i) we find that the relative score metric of GPT-4 evaluation (Vicuna, 2023) is quite consistent, both in terms of different opponent models (i.e., ChatGPT or GPT-4) and languages (i.e., English or Chinese). $(i i)$ For GPT-4 results alone, the translated responses show superior performance over the generated response in Chinese, probably because GPT-4 is trained in richer English corpus than Chinese, which leads to stronger English instruction-following ability. In Figure 5 (c), we show results for all models who are asked to answer in Chinese.  \\n\\nWe compare LLaMA-GPT4 with GPT-4 and Alpaca unnatural instructions in Figure 6. In terms of the average ROUGE-L scores, Alpaca outperforms the other two models. We note that LLaMA-GPT4 and GPT4 is gradually performing better when the ground truth response length is increasing, eventually showing higher performance when the length is longer than 4. This means that they can better follow instructions when the scenarios are more creative. Across different subsets, LLaMA-GPT4 can closely follow the behavior of GPT-4. When the sequence length is short, both LLaMA-GPT4 and GPT-4 can generate responses that contains the simple ground truth answers, but add extra words to make the response more chat-like, which probably leads to lower ROUGE-L scores.  \\n\\n![](https://cdn-mineru.openxlab.org.cn/extract/1c083c34-6489-47d8-b145-80ab407aa524/8847ff8909dd69a4675f2f75297c311b4a91c57f7f130110f30639fe999b517d.jpg)  \\nFigure 6: ROUGE-L on unnatural instructions evaluated with 9K samples. The instructions are grouped into four subsets based on the ground-truth response length. The mean values are reported in the legend. The difference with GPT-4 is reported on the bar per group. LLaMA-GPT4 is a closer proxy to GPT-4 than Alpaca.', '5 RELATED WORK': 'Instruction Tuning. Instruction tuning of LLMs is an increasingly popular research direction in NLP (Zhong et al., 2021; Ouyang et al., 2022; Wei et al., 2021). Existing works aim to improve the quality and scale of three factors in the development pipeline, including instruction-following data, foundation language models and evaluation benchmarks. Each group typically maintains its own pipeline. For example, scaling instruction-finetuned language models (Chung et al., 2022) is built on top of FLAN (Wei et al., 2021). PromptSource contains a growing collection of prompts (which is also called P3: Public Pool of Prompts) (Bach et al., 2022). T0 is a series of models trained on P3 via multitask prompted training (Sanh et al., 2021). Instruction-tuning of OPT models is considered in (Iyer et al., 2022), where a larger and more comprehensive benchmark OPT-IML Bench is employed, covering FLAN (Wei et al., 2021), Super-NaturalInstructions (Wang et al., 2022b), and UnifiedSKG (Xie et al., 2022).  \\n\\nOpen-Source Efforts. Given the broad capabilities of LLMs exhibited by ChatGPT, open-source models have drawn a significant interest and promoted work towards open, general-purpose, textbased assistants that are aligned with human values. Early attempts on foundation LLMs include BLOOM (Scao et al., 2022), GPT-J (Wang & Komatsuzaki, 2021), GPT-NEO (Black et al., 2021) OPT (Zhang et al., 2022) and LLaMA (Zhang et al., 2023). To align LLMs with chat-based assistance, Open-Assistant (LAION-AI, 2023) is built on GPT-J, and Alpaca/Vicuna are built on LLaMA. Furthermore, OpenFlamingo (Awadalla et al., 2023) and LLaMA-Adapter (Zhang et al., 2023) connect LLaMA with image inputs, paving a way to build open-source multi-modal LLMs.'}\n",
      "\n",
      "Processing sections:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: ABSTRACT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections:  10%|â–ˆ         | 1/10 [00:04<00:41,  4.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: INTRODUCTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections:  20%|â–ˆâ–ˆ        | 2/10 [00:19<01:27, 10.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: 2 DATASET\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:37<01:37, 13.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: 3 INSTRUCTION-TUNING LANGUAGE MODELS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:39<00:55,  9.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: 3.1 SELF-INSTRUCT TUNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:44<00:39,  7.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: 3.2 REWARD MODELS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:55<00:34,  8.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: 4 EXPERIMENTAL RESULTS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:57<00:19,  6.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: 4.1 BENCHMARKS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [01:13<00:19,  9.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: 4.3 COMPARISONS WITH SOTA USING AUTOMATIC EVALUATION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [01:26<00:10, 10.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: 5 RELATED WORK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:44<00:00, 10.47s/it]\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "markdown_path = \"input_file/INSTRUCTION TUNING WITH GPT-4.md\"\n",
    "output_dir = \"output_file\"\n",
    "\n",
    "explanations, qa = process_paper(markdown_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Explanations saved to: output_file/INSTRUCTION TUNING WITH GPT-4_explained.md\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"output_file\"\n",
    "\n",
    "input_filename = Path(markdown_path).stem  # íŒŒì¼ ì´ë¦„ë§Œ ì¶”ì¶œ (í™•ì¥ì ì œì™¸)\n",
    "output_path = os.path.join(output_dir, f\"{input_filename}_explained.md\")\n",
    "\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    for section, explanation in explanations.items():\n",
    "        f.write(f\"\\n## {section}\\n\\n\")\n",
    "        f.write(explanation)\n",
    "        f.write(\"\\n\\n---\\n\")\n",
    "\n",
    "print(f\"\\nExplanations saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "                                                     <span style=\"font-weight: bold; text-decoration: underline\">ABSTRACT</span>                                                      \n",
       "\n",
       "ì´ ì—°êµ¬ì—ì„œëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì„ ê¸°ê³„ê°€ ìƒì„±í•œ ì§€ì‹œë¬¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¯¸ì„¸ ì¡°ì •(finetuning)í•˜ë©´ ìƒˆë¡œìš´        \n",
       "ì‘ì—…ì—ì„œë„ ë›°ì–´ë‚œ ì œë¡œìƒ·(Zero-shot) ì„±ëŠ¥ì„ ë°œíœ˜í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì´ì „ ì—°êµ¬ê°€ ë³´ì—¬ì£¼ì—ˆë‹¤ê³  ì„¤ëª…í•©ë‹ˆë‹¤. ì¦‰, ì¸ê°„ì´   \n",
       "ì‘ì„±í•œ ì§€ì‹œë¬¸ì´ í•„ìš” ì—†ê²Œ ë©ë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì€ GPT-4ë¥¼ í™œìš©í•˜ì—¬ LLM ë¯¸ì„¸ ì¡°ì •ì„ ìœ„í•œ ì§€ì‹œë¬¸ ë°ì´í„°ë¥¼ ìƒì„±í•œ ì²« ì‹œë„ë¥¼ \n",
       "ì†Œê°œí•©ë‹ˆë‹¤. ì—°êµ¬ ê²°ê³¼, GPT-4ê°€ ìƒì„±í•œ 5ë§Œ 2ì²œ ê°œì˜ ì˜ì–´ ë° ì¤‘êµ­ì–´ ì§€ì‹œë¬¸ ë°ì´í„°ê°€ ê¸°ì¡´ ìµœì²¨ë‹¨ ëª¨ë¸ë“¤ì´ ìƒì„±í•œ      \n",
       "ì§€ì‹œë¬¸ ë°ì´í„°ë³´ë‹¤ ìƒˆë¡œìš´ ì‘ì—…ì—ì„œ ë›°ì–´ë‚œ ì œë¡œìƒ· ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤. ë˜í•œ, í¬ê´„ì ì¸ í‰ê°€ ë° ë³´ìƒ ëª¨ë¸ í›ˆë ¨ì„ ìœ„í•´\n",
       "GPT-4ë¡œë¶€í„° í”¼ë“œë°± ë° ë¹„êµ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì˜€ìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” GPT-4ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒì„±í•œ ë°ì´í„°ì™€ ì½”ë“œë² ì´ìŠ¤ë¥¼ ê³µê°œí•˜ê³   \n",
       "ìˆìŠµë‹ˆë‹¤.                                                                                                          \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>\n",
       "\n",
       "                                                   <span style=\"font-weight: bold; text-decoration: underline\">INTRODUCTION</span>                                                    \n",
       "\n",
       "ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ë›°ì–´ë‚œ ì¼ë°˜í™” ëŠ¥ë ¥ì„ ì„¤ëª…í•˜ë©°, ì´ëŸ¬í•œ ëª¨ë¸ë“¤ì´ ìì—°ì–´ ì§€ì‹œë¥¼ ë”°ë¥´ê³  ì‹¤ì œ ì„¸ìƒì—ì„œì˜\n",
       "ì‘ì—…ì„ ì™„ë£Œí•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ë°©ë²•ì„ íƒêµ¬í•˜ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ ë°©ë²• ì¤‘ í•˜ë‚˜ë¡œ ì§€ì‹œë¬¸ íŠœë‹(instruction-tuning)ì´ ìˆìœ¼ë©°, \n",
       "ì´ëŠ” ì¸ê°„ì´ ì£¼ì„ì„ ë‹¨ í”„ë¡¬í”„íŠ¸ì™€ í”¼ë“œë°±ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë‹¤ì–‘í•œ ì‘ì—…ì— ë§ê²Œ ë¯¸ì„¸ ì¡°ì •í•˜ê±°ë‚˜, ìˆ˜ë™ ë˜ëŠ” ìë™ìœ¼ë¡œ   \n",
       "ìƒì„±ëœ ì§€ì‹œë¬¸ì„ ì¶”ê°€í•œ ê³µê°œ ë²¤ì¹˜ë§ˆí¬ì™€ ë°ì´í„°ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ êµ¬í˜„ë©ë‹ˆë‹¤.                                  \n",
       "\n",
       "íŠ¹íˆ Self-Instruct íŠœë‹ì€ ìµœì‹ ì˜ ì§€ì‹œë¬¸ íŠœë‹ëœ êµì‚¬ LLMì´ ìƒì„±í•œ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ì—¬, LLMì„ ì¸ê°„ ì˜ë„ì— ë§ì¶”ëŠ”       \n",
       "ê°„ë‹¨í•˜ê³  íš¨ê³¼ì ì¸ ë°©ë²•ì…ë‹ˆë‹¤. ì´ ì—°êµ¬ ë°©í–¥ì€ LLMì˜ ì œë¡œìƒ· ë° ì†Œìˆ˜ìƒ· ì¼ë°˜í™” ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” íš¨ê³¼ì ì¸ ìˆ˜ë‹¨ì„ ì œê³µí•´\n",
       "ì™”ìŠµë‹ˆë‹¤. ìµœê·¼ ChatGPTì™€ GPT-4ì˜ ì„±ê³µì€ ì´ëŸ¬í•œ ì§€ì‹œë¬¸ íŠœë‹ì„ í†µí•´ ì˜¤í”ˆì†ŒìŠ¤ LLMì„ ê°œì„ í•  í° ê¸°íšŒë¥¼ ì œê³µí•©ë‹ˆë‹¤.      \n",
       "LLaMAëŠ” ì´ëŸ¬í•œ ì˜¤í”ˆì†ŒìŠ¤ LLMë“¤ ì¤‘ í•˜ë‚˜ë¡œ, ìƒìš© LLMì¸ GPT-3ì™€ ì„±ëŠ¥ì„ ê²¬ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. LLaMAê°€ ì§€ì‹œë¥¼ ë”°ë¥¼ ìˆ˜ ìˆë„ë¡ \n",
       "í•˜ë ¤ë©´, ë›°ì–´ë‚œ ì„±ëŠ¥ê³¼ ì €ë¹„ìš©ì˜ Self-Instruct íŠœë‹ì´ ë¹ ë¥´ê²Œ ì±„íƒë˜ê³  ìˆìŠµë‹ˆë‹¤.                                      \n",
       "\n",
       "ë³¸ ë…¼ë¬¸ì—ì„œ ìš°ë¦¬ëŠ” ì²˜ìŒìœ¼ë¡œ GPT-4ë¥¼ Self-Instruct íŠœë‹ì˜ êµì‚¬ë¡œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì£¼ìš” ê¸°ì—¬ëŠ”     \n",
       "ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:                                                                                                   \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span><span style=\"font-weight: bold\">GPT-4 ë°ì´í„°:</span> ìš°ë¦¬ëŠ” ì˜ì–´ì™€ ì¤‘êµ­ì–´ë¡œ ëœ 52K ê°œì˜ ì§€ì‹œë¬¸ ë°ì´í„°ì…‹ê³¼ ì„¸ ê°œì˜ ì§€ì‹œë¬¸ íŠœë‹ëœ ëª¨ë¸ì˜ ì¶œë ¥ì„ í‰ê°€í•œ   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>í”¼ë“œë°± ë°ì´í„°ë¥¼ í¬í•¨í•˜ì—¬ GPT-4ê°€ ìƒì„±í•œ ë°ì´í„°ë¥¼ ê³µê°œí•©ë‹ˆë‹¤.                                                    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span><span style=\"font-weight: bold\">ëª¨ë¸ ë° í‰ê°€:</span> GPT-4ê°€ ìƒì„±í•œ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§€ì‹œë¬¸ íŠœë‹ëœ LLaMA ëª¨ë¸ê³¼ ë³´ìƒ ëª¨ë¸ì„ ê°œë°œí–ˆìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì„¸   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>ê°€ì§€ ê¸°ì¤€ì— ëŒ€í•œ ì¸ê°„ í‰ê°€, GPT-4 í”¼ë“œë°±ì„ ì‚¬ìš©í•œ ìë™ í‰ê°€, ê·¸ë¦¬ê³  ë¹„ì •í˜• ì§€ì‹œë¬¸ì— ëŒ€í•œ ROUGE-L ì ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>ì§€ì‹œë¬¸ íŠœë‹ëœ LLMì˜ í’ˆì§ˆì„ í‰ê°€í•©ë‹ˆë‹¤.                                                                          \n",
       "\n",
       "ì´ ì—°êµ¬ëŠ” GPT-4ë¡œ ìƒì„±ëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ LLM ì§€ì‹œë¬¸ íŠœë‹ì˜ íš¨ê³¼ì„±ì„ í™•ì¸í•˜ê³ , ì¼ë°˜ ëª©ì ì˜ ì§€ì‹œë¬¸ì„ ë”°ë¥´ëŠ”          \n",
       "ì—ì´ì „íŠ¸ë¥¼ êµ¬ì¶•í•˜ëŠ” ë° ìˆì–´ ì‹¤ìš©ì ì¸ íŒì„ ì œì•ˆí•©ë‹ˆë‹¤.                                                              \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>\n",
       "\n",
       "                                                     <span style=\"font-weight: bold; text-decoration: underline\">2 DATASET</span>                                                     \n",
       "\n",
       "ì´ ë…¼ë¬¸ì˜ '2 ë°ì´í„°ì…‹' ì„¹ì…˜ì—ì„œëŠ” ë°ì´í„° ìˆ˜ì§‘ ê³¼ì •ê³¼ GPT-4ë¥¼ ì‚¬ìš©í•œ ë°ì´í„° ìƒì„± ë°©ë²•ì— ëŒ€í•´ ì„¤ëª…í•©ë‹ˆë‹¤.            \n",
       "\n",
       "ë°ì´í„° ìˆ˜ì§‘: Alpaca ë°ì´í„°ì…‹(Taori et al., 2023)ì—ì„œ ìˆ˜ì§‘ëœ 52,000ê°œì˜ ê³ ìœ í•œ ì§€ì‹œë¬¸ì„ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤. ê° ì§€ì‹œë¬¸ì€   \n",
       "ëª¨ë¸ì´ ìˆ˜í–‰í•´ì•¼ í•  ì‘ì—…ì„ ì„¤ëª…í•©ë‹ˆë‹¤. ì§€ì‹œë¬¸ì—ëŠ” ì‘ì—…ì— ëŒ€í•œ ì„ íƒì  ë§¥ë½ì´ë‚˜ ì…ë ¥ì´ ìˆì„ ìˆ˜ë„ ìˆê³  ì—†ì„ ìˆ˜ë„       \n",
       "ìˆìŠµë‹ˆë‹¤. Alpaca ë°ì´í„°ì…‹ì—ì„œëŠ” GPT-3.5(text-davinci-003)ë¥¼ ì‚¬ìš©í•´ ì¶œë ¥ì„ ìƒì„±í–ˆì§€ë§Œ, ìš°ë¦¬ëŠ” GPT-4(gpt-4)ë¥¼        \n",
       "í™œìš©í•˜ì—¬ ì¶œë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤. GPT-4ë¥¼ ì‚¬ìš©í•´ ë‹¤ìŒ ë„¤ ê°€ì§€ ë°ì´í„°ì…‹ì„ ìƒì„±í•©ë‹ˆë‹¤.                                     \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 1 </span><span style=\"font-weight: bold\">ì˜ì–´ ì§€ì‹œë¬¸-ë‹µë³€ ë°ì´í„°:</span> Alpacaì—ì„œ ìˆ˜ì§‘í•œ 52,000ê°œì˜ ì§€ì‹œë¬¸ ê°ê°ì— ëŒ€í•´ GPT-4ê°€ ì˜ì–´ë¡œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>ìì„¸í•œ ë‚´ìš©ì€ ì•Œê³ ë¦¬ì¦˜ 1ì— ë‚˜ì™€ ìˆìŠµë‹ˆë‹¤. GPT-4ì™€ Self-Instructë¥¼ ì‚¬ìš©í•˜ì—¬ ìì²´ ì§€ì‹œ ì„¸íŠ¸ë¥¼ êµ¬ì„±í•˜ëŠ” ë°˜ë³µì      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>ê³¼ì •ì„ í–¥í›„ ì‘ì—…ìœ¼ë¡œ ë‚¨ê²¨ë‘¡ë‹ˆë‹¤.                                                                                \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 2 </span><span style=\"font-weight: bold\">ì¤‘êµ­ì–´ ì§€ì‹œë¬¸-ë‹µë³€ ë°ì´í„°:</span> ChatGPTë¥¼ ì‚¬ìš©í•˜ì—¬ 52,000ê°œì˜ ì§€ì‹œë¬¸ì„ ì¤‘êµ­ì–´ë¡œ ë²ˆì—­í•˜ê³ , GPT-4ê°€ ì¤‘êµ­ì–´ë¡œ ë‹µë³€í•˜ê²Œ  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ LLaMAì— ê¸°ë°˜í•œ ì¤‘êµ­ì–´ ì§€ì‹œë¬¸-ë”°ë¥´ê¸° ëª¨ë¸ì„ êµ¬ì¶•í•˜ê³ , ì§€ì‹œë¬¸ íŠœë‹ì˜ ì–¸ì–´ ê°„ ì¼ë°˜í™” ëŠ¥ë ¥ì„      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>ì—°êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.                                                                                             \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 3 </span><span style=\"font-weight: bold\">ë¹„êµ ë°ì´í„°:</span> GPT-4ì—ê²Œ ìì‹ ì˜ ì‘ë‹µì„ 1ë¶€í„° 10ê¹Œì§€ í‰ê°€í•˜ë„ë¡ ìš”ì²­í•©ë‹ˆë‹¤. ë˜í•œ, GPT-4ì—ê²Œ GPT-4, GPT-3.5, ê·¸ë¦¬ê³  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>OPT-IMLì„ í¬í•¨í•œ ì„¸ ëª¨ë¸ì˜ ì‘ë‹µì„ ë¹„êµí•˜ì—¬ í‰ê°€í•˜ë„ë¡ ìš”ì²­í•©ë‹ˆë‹¤. ì´ëŠ” ë³´ìƒ ëª¨ë¸ì„ í›ˆë ¨í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 4 </span><span style=\"font-weight: bold\">ë¹„ì •ìƒ ì§€ì‹œë¬¸ì— ëŒ€í•œ ë‹µë³€:</span> 68,000ê°œì˜ ì§€ì‹œë¬¸-ì…ë ¥-ì¶œë ¥ ì‚¼ì¤‘ ì½”ì–´ ë°ì´í„°ì…‹ì— ëŒ€í•´ GPT-4ì˜ ë‹µë³€ì„ ë””ì½”ë”©í•©ë‹ˆë‹¤. ì´\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>í•˜ìœ„ ì„¸íŠ¸ëŠ” GPT-4ì™€ ìš°ë¦¬ ì§€ì‹œë¬¸ íŠœë‹ëœ ëª¨ë¸ë“¤ ê°„ì˜ ì°¨ì´ë¥¼ ê³„ëŸ‰í™”í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.                             \n",
       "\n",
       "ë°ì´í„° í†µê³„: ê·¸ë¦¼ 1ì—ì„œëŠ” GPT-4ì™€ GPT-3.5ì˜ ì˜ì–´ ì¶œë ¥ ì‘ë‹µ ì„¸íŠ¸ë¥¼ ë¹„êµí•©ë‹ˆë‹¤. ê° ì¶œë ¥ì— ëŒ€í•´ ë™ì‚¬ ë¿Œë¦¬ì™€ ì§ì ‘      \n",
       "ëª©ì ì–´ ëª…ì‚¬ë¥¼ ì¶”ì¶œí•˜ê³ , ê° ì¶œë ¥ ì„¸íŠ¸ì—ì„œ ê³ ìœ í•œ ë™ì‚¬-ëª…ì‚¬ ìŒì˜ ë¹ˆë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. ë¹ˆë„ê°€ 10 ì´ìƒì¸ ë™ì‚¬-ëª…ì‚¬ ìŒì€  \n",
       "ê·¸ë¦¼ 1(a)ì™€ (b)ì— í‘œì‹œë˜ë©°, ë‘ ì„¸íŠ¸ì˜ ê°€ì¥ ë¹ˆë²ˆí•œ 25ê°œì˜ ìŒì„ ê·¸ë¦¼ 1(c)ì— ë¹„êµí•©ë‹ˆë‹¤. ì¶œë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´ì˜ ë¹ˆë„      \n",
       "ë¶„í¬ëŠ” ê·¸ë¦¼ 1(d)ì— ë¹„êµë˜ì–´ ìˆìŠµë‹ˆë‹¤. GPT-4ëŠ” GPT-3.5ë³´ë‹¤ ë” ê¸´ ì‹œí€€ìŠ¤ë¥¼ ìƒì„±í•˜ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤. Alpacaì˜ GPT-3.5\n",
       "ë°ì´í„°ëŠ” ë°˜ë³µì ì¸ ë°ì´í„° ìˆ˜ì§‘ ê³¼ì •ì„ í†µí•´ ê° ë°˜ë³µì—ì„œ ìœ ì‚¬í•œ ì§€ì‹œë¬¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì œê±°í•˜ì—¬ ë” ê¸´ ê¼¬ë¦¬ë¥¼ ê°€ì§„ ì¶œë ¥     \n",
       "ë¶„í¬ë¥¼ ë³´ì…ë‹ˆë‹¤. ì´ëŠ” í˜„ ì‹œì ì˜ ì¼íšŒì„± ë°ì´í„° ìƒì„± ê³¼ì •ì—ì„œëŠ” ì—†ëŠ” íŠ¹ì§•ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ê°„ë‹¨í•œ ê³¼ì •ì—ë„ ë¶ˆêµ¬í•˜ê³ ,    \n",
       "GPT-4ê°€ ìƒì„±í•œ ì§€ì‹œë¬¸-ë”°ë¥´ê¸° ë°ì´í„°ëŠ” ì´í›„ ì‹¤í—˜ì—ì„œ ë” ë‚˜ì€ ì •ë ¬ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.                                \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>\n",
       "\n",
       "                                       <span style=\"font-weight: bold; text-decoration: underline\">3 INSTRUCTION-TUNING LANGUAGE MODELS</span>                                        \n",
       "\n",
       "I apologize, but it seems that the '3 INSTRUCTION-TUNING LANGUAGE MODELS' section content is missing from your     \n",
       "request. If you can provide the full text of that section, I will be happy to explain it in detail.                \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>\n",
       "\n",
       "                                             <span style=\"font-weight: bold; text-decoration: underline\">3.1 SELF-INSTRUCT TUNING</span>                                              \n",
       "\n",
       "ì´ ì„¹ì…˜ì—ì„œëŠ” Self-Instruct íŠœë‹ì„ í†µí•´ ì–¸ì–´ ëª¨ë¸ì„ í›ˆë ¨í•˜ëŠ” ê³¼ì •ì— ëŒ€í•´ ì„¤ëª…í•©ë‹ˆë‹¤. ì—°êµ¬ì—ì„œëŠ” LLaMA 7B           \n",
       "ì²´í¬í¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‘ ê°€ì§€ ëª¨ë¸ì„ ì§€ë„ í•™ìŠµ ë°©ì‹ìœ¼ë¡œ ë¯¸ì„¸ ì¡°ì •í–ˆìŠµë‹ˆë‹¤.                                         \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 1 </span><span style=\"font-weight: bold\">LLaMA-GPT4 ëª¨ë¸</span>: ì´ ëª¨ë¸ì€ GPT-4ê°€ ìƒì„±í•œ 52,000ê°œì˜ ì˜ì–´ ì§€ì‹œë¬¸-ë”°ë¥´ê¸° ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ í›ˆë ¨ë˜ì—ˆìŠµë‹ˆë‹¤. ì´    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>ë°ì´í„°ì˜ ë¶„í¬ëŠ” ì´ì „ì˜ ê·¸ë¦¼ 1ì—ì„œ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤.                                                               \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 2 </span><span style=\"font-weight: bold\">LLaMA-GPT4-CN ëª¨ë¸</span>: ì´ ëª¨ë¸ì€ GPT-4ê°€ ìƒì„±í•œ 52,000ê°œì˜ ì¤‘êµ­ì–´ ì§€ì‹œë¬¸-ë”°ë¥´ê¸° ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ í›ˆë ¨ë˜ì—ˆìŠµë‹ˆë‹¤.  \n",
       "\n",
       "í›ˆë ¨ ê³¼ì •ì—ì„œëŠ” ê³µì •í•œ ë¹„êµë¥¼ ìœ„í•´ Taori et al.(2023)ì˜ í›ˆë ¨ ìŠ¤ì¼€ì¤„ì„ ë”°ëìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ëª¨ë¸ë“¤ì€ GPT-4ê°€ ìƒì„±í•œ   \n",
       "ë°ì´í„°ì˜ í’ˆì§ˆì„ ì—°êµ¬í•˜ê³ , í•˜ë‚˜ì˜ ì–¸ì–´ë¡œ ì§€ì‹œë¬¸ íŠœë‹ì„ í–ˆì„ ë•Œì˜ êµì°¨ ì–¸ì–´ ì¼ë°˜í™” íŠ¹ì„±ì„ ì—°êµ¬í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.    \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>\n",
       "\n",
       "                                                 <span style=\"font-weight: bold; text-decoration: underline\">3.2 REWARD MODELS</span>                                                 \n",
       "\n",
       "ì´ ì„¹ì…˜ì—ì„œëŠ” ì¸ê°„ í”¼ë“œë°±ì„ í†µí•œ ê°•í™” í•™ìŠµ(RLHF, Reinforcement Learning from Human Feedback)ì˜ í•µì‹¬ ìš”ì†Œì¸ ë³´ìƒ    \n",
       "ëª¨ë¸ë§ì— ëŒ€í•´ ì„¤ëª…í•©ë‹ˆë‹¤. RLHFëŠ” LLMì˜ í–‰ë™ì„ ì¸ê°„ì˜ ì„ í˜¸ì™€ ë§ì¶° ëª¨ë¸ì„ ë”ìš± ìœ ìš©í•˜ê²Œ ë§Œë“¤ë ¤ëŠ” ëª©ì ì„ ê°€ì§€ê³        \n",
       "ìˆìŠµë‹ˆë‹¤. ë³´ìƒ ëª¨ë¸ë§ì€ í”„ë¡¬í”„íŠ¸ì™€ ì‘ë‹µì´ ì£¼ì–´ì¡Œì„ ë•Œ ìŠ¤ì¹¼ë¼ ë³´ìƒì„ ì˜ˆì¸¡í•˜ëŠ” íšŒê·€ ê³¼ì œë¡œ ë¬¸ì œë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ \n",
       "í•˜ë ¤ë©´ ëŒ€ê·œëª¨ ë¹„êµ ë°ì´í„°ê°€ í•„ìš”í•œë°, ë‘ ê°€ì§€ ëª¨ë¸ì˜ ì‘ë‹µì„ ê°™ì€ í”„ë¡¬í”„íŠ¸ì— ëŒ€í•´ ë¹„êµí•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ       \n",
       "Alpaca, Vicuna, Dolly ë“±ì˜ ê¸°ì¡´ ì˜¤í”ˆì†ŒìŠ¤ ì‘ì—…ì€ ë¹„êµ ë°ì´í„° ë ˆì´ë¸”ë§ ë¹„ìš©ì´ ë†’ì•„ RLHFë¥¼ í¬í•¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.         \n",
       "\n",
       "í•œí¸, ìµœê·¼ ì—°êµ¬ëŠ” GPT-4ê°€ ìì‹ ì˜ ì‹¤ìˆ˜ë¥¼ ì‹ë³„í•˜ê³  ìˆ˜ì •í•˜ë©°, ì‘ë‹µ í’ˆì§ˆì„ ì •í™•íˆ í‰ê°€í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤.     \n",
       "ë”°ë¼ì„œ RLHF ì—°êµ¬ë¥¼ ì´‰ì§„í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” 2ì ˆì—ì„œ ì„¤ëª…í•œ ë°”ì™€ ê°™ì´ GPT-4ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„êµ ë°ì´í„°ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.     \n",
       "\n",
       "ë°ì´í„° í’ˆì§ˆ í‰ê°€ë¥¼ ìœ„í•´, ìš°ë¦¬ëŠ” OPT 1.3B ê¸°ë°˜ìœ¼ë¡œ ë³´ìƒ ëª¨ë¸ì„ í›ˆë ¨í•˜ì—¬ ì„œë¡œ ë‹¤ë¥¸ ì‘ë‹µë“¤ì„ í‰ê°€í•©ë‹ˆë‹¤. ê° ë¹„êµ      \n",
       "ë°ì´í„° ì¸ìŠ¤í„´ìŠ¤ëŠ” í•œ í”„ë¡¬í”„íŠ¸ $\\textbf{\\em x}$ì™€ $K$ê°œì˜ ì‘ë‹µì„ í¬í•¨í•˜ê³ , GPT-4ëŠ” ê° ì‘ë‹µì— ëŒ€í•´ 1ë¶€í„° 10ê¹Œì§€ì˜    \n",
       "ì ìˆ˜ $s$ë¥¼ ì¤ë‹ˆë‹¤. ì´ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ê³ ìœ í•œ ìŒ $C_{2}^{K}$ë¥¼ êµ¬ì„±í•  ìˆ˜ ìˆìœ¼ë©°, ê° ìŒì€ $(y_{l}, y_{h})$ë¡œ í‘œí˜„ë©ë‹ˆë‹¤.\n",
       "ì—¬ê¸°ì„œ $y_{l}$ì˜ ì ìˆ˜ëŠ” $y_{h}$ì˜ ì ìˆ˜ë³´ë‹¤ ë‚®ìŠµë‹ˆë‹¤($s_{l} &lt; s_{h}$). ë³´ìƒ ëª¨ë¸ $r_{\\theta}$ëŠ” ë‹¤ìŒ ëª©í‘œë¡œ         \n",
       "í›ˆë ¨ë©ë‹ˆë‹¤: $\\operatorname*{min}\\log(\\sigma(r_{\\pmb\\theta}(x,y_{h})-r_{\\pmb\\theta}(\\pmb x,y_{l})))$, ì—¬ê¸°ì„œ        \n",
       "$\\sigma$ëŠ” ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ì…ë‹ˆë‹¤. ë¹„êµ ë°ì´í„°ì˜ ë¶„í¬ëŠ” ê·¸ë¦¼ 2ì— ë‚˜íƒ€ë‚˜ ìˆìŠµë‹ˆë‹¤.                                   \n",
       "\n",
       "ì´ ì ‘ê·¼ ë°©ì‹ì€ ë³´ìƒ ëª¨ë¸ì´ ë‹¤ì–‘í•œ ì‘ë‹µ í’ˆì§ˆì„ í‰ê°€í•˜ê³  RLHFì˜ íš¨ê³¼ì„±ì„ ë†’ì´ëŠ” ë° ë„ì›€ì„ ì¤„ ìˆ˜ ìˆë„ë¡               \n",
       "ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.                                                                                                    \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>\n",
       "\n",
       "                                              <span style=\"font-weight: bold; text-decoration: underline\">4 EXPERIMENTAL RESULTS</span>                                               \n",
       "\n",
       "It looks like the content from the '4 EXPERIMENTAL RESULTS' section is missing from your request. If you can       \n",
       "provide the full text of that section, I would be happy to explain it in detail.                                   \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>\n",
       "\n",
       "                                                  <span style=\"font-weight: bold; text-decoration: underline\">4.1 BENCHMARKS</span>                                                   \n",
       "\n",
       "ì´ ì„¹ì…˜ì—ì„œëŠ” LLM(ëŒ€í˜• ì–¸ì–´ ëª¨ë¸)ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•œ ë²¤ì¹˜ë§ˆí¬ë¥¼ ì†Œê°œí•˜ê³ , ì¸ê°„ í‰ê°€ ë°©ë²•ê³¼ ê·¸ ê²°ê³¼ë¥¼           \n",
       "ì„¤ëª…í•©ë‹ˆë‹¤.                                                                                                        \n",
       "\n",
       "                                                     <span style=\"font-weight: bold\">ë²¤ì¹˜ë§ˆí¬:</span>                                                     \n",
       "\n",
       "ì´ ì—°êµ¬ì˜ ëª©í‘œëŠ” GPT-4 ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµëœ Self-Instruct ëª¨ë¸ë“¤ì´ ìƒˆë¡œìš´ ì§€ì‹œë¬¸ì— ëŒ€í•´ ì–¼ë§ˆë‚˜ ì˜ ë°˜ì‘í•˜ëŠ”ì§€   \n",
       "í‰ê°€í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì—°êµ¬ì—ì„œ ì‚¬ìš©í•œ ë°ì´í„°ì…‹ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:                                                     \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span><span style=\"font-weight: bold\">User-Oriented-Instructions- $252^{,2}$</span>: ì´ ë°ì´í„°ì…‹ì€ 71ê°œì˜ ì‚¬ìš©ì ì¤‘ì‹¬ ì• í”Œë¦¬ì¼€ì´ì…˜(ì˜ˆ: Grammarly,            \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>StackOverflow, Overleaf)ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ 252ê°œì˜ ì§€ì‹œë¬¸ì„ í¬í•¨í•©ë‹ˆë‹¤.                                             \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span><span style=\"font-weight: bold\">Vicuna-Instructions- $80^{3}$</span>: GPT-4ê°€ ìƒì„±í•œ 80ê°œì˜ ë„ì „ì ì¸ ì§ˆë¬¸ì´ í¬í•¨ë˜ì–´ ìˆìœ¼ë©°, ê¸°ë³¸ ëª¨ë¸ë“¤ì´ ì–´ë ¤ì›Œí•˜ëŠ”  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>ì§ˆë¬¸ë“¤ì…ë‹ˆë‹¤. ì§€ì‹, ìˆ˜í•™, Fermi ë¬¸ì œ, ë°˜ì‚¬ì‹¤ì  ì‹œë‚˜ë¦¬ì˜¤, ë¡¤í”Œë ˆì‰, ì¼ë°˜, ì½”ë”©, ê¸€ì“°ê¸°, ìƒì‹ ë“± 8ê°œì˜ ì¹´í…Œê³ ë¦¬ê°€ \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>ìˆìŠµë‹ˆë‹¤.                                                                                                       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span><span style=\"font-weight: bold\">Unnatural Instructions</span>: 68,478ê°œì˜ ìƒ˜í”Œë¡œ êµ¬ì„±ëœ ì´ ë°ì´í„°ì…‹ì€ 15ê°œì˜ ìˆ˜ì‘ì—… ì˜ˆì œë¥¼ ì‚¬ìš©í•œ 3-ìƒ· ë§¥ë½ í•™ìŠµìœ¼ë¡œ   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>text-davinci-002ê°€ í•©ì„±í•œ ê²ƒì…ë‹ˆë‹¤.                                                                             \n",
       "\n",
       "                                                    <span style=\"font-weight: bold\">ì¸ê°„ í‰ê°€:</span>                                                     \n",
       "\n",
       "ëª¨ë¸ì˜ ì •ë ¬ í’ˆì§ˆì„ í‰ê°€í•˜ê¸° ìœ„í•´ Anthropic Askell et al.(2021)ì—ì„œ ì œì•ˆí•œ ì •ë ¬ ê¸°ì¤€ì„ ë”°ëìŠµë‹ˆë‹¤. ì´ ê¸°ì¤€ì€ AI     \n",
       "ì‹œìŠ¤í…œì´ ì¸ê°„ì˜ ê°€ì¹˜ì— ì–¼ë§ˆë‚˜ ì˜ ì •ë ¬ë˜ì–´ ìˆëŠ”ì§€ë¥¼ í‰ê°€í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ê¸°ì¤€ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:                 \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span><span style=\"font-weight: bold\">ë„ì›€ë¨</span>: ì¸ê°„ì´ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ”ì§€ ì—¬ë¶€ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.                                              \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span><span style=\"font-weight: bold\">ì •ì§ì„±</span>: ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•˜ê³  í•„ìš”ì‹œ ë¶ˆí™•ì‹¤ì„±ì„ í‘œí˜„í•˜ì—¬ ì¸ê°„ ì‚¬ìš©ìë¥¼ ì˜¤ë„í•˜ì§€ ì•ŠëŠ”ì§€ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.           \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span><span style=\"font-weight: bold\">ë¬´í•´ì„±</span>: ì¸ê°„ì—ê²Œ í•´ë¥¼ ë¼ì¹˜ì§€ ì•ŠëŠ”ì§€ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.                                                               \n",
       "\n",
       "Amazon Mechanical Turkë¥¼ í†µí•´ ì¸ê°„ í‰ê°€ë¥¼ ìˆ˜í–‰í–ˆìœ¼ë©°, ê²°ê³¼ëŠ” ê·¸ë¦¼ 3ì˜ íŒŒì´ ì°¨íŠ¸ì— ë‚˜íƒ€ë‚˜ ìˆìŠµë‹ˆë‹¤.                 \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span><span style=\"font-weight: bold\">ì²« ë²ˆì§¸ ë¹„êµ</span>: GPT-4 ë°ì´í„°ë¡œ ë¯¸ì„¸ ì¡°ì •ëœ LLaMA ëª¨ë¸ê³¼ GPT-3 ë°ì´í„°ë¡œ ë¯¸ì„¸ ì¡°ì •ëœ LLaMA ëª¨ë¸(Stanford Alpaca     \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>ëª¨ë¸)ì„ ë¹„êµí•˜ì—¬, \"ë„ì›€ë¨\" ê¸°ì¤€ì—ì„œ GPT-4ê°€ 54.12%ë¡œ ìš°ì„¸í–ˆìœ¼ë©°, \"ì •ì§ì„±\"ê³¼ \"ë¬´í•´ì„±\"ì—ì„œëŠ” GPT-3(Alpaca)ì´ ë‹¤ì†Œ \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>ìš°ì„¸í•˜ë‚˜ ë¹„ìŠ·í•œ ìˆ˜ì¤€ì˜ ê²°ê³¼ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤.                                                                       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span><span style=\"font-weight: bold\">ë‘ ë²ˆì§¸ ë¹„êµ</span>: GPT-4 ì§€ì‹œë¥¼ ë”°ë¼ íŠœë‹ëœ LLaMA ëª¨ë¸ê³¼ ì›ë˜ì˜ GPT-4 ëª¨ë¸ì„ ë¹„êµí–ˆìœ¼ë©°, ì„¸ ê°€ì§€ ê¸°ì¤€ ëª¨ë‘ì—ì„œ ìœ ì‚¬í•œ\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>ì„±ëŠ¥ì„ ë³´ì—¬ì¤¬ìŠµë‹ˆë‹¤. ì´ëŠ” GPT-4ê°€ ìƒì„±í•œ ë°ì´í„°ë¡œ í•™ìŠµí•¨ìœ¼ë¡œì¨ ì›ë˜ GPT-4 ìˆ˜ì¤€ì˜ ì„±ëŠ¥ì„ ë‹¬ì„±í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>ì˜ë¯¸í•©ë‹ˆë‹¤.                                                                                                     \n",
       "\n",
       "                                             <span style=\"font-weight: bold\">ì¶”ê°€ ê·¸ë¦¼ ì„¤ëª… (ê·¸ë¦¼ 4):</span>                                              \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span>GPT-4ë¡œ í‰ê°€ëœ ì„±ëŠ¥ ë¹„êµë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ê° ë§‰ëŒ€ëŠ” ë‘ ëª¨ë¸ ê°„ì˜ í‰ê°€ ê²°ê³¼ë¥¼ ë‚˜íƒ€ë‚´ë©°, ì´ ì ìˆ˜(ìµœëŒ€ 800ì )ì™€ ìƒëŒ€  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>ì ìˆ˜ ë¹„ìœ¨(ê°•ë ¥í•œ ìƒëŒ€ ëª¨ë¸ ëŒ€ë¹„)ì´ ë³´ê³ ë©ë‹ˆë‹¤.                                                                  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span>LLaMA GPT4 ëª¨ë¸ì˜ ì‘ë‹µì„ ìš°ë¦¬ ë³´ìƒ ëª¨ë¸ë¡œ í‰ê°€í•œ ê²°ê³¼ë¥¼ ë‚˜íƒ€ë‚´ë©°, ì „ë°˜ì ìœ¼ë¡œ ChatGPT ë° GPT-4ì™€ ë¹„êµí•˜ì—¬ ì„±ëŠ¥ì„ \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>í‰ê°€í–ˆìŠµë‹ˆë‹¤.                                                                                                   \n",
       "\n",
       "ì´ ì—°êµ¬ëŠ” GPT-4 ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ í•™ìŠµì´ ìƒˆë¡œìš´ ì§€ì‹œë¬¸ ì‘ì—…ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë‚¸ë‹¤ëŠ” ê°€ëŠ¥ì„±ì„ ì‹œì‚¬í•©ë‹ˆë‹¤.            \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>\n",
       "\n",
       "                               <span style=\"font-weight: bold; text-decoration: underline\">4.3 COMPARISONS WITH SOTA USING AUTOMATIC EVALUATION</span>                                \n",
       "\n",
       "ì´ ì„¹ì…˜ì—ì„œëŠ” ìë™ í‰ê°€ë¥¼ ì‚¬ìš©í•˜ì—¬ LLaMA-GPT4 ëª¨ë¸ê³¼ ë‹¤ë¥¸ ìµœì²¨ë‹¨(State-of-the-Art, SOTA) ëª¨ë¸ë“¤ì„ ë¹„êµí•œ ê²°ê³¼ë¥¼    \n",
       "ì„¤ëª…í•©ë‹ˆë‹¤.                                                                                                        \n",
       "\n",
       "                                                    <span style=\"font-weight: bold\">ìë™ í‰ê°€:</span>                                                     \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span><span style=\"font-weight: bold\">í‰ê°€ ë°©ë²•</span>: GPT-4ë¥¼ í™œìš©í•´ ë‹¤ì–‘í•œ ëª¨ë¸ë“¤ì´ 80ê°œì˜ ìƒˆë¡œìš´ ì§ˆë¬¸ì— ëŒ€í•œ ìƒì„± ì‘ë‹µì˜ í’ˆì§ˆì„ ìë™ í‰ê°€í–ˆìŠµë‹ˆë‹¤.       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>LLaMA-GPT4(7B)ì™€ GPT-4ì˜ ì‘ë‹µì„ ìˆ˜ì§‘í•˜ê³ , Vicuna(2023)ì—ì„œ ê³µê°œëœ ë‹¤ë¥¸ ì±„íŒ… ë´‡ë“¤(LLaMA 13B, Alpaca 13B, Vicuna  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>13B, Bard, ChatGPT)ì˜ ì‘ë‹µì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ë‘ ëª¨ë¸ ê°„ì˜ ì‘ë‹µ í’ˆì§ˆì„ 1ì—ì„œ 10ê¹Œì§€ì˜ ì ìˆ˜ë¡œ í‰ê°€í–ˆìŠµë‹ˆë‹¤.         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span><span style=\"font-weight: bold\">ê²°ê³¼</span>: LLaMA-GPT4ë¥¼ ë‘ ê°€ì§€ ë””ì½”ë”© ê²°ê³¼ë¡œ í‰ê°€í–ˆìŠµë‹ˆë‹¤. ì²«ì§¸ëŠ” ì§ˆë¬¸ë‹¹ í•˜ë‚˜ì˜ ì‘ë‹µë§Œì„ ìƒì„±í•˜ëŠ” ê¸°ë³¸ ë””ì½”ë”© ê²°ê³¼, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>ë‘˜ì§¸ëŠ” ì§ˆë¬¸ë‹¹ ë‹¤ì„¯ ê°œì˜ ì‘ë‹µì„ ìƒì„±í•œ í›„ ë³´ìƒ ëª¨ë¸ì´ ìƒìœ„ì—ì„œ í•˜ìœ„ë¡œ ìˆœìœ„ ë§¤ê¸´ ì‘ë‹µ ê·¸ë£¹ì…ë‹ˆë‹¤. ì´ ê·¸ë£¹ë“¤ì€ ê¸°ë³¸\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>ê²°ê³¼ì™€ ë¹„êµë˜ì—ˆê³ , í‰ê°€ ê²°ê³¼ì—ì„œ í”¼ë“œë°± ë°ì´í„°ì™€ ë³´ìƒ ëª¨ë¸ì˜ ìœ ìš©ì„±ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.                               \n",
       "\n",
       "                                                       <span style=\"font-weight: bold\">ê²°ê³¼:</span>                                                       \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span>LLaMA-GPT4ëŠ” text-davinci-003(ì¦‰, Alpaca)ë¡œ íŠœë‹í•œ ê²½ìš°ë³´ë‹¤ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì—ˆì§€ë§Œ, ì—¬ì „íˆ ìƒì—…ìš© ëŒ€í˜•        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>ì±—ë´‡(e.g., GPT-4)ê³¼ ë¹„êµí•´ì„œëŠ” ì°¨ì´ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.                                                               \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span><span style=\"font-weight: bold\">ì¤‘êµ­ì–´ í‰ê°€</span>: ì˜ì–´ ì‘ë‹µì„ ì¤‘êµ­ì–´ë¡œ ë²ˆì—­í•œ ê²ƒì´ë‚˜, ì¤‘êµ­ì–´ë¡œ ì§ì ‘ ì§ˆë¬¸í•˜ì—¬ ìƒì„±ëœ ì‘ë‹µì„ ë¹„êµí–ˆì„ ë•Œ, ë²ˆì—­ëœ ì‘ë‹µì´\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>ë” ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤. ì´ëŠ” GPT-4ê°€ ì˜ì–´ ì½”í¼ìŠ¤ì—ì„œ ë” ë§ì´ í›ˆë ¨ë˜ì–´ ì˜ì–´ ì§€ì‹œë¬¸ì„ ë”°ë¥´ëŠ” ëŠ¥ë ¥ì´ ë”   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>ê°•ë ¥í•˜ê¸° ë•Œë¬¸ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.                                                                                    \n",
       "\n",
       "                             <span style=\"font-weight: bold\">ë¹„ì •ìƒ ì§€ì‹œë¬¸(unnatural instructions)ì—ì„œì˜ ê²°ê³¼(ê·¸ë¦¼ 6):</span>                             \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span>LLaMA-GPT4ì™€ GPT-4, ê·¸ë¦¬ê³  Alpaca ëª¨ë¸ì„ ë¹„êµí–ˆì„ ë•Œ, Alpacaê°€ ë‹¤ë¥¸ ë‘ ëª¨ë¸ì— ë¹„í•´ í‰ê·  ROUGE-L ì ìˆ˜ì—ì„œ        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>ìš°ìˆ˜í•˜ì˜€ìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì‘ë‹µì˜ ê¸¸ì´ê°€ ê¸¸ì–´ì§ˆìˆ˜ë¡ LLaMA-GPT4ì™€ GPT-4ê°€ ë” ë†’ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ê¸° ì‹œì‘í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>ë” ì°½ì˜ì ì¸ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ì´ ëª¨ë¸ë“¤ì´ ì§€ì‹œë¬¸ì„ ë” ì˜ ë”°ë¥¼ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.                             \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span>ê°ê¸° ë‹¤ë¥¸ ì‘ë‹µ ê¸¸ì´ í•˜ìœ„ ì§‘í•©ì— ëŒ€í•´, LLaMA-GPT4ëŠ” GPT-4ì˜ í–‰ë™ì„ ê°€ê¹ê²Œ ë”°ë¦…ë‹ˆë‹¤. ì‘ë‹µ ê¸¸ì´ê°€ ì§§ì„ ë•Œ,         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>LLaMA-GPT4ì™€ GPT-4ëŠ” ë‹¨ìˆœí•œ ì •ë‹µì„ í¬í•¨í•˜ë˜, ì‘ë‹µì„ ë” ëŒ€í™”í˜•ìœ¼ë¡œ ë§Œë“œëŠ” ì¶”ê°€ì ì¸ ë‹¨ì–´ë¥¼ í¬í•¨í•˜ê¸° ë•Œë¬¸ì—, ë‚®ì€  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>ROUGE-L ì ìˆ˜ë¥¼ ë°›ì„ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.                                                                          \n",
       "\n",
       "ì´ ì‹¤í—˜ ê²°ê³¼ëŠ” LLaMA-GPT4ê°€ GPT-4ì˜ ì¶œë ¥ì„ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµí–ˆì„ ë•Œ, ìƒë‹¹í•œ ì„±ëŠ¥ í–¥ìƒì„ ì´ë£° ìˆ˜ ìˆìŒì„ ë³´ì—¬ì£¼ë©°, ì´ëŠ”  \n",
       "í˜ì‹ ì ì¸ ì§€ì‹œë¬¸-ë”°ë¥´ê¸° LLM ê°œë°œì— ìˆì–´ì„œ ìœ ë§í•œ ë°©í–¥ì„ì„ ì‹œì‚¬í•©ë‹ˆë‹¤.                                               \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>\n",
       "\n",
       "                                                  <span style=\"font-weight: bold; text-decoration: underline\">5 RELATED WORK</span>                                                   \n",
       "\n",
       "ì´ ì„¹ì…˜ì—ì„œëŠ” ì§€ì‹œë¬¸ íŠœë‹ê³¼ ì˜¤í”ˆì†ŒìŠ¤ ë…¸ë ¥ì´ ê´€ë ¨ëœ ì—°êµ¬ì— ëŒ€í•´ ì„¤ëª…í•©ë‹ˆë‹¤.                                         \n",
       "\n",
       "                                         <span style=\"font-weight: bold\">ì§€ì‹œë¬¸ íŠœë‹(Instruction Tuning):</span>                                          \n",
       "\n",
       "ì§€ì‹œë¬¸ íŠœë‹ì€ NLP ë¶„ì•¼ì—ì„œ ì ì  ì£¼ëª©ë°›ê³  ìˆëŠ” ì—°êµ¬ ë°©í–¥ì…ë‹ˆë‹¤. ê¸°ì¡´ ì—°êµ¬ë“¤ì€ í¬ê²Œ ì„¸ ê°€ì§€ ìš”ì†Œì¸ ì§€ì‹œë¬¸-ë”°ë¥´ê¸°     \n",
       "ë°ì´í„°, ê¸°ë³¸ ì–¸ì–´ ëª¨ë¸, í‰ê°€ ë²¤ì¹˜ë§ˆí¬ì˜ í’ˆì§ˆê³¼ ê·œëª¨ë¥¼ í–¥ìƒì‹œí‚¤ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. ê° ì—°êµ¬ ê·¸ë£¹ì€ ì¼ë°˜ì ìœ¼ë¡œ     \n",
       "ìì²´ì ì¸ ê°œë°œ íŒŒì´í”„ë¼ì¸ì„ ìœ ì§€í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, í”Œëœ(FLAN)ì„ ê¸°ë°˜ìœ¼ë¡œ ì§€ì‹œë¬¸ ë¯¸ì„¸ ì¡°ì • ì–¸ì–´ ëª¨ë¸ì˜ ê·œëª¨ë¥¼        \n",
       "í™•ëŒ€í•˜ëŠ” ì—°êµ¬(Chung et al., 2022)ë‚˜, ì ì  ë§ì€ í”„ë¡¬í”„íŠ¸ë¥¼ í¬í•¨í•˜ëŠ” PromptSource(ê³µê³µ í”„ë¡¬í”„íŠ¸ í’€, P3)ì˜ ê°œë°œ(Bach  \n",
       "et al., 2022) ë“±ì´ ìˆìŠµë‹ˆë‹¤. T0ëŠ” P3ì— ê¸°ë°˜í•œ ë‹¤ì¤‘ ê³¼ì œ í”„ë¡¬í”„íŠ¸ í›ˆë ¨ì„ í†µí•´ ê°œë°œëœ ì¼ë ¨ì˜ ëª¨ë¸(Sanh et al.,       \n",
       "2021)ì…ë‹ˆë‹¤. OPT ëª¨ë¸ì˜ ì§€ì‹œë¬¸ íŠœë‹ì€ ë” í¬ê³  í¬ê´„ì ì¸ ë²¤ì¹˜ë§ˆí¬ì¸ OPT-IML Benchë¥¼ ì‚¬ìš©í•˜ì—¬ ì§„í–‰ë˜ì—ˆìœ¼ë©°, ì´ëŠ” FLAN,\n",
       "Super-NaturalInstructions(Wang et al., 2022b), UnifiedSKG(Xie et al., 2022)ì„ í¬í•¨í•©ë‹ˆë‹¤.                          \n",
       "\n",
       "                                        <span style=\"font-weight: bold\">ì˜¤í”ˆì†ŒìŠ¤ ë…¸ë ¥(Open-Source Efforts):</span>                                        \n",
       "\n",
       "ChatGPTì˜ í­ë„“ì€ ëŠ¥ë ¥ ë•ë¶„ì—, ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ì€ í° ê´€ì‹¬ì„ ë°›ìœ¼ë©° ì¸ê°„ì˜ ê°€ì¹˜ì— ë§ëŠ” ì¼ë°˜ ëª©ì ì˜ í…ìŠ¤íŠ¸ ê¸°ë°˜          \n",
       "ì–´ì‹œìŠ¤í„´íŠ¸ë¥¼ ê°œë°œí•˜ëŠ” ë° ê¸°ì—¬í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ˆê¸°ì˜ ê¸°ì´ˆ LLM ë…¸ë ¥ì—ëŠ” BLOOM(Scao et al., 2022), GPT-J(Wang &amp;        \n",
       "Komatsuzaki, 2021), GPT-NEO(Black et al., 2021), OPT(Zhang et al., 2022), LLaMA(Zhang et al., 2023)ê°€ í¬í•¨ë©ë‹ˆë‹¤.  \n",
       "LLMì„ ëŒ€í™” ê¸°ë°˜ ì–´ì‹œìŠ¤í„´ìŠ¤ì— ë§ì¶”ê¸° ìœ„í•´ Open-AssistantëŠ” GPT-Jë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ê³ , Alpaca/VicunaëŠ” LLaMAë¥¼ ê¸°ë°˜ìœ¼ë¡œ  \n",
       "êµ¬ì¶•ë˜ì—ˆìŠµë‹ˆë‹¤. ë˜í•œ, OpenFlamingo(Awadalla et al., 2023)ì™€ LLaMA-Adapter(Zhang et al., 2023)ëŠ” LLaMAë¥¼ ì´ë¯¸ì§€     \n",
       "ì…ë ¥ê³¼ ì—°ê²°í•˜ì—¬ ì˜¤í”ˆì†ŒìŠ¤ ë©€í‹°ëª¨ë‹¬ LLMì„ êµ¬ì¶•í•  ê¸¸ì„ ë§ˆë ¨í–ˆìŠµë‹ˆë‹¤.                                                  \n",
       "\n",
       "ì´ëŸ¬í•œ ì—°êµ¬ì™€ ë…¸ë ¥ì€ LLMì´ ë” ë‹¤ì–‘í•˜ê³  ê°•ë ¥í•œ ëŠ¥ë ¥ì„ ë°œíœ˜í•  ìˆ˜ ìˆë„ë¡ í•˜ê³ , ì˜¤í”ˆì†ŒìŠ¤ í™˜ê²½ì—ì„œì˜ ë°œì „ì„ ì´‰ì§„í•˜ëŠ” ë° \n",
       "ê·¸ ëª©ì ì´ ìˆìŠµë‹ˆë‹¤.                                                                                                \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "                                                     \u001b[1;4mABSTRACT\u001b[0m                                                      \n",
       "\n",
       "ì´ ì—°êµ¬ì—ì„œëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì„ ê¸°ê³„ê°€ ìƒì„±í•œ ì§€ì‹œë¬¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¯¸ì„¸ ì¡°ì •(finetuning)í•˜ë©´ ìƒˆë¡œìš´        \n",
       "ì‘ì—…ì—ì„œë„ ë›°ì–´ë‚œ ì œë¡œìƒ·(Zero-shot) ì„±ëŠ¥ì„ ë°œíœ˜í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì´ì „ ì—°êµ¬ê°€ ë³´ì—¬ì£¼ì—ˆë‹¤ê³  ì„¤ëª…í•©ë‹ˆë‹¤. ì¦‰, ì¸ê°„ì´   \n",
       "ì‘ì„±í•œ ì§€ì‹œë¬¸ì´ í•„ìš” ì—†ê²Œ ë©ë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì€ GPT-4ë¥¼ í™œìš©í•˜ì—¬ LLM ë¯¸ì„¸ ì¡°ì •ì„ ìœ„í•œ ì§€ì‹œë¬¸ ë°ì´í„°ë¥¼ ìƒì„±í•œ ì²« ì‹œë„ë¥¼ \n",
       "ì†Œê°œí•©ë‹ˆë‹¤. ì—°êµ¬ ê²°ê³¼, GPT-4ê°€ ìƒì„±í•œ 5ë§Œ 2ì²œ ê°œì˜ ì˜ì–´ ë° ì¤‘êµ­ì–´ ì§€ì‹œë¬¸ ë°ì´í„°ê°€ ê¸°ì¡´ ìµœì²¨ë‹¨ ëª¨ë¸ë“¤ì´ ìƒì„±í•œ      \n",
       "ì§€ì‹œë¬¸ ë°ì´í„°ë³´ë‹¤ ìƒˆë¡œìš´ ì‘ì—…ì—ì„œ ë›°ì–´ë‚œ ì œë¡œìƒ· ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤. ë˜í•œ, í¬ê´„ì ì¸ í‰ê°€ ë° ë³´ìƒ ëª¨ë¸ í›ˆë ¨ì„ ìœ„í•´\n",
       "GPT-4ë¡œë¶€í„° í”¼ë“œë°± ë° ë¹„êµ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì˜€ìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” GPT-4ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒì„±í•œ ë°ì´í„°ì™€ ì½”ë“œë² ì´ìŠ¤ë¥¼ ê³µê°œí•˜ê³   \n",
       "ìˆìŠµë‹ˆë‹¤.                                                                                                          \n",
       "\n",
       "\u001b[33mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
       "\n",
       "                                                   \u001b[1;4mINTRODUCTION\u001b[0m                                                    \n",
       "\n",
       "ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ë›°ì–´ë‚œ ì¼ë°˜í™” ëŠ¥ë ¥ì„ ì„¤ëª…í•˜ë©°, ì´ëŸ¬í•œ ëª¨ë¸ë“¤ì´ ìì—°ì–´ ì§€ì‹œë¥¼ ë”°ë¥´ê³  ì‹¤ì œ ì„¸ìƒì—ì„œì˜\n",
       "ì‘ì—…ì„ ì™„ë£Œí•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ë°©ë²•ì„ íƒêµ¬í•˜ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ ë°©ë²• ì¤‘ í•˜ë‚˜ë¡œ ì§€ì‹œë¬¸ íŠœë‹(instruction-tuning)ì´ ìˆìœ¼ë©°, \n",
       "ì´ëŠ” ì¸ê°„ì´ ì£¼ì„ì„ ë‹¨ í”„ë¡¬í”„íŠ¸ì™€ í”¼ë“œë°±ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë‹¤ì–‘í•œ ì‘ì—…ì— ë§ê²Œ ë¯¸ì„¸ ì¡°ì •í•˜ê±°ë‚˜, ìˆ˜ë™ ë˜ëŠ” ìë™ìœ¼ë¡œ   \n",
       "ìƒì„±ëœ ì§€ì‹œë¬¸ì„ ì¶”ê°€í•œ ê³µê°œ ë²¤ì¹˜ë§ˆí¬ì™€ ë°ì´í„°ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ êµ¬í˜„ë©ë‹ˆë‹¤.                                  \n",
       "\n",
       "íŠ¹íˆ Self-Instruct íŠœë‹ì€ ìµœì‹ ì˜ ì§€ì‹œë¬¸ íŠœë‹ëœ êµì‚¬ LLMì´ ìƒì„±í•œ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ì—¬, LLMì„ ì¸ê°„ ì˜ë„ì— ë§ì¶”ëŠ”       \n",
       "ê°„ë‹¨í•˜ê³  íš¨ê³¼ì ì¸ ë°©ë²•ì…ë‹ˆë‹¤. ì´ ì—°êµ¬ ë°©í–¥ì€ LLMì˜ ì œë¡œìƒ· ë° ì†Œìˆ˜ìƒ· ì¼ë°˜í™” ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” íš¨ê³¼ì ì¸ ìˆ˜ë‹¨ì„ ì œê³µí•´\n",
       "ì™”ìŠµë‹ˆë‹¤. ìµœê·¼ ChatGPTì™€ GPT-4ì˜ ì„±ê³µì€ ì´ëŸ¬í•œ ì§€ì‹œë¬¸ íŠœë‹ì„ í†µí•´ ì˜¤í”ˆì†ŒìŠ¤ LLMì„ ê°œì„ í•  í° ê¸°íšŒë¥¼ ì œê³µí•©ë‹ˆë‹¤.      \n",
       "LLaMAëŠ” ì´ëŸ¬í•œ ì˜¤í”ˆì†ŒìŠ¤ LLMë“¤ ì¤‘ í•˜ë‚˜ë¡œ, ìƒìš© LLMì¸ GPT-3ì™€ ì„±ëŠ¥ì„ ê²¬ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. LLaMAê°€ ì§€ì‹œë¥¼ ë”°ë¥¼ ìˆ˜ ìˆë„ë¡ \n",
       "í•˜ë ¤ë©´, ë›°ì–´ë‚œ ì„±ëŠ¥ê³¼ ì €ë¹„ìš©ì˜ Self-Instruct íŠœë‹ì´ ë¹ ë¥´ê²Œ ì±„íƒë˜ê³  ìˆìŠµë‹ˆë‹¤.                                      \n",
       "\n",
       "ë³¸ ë…¼ë¬¸ì—ì„œ ìš°ë¦¬ëŠ” ì²˜ìŒìœ¼ë¡œ GPT-4ë¥¼ Self-Instruct íŠœë‹ì˜ êµì‚¬ë¡œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì£¼ìš” ê¸°ì—¬ëŠ”     \n",
       "ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:                                                                                                   \n",
       "\n",
       "\u001b[1;33m â€¢ \u001b[0m\u001b[1mGPT-4 ë°ì´í„°:\u001b[0m ìš°ë¦¬ëŠ” ì˜ì–´ì™€ ì¤‘êµ­ì–´ë¡œ ëœ 52K ê°œì˜ ì§€ì‹œë¬¸ ë°ì´í„°ì…‹ê³¼ ì„¸ ê°œì˜ ì§€ì‹œë¬¸ íŠœë‹ëœ ëª¨ë¸ì˜ ì¶œë ¥ì„ í‰ê°€í•œ   \n",
       "\u001b[1;33m   \u001b[0mí”¼ë“œë°± ë°ì´í„°ë¥¼ í¬í•¨í•˜ì—¬ GPT-4ê°€ ìƒì„±í•œ ë°ì´í„°ë¥¼ ê³µê°œí•©ë‹ˆë‹¤.                                                    \n",
       "\u001b[1;33m â€¢ \u001b[0m\u001b[1mëª¨ë¸ ë° í‰ê°€:\u001b[0m GPT-4ê°€ ìƒì„±í•œ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§€ì‹œë¬¸ íŠœë‹ëœ LLaMA ëª¨ë¸ê³¼ ë³´ìƒ ëª¨ë¸ì„ ê°œë°œí–ˆìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì„¸   \n",
       "\u001b[1;33m   \u001b[0mê°€ì§€ ê¸°ì¤€ì— ëŒ€í•œ ì¸ê°„ í‰ê°€, GPT-4 í”¼ë“œë°±ì„ ì‚¬ìš©í•œ ìë™ í‰ê°€, ê·¸ë¦¬ê³  ë¹„ì •í˜• ì§€ì‹œë¬¸ì— ëŒ€í•œ ROUGE-L ì ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬\n",
       "\u001b[1;33m   \u001b[0mì§€ì‹œë¬¸ íŠœë‹ëœ LLMì˜ í’ˆì§ˆì„ í‰ê°€í•©ë‹ˆë‹¤.                                                                          \n",
       "\n",
       "ì´ ì—°êµ¬ëŠ” GPT-4ë¡œ ìƒì„±ëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ LLM ì§€ì‹œë¬¸ íŠœë‹ì˜ íš¨ê³¼ì„±ì„ í™•ì¸í•˜ê³ , ì¼ë°˜ ëª©ì ì˜ ì§€ì‹œë¬¸ì„ ë”°ë¥´ëŠ”          \n",
       "ì—ì´ì „íŠ¸ë¥¼ êµ¬ì¶•í•˜ëŠ” ë° ìˆì–´ ì‹¤ìš©ì ì¸ íŒì„ ì œì•ˆí•©ë‹ˆë‹¤.                                                              \n",
       "\n",
       "\u001b[33mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
       "\n",
       "                                                     \u001b[1;4m2 DATASET\u001b[0m                                                     \n",
       "\n",
       "ì´ ë…¼ë¬¸ì˜ '2 ë°ì´í„°ì…‹' ì„¹ì…˜ì—ì„œëŠ” ë°ì´í„° ìˆ˜ì§‘ ê³¼ì •ê³¼ GPT-4ë¥¼ ì‚¬ìš©í•œ ë°ì´í„° ìƒì„± ë°©ë²•ì— ëŒ€í•´ ì„¤ëª…í•©ë‹ˆë‹¤.            \n",
       "\n",
       "ë°ì´í„° ìˆ˜ì§‘: Alpaca ë°ì´í„°ì…‹(Taori et al., 2023)ì—ì„œ ìˆ˜ì§‘ëœ 52,000ê°œì˜ ê³ ìœ í•œ ì§€ì‹œë¬¸ì„ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤. ê° ì§€ì‹œë¬¸ì€   \n",
       "ëª¨ë¸ì´ ìˆ˜í–‰í•´ì•¼ í•  ì‘ì—…ì„ ì„¤ëª…í•©ë‹ˆë‹¤. ì§€ì‹œë¬¸ì—ëŠ” ì‘ì—…ì— ëŒ€í•œ ì„ íƒì  ë§¥ë½ì´ë‚˜ ì…ë ¥ì´ ìˆì„ ìˆ˜ë„ ìˆê³  ì—†ì„ ìˆ˜ë„       \n",
       "ìˆìŠµë‹ˆë‹¤. Alpaca ë°ì´í„°ì…‹ì—ì„œëŠ” GPT-3.5(text-davinci-003)ë¥¼ ì‚¬ìš©í•´ ì¶œë ¥ì„ ìƒì„±í–ˆì§€ë§Œ, ìš°ë¦¬ëŠ” GPT-4(gpt-4)ë¥¼        \n",
       "í™œìš©í•˜ì—¬ ì¶œë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤. GPT-4ë¥¼ ì‚¬ìš©í•´ ë‹¤ìŒ ë„¤ ê°€ì§€ ë°ì´í„°ì…‹ì„ ìƒì„±í•©ë‹ˆë‹¤.                                     \n",
       "\n",
       "\u001b[1;33m 1 \u001b[0m\u001b[1mì˜ì–´ ì§€ì‹œë¬¸-ë‹µë³€ ë°ì´í„°:\u001b[0m Alpacaì—ì„œ ìˆ˜ì§‘í•œ 52,000ê°œì˜ ì§€ì‹œë¬¸ ê°ê°ì— ëŒ€í•´ GPT-4ê°€ ì˜ì–´ë¡œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.      \n",
       "\u001b[1;33m   \u001b[0mìì„¸í•œ ë‚´ìš©ì€ ì•Œê³ ë¦¬ì¦˜ 1ì— ë‚˜ì™€ ìˆìŠµë‹ˆë‹¤. GPT-4ì™€ Self-Instructë¥¼ ì‚¬ìš©í•˜ì—¬ ìì²´ ì§€ì‹œ ì„¸íŠ¸ë¥¼ êµ¬ì„±í•˜ëŠ” ë°˜ë³µì      \n",
       "\u001b[1;33m   \u001b[0mê³¼ì •ì„ í–¥í›„ ì‘ì—…ìœ¼ë¡œ ë‚¨ê²¨ë‘¡ë‹ˆë‹¤.                                                                                \n",
       "\u001b[1;33m 2 \u001b[0m\u001b[1mì¤‘êµ­ì–´ ì§€ì‹œë¬¸-ë‹µë³€ ë°ì´í„°:\u001b[0m ChatGPTë¥¼ ì‚¬ìš©í•˜ì—¬ 52,000ê°œì˜ ì§€ì‹œë¬¸ì„ ì¤‘êµ­ì–´ë¡œ ë²ˆì—­í•˜ê³ , GPT-4ê°€ ì¤‘êµ­ì–´ë¡œ ë‹µë³€í•˜ê²Œ  \n",
       "\u001b[1;33m   \u001b[0mí•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ LLaMAì— ê¸°ë°˜í•œ ì¤‘êµ­ì–´ ì§€ì‹œë¬¸-ë”°ë¥´ê¸° ëª¨ë¸ì„ êµ¬ì¶•í•˜ê³ , ì§€ì‹œë¬¸ íŠœë‹ì˜ ì–¸ì–´ ê°„ ì¼ë°˜í™” ëŠ¥ë ¥ì„      \n",
       "\u001b[1;33m   \u001b[0mì—°êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.                                                                                             \n",
       "\u001b[1;33m 3 \u001b[0m\u001b[1më¹„êµ ë°ì´í„°:\u001b[0m GPT-4ì—ê²Œ ìì‹ ì˜ ì‘ë‹µì„ 1ë¶€í„° 10ê¹Œì§€ í‰ê°€í•˜ë„ë¡ ìš”ì²­í•©ë‹ˆë‹¤. ë˜í•œ, GPT-4ì—ê²Œ GPT-4, GPT-3.5, ê·¸ë¦¬ê³  \n",
       "\u001b[1;33m   \u001b[0mOPT-IMLì„ í¬í•¨í•œ ì„¸ ëª¨ë¸ì˜ ì‘ë‹µì„ ë¹„êµí•˜ì—¬ í‰ê°€í•˜ë„ë¡ ìš”ì²­í•©ë‹ˆë‹¤. ì´ëŠ” ë³´ìƒ ëª¨ë¸ì„ í›ˆë ¨í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.      \n",
       "\u001b[1;33m 4 \u001b[0m\u001b[1më¹„ì •ìƒ ì§€ì‹œë¬¸ì— ëŒ€í•œ ë‹µë³€:\u001b[0m 68,000ê°œì˜ ì§€ì‹œë¬¸-ì…ë ¥-ì¶œë ¥ ì‚¼ì¤‘ ì½”ì–´ ë°ì´í„°ì…‹ì— ëŒ€í•´ GPT-4ì˜ ë‹µë³€ì„ ë””ì½”ë”©í•©ë‹ˆë‹¤. ì´\n",
       "\u001b[1;33m   \u001b[0mí•˜ìœ„ ì„¸íŠ¸ëŠ” GPT-4ì™€ ìš°ë¦¬ ì§€ì‹œë¬¸ íŠœë‹ëœ ëª¨ë¸ë“¤ ê°„ì˜ ì°¨ì´ë¥¼ ê³„ëŸ‰í™”í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.                             \n",
       "\n",
       "ë°ì´í„° í†µê³„: ê·¸ë¦¼ 1ì—ì„œëŠ” GPT-4ì™€ GPT-3.5ì˜ ì˜ì–´ ì¶œë ¥ ì‘ë‹µ ì„¸íŠ¸ë¥¼ ë¹„êµí•©ë‹ˆë‹¤. ê° ì¶œë ¥ì— ëŒ€í•´ ë™ì‚¬ ë¿Œë¦¬ì™€ ì§ì ‘      \n",
       "ëª©ì ì–´ ëª…ì‚¬ë¥¼ ì¶”ì¶œí•˜ê³ , ê° ì¶œë ¥ ì„¸íŠ¸ì—ì„œ ê³ ìœ í•œ ë™ì‚¬-ëª…ì‚¬ ìŒì˜ ë¹ˆë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. ë¹ˆë„ê°€ 10 ì´ìƒì¸ ë™ì‚¬-ëª…ì‚¬ ìŒì€  \n",
       "ê·¸ë¦¼ 1(a)ì™€ (b)ì— í‘œì‹œë˜ë©°, ë‘ ì„¸íŠ¸ì˜ ê°€ì¥ ë¹ˆë²ˆí•œ 25ê°œì˜ ìŒì„ ê·¸ë¦¼ 1(c)ì— ë¹„êµí•©ë‹ˆë‹¤. ì¶œë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´ì˜ ë¹ˆë„      \n",
       "ë¶„í¬ëŠ” ê·¸ë¦¼ 1(d)ì— ë¹„êµë˜ì–´ ìˆìŠµë‹ˆë‹¤. GPT-4ëŠ” GPT-3.5ë³´ë‹¤ ë” ê¸´ ì‹œí€€ìŠ¤ë¥¼ ìƒì„±í•˜ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤. Alpacaì˜ GPT-3.5\n",
       "ë°ì´í„°ëŠ” ë°˜ë³µì ì¸ ë°ì´í„° ìˆ˜ì§‘ ê³¼ì •ì„ í†µí•´ ê° ë°˜ë³µì—ì„œ ìœ ì‚¬í•œ ì§€ì‹œë¬¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì œê±°í•˜ì—¬ ë” ê¸´ ê¼¬ë¦¬ë¥¼ ê°€ì§„ ì¶œë ¥     \n",
       "ë¶„í¬ë¥¼ ë³´ì…ë‹ˆë‹¤. ì´ëŠ” í˜„ ì‹œì ì˜ ì¼íšŒì„± ë°ì´í„° ìƒì„± ê³¼ì •ì—ì„œëŠ” ì—†ëŠ” íŠ¹ì§•ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ê°„ë‹¨í•œ ê³¼ì •ì—ë„ ë¶ˆêµ¬í•˜ê³ ,    \n",
       "GPT-4ê°€ ìƒì„±í•œ ì§€ì‹œë¬¸-ë”°ë¥´ê¸° ë°ì´í„°ëŠ” ì´í›„ ì‹¤í—˜ì—ì„œ ë” ë‚˜ì€ ì •ë ¬ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.                                \n",
       "\n",
       "\u001b[33mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
       "\n",
       "                                       \u001b[1;4m3 INSTRUCTION-TUNING LANGUAGE MODELS\u001b[0m                                        \n",
       "\n",
       "I apologize, but it seems that the '3 INSTRUCTION-TUNING LANGUAGE MODELS' section content is missing from your     \n",
       "request. If you can provide the full text of that section, I will be happy to explain it in detail.                \n",
       "\n",
       "\u001b[33mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
       "\n",
       "                                             \u001b[1;4m3.1 SELF-INSTRUCT TUNING\u001b[0m                                              \n",
       "\n",
       "ì´ ì„¹ì…˜ì—ì„œëŠ” Self-Instruct íŠœë‹ì„ í†µí•´ ì–¸ì–´ ëª¨ë¸ì„ í›ˆë ¨í•˜ëŠ” ê³¼ì •ì— ëŒ€í•´ ì„¤ëª…í•©ë‹ˆë‹¤. ì—°êµ¬ì—ì„œëŠ” LLaMA 7B           \n",
       "ì²´í¬í¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‘ ê°€ì§€ ëª¨ë¸ì„ ì§€ë„ í•™ìŠµ ë°©ì‹ìœ¼ë¡œ ë¯¸ì„¸ ì¡°ì •í–ˆìŠµë‹ˆë‹¤.                                         \n",
       "\n",
       "\u001b[1;33m 1 \u001b[0m\u001b[1mLLaMA-GPT4 ëª¨ë¸\u001b[0m: ì´ ëª¨ë¸ì€ GPT-4ê°€ ìƒì„±í•œ 52,000ê°œì˜ ì˜ì–´ ì§€ì‹œë¬¸-ë”°ë¥´ê¸° ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ í›ˆë ¨ë˜ì—ˆìŠµë‹ˆë‹¤. ì´    \n",
       "\u001b[1;33m   \u001b[0më°ì´í„°ì˜ ë¶„í¬ëŠ” ì´ì „ì˜ ê·¸ë¦¼ 1ì—ì„œ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤.                                                               \n",
       "\u001b[1;33m 2 \u001b[0m\u001b[1mLLaMA-GPT4-CN ëª¨ë¸\u001b[0m: ì´ ëª¨ë¸ì€ GPT-4ê°€ ìƒì„±í•œ 52,000ê°œì˜ ì¤‘êµ­ì–´ ì§€ì‹œë¬¸-ë”°ë¥´ê¸° ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ í›ˆë ¨ë˜ì—ˆìŠµë‹ˆë‹¤.  \n",
       "\n",
       "í›ˆë ¨ ê³¼ì •ì—ì„œëŠ” ê³µì •í•œ ë¹„êµë¥¼ ìœ„í•´ Taori et al.(2023)ì˜ í›ˆë ¨ ìŠ¤ì¼€ì¤„ì„ ë”°ëìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ëª¨ë¸ë“¤ì€ GPT-4ê°€ ìƒì„±í•œ   \n",
       "ë°ì´í„°ì˜ í’ˆì§ˆì„ ì—°êµ¬í•˜ê³ , í•˜ë‚˜ì˜ ì–¸ì–´ë¡œ ì§€ì‹œë¬¸ íŠœë‹ì„ í–ˆì„ ë•Œì˜ êµì°¨ ì–¸ì–´ ì¼ë°˜í™” íŠ¹ì„±ì„ ì—°êµ¬í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.    \n",
       "\n",
       "\u001b[33mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
       "\n",
       "                                                 \u001b[1;4m3.2 REWARD MODELS\u001b[0m                                                 \n",
       "\n",
       "ì´ ì„¹ì…˜ì—ì„œëŠ” ì¸ê°„ í”¼ë“œë°±ì„ í†µí•œ ê°•í™” í•™ìŠµ(RLHF, Reinforcement Learning from Human Feedback)ì˜ í•µì‹¬ ìš”ì†Œì¸ ë³´ìƒ    \n",
       "ëª¨ë¸ë§ì— ëŒ€í•´ ì„¤ëª…í•©ë‹ˆë‹¤. RLHFëŠ” LLMì˜ í–‰ë™ì„ ì¸ê°„ì˜ ì„ í˜¸ì™€ ë§ì¶° ëª¨ë¸ì„ ë”ìš± ìœ ìš©í•˜ê²Œ ë§Œë“¤ë ¤ëŠ” ëª©ì ì„ ê°€ì§€ê³        \n",
       "ìˆìŠµë‹ˆë‹¤. ë³´ìƒ ëª¨ë¸ë§ì€ í”„ë¡¬í”„íŠ¸ì™€ ì‘ë‹µì´ ì£¼ì–´ì¡Œì„ ë•Œ ìŠ¤ì¹¼ë¼ ë³´ìƒì„ ì˜ˆì¸¡í•˜ëŠ” íšŒê·€ ê³¼ì œë¡œ ë¬¸ì œë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ \n",
       "í•˜ë ¤ë©´ ëŒ€ê·œëª¨ ë¹„êµ ë°ì´í„°ê°€ í•„ìš”í•œë°, ë‘ ê°€ì§€ ëª¨ë¸ì˜ ì‘ë‹µì„ ê°™ì€ í”„ë¡¬í”„íŠ¸ì— ëŒ€í•´ ë¹„êµí•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ       \n",
       "Alpaca, Vicuna, Dolly ë“±ì˜ ê¸°ì¡´ ì˜¤í”ˆì†ŒìŠ¤ ì‘ì—…ì€ ë¹„êµ ë°ì´í„° ë ˆì´ë¸”ë§ ë¹„ìš©ì´ ë†’ì•„ RLHFë¥¼ í¬í•¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.         \n",
       "\n",
       "í•œí¸, ìµœê·¼ ì—°êµ¬ëŠ” GPT-4ê°€ ìì‹ ì˜ ì‹¤ìˆ˜ë¥¼ ì‹ë³„í•˜ê³  ìˆ˜ì •í•˜ë©°, ì‘ë‹µ í’ˆì§ˆì„ ì •í™•íˆ í‰ê°€í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤.     \n",
       "ë”°ë¼ì„œ RLHF ì—°êµ¬ë¥¼ ì´‰ì§„í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” 2ì ˆì—ì„œ ì„¤ëª…í•œ ë°”ì™€ ê°™ì´ GPT-4ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„êµ ë°ì´í„°ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.     \n",
       "\n",
       "ë°ì´í„° í’ˆì§ˆ í‰ê°€ë¥¼ ìœ„í•´, ìš°ë¦¬ëŠ” OPT 1.3B ê¸°ë°˜ìœ¼ë¡œ ë³´ìƒ ëª¨ë¸ì„ í›ˆë ¨í•˜ì—¬ ì„œë¡œ ë‹¤ë¥¸ ì‘ë‹µë“¤ì„ í‰ê°€í•©ë‹ˆë‹¤. ê° ë¹„êµ      \n",
       "ë°ì´í„° ì¸ìŠ¤í„´ìŠ¤ëŠ” í•œ í”„ë¡¬í”„íŠ¸ $\\textbf{\\em x}$ì™€ $K$ê°œì˜ ì‘ë‹µì„ í¬í•¨í•˜ê³ , GPT-4ëŠ” ê° ì‘ë‹µì— ëŒ€í•´ 1ë¶€í„° 10ê¹Œì§€ì˜    \n",
       "ì ìˆ˜ $s$ë¥¼ ì¤ë‹ˆë‹¤. ì´ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ê³ ìœ í•œ ìŒ $C_{2}^{K}$ë¥¼ êµ¬ì„±í•  ìˆ˜ ìˆìœ¼ë©°, ê° ìŒì€ $(y_{l}, y_{h})$ë¡œ í‘œí˜„ë©ë‹ˆë‹¤.\n",
       "ì—¬ê¸°ì„œ $y_{l}$ì˜ ì ìˆ˜ëŠ” $y_{h}$ì˜ ì ìˆ˜ë³´ë‹¤ ë‚®ìŠµë‹ˆë‹¤($s_{l} < s_{h}$). ë³´ìƒ ëª¨ë¸ $r_{\\theta}$ëŠ” ë‹¤ìŒ ëª©í‘œë¡œ         \n",
       "í›ˆë ¨ë©ë‹ˆë‹¤: $\\operatorname*{min}\\log(\\sigma(r_{\\pmb\\theta}(x,y_{h})-r_{\\pmb\\theta}(\\pmb x,y_{l})))$, ì—¬ê¸°ì„œ        \n",
       "$\\sigma$ëŠ” ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ì…ë‹ˆë‹¤. ë¹„êµ ë°ì´í„°ì˜ ë¶„í¬ëŠ” ê·¸ë¦¼ 2ì— ë‚˜íƒ€ë‚˜ ìˆìŠµë‹ˆë‹¤.                                   \n",
       "\n",
       "ì´ ì ‘ê·¼ ë°©ì‹ì€ ë³´ìƒ ëª¨ë¸ì´ ë‹¤ì–‘í•œ ì‘ë‹µ í’ˆì§ˆì„ í‰ê°€í•˜ê³  RLHFì˜ íš¨ê³¼ì„±ì„ ë†’ì´ëŠ” ë° ë„ì›€ì„ ì¤„ ìˆ˜ ìˆë„ë¡               \n",
       "ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.                                                                                                    \n",
       "\n",
       "\u001b[33mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
       "\n",
       "                                              \u001b[1;4m4 EXPERIMENTAL RESULTS\u001b[0m                                               \n",
       "\n",
       "It looks like the content from the '4 EXPERIMENTAL RESULTS' section is missing from your request. If you can       \n",
       "provide the full text of that section, I would be happy to explain it in detail.                                   \n",
       "\n",
       "\u001b[33mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
       "\n",
       "                                                  \u001b[1;4m4.1 BENCHMARKS\u001b[0m                                                   \n",
       "\n",
       "ì´ ì„¹ì…˜ì—ì„œëŠ” LLM(ëŒ€í˜• ì–¸ì–´ ëª¨ë¸)ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•œ ë²¤ì¹˜ë§ˆí¬ë¥¼ ì†Œê°œí•˜ê³ , ì¸ê°„ í‰ê°€ ë°©ë²•ê³¼ ê·¸ ê²°ê³¼ë¥¼           \n",
       "ì„¤ëª…í•©ë‹ˆë‹¤.                                                                                                        \n",
       "\n",
       "                                                     \u001b[1më²¤ì¹˜ë§ˆí¬:\u001b[0m                                                     \n",
       "\n",
       "ì´ ì—°êµ¬ì˜ ëª©í‘œëŠ” GPT-4 ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµëœ Self-Instruct ëª¨ë¸ë“¤ì´ ìƒˆë¡œìš´ ì§€ì‹œë¬¸ì— ëŒ€í•´ ì–¼ë§ˆë‚˜ ì˜ ë°˜ì‘í•˜ëŠ”ì§€   \n",
       "í‰ê°€í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì—°êµ¬ì—ì„œ ì‚¬ìš©í•œ ë°ì´í„°ì…‹ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:                                                     \n",
       "\n",
       "\u001b[1;33m â€¢ \u001b[0m\u001b[1mUser-Oriented-Instructions- $252^{,2}$\u001b[0m: ì´ ë°ì´í„°ì…‹ì€ 71ê°œì˜ ì‚¬ìš©ì ì¤‘ì‹¬ ì• í”Œë¦¬ì¼€ì´ì…˜(ì˜ˆ: Grammarly,            \n",
       "\u001b[1;33m   \u001b[0mStackOverflow, Overleaf)ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ 252ê°œì˜ ì§€ì‹œë¬¸ì„ í¬í•¨í•©ë‹ˆë‹¤.                                             \n",
       "\u001b[1;33m â€¢ \u001b[0m\u001b[1mVicuna-Instructions- $80^{3}$\u001b[0m: GPT-4ê°€ ìƒì„±í•œ 80ê°œì˜ ë„ì „ì ì¸ ì§ˆë¬¸ì´ í¬í•¨ë˜ì–´ ìˆìœ¼ë©°, ê¸°ë³¸ ëª¨ë¸ë“¤ì´ ì–´ë ¤ì›Œí•˜ëŠ”  \n",
       "\u001b[1;33m   \u001b[0mì§ˆë¬¸ë“¤ì…ë‹ˆë‹¤. ì§€ì‹, ìˆ˜í•™, Fermi ë¬¸ì œ, ë°˜ì‚¬ì‹¤ì  ì‹œë‚˜ë¦¬ì˜¤, ë¡¤í”Œë ˆì‰, ì¼ë°˜, ì½”ë”©, ê¸€ì“°ê¸°, ìƒì‹ ë“± 8ê°œì˜ ì¹´í…Œê³ ë¦¬ê°€ \n",
       "\u001b[1;33m   \u001b[0mìˆìŠµë‹ˆë‹¤.                                                                                                       \n",
       "\u001b[1;33m â€¢ \u001b[0m\u001b[1mUnnatural Instructions\u001b[0m: 68,478ê°œì˜ ìƒ˜í”Œë¡œ êµ¬ì„±ëœ ì´ ë°ì´í„°ì…‹ì€ 15ê°œì˜ ìˆ˜ì‘ì—… ì˜ˆì œë¥¼ ì‚¬ìš©í•œ 3-ìƒ· ë§¥ë½ í•™ìŠµìœ¼ë¡œ   \n",
       "\u001b[1;33m   \u001b[0mtext-davinci-002ê°€ í•©ì„±í•œ ê²ƒì…ë‹ˆë‹¤.                                                                             \n",
       "\n",
       "                                                    \u001b[1mì¸ê°„ í‰ê°€:\u001b[0m                                                     \n",
       "\n",
       "ëª¨ë¸ì˜ ì •ë ¬ í’ˆì§ˆì„ í‰ê°€í•˜ê¸° ìœ„í•´ Anthropic Askell et al.(2021)ì—ì„œ ì œì•ˆí•œ ì •ë ¬ ê¸°ì¤€ì„ ë”°ëìŠµë‹ˆë‹¤. ì´ ê¸°ì¤€ì€ AI     \n",
       "ì‹œìŠ¤í…œì´ ì¸ê°„ì˜ ê°€ì¹˜ì— ì–¼ë§ˆë‚˜ ì˜ ì •ë ¬ë˜ì–´ ìˆëŠ”ì§€ë¥¼ í‰ê°€í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ê¸°ì¤€ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:                 \n",
       "\n",
       "\u001b[1;33m â€¢ \u001b[0m\u001b[1më„ì›€ë¨\u001b[0m: ì¸ê°„ì´ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ”ì§€ ì—¬ë¶€ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.                                              \n",
       "\u001b[1;33m â€¢ \u001b[0m\u001b[1mì •ì§ì„±\u001b[0m: ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•˜ê³  í•„ìš”ì‹œ ë¶ˆí™•ì‹¤ì„±ì„ í‘œí˜„í•˜ì—¬ ì¸ê°„ ì‚¬ìš©ìë¥¼ ì˜¤ë„í•˜ì§€ ì•ŠëŠ”ì§€ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.           \n",
       "\u001b[1;33m â€¢ \u001b[0m\u001b[1më¬´í•´ì„±\u001b[0m: ì¸ê°„ì—ê²Œ í•´ë¥¼ ë¼ì¹˜ì§€ ì•ŠëŠ”ì§€ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.                                                               \n",
       "\n",
       "Amazon Mechanical Turkë¥¼ í†µí•´ ì¸ê°„ í‰ê°€ë¥¼ ìˆ˜í–‰í–ˆìœ¼ë©°, ê²°ê³¼ëŠ” ê·¸ë¦¼ 3ì˜ íŒŒì´ ì°¨íŠ¸ì— ë‚˜íƒ€ë‚˜ ìˆìŠµë‹ˆë‹¤.                 \n",
       "\n",
       "\u001b[1;33m â€¢ \u001b[0m\u001b[1mì²« ë²ˆì§¸ ë¹„êµ\u001b[0m: GPT-4 ë°ì´í„°ë¡œ ë¯¸ì„¸ ì¡°ì •ëœ LLaMA ëª¨ë¸ê³¼ GPT-3 ë°ì´í„°ë¡œ ë¯¸ì„¸ ì¡°ì •ëœ LLaMA ëª¨ë¸(Stanford Alpaca     \n",
       "\u001b[1;33m   \u001b[0mëª¨ë¸)ì„ ë¹„êµí•˜ì—¬, \"ë„ì›€ë¨\" ê¸°ì¤€ì—ì„œ GPT-4ê°€ 54.12%ë¡œ ìš°ì„¸í–ˆìœ¼ë©°, \"ì •ì§ì„±\"ê³¼ \"ë¬´í•´ì„±\"ì—ì„œëŠ” GPT-3(Alpaca)ì´ ë‹¤ì†Œ \n",
       "\u001b[1;33m   \u001b[0mìš°ì„¸í•˜ë‚˜ ë¹„ìŠ·í•œ ìˆ˜ì¤€ì˜ ê²°ê³¼ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤.                                                                       \n",
       "\u001b[1;33m â€¢ \u001b[0m\u001b[1më‘ ë²ˆì§¸ ë¹„êµ\u001b[0m: GPT-4 ì§€ì‹œë¥¼ ë”°ë¼ íŠœë‹ëœ LLaMA ëª¨ë¸ê³¼ ì›ë˜ì˜ GPT-4 ëª¨ë¸ì„ ë¹„êµí–ˆìœ¼ë©°, ì„¸ ê°€ì§€ ê¸°ì¤€ ëª¨ë‘ì—ì„œ ìœ ì‚¬í•œ\n",
       "\u001b[1;33m   \u001b[0mì„±ëŠ¥ì„ ë³´ì—¬ì¤¬ìŠµë‹ˆë‹¤. ì´ëŠ” GPT-4ê°€ ìƒì„±í•œ ë°ì´í„°ë¡œ í•™ìŠµí•¨ìœ¼ë¡œì¨ ì›ë˜ GPT-4 ìˆ˜ì¤€ì˜ ì„±ëŠ¥ì„ ë‹¬ì„±í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„   \n",
       "\u001b[1;33m   \u001b[0mì˜ë¯¸í•©ë‹ˆë‹¤.                                                                                                     \n",
       "\n",
       "                                             \u001b[1mì¶”ê°€ ê·¸ë¦¼ ì„¤ëª… (ê·¸ë¦¼ 4):\u001b[0m                                              \n",
       "\n",
       "\u001b[1;33m â€¢ \u001b[0mGPT-4ë¡œ í‰ê°€ëœ ì„±ëŠ¥ ë¹„êµë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ê° ë§‰ëŒ€ëŠ” ë‘ ëª¨ë¸ ê°„ì˜ í‰ê°€ ê²°ê³¼ë¥¼ ë‚˜íƒ€ë‚´ë©°, ì´ ì ìˆ˜(ìµœëŒ€ 800ì )ì™€ ìƒëŒ€  \n",
       "\u001b[1;33m   \u001b[0mì ìˆ˜ ë¹„ìœ¨(ê°•ë ¥í•œ ìƒëŒ€ ëª¨ë¸ ëŒ€ë¹„)ì´ ë³´ê³ ë©ë‹ˆë‹¤.                                                                  \n",
       "\u001b[1;33m â€¢ \u001b[0mLLaMA GPT4 ëª¨ë¸ì˜ ì‘ë‹µì„ ìš°ë¦¬ ë³´ìƒ ëª¨ë¸ë¡œ í‰ê°€í•œ ê²°ê³¼ë¥¼ ë‚˜íƒ€ë‚´ë©°, ì „ë°˜ì ìœ¼ë¡œ ChatGPT ë° GPT-4ì™€ ë¹„êµí•˜ì—¬ ì„±ëŠ¥ì„ \n",
       "\u001b[1;33m   \u001b[0mí‰ê°€í–ˆìŠµë‹ˆë‹¤.                                                                                                   \n",
       "\n",
       "ì´ ì—°êµ¬ëŠ” GPT-4 ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ í•™ìŠµì´ ìƒˆë¡œìš´ ì§€ì‹œë¬¸ ì‘ì—…ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë‚¸ë‹¤ëŠ” ê°€ëŠ¥ì„±ì„ ì‹œì‚¬í•©ë‹ˆë‹¤.            \n",
       "\n",
       "\u001b[33mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
       "\n",
       "                               \u001b[1;4m4.3 COMPARISONS WITH SOTA USING AUTOMATIC EVALUATION\u001b[0m                                \n",
       "\n",
       "ì´ ì„¹ì…˜ì—ì„œëŠ” ìë™ í‰ê°€ë¥¼ ì‚¬ìš©í•˜ì—¬ LLaMA-GPT4 ëª¨ë¸ê³¼ ë‹¤ë¥¸ ìµœì²¨ë‹¨(State-of-the-Art, SOTA) ëª¨ë¸ë“¤ì„ ë¹„êµí•œ ê²°ê³¼ë¥¼    \n",
       "ì„¤ëª…í•©ë‹ˆë‹¤.                                                                                                        \n",
       "\n",
       "                                                    \u001b[1mìë™ í‰ê°€:\u001b[0m                                                     \n",
       "\n",
       "\u001b[1;33m â€¢ \u001b[0m\u001b[1mí‰ê°€ ë°©ë²•\u001b[0m: GPT-4ë¥¼ í™œìš©í•´ ë‹¤ì–‘í•œ ëª¨ë¸ë“¤ì´ 80ê°œì˜ ìƒˆë¡œìš´ ì§ˆë¬¸ì— ëŒ€í•œ ìƒì„± ì‘ë‹µì˜ í’ˆì§ˆì„ ìë™ í‰ê°€í–ˆìŠµë‹ˆë‹¤.       \n",
       "\u001b[1;33m   \u001b[0mLLaMA-GPT4(7B)ì™€ GPT-4ì˜ ì‘ë‹µì„ ìˆ˜ì§‘í•˜ê³ , Vicuna(2023)ì—ì„œ ê³µê°œëœ ë‹¤ë¥¸ ì±„íŒ… ë´‡ë“¤(LLaMA 13B, Alpaca 13B, Vicuna  \n",
       "\u001b[1;33m   \u001b[0m13B, Bard, ChatGPT)ì˜ ì‘ë‹µì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ë‘ ëª¨ë¸ ê°„ì˜ ì‘ë‹µ í’ˆì§ˆì„ 1ì—ì„œ 10ê¹Œì§€ì˜ ì ìˆ˜ë¡œ í‰ê°€í–ˆìŠµë‹ˆë‹¤.         \n",
       "\u001b[1;33m â€¢ \u001b[0m\u001b[1mê²°ê³¼\u001b[0m: LLaMA-GPT4ë¥¼ ë‘ ê°€ì§€ ë””ì½”ë”© ê²°ê³¼ë¡œ í‰ê°€í–ˆìŠµë‹ˆë‹¤. ì²«ì§¸ëŠ” ì§ˆë¬¸ë‹¹ í•˜ë‚˜ì˜ ì‘ë‹µë§Œì„ ìƒì„±í•˜ëŠ” ê¸°ë³¸ ë””ì½”ë”© ê²°ê³¼, \n",
       "\u001b[1;33m   \u001b[0më‘˜ì§¸ëŠ” ì§ˆë¬¸ë‹¹ ë‹¤ì„¯ ê°œì˜ ì‘ë‹µì„ ìƒì„±í•œ í›„ ë³´ìƒ ëª¨ë¸ì´ ìƒìœ„ì—ì„œ í•˜ìœ„ë¡œ ìˆœìœ„ ë§¤ê¸´ ì‘ë‹µ ê·¸ë£¹ì…ë‹ˆë‹¤. ì´ ê·¸ë£¹ë“¤ì€ ê¸°ë³¸\n",
       "\u001b[1;33m   \u001b[0mê²°ê³¼ì™€ ë¹„êµë˜ì—ˆê³ , í‰ê°€ ê²°ê³¼ì—ì„œ í”¼ë“œë°± ë°ì´í„°ì™€ ë³´ìƒ ëª¨ë¸ì˜ ìœ ìš©ì„±ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.                               \n",
       "\n",
       "                                                       \u001b[1mê²°ê³¼:\u001b[0m                                                       \n",
       "\n",
       "\u001b[1;33m â€¢ \u001b[0mLLaMA-GPT4ëŠ” text-davinci-003(ì¦‰, Alpaca)ë¡œ íŠœë‹í•œ ê²½ìš°ë³´ë‹¤ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì—ˆì§€ë§Œ, ì—¬ì „íˆ ìƒì—…ìš© ëŒ€í˜•        \n",
       "\u001b[1;33m   \u001b[0mì±—ë´‡(e.g., GPT-4)ê³¼ ë¹„êµí•´ì„œëŠ” ì°¨ì´ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.                                                               \n",
       "\u001b[1;33m â€¢ \u001b[0m\u001b[1mì¤‘êµ­ì–´ í‰ê°€\u001b[0m: ì˜ì–´ ì‘ë‹µì„ ì¤‘êµ­ì–´ë¡œ ë²ˆì—­í•œ ê²ƒì´ë‚˜, ì¤‘êµ­ì–´ë¡œ ì§ì ‘ ì§ˆë¬¸í•˜ì—¬ ìƒì„±ëœ ì‘ë‹µì„ ë¹„êµí–ˆì„ ë•Œ, ë²ˆì—­ëœ ì‘ë‹µì´\n",
       "\u001b[1;33m   \u001b[0më” ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤. ì´ëŠ” GPT-4ê°€ ì˜ì–´ ì½”í¼ìŠ¤ì—ì„œ ë” ë§ì´ í›ˆë ¨ë˜ì–´ ì˜ì–´ ì§€ì‹œë¬¸ì„ ë”°ë¥´ëŠ” ëŠ¥ë ¥ì´ ë”   \n",
       "\u001b[1;33m   \u001b[0mê°•ë ¥í•˜ê¸° ë•Œë¬¸ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.                                                                                    \n",
       "\n",
       "                             \u001b[1më¹„ì •ìƒ ì§€ì‹œë¬¸(unnatural instructions)ì—ì„œì˜ ê²°ê³¼(ê·¸ë¦¼ 6):\u001b[0m                             \n",
       "\n",
       "\u001b[1;33m â€¢ \u001b[0mLLaMA-GPT4ì™€ GPT-4, ê·¸ë¦¬ê³  Alpaca ëª¨ë¸ì„ ë¹„êµí–ˆì„ ë•Œ, Alpacaê°€ ë‹¤ë¥¸ ë‘ ëª¨ë¸ì— ë¹„í•´ í‰ê·  ROUGE-L ì ìˆ˜ì—ì„œ        \n",
       "\u001b[1;33m   \u001b[0mìš°ìˆ˜í•˜ì˜€ìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì‘ë‹µì˜ ê¸¸ì´ê°€ ê¸¸ì–´ì§ˆìˆ˜ë¡ LLaMA-GPT4ì™€ GPT-4ê°€ ë” ë†’ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ê¸° ì‹œì‘í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” \n",
       "\u001b[1;33m   \u001b[0më” ì°½ì˜ì ì¸ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ì´ ëª¨ë¸ë“¤ì´ ì§€ì‹œë¬¸ì„ ë” ì˜ ë”°ë¥¼ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.                             \n",
       "\u001b[1;33m â€¢ \u001b[0mê°ê¸° ë‹¤ë¥¸ ì‘ë‹µ ê¸¸ì´ í•˜ìœ„ ì§‘í•©ì— ëŒ€í•´, LLaMA-GPT4ëŠ” GPT-4ì˜ í–‰ë™ì„ ê°€ê¹ê²Œ ë”°ë¦…ë‹ˆë‹¤. ì‘ë‹µ ê¸¸ì´ê°€ ì§§ì„ ë•Œ,         \n",
       "\u001b[1;33m   \u001b[0mLLaMA-GPT4ì™€ GPT-4ëŠ” ë‹¨ìˆœí•œ ì •ë‹µì„ í¬í•¨í•˜ë˜, ì‘ë‹µì„ ë” ëŒ€í™”í˜•ìœ¼ë¡œ ë§Œë“œëŠ” ì¶”ê°€ì ì¸ ë‹¨ì–´ë¥¼ í¬í•¨í•˜ê¸° ë•Œë¬¸ì—, ë‚®ì€  \n",
       "\u001b[1;33m   \u001b[0mROUGE-L ì ìˆ˜ë¥¼ ë°›ì„ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.                                                                          \n",
       "\n",
       "ì´ ì‹¤í—˜ ê²°ê³¼ëŠ” LLaMA-GPT4ê°€ GPT-4ì˜ ì¶œë ¥ì„ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµí–ˆì„ ë•Œ, ìƒë‹¹í•œ ì„±ëŠ¥ í–¥ìƒì„ ì´ë£° ìˆ˜ ìˆìŒì„ ë³´ì—¬ì£¼ë©°, ì´ëŠ”  \n",
       "í˜ì‹ ì ì¸ ì§€ì‹œë¬¸-ë”°ë¥´ê¸° LLM ê°œë°œì— ìˆì–´ì„œ ìœ ë§í•œ ë°©í–¥ì„ì„ ì‹œì‚¬í•©ë‹ˆë‹¤.                                               \n",
       "\n",
       "\u001b[33mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
       "\n",
       "                                                  \u001b[1;4m5 RELATED WORK\u001b[0m                                                   \n",
       "\n",
       "ì´ ì„¹ì…˜ì—ì„œëŠ” ì§€ì‹œë¬¸ íŠœë‹ê³¼ ì˜¤í”ˆì†ŒìŠ¤ ë…¸ë ¥ì´ ê´€ë ¨ëœ ì—°êµ¬ì— ëŒ€í•´ ì„¤ëª…í•©ë‹ˆë‹¤.                                         \n",
       "\n",
       "                                         \u001b[1mì§€ì‹œë¬¸ íŠœë‹(Instruction Tuning):\u001b[0m                                          \n",
       "\n",
       "ì§€ì‹œë¬¸ íŠœë‹ì€ NLP ë¶„ì•¼ì—ì„œ ì ì  ì£¼ëª©ë°›ê³  ìˆëŠ” ì—°êµ¬ ë°©í–¥ì…ë‹ˆë‹¤. ê¸°ì¡´ ì—°êµ¬ë“¤ì€ í¬ê²Œ ì„¸ ê°€ì§€ ìš”ì†Œì¸ ì§€ì‹œë¬¸-ë”°ë¥´ê¸°     \n",
       "ë°ì´í„°, ê¸°ë³¸ ì–¸ì–´ ëª¨ë¸, í‰ê°€ ë²¤ì¹˜ë§ˆí¬ì˜ í’ˆì§ˆê³¼ ê·œëª¨ë¥¼ í–¥ìƒì‹œí‚¤ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. ê° ì—°êµ¬ ê·¸ë£¹ì€ ì¼ë°˜ì ìœ¼ë¡œ     \n",
       "ìì²´ì ì¸ ê°œë°œ íŒŒì´í”„ë¼ì¸ì„ ìœ ì§€í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, í”Œëœ(FLAN)ì„ ê¸°ë°˜ìœ¼ë¡œ ì§€ì‹œë¬¸ ë¯¸ì„¸ ì¡°ì • ì–¸ì–´ ëª¨ë¸ì˜ ê·œëª¨ë¥¼        \n",
       "í™•ëŒ€í•˜ëŠ” ì—°êµ¬(Chung et al., 2022)ë‚˜, ì ì  ë§ì€ í”„ë¡¬í”„íŠ¸ë¥¼ í¬í•¨í•˜ëŠ” PromptSource(ê³µê³µ í”„ë¡¬í”„íŠ¸ í’€, P3)ì˜ ê°œë°œ(Bach  \n",
       "et al., 2022) ë“±ì´ ìˆìŠµë‹ˆë‹¤. T0ëŠ” P3ì— ê¸°ë°˜í•œ ë‹¤ì¤‘ ê³¼ì œ í”„ë¡¬í”„íŠ¸ í›ˆë ¨ì„ í†µí•´ ê°œë°œëœ ì¼ë ¨ì˜ ëª¨ë¸(Sanh et al.,       \n",
       "2021)ì…ë‹ˆë‹¤. OPT ëª¨ë¸ì˜ ì§€ì‹œë¬¸ íŠœë‹ì€ ë” í¬ê³  í¬ê´„ì ì¸ ë²¤ì¹˜ë§ˆí¬ì¸ OPT-IML Benchë¥¼ ì‚¬ìš©í•˜ì—¬ ì§„í–‰ë˜ì—ˆìœ¼ë©°, ì´ëŠ” FLAN,\n",
       "Super-NaturalInstructions(Wang et al., 2022b), UnifiedSKG(Xie et al., 2022)ì„ í¬í•¨í•©ë‹ˆë‹¤.                          \n",
       "\n",
       "                                        \u001b[1mì˜¤í”ˆì†ŒìŠ¤ ë…¸ë ¥(Open-Source Efforts):\u001b[0m                                        \n",
       "\n",
       "ChatGPTì˜ í­ë„“ì€ ëŠ¥ë ¥ ë•ë¶„ì—, ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ì€ í° ê´€ì‹¬ì„ ë°›ìœ¼ë©° ì¸ê°„ì˜ ê°€ì¹˜ì— ë§ëŠ” ì¼ë°˜ ëª©ì ì˜ í…ìŠ¤íŠ¸ ê¸°ë°˜          \n",
       "ì–´ì‹œìŠ¤í„´íŠ¸ë¥¼ ê°œë°œí•˜ëŠ” ë° ê¸°ì—¬í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ˆê¸°ì˜ ê¸°ì´ˆ LLM ë…¸ë ¥ì—ëŠ” BLOOM(Scao et al., 2022), GPT-J(Wang &        \n",
       "Komatsuzaki, 2021), GPT-NEO(Black et al., 2021), OPT(Zhang et al., 2022), LLaMA(Zhang et al., 2023)ê°€ í¬í•¨ë©ë‹ˆë‹¤.  \n",
       "LLMì„ ëŒ€í™” ê¸°ë°˜ ì–´ì‹œìŠ¤í„´ìŠ¤ì— ë§ì¶”ê¸° ìœ„í•´ Open-AssistantëŠ” GPT-Jë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ê³ , Alpaca/VicunaëŠ” LLaMAë¥¼ ê¸°ë°˜ìœ¼ë¡œ  \n",
       "êµ¬ì¶•ë˜ì—ˆìŠµë‹ˆë‹¤. ë˜í•œ, OpenFlamingo(Awadalla et al., 2023)ì™€ LLaMA-Adapter(Zhang et al., 2023)ëŠ” LLaMAë¥¼ ì´ë¯¸ì§€     \n",
       "ì…ë ¥ê³¼ ì—°ê²°í•˜ì—¬ ì˜¤í”ˆì†ŒìŠ¤ ë©€í‹°ëª¨ë‹¬ LLMì„ êµ¬ì¶•í•  ê¸¸ì„ ë§ˆë ¨í–ˆìŠµë‹ˆë‹¤.                                                  \n",
       "\n",
       "ì´ëŸ¬í•œ ì—°êµ¬ì™€ ë…¸ë ¥ì€ LLMì´ ë” ë‹¤ì–‘í•˜ê³  ê°•ë ¥í•œ ëŠ¥ë ¥ì„ ë°œíœ˜í•  ìˆ˜ ìˆë„ë¡ í•˜ê³ , ì˜¤í”ˆì†ŒìŠ¤ í™˜ê²½ì—ì„œì˜ ë°œì „ì„ ì´‰ì§„í•˜ëŠ” ë° \n",
       "ê·¸ ëª©ì ì´ ìˆìŠµë‹ˆë‹¤.                                                                                                \n",
       "\n",
       "\u001b[33mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich.console import Console\n",
    "from rich.markdown import Markdown\n",
    "from rich.panel import Panel\n",
    "from rich.syntax import Syntax\n",
    "from rich.table import Table\n",
    "from typing import Dict\n",
    "import os\n",
    "\n",
    "class MarkdownPrinter:\n",
    "    def __init__(self):\n",
    "        self.console = Console()\n",
    "        \n",
    "    def print_markdown_file(self, file_path: str):\n",
    "        \"\"\"ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì„ ì´ì˜ê²Œ ì¶œë ¥\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                markdown_content = f.read()\n",
    "            \n",
    "            # ë§ˆí¬ë‹¤ìš´ ë Œë”ë§\n",
    "            md = Markdown(markdown_content)\n",
    "            \n",
    "            # ë§ˆí¬ë‹¤ìš´ ë‚´ìš© ì¶œë ¥\n",
    "            self.console.print(md)\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.console.print(f\"[bold red]Error reading file: {str(e)}[/]\")\n",
    "            \n",
    "    def print_sections(self, sections: Dict[str, str]):\n",
    "        \"\"\"ì„¹ì…˜ë³„ë¡œ êµ¬ë¶„í•˜ì—¬ ì¶œë ¥\"\"\"\n",
    "        for section, content in sections.items():\n",
    "            # ì„¹ì…˜ ì œëª©\n",
    "            self.console.print(\"\\n\")\n",
    "            self.console.print(Panel(\n",
    "                f\"[bold cyan]{section}[/]\",\n",
    "                border_style=\"cyan\"\n",
    "            ))\n",
    "            \n",
    "            # ì„¹ì…˜ ë‚´ìš©\n",
    "            md = Markdown(content)\n",
    "            self.console.print(md)\n",
    "            \n",
    "            # êµ¬ë¶„ì„ \n",
    "            self.console.print(\"[dim]\" + \"=\"*80 + \"[/]\")\n",
    "\n",
    "printer = MarkdownPrinter()\n",
    "\n",
    "# ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì¶œë ¥\n",
    "printer.print_markdown_file(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë„¤, ë§ìŠµë‹ˆë‹¤. ReSTì—ì„œ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ê³¼ì •ì€ ì›ë³¸ ë°ì´í„°ì…‹ì„ ìƒ˜í”Œë§í•˜ì—¬ ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ ì„ íƒí•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¡°ê±´í™”ëœ ìƒˆë¡œìš´ ì¶œë ¥ ì‹œí€€ìŠ¤ë¥¼ ìƒì„±í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤. ì—¬ê¸°ì„œ \"ì¡°ê±´í™”\"ë¼ëŠ” ê²ƒì€ ì£¼ì–´ì§„ ì…ë ¥ ì‹œí€€ìŠ¤ \\(\\pmb{x}\\)ì— ëŒ€í•´ ê·¸ ì¡°ê±´ í•˜ì—ì„œ ê°€ëŠ¥í•œ ì¶œë ¥ ì‹œí€€ìŠ¤ \\(\\pmb{y}\\)ë¥¼ ìƒì„±í•œë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.\n",
      "\n",
      "ì¡°ê±´í™”ëŠ” í™•ë¥ ì  ëª¨ë¸ë§ì—ì„œ ë§¤ìš° ì¤‘ìš”í•œ ê°œë…ì¸ë°, ì´ëŠ” ì£¼ì–´ì§„ ì…ë ¥ì´ ì–´ë–¤ íŠ¹ì •í•œ ìƒíƒœë‚˜ ê°’ì¼ ë•Œì˜ ì¶œë ¥ í™•ë¥ ì„ ëœ»í•©ë‹ˆë‹¤. ì–¸ì–´ ëª¨ë¸ë§ì—ì„œëŠ” ì…ë ¥ ì‹œí€€ìŠ¤(ë˜ëŠ” ë¬¸ë§¥)ê°€ ì£¼ì–´ì¡Œì„ ë•Œ ë‹¤ìŒ ê°€ëŠ¥í•œ ì¶œë ¥ ì‹œí€€ìŠ¤ë¥¼ ìƒì„±í•˜ëŠ” ê³¼ì •ì„ ë§í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì€ ìë™ íšŒê·€ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ êµ¬í˜„ë˜ë©°, ëª¨ë¸ì´ ì´ë¯¸ í•™ìŠµí•œ í™•ë¥  ë¶„í¬ì— ê¸°ë°˜í•´ \\(\\pi_{\\theta}(\\pmb{y}|\\pmb{x})=\\prod_{t=1}^{T}\\pi_{\\theta}(y_{t}|\\pmb{y}_{1:t-1}, \\pmb{x})\\) í˜•íƒœë¡œ ë‹¤ìŒ í† í° \\(y_t\\)ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
      "\n",
      "ReSTì—ì„œëŠ” ì´ë¥¼ í†µí•´ í˜„ì¬ ì •ì±… \\(\\pi_{\\theta}\\)ë¡œ ì…ë ¥ ì‹œí€€ìŠ¤ì— ë§ëŠ” ìƒˆë¡œìš´ ì¶œë ¥ ì‹œí€€ìŠ¤ë¥¼ ìƒì„±í•˜ê³ , ê·¸ ê²°ê³¼ë¥¼ ë°ì´í„°ì…‹ì— ì¶”ê°€í•˜ì—¬ ë°ì´í„°ë¥¼ í™•ì¥ì‹œí‚µë‹ˆë‹¤. ì´ëŸ¬í•œ ë°©ì‹ì€ ëª¨ë¸ì´ ì´ë¯¸ ì•Œê³  ìˆëŠ” ë¬¸ë§¥ì— ê¸°ë°˜í•´ ì¶”ê°€ì ì¸ í•™ìŠµ ë°ì´í„°ë¥¼ ìƒì„±í•  ìˆ˜ ìˆë„ë¡ í•˜ë©°, ì´ëŠ” Grow ë‹¨ê³„ì—ì„œ ì›ë³¸ ë°ì´í„°ì…‹ì˜ ìƒ˜í”Œì„ ì´ìš©í•´ ìƒˆë¡œìš´ ì‹œí€€ìŠ¤ë¥¼ ìƒì„±í•˜ëŠ” ê³¼ì •ì—ì„œ í•µì‹¬ì ì¸ ì—­í• ì„ í•©ë‹ˆë‹¤. ì´ëŠ” Section 3ì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì„¤ëª…í•œ ê²ƒì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\" \n",
    "ì›ë³¸ ë°ì´í„° ì…‹ì„ ìƒ˜í”Œë§í•´ì„œ ì…ë ¥ ì‹œí€€ìŠ¤ì— ì¡°ê±´í™”ë¥¼ í•œ í›„ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒ ê°™ì€ë° ì´ê²Œ ë§ì•„? ê·¸ë¦¬ê³  ì¡°ê±´í™”ê°€ ë­ì„?\n",
    "\"\"\"\n",
    "\n",
    "qa = PaperQA(qa.conversation_history)\n",
    "response = qa.ask_question(question)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
