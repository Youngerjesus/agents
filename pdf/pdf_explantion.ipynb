{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Invalid requirement: 'anthropic,': Expected end or semicolon (after name and no valid version specifier)\n",
      "    anthropic,\n",
      "             ^\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install anthropic\n",
    "%pip install rich\n",
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from typing import Dict, List\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def get_completion(messages: List[Dict[str, str]]) -> str:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=messages,\n",
    "        max_completion_tokens=4092, \n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Dict, List\n",
    "from pathlib import Path\n",
    "\n",
    "class MarkdownSectionParser:\n",
    "    def __init__(self):\n",
    "        self.section_pattern = r'^#+ .*$'  # '#'으로 시작하는 헤더 패턴\n",
    "        self.end_sections = {'ACKNOWLEDGEMENTS', 'REFERENCES', 'CONCLUSION', 'CONCLUSIONS'}\n",
    "        \n",
    "    def parse_sections(self, markdown_path: str) -> Dict[str, str]:\n",
    "        sections = {}\n",
    "        current_section = None\n",
    "        current_content = []\n",
    "        \n",
    "        with open(markdown_path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            \n",
    "        for line in lines:\n",
    "            if re.match(self.section_pattern, line):\n",
    "                section_title = line.strip('# \\n')\n",
    "                \n",
    "                if current_section and current_content:\n",
    "                    sections[current_section] = ''.join(current_content).strip()\n",
    "                \n",
    "                # 대문자로 변환하여 비교\n",
    "                if any(end_sec in section_title.upper() for end_sec in self.end_sections):\n",
    "                    break\n",
    "                    \n",
    "                current_section = section_title\n",
    "                current_content = []\n",
    "            else:\n",
    "                if current_section is None and 'ABSTRACT' not in line.upper():\n",
    "                    continue\n",
    "                current_content.append(line)\n",
    "        \n",
    "        # 마지막 섹션 저장\n",
    "        if current_section and current_content:\n",
    "            sections[current_section] = ''.join(current_content).strip()\n",
    "        \n",
    "        # Abstract 필터링\n",
    "        filtered_sections = {}\n",
    "        include_section = False\n",
    "        \n",
    "        for section, content in sections.items():\n",
    "            if 'ABSTRACT' in section.upper():\n",
    "                include_section = True\n",
    "            \n",
    "            if include_section:\n",
    "                filtered_sections[section] = content\n",
    "                \n",
    "        return filtered_sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "from typing import Dict, List\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "class PaperExplainer:\n",
    "    def __init__(self):\n",
    "        self.conversation_history = []\n",
    "        self.delay = 1  # API 호출 간 딜레이 (초)\n",
    "        \n",
    "    def _create_section_prompt(self, section_title: str, section_content: str, is_first: bool = False) -> str:\n",
    "        if is_first:\n",
    "            return f\"\"\"You are an expert academic paper explainer. Please explain the following section '{section_title}' \n",
    "            from an academic paper in a clear and concise manner. Please explain in Korean.\n",
    "\n",
    "            Section content:\n",
    "            {section_content}\"\"\"\n",
    "        else:\n",
    "            return f\"\"\"Based on our previous discussion of the paper, please explain the following section '{section_title}'.\n",
    "            \n",
    "            Section content:\n",
    "            {section_content}\"\"\"\n",
    "    \n",
    "    def explain_section(self, section_title: str, section_content: str) -> str:\n",
    "        try:\n",
    "            # Create prompt based on whether this is the first section\n",
    "            is_first = len(self.conversation_history) == 0\n",
    "            prompt = self._create_section_prompt(section_title, section_content, is_first)\n",
    "            \n",
    "            # Add previous conversation for context\n",
    "            self.conversation_history.append({\"role\": \"user\", \"content\": prompt})\n",
    "            \n",
    "            response = get_completion(self.conversation_history)\n",
    "            \n",
    "            # Update conversation history\n",
    "            self.conversation_history.append({\"role\": \"assistant\", \"content\": response})            \n",
    "            # API 호출 간 딜레이\n",
    "            time.sleep(self.delay)\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error explaining section {section_title}: {str(e)}\")\n",
    "            return f\"Error: Failed to explain section {section_title}\"\n",
    "\n",
    "    def explain_paper(self, sections: Dict[str, str]) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        논문의 각 섹션을 순차적으로 설명\n",
    "        \n",
    "        Args:\n",
    "            sections: 섹션 제목과 내용을 매핑한 딕셔너리\n",
    "            \n",
    "        Returns:\n",
    "            섹션 제목과 설명을 매핑한 딕셔너리\n",
    "        \"\"\"\n",
    "        explanations = {}\n",
    "        \n",
    "        print(\"\\nProcessing sections:\")\n",
    "        for title, content in tqdm(sections.items(), desc=\"Explaining sections\"):\n",
    "            print(f\"\\nProcessing: {title}\")\n",
    "            explanation = self.explain_section(title, content)\n",
    "            explanations[title] = explanation\n",
    "            \n",
    "        return explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Optional, List\n",
    "class PaperQA:\n",
    "    def __init__(self, context: Optional[List[Dict[str, str]]] = None):\n",
    "        self.conversation_history = context or []\n",
    "        self.delay = 1\n",
    "        \n",
    "    def load_paper_context(self, explanations: Dict[str, str]):\n",
    "        \"\"\"논문 설명을 대화 기록에 로드\"\"\"\n",
    "        context = \"Here's the paper summary:\\n\\n\"\n",
    "        for section, explanation in explanations.items():\n",
    "            context += f\"## {section}\\n{explanation}\\n\\n\"\n",
    "            \n",
    "        # 논문 컨텍스트를 대화 기록에 추가\n",
    "        self.conversation_history.append({\"role\": \"user\", \"content\": context})\n",
    "\n",
    "    def ask_question(self, question: str) -> str:\n",
    "        \"\"\"논문에 대한 질문에 답변\"\"\"\n",
    "        try:\n",
    "            # 질문 프롬프트 생성\n",
    "            prompt = f\"\"\"Based on the paper we discussed, please answer the following question in Korean. \n",
    "            Be specific and cite relevant sections when possible.\n",
    "\n",
    "            Question: {question}\"\"\"\n",
    "            \n",
    "            # 이전 대화 기록과 함께 질문 전송\n",
    "            self.conversation_history.append({\"role\": \"user\", \"content\": prompt})\n",
    "            \n",
    "            # Claude에 질문\n",
    "            response = get_completion(self.conversation_history)\n",
    "            \n",
    "            # 대화 기록 업데이트\n",
    "            self.conversation_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "            \n",
    "            time.sleep(self.delay)\n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question: {str(e)}\")\n",
    "            return f\"Error: Failed to process question\"\n",
    "    \n",
    "    def view_conversation_history(self, start_idx: int = 0, end_idx: Optional[int] = None) -> None:\n",
    "        \"\"\"대화 내역을 출력하는 함수\n",
    "        \n",
    "        Args:\n",
    "            start_idx: 시작 인덱스 (기본값: 0)\n",
    "            end_idx: 종료 인덱스 (기본값: None, None일 경우 끝까지 출력)\n",
    "        \"\"\"\n",
    "        # 논문 컨텍스트는 제외하고 실제 대화만 출력\n",
    "        conversations = [\n",
    "            msg for msg in self.conversation_history \n",
    "            if not msg[\"content\"].startswith(\"Here's the paper summary:\")\n",
    "        ]\n",
    "        \n",
    "        # end_idx가 None이면 리스트 끝까지\n",
    "        end_idx = end_idx if end_idx is not None else len(conversations)\n",
    "        \n",
    "        print(\"\\n=== 대화 내역 ===\\n\")\n",
    "        for i, msg in enumerate(conversations[start_idx:end_idx], start=start_idx):\n",
    "            role = msg[\"role\"].upper()\n",
    "            if role == \"ASSISTANT\":\n",
    "                print(f\"\\n🤖 Assistant ({i}):\\n{msg['content']}\\n\")\n",
    "                print(\"-\" * 80)\n",
    "            elif role == \"USER\":\n",
    "                print(f\"\\n👤 User ({i}):\\n{msg['content']}\\n\")\n",
    "                print(\"-\" * 80)\n",
    "    \n",
    "    \n",
    "    def get_last_n_conversations(self, n: int = 1) -> None:\n",
    "        \"\"\"최근 n개의 대화 내역을 출력\n",
    "        \n",
    "        Args:\n",
    "            n: 출력할 최근 대화 개수 (기본값: 1)\n",
    "        \"\"\"\n",
    "        conversations = [\n",
    "            msg for msg in self.conversation_history \n",
    "            if not msg[\"content\"].startswith(\"Here's the paper summary:\")\n",
    "        ]\n",
    "        start_idx = max(0, len(conversations) - n)\n",
    "        self.view_conversation_history(start_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def process_paper(markdown_path: str) -> Tuple[Dict[str, str], PaperQA]:\n",
    "    \"\"\"\n",
    "    전체 논문 처리 프로세스\n",
    "    \"\"\"\n",
    "    # 1. Markdown 파싱\n",
    "    parser = MarkdownSectionParser()\n",
    "    sections = parser.parse_sections(markdown_path)\n",
    "    \n",
    "    print(sections)\n",
    "    # 2. 섹션별 설명 생성\n",
    "    explainer = PaperExplainer()\n",
    "    explanations = explainer.explain_paper(sections)\n",
    "\n",
    "    # 3. 질문 답변 준비 \n",
    "    qa = PaperQA()\n",
    "    qa.load_paper_context(explanations)\n",
    "\n",
    "    return explanations, qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ABSTRACT': 'Prior work has shown that finetuning large language models (LLMs) using machinegenerated instruction-following data enables such models to achieve remarkable zero-shot capabilities on new tasks, and no human-written instructions are needed. In this paper, we present the first attempt to use GPT-4 to generate instructionfollowing data for LLM finetuning. Our early experiments on instruction-tuned LLaMA models show that the 52K English and Chinese instruction-following data generated by GPT-4 leads to superior zero-shot performance on new tasks to the instruction-following data generated by previous state-of-the-art models. We also collect feedback and comparison data from GPT-4 to enable a comprehensive evaluation and reward model training. We make our data generated using GPT-4 as well as our codebase publicly available. 1', 'INTRODUCTION': 'Large Language Models (LLMs) have shown impressive generalization capabilities such as incontext-learning (Brown et al., 2020) and chain-of-thoughts reasoning (Wei et al., 2022). To enable LLMs to follow natural language instructions and complete real-world tasks, researchers have been exploring methods of instruction-tuning of LLMs. This is implemented by either finetuning the model on a wide range of tasks using human-annotated prompts and feedback (Ouyang et al., 2022), or supervised finetuning using public benchmarks and datasets augmented with manually or automatically generated instructions (Wang et al., 2022b). Among these methods, Self-Instruct tuning (Wang et al., 2022a) is a simple and effective method of aligning LLMs to human intent, by learning from instruction-following data generated by state-of-the-art instruction-tuned teacher LLMs. It turns out that the line of instruction-tuning research has produced effective means to improve the zero and few-shot generalization abilities of LLMs. The recent success of ChatGPT (OpenAI, 2023a) and GPT-4 (OpenAI, 2023b) offers tremendous opportunities to improve open-source LLMs using instruction-tuning. LLaMA (Touvron et al., 2023) is a series of open-sourced LLMs, which match the performance of proprietary LLMs such as GPT-3. To teach LLaMA to follow instructions, Self-Instruct tuning has been quickly adopted given its superior performance and low cost. For example, Stanford Alpaca (Taori et al., 2023) uses 52K instruction-following samples generated by GPT-3.5, while Vicuna (Vicuna, 2023) uses around 700K instruction-following samples (70K conversions) shared user-ChatGPT (ShareGPT, 2023).  \\n\\nTo advance the state of the art of instruction-tuning for LLMs, we propose for the first time to use GPT-4 as a teacher for self-instruct tuning. Our paper makes the following contributions:  \\n\\n• GPT-4 data. We release data generated by GPT-4, including the 52K instruction-following dataset in both English and Chinese, and the GPT-4-generated feedback data that rate the outputs of three instruction-tuned models. • Models & Evaluation. Based on the GPT-4-generated data, we have developed instruction-tuned LLaMA models and reward models. To evaluate the quality of instruction-tuned LLMs, we use three metrics evaluated on test samples (i.e., unseen instructions): human evaluation on three alignment criteria, automatic evaluation using GPT-4 feedback, and ROUGE-L on un-natural  \\n\\nAlgorithm 1: Pseudo code for prompt engineering, GPT-4 call and hyper-parameters in data generation. Each instruction instance is used as variables in the prompt template, the data flow is highlighted in blue. 1 PROMPT DICT{ 2 prompt input: ( 3 “Below is an instruction that describes a task, paired with an input that provides further context.” 4 “Write a response that appropriately completes the request. $\\\\bar{\\\\langle{\\\\bf n}\\\\backslash{\\\\bf n}^{\\\\circ}\\\\rangle}$ 5 “### Instruction: \\\\n {instruction} $\\\\backslash{\\\\mathfrak{n}}\\\\backslash{\\\\mathfrak{n}}$ ### Input: $\\\\{{\\\\tt i n p u t}\\\\}\\\\setminus{\\\\tt n\\\\backslash n}$ ### Response:” 6 ), 7 prompt no input: ( “Below is an instruction that describes a task. ” “Write a response that appropriately completes the request. $\\\\langle\\\\mathbf{n}\\\\mid\\\\mathbf{n}^{\\\\ast}$ 10 “### Instruction: $\\\\setminus\\\\mathbf{n}$ {instruction} $\\\\backslash{\\\\mathfrak{n}}\\\\backslash{\\\\mathfrak{n}}$ ### Response:” ) 11 $\\\\}$ 12 output $=$ openai.ChatCompletion.create( 13 model $=$ \"gpt-4\", 14 messages $=$ [\"role\": \"user\", \"content\": prompt], 15 temperature $=\\\\textsf{1}.0$ , 16 top p=1.0, # nucleus sampling over entire vocabulary 17 max tokens $=\\\\!512$ # the max number of generated tokens 18 )  \\n\\ninstructions (Honovich et al., 2022). Our empirical study validates the effectiveness of using GPT-4-generated data for LLM instruction-tuning, and suggests practical tips of building a general-purpose instruction-following agent powered by LLMs.', '2 DATASET': 'Data Collection. We reuse 52K unique instructions in the instruction-following data collected in the Alpaca dataset (Taori et al., 2023). Each instruction describes the task the model should perform. We follow the same prompting strategy to consider cases with and without input, which is the optional context or input for the task. The output answers to the instruction instance using LLMs. In the Alpaca dataset, the output is generated using GPT-3.5 (text-davinci-003) but we instead consider GPT-4 (gpt-4) for data generation. Specifically, we generate the following four datasets with GPT-4:  \\n\\n(1) English Instruction-Following Data: For the 52K instructions collected in Alpaca (Taori et al., 2023), one English GPT-4 answer is provided for each. The details are described in Algorithm 1. We leave it as future work to follow an iterative process to construct our own instruction set using GPT-4 and self-instruct (Wang et al., 2022a).   \\n(2) Chinese Instruction-Following Data: We use ChatGPT to translate the 52K instructions into Chinese and ask GPT-4 to answer them in Chinese. This allows us to build a Chinese instruction-following model based on LLaMA, and study cross-language generalization ability of instruction-tuning.   \\n(3) Comparison Data: We ask GPT-4 to rate its own response from 1 to 10. Furthermore, we ask GPT-4 to compare and rate the responses from the three models, including GPT-4, GPT-3.5 and OPT-IML (Iyer et al., 2022). This is used to train reward models.   \\n(4) Answers on Unnatural Instructions: The GPT-4 answers are decoded on the core dataset of 68K instruction-input-output triplets (Honovich et al., 2022). The subset is used to quantify the gap between GPT-4 and our instruction-tuned models at scale.  \\n\\nData Statistics. We compare the English output response sets of GPT-4 and GPT-3.5 in Figure 1. For each output, the root verb and the direct-object noun are extracted; The frequency over the unique verb-noun pairs are computed over each output set. The verb-noun pairs whose frequency are higher than 10 are displayed in Figure 1(a) and (b), and the most frequent 25 pairs of two sets are compared in Figure 1(c). The frequency distributions of the sequence length are compared in Figure 1(d). GPT-4 tends to generated longer sequences than GPT-3.5. The GPT-3.5 data in Alpaca exhibits an output distribution with a longer tail than our GPT-4-generated output distribution, probably because the Alpaca dataset involves an iterative data collection process to remove similar instruction instances at each iteration, which is absent in our current one-time data generation. Despite this simple process, the GPT-4 generated instruction-following data demonstrates more favorable alignment performance, as shown in experiments later.  \\n\\n![](https://cdn-mineru.openxlab.org.cn/extract/1c083c34-6489-47d8-b145-80ab407aa524/648e7b3ac7ef1b964ee54fec1f56ea8817604666a8cb6a75cd4cb4ed720ff2bf.jpg)  \\nFigure 1: Comparison of generated responses using GPT-4 and GPT-3: (a,b) The root verb-noun pairs of GPT-4 and GPT-3, where the inner circle of the plot represents the root verb of the output response, and the outer circle represents the direct nouns. (c) The top 25 verb-noun pairs and their frequencies. (d) Comparison of output sequence length.', '3 INSTRUCTION-TUNING LANGUAGE MODELS': '', '3.1 SELF-INSTRUCT TUNING': 'We train two models using supervised finetuning using the LLaMA 7B checkpoint: $(i)$ LLaMA-GPT4 is trained on 52K English instruction-following data generated by GPT-4, which distribution is displayed in Figure 1. (ii) LLaMA-GPT4-CN is trained on 52K Chinese instruction-following data from GPT-4. We follow the training schedule in (Taori et al., 2023) for fair comparisons. These models are used to study the data quality of GPT-4 and the cross-language generalization properties when instruction-tuning LLMs in one language.', '3.2 REWARD MODELS': 'Reinforcement Learning from Human Feedback (RLHF) aims to align the LLM behavior with human preferences in order to make it more useful. One key component of RLHF is reward modeling, where the problem is formulated as a regression task to predict a scalar reward given a prompt and a response (Askell et al., 2021; Ouyang et al., 2022). This approach typically requires large-scale comparison data, where two model responses on the same prompt are compared Ouyang et al. (2022). Existing open-source works such as Alpaca, Vicuna, and Dolly (Databricks, 2023) do not involve RLHF due to the high cost of labeling comparison data. Meanwhile, recent studies show that GPT-4 is capable of identifying and fixing its own mistakes, and accurately judging the quality of responses(Peng et al., 2023; Bai et al., 2022; Madaan et al., 2023; Kim et al., 2023). Therefore, to facilitate research on RLHF, we have created comparison data using GPT-4, as described in Section 2.  \\n\\nTo evaluate data quality, we train a reward model based on OPT 1.3B (Iyer et al., 2022) to rate different responses. For each instance of the comparison data involving one prompt $\\\\textbf{\\\\em x}$ and $K$ responses, GPT-4 assigns a score $s\\\\in[1,10]$ for each response. There are $C_{2}^{K}$ unique pairs constructed from this instance, each pair is $(y_{l},y_{h})$ , whose corresponding scores follow $s_{l}~<~s_{h}$ . A reward model $r_{\\\\theta}$ parameterized by $\\\\pmb{\\\\theta}$ is trained with the objective: $\\\\operatorname*{min}\\\\log(\\\\sigma(r_{\\\\pmb\\\\theta}(x,y_{h})-r_{\\\\pmb\\\\theta}(\\\\pmb x,y_{l})))$ , where $\\\\sigma$ is the sigmoid function. The distribution of the comparison data is shown in Figure 2.  \\n\\n![](https://cdn-mineru.openxlab.org.cn/extract/1c083c34-6489-47d8-b145-80ab407aa524/f3c4e0d3b5af16b40081d39fc62beab25c81f465d0774242ad96e88ab334ad4b.jpg)  \\nFigure 2: The distribution of comparison data.', '4 EXPERIMENTAL RESULTS': '', '4.1 BENCHMARKS': 'It is known that LLM evaluation remains a significant challenge. Our goal is to evaluate self-instruct tuned models on GPT-4 data on unseen instructions, to study their ability to follow instructions for arbitrary tasks. Specifically, we use three established datasets in our study:  \\n\\n• User-Oriented-Instructions- $252^{\\\\,2}$ (Wang et al., 2022a) is a manually curated set involving 252 instructions, motivated by 71 user-oriented applications such as Grammarly, StackOverflow, Overleaf, rather than well-studied NLP tasks.   \\n• Vicuna-Instructions- $80^{3}$ (Vicuna, 2023) is a dataset synthesized by gpt-4 with 80 challenging questions that baseline models find challenging. Beside generic instructions, there are 8 categories, including knowledge, math, Fermi, counterfactual, roleplay, generic, coding, writing, common-sense.   \\n• Unnatural Instructions4 (Honovich et al., 2022) is a dataset of 68,478 samples synthesized by text-davinci-002 using 3-shot in-context-learning from 15 manually-constructed examples.  \\n\\n![](https://cdn-mineru.openxlab.org.cn/extract/1c083c34-6489-47d8-b145-80ab407aa524/87f25a37070ba8649c23d9507c8779372aa5ec3ca4cff0989798cfa82d65fe70.jpg)  \\nFigure 3: Human evaluation.  \\n\\n4.2 HUMAN EVALUATION WITH ALIGNMENT CRITERIA  \\n\\nTo evaluate the alignment quality of our instruction-tuned LLMs, we follow alignment criteria from Anthropic Askell et al. (2021): an assistant is aligned if it is helpful, honest, and harmless (HHH). These criteria are used to evaluate how well an AI system is aligned with human values.  \\n\\n• Helpfulness: whether it helps humans achieve their goals. A model that can answer questions accurately is helpful.   \\n• Honesty: whether it provides true information, and expresses its uncertainty to avoid misleading human users when necessary. A model that provides false information is not honest.   \\n• Harmlessness: whether it does not cause harm to humans. A model that generates hate speech or promotes violence is not harmless.  \\n\\nBased on HHH alignment criteria, we used Amazon Mechanical Turk to perform human evaluation on the model generation results. Please find the interface in Appendix Section A.1. Following (Wang et al., 2022a; Taori et al., 2023), we consider 252 user-oriented instructions for evaluation. We display the human evaluation results in pie charts in Figure 3.  \\n\\nFirst, we compare the quality of generated responses from two instruction-tuned LLaMA models, which are fine-tuned on data generated by GPT-4 and GPT-3, respectively. Note that aligning LLaMA to GPT-3 corresponds to the Stanford Alpaca model. From Figure 3(a), we observe that $(i)$ For the “Helpfulness” criterion, GPT-4 is the clear winner with $54.12\\\\%$ of the votes. GPT-3 only wins $19.74\\\\%$ of the time. (ii) For the “Honesty” and “Harmlessness” criteria, the largest portion of votes goes to the tie category, which is substantially higher than the winning categories but GPT-3 (Alpaca) is slightly superior.  \\n\\nSecond, we compare GPT-4-instruction-tuned LLaMA models against the teacher model GPT-4 in Figure 3(b). The observations are quite consistent over the three criteria: GPT-4-instruction-tuned LLaMA performs similarly to the original GPT-4. We conclude that learning from GPT-4 generated data can lead to very comparable performance with the original GPT-4 on the unseen instructional tasks, which suggests a promising direction to developing state-of-the-art instruction-following LLMs.  \\n\\n![](https://cdn-mineru.openxlab.org.cn/extract/1c083c34-6489-47d8-b145-80ab407aa524/391ce593cd636aa1d97e33668f4fce955396fd95f6d83492832bb0978f31573b.jpg)  \\nFigure 4: Performance comparisons evaluated by GPT-4. Each bar represents an evaluation result between two models; the sum of scores are computed and reported (the full score is 800). The relative score is reported in percentage, which is computed as the ratio against a strong opponent model. (a,b) The comparisons of responses from LLaMA GPT4 ranked by our reward model. ‘B’ indicates the baseline that the model decodes one response per question. (c,d) All chatbots are compared against ChatGPT and GPT-4, respectively.', '4.3 COMPARISONS WITH SOTA USING AUTOMATIC EVALUATION': 'Automatic Evaluation with GPT-4. Following (Vicuna, 2023), we employ GPT-4 to automatically evaluate the generated responses of different models on 80 unseen questions in (Vicuna, 2023). We first collect answers from two chatbots, including LLaMA-GPT-4 (7B) and GPT-4, and use the release answers of other chatbots from (Vicuna, 2023), including LLaMA (13B), Alpaca (13B), Vicuna (13B), Bard (Google, 2023), and ChatGPT. For each evaluation, we ask GPT-4 to rate the response quality between two models with scores from 1 to 10. We compare all models against a strong competing model such as ChatGPT and GPT-4, respectively. The results are shown in Figure 4.  \\n\\nFor LLaMA instruction-tuned with GPT-4, we provide two sets of decoding results: $(i)$ One response per question, which is considered the baseline decoding result. $(i i)$ Five responses per questions. For the latter, the reward model is used to rank the responses which are then grouped into five subsets ranked from top 1 to top 5. We compare the five ranked groups against the baseline, and show the relative scores in Figure 4 (a,b). The ChatGPT and GPT-4 evaluation is consistent with the orders suggested by our reward model, which demonstrate the value of the feedback data and effectiveness of the reward model.  \\n\\n![](https://cdn-mineru.openxlab.org.cn/extract/1c083c34-6489-47d8-b145-80ab407aa524/c661bdcdb2121dac7141676b9ecf33a671b85c5f15bb0a86f36646d2ce185a86.jpg)  \\n\\n![](https://cdn-mineru.openxlab.org.cn/extract/1c083c34-6489-47d8-b145-80ab407aa524/29a8da4f1641549379f9319f390757157c475a89f73378094a0d7bdcec722adb.jpg)  \\n(b) All chatbots against GPT-4, whose Chinese responses are generated by asking Chinese questions  \\n\\n![](https://cdn-mineru.openxlab.org.cn/extract/1c083c34-6489-47d8-b145-80ab407aa524/4aa569edc6e700e968f12c9c118787baa83f754d0997493bb5794f772e8dbffe.jpg)  \\nFigure 5: Performance comparisons of Chinese instruction-following evaluated by GPT-4. In (a,b), all models are asked to respond in English, and the responses are translated into Chinese; the scores are computed against translated Chinese in (a) and model generated Chinese in (b). In (c), all models are asked to respond in Chinese.  \\n\\nWe compare all the chatbots in Figure 4(c,d). Instruction tuning of LLaMA with GPT-4 often achieves higher performance than tuning with text-davinci-003 (i.e., Alpaca) and no tuning (i.e., LLaMA): The 7B LLaMA GPT4 outperforms the 13B Alpaca and LLaMA. However, there is still a gap compared with large commercial chatbots such as GPT-4.  \\n\\nWe further study the performance of all the chatbots in Chinese in Figure 5. We first translate English responses of chatbots into Chinese using GPT-4. We also translate English questions into Chinese to obtain answers with GPT-4. The comparisons against translated and generated Chinese responses from GPT-4 are shown in Figure 5 (a) and (b), respectively. There are two interesting observations: (i) we find that the relative score metric of GPT-4 evaluation (Vicuna, 2023) is quite consistent, both in terms of different opponent models (i.e., ChatGPT or GPT-4) and languages (i.e., English or Chinese). $(i i)$ For GPT-4 results alone, the translated responses show superior performance over the generated response in Chinese, probably because GPT-4 is trained in richer English corpus than Chinese, which leads to stronger English instruction-following ability. In Figure 5 (c), we show results for all models who are asked to answer in Chinese.  \\n\\nWe compare LLaMA-GPT4 with GPT-4 and Alpaca unnatural instructions in Figure 6. In terms of the average ROUGE-L scores, Alpaca outperforms the other two models. We note that LLaMA-GPT4 and GPT4 is gradually performing better when the ground truth response length is increasing, eventually showing higher performance when the length is longer than 4. This means that they can better follow instructions when the scenarios are more creative. Across different subsets, LLaMA-GPT4 can closely follow the behavior of GPT-4. When the sequence length is short, both LLaMA-GPT4 and GPT-4 can generate responses that contains the simple ground truth answers, but add extra words to make the response more chat-like, which probably leads to lower ROUGE-L scores.  \\n\\n![](https://cdn-mineru.openxlab.org.cn/extract/1c083c34-6489-47d8-b145-80ab407aa524/8847ff8909dd69a4675f2f75297c311b4a91c57f7f130110f30639fe999b517d.jpg)  \\nFigure 6: ROUGE-L on unnatural instructions evaluated with 9K samples. The instructions are grouped into four subsets based on the ground-truth response length. The mean values are reported in the legend. The difference with GPT-4 is reported on the bar per group. LLaMA-GPT4 is a closer proxy to GPT-4 than Alpaca.', '5 RELATED WORK': 'Instruction Tuning. Instruction tuning of LLMs is an increasingly popular research direction in NLP (Zhong et al., 2021; Ouyang et al., 2022; Wei et al., 2021). Existing works aim to improve the quality and scale of three factors in the development pipeline, including instruction-following data, foundation language models and evaluation benchmarks. Each group typically maintains its own pipeline. For example, scaling instruction-finetuned language models (Chung et al., 2022) is built on top of FLAN (Wei et al., 2021). PromptSource contains a growing collection of prompts (which is also called P3: Public Pool of Prompts) (Bach et al., 2022). T0 is a series of models trained on P3 via multitask prompted training (Sanh et al., 2021). Instruction-tuning of OPT models is considered in (Iyer et al., 2022), where a larger and more comprehensive benchmark OPT-IML Bench is employed, covering FLAN (Wei et al., 2021), Super-NaturalInstructions (Wang et al., 2022b), and UnifiedSKG (Xie et al., 2022).  \\n\\nOpen-Source Efforts. Given the broad capabilities of LLMs exhibited by ChatGPT, open-source models have drawn a significant interest and promoted work towards open, general-purpose, textbased assistants that are aligned with human values. Early attempts on foundation LLMs include BLOOM (Scao et al., 2022), GPT-J (Wang & Komatsuzaki, 2021), GPT-NEO (Black et al., 2021) OPT (Zhang et al., 2022) and LLaMA (Zhang et al., 2023). To align LLMs with chat-based assistance, Open-Assistant (LAION-AI, 2023) is built on GPT-J, and Alpaca/Vicuna are built on LLaMA. Furthermore, OpenFlamingo (Awadalla et al., 2023) and LLaMA-Adapter (Zhang et al., 2023) connect LLaMA with image inputs, paving a way to build open-source multi-modal LLMs.'}\n",
      "\n",
      "Processing sections:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: ABSTRACT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections:  10%|█         | 1/10 [00:04<00:41,  4.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: INTRODUCTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections:  20%|██        | 2/10 [00:19<01:27, 10.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: 2 DATASET\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections:  30%|███       | 3/10 [00:37<01:37, 13.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: 3 INSTRUCTION-TUNING LANGUAGE MODELS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections:  40%|████      | 4/10 [00:39<00:55,  9.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: 3.1 SELF-INSTRUCT TUNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections:  50%|█████     | 5/10 [00:44<00:39,  7.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: 3.2 REWARD MODELS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections:  60%|██████    | 6/10 [00:55<00:34,  8.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: 4 EXPERIMENTAL RESULTS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections:  70%|███████   | 7/10 [00:57<00:19,  6.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: 4.1 BENCHMARKS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections:  80%|████████  | 8/10 [01:13<00:19,  9.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: 4.3 COMPARISONS WITH SOTA USING AUTOMATIC EVALUATION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections:  90%|█████████ | 9/10 [01:26<00:10, 10.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: 5 RELATED WORK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections: 100%|██████████| 10/10 [01:44<00:00, 10.47s/it]\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "markdown_path = \"input_file/INSTRUCTION TUNING WITH GPT-4.md\"\n",
    "output_dir = \"output_file\"\n",
    "\n",
    "explanations, qa = process_paper(markdown_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Explanations saved to: output_file/INSTRUCTION TUNING WITH GPT-4_explained.md\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"output_file\"\n",
    "\n",
    "input_filename = Path(markdown_path).stem  # 파일 이름만 추출 (확장자 제외)\n",
    "output_path = os.path.join(output_dir, f\"{input_filename}_explained.md\")\n",
    "\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    for section, explanation in explanations.items():\n",
    "        f.write(f\"\\n## {section}\\n\\n\")\n",
    "        f.write(explanation)\n",
    "        f.write(\"\\n\\n---\\n\")\n",
    "\n",
    "print(f\"\\nExplanations saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "                                                     <span style=\"font-weight: bold; text-decoration: underline\">ABSTRACT</span>                                                      \n",
       "\n",
       "이 연구에서는 대형 언어 모델(LLM)을 기계가 생성한 지시문 데이터를 사용하여 미세 조정(finetuning)하면 새로운        \n",
       "작업에서도 뛰어난 제로샷(Zero-shot) 성능을 발휘할 수 있다는 것을 이전 연구가 보여주었다고 설명합니다. 즉, 인간이   \n",
       "작성한 지시문이 필요 없게 됩니다. 본 논문은 GPT-4를 활용하여 LLM 미세 조정을 위한 지시문 데이터를 생성한 첫 시도를 \n",
       "소개합니다. 연구 결과, GPT-4가 생성한 5만 2천 개의 영어 및 중국어 지시문 데이터가 기존 최첨단 모델들이 생성한      \n",
       "지시문 데이터보다 새로운 작업에서 뛰어난 제로샷 성능을 보여주었습니다. 또한, 포괄적인 평가 및 보상 모델 훈련을 위해\n",
       "GPT-4로부터 피드백 및 비교 데이터를 수집하였습니다. 우리는 GPT-4를 사용하여 생성한 데이터와 코드베이스를 공개하고  \n",
       "있습니다.                                                                                                          \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "\n",
       "                                                   <span style=\"font-weight: bold; text-decoration: underline\">INTRODUCTION</span>                                                    \n",
       "\n",
       "이 논문은 대형 언어 모델(LLM)의 뛰어난 일반화 능력을 설명하며, 이러한 모델들이 자연어 지시를 따르고 실제 세상에서의\n",
       "작업을 완료할 수 있도록 하는 방법을 탐구하고 있습니다. 그 방법 중 하나로 지시문 튜닝(instruction-tuning)이 있으며, \n",
       "이는 인간이 주석을 단 프롬프트와 피드백을 사용하여 모델을 다양한 작업에 맞게 미세 조정하거나, 수동 또는 자동으로   \n",
       "생성된 지시문을 추가한 공개 벤치마크와 데이터세트를 사용하는 방식으로 구현됩니다.                                  \n",
       "\n",
       "특히 Self-Instruct 튜닝은 최신의 지시문 튜닝된 교사 LLM이 생성한 데이터를 학습하여, LLM을 인간 의도에 맞추는       \n",
       "간단하고 효과적인 방법입니다. 이 연구 방향은 LLM의 제로샷 및 소수샷 일반화 능력을 향상시키는 효과적인 수단을 제공해\n",
       "왔습니다. 최근 ChatGPT와 GPT-4의 성공은 이러한 지시문 튜닝을 통해 오픈소스 LLM을 개선할 큰 기회를 제공합니다.      \n",
       "LLaMA는 이러한 오픈소스 LLM들 중 하나로, 상용 LLM인 GPT-3와 성능을 견줄 수 있습니다. LLaMA가 지시를 따를 수 있도록 \n",
       "하려면, 뛰어난 성능과 저비용의 Self-Instruct 튜닝이 빠르게 채택되고 있습니다.                                      \n",
       "\n",
       "본 논문에서 우리는 처음으로 GPT-4를 Self-Instruct 튜닝의 교사로 사용하는 방법을 제안합니다. 우리의 주요 기여는     \n",
       "다음과 같습니다:                                                                                                   \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">GPT-4 데이터:</span> 우리는 영어와 중국어로 된 52K 개의 지시문 데이터셋과 세 개의 지시문 튜닝된 모델의 출력을 평가한   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>피드백 데이터를 포함하여 GPT-4가 생성한 데이터를 공개합니다.                                                    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">모델 및 평가:</span> GPT-4가 생성한 데이터를 기반으로 지시문 튜닝된 LLaMA 모델과 보상 모델을 개발했습니다. 우리는 세   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>가지 기준에 대한 인간 평가, GPT-4 피드백을 사용한 자동 평가, 그리고 비정형 지시문에 대한 ROUGE-L 점수를 사용하여\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>지시문 튜닝된 LLM의 품질을 평가합니다.                                                                          \n",
       "\n",
       "이 연구는 GPT-4로 생성된 데이터를 사용한 LLM 지시문 튜닝의 효과성을 확인하고, 일반 목적의 지시문을 따르는          \n",
       "에이전트를 구축하는 데 있어 실용적인 팁을 제안합니다.                                                              \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "\n",
       "                                                     <span style=\"font-weight: bold; text-decoration: underline\">2 DATASET</span>                                                     \n",
       "\n",
       "이 논문의 '2 데이터셋' 섹션에서는 데이터 수집 과정과 GPT-4를 사용한 데이터 생성 방법에 대해 설명합니다.            \n",
       "\n",
       "데이터 수집: Alpaca 데이터셋(Taori et al., 2023)에서 수집된 52,000개의 고유한 지시문을 재사용합니다. 각 지시문은   \n",
       "모델이 수행해야 할 작업을 설명합니다. 지시문에는 작업에 대한 선택적 맥락이나 입력이 있을 수도 있고 없을 수도       \n",
       "있습니다. Alpaca 데이터셋에서는 GPT-3.5(text-davinci-003)를 사용해 출력을 생성했지만, 우리는 GPT-4(gpt-4)를        \n",
       "활용하여 출력을 생성합니다. GPT-4를 사용해 다음 네 가지 데이터셋을 생성합니다.                                     \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 1 </span><span style=\"font-weight: bold\">영어 지시문-답변 데이터:</span> Alpaca에서 수집한 52,000개의 지시문 각각에 대해 GPT-4가 영어로 답변을 제공합니다.      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>자세한 내용은 알고리즘 1에 나와 있습니다. GPT-4와 Self-Instruct를 사용하여 자체 지시 세트를 구성하는 반복적     \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>과정을 향후 작업으로 남겨둡니다.                                                                                \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 2 </span><span style=\"font-weight: bold\">중국어 지시문-답변 데이터:</span> ChatGPT를 사용하여 52,000개의 지시문을 중국어로 번역하고, GPT-4가 중국어로 답변하게  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>합니다. 이를 통해 LLaMA에 기반한 중국어 지시문-따르기 모델을 구축하고, 지시문 튜닝의 언어 간 일반화 능력을      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>연구할 수 있습니다.                                                                                             \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 3 </span><span style=\"font-weight: bold\">비교 데이터:</span> GPT-4에게 자신의 응답을 1부터 10까지 평가하도록 요청합니다. 또한, GPT-4에게 GPT-4, GPT-3.5, 그리고 \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>OPT-IML을 포함한 세 모델의 응답을 비교하여 평가하도록 요청합니다. 이는 보상 모델을 훈련하는 데 사용됩니다.      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 4 </span><span style=\"font-weight: bold\">비정상 지시문에 대한 답변:</span> 68,000개의 지시문-입력-출력 삼중 코어 데이터셋에 대해 GPT-4의 답변을 디코딩합니다. 이\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>하위 세트는 GPT-4와 우리 지시문 튜닝된 모델들 간의 차이를 계량화하는 데 사용됩니다.                             \n",
       "\n",
       "데이터 통계: 그림 1에서는 GPT-4와 GPT-3.5의 영어 출력 응답 세트를 비교합니다. 각 출력에 대해 동사 뿌리와 직접      \n",
       "목적어 명사를 추출하고, 각 출력 세트에서 고유한 동사-명사 쌍의 빈도를 계산합니다. 빈도가 10 이상인 동사-명사 쌍은  \n",
       "그림 1(a)와 (b)에 표시되며, 두 세트의 가장 빈번한 25개의 쌍을 그림 1(c)에 비교합니다. 출력 시퀀스 길이의 빈도      \n",
       "분포는 그림 1(d)에 비교되어 있습니다. GPT-4는 GPT-3.5보다 더 긴 시퀀스를 생성하는 경향이 있습니다. Alpaca의 GPT-3.5\n",
       "데이터는 반복적인 데이터 수집 과정을 통해 각 반복에서 유사한 지시문 인스턴스를 제거하여 더 긴 꼬리를 가진 출력     \n",
       "분포를 보입니다. 이는 현 시점의 일회성 데이터 생성 과정에서는 없는 특징입니다. 이러한 간단한 과정에도 불구하고,    \n",
       "GPT-4가 생성한 지시문-따르기 데이터는 이후 실험에서 더 나은 정렬 성능을 보여줍니다.                                \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "\n",
       "                                       <span style=\"font-weight: bold; text-decoration: underline\">3 INSTRUCTION-TUNING LANGUAGE MODELS</span>                                        \n",
       "\n",
       "I apologize, but it seems that the '3 INSTRUCTION-TUNING LANGUAGE MODELS' section content is missing from your     \n",
       "request. If you can provide the full text of that section, I will be happy to explain it in detail.                \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "\n",
       "                                             <span style=\"font-weight: bold; text-decoration: underline\">3.1 SELF-INSTRUCT TUNING</span>                                              \n",
       "\n",
       "이 섹션에서는 Self-Instruct 튜닝을 통해 언어 모델을 훈련하는 과정에 대해 설명합니다. 연구에서는 LLaMA 7B           \n",
       "체크포인트를 사용하여 두 가지 모델을 지도 학습 방식으로 미세 조정했습니다.                                         \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 1 </span><span style=\"font-weight: bold\">LLaMA-GPT4 모델</span>: 이 모델은 GPT-4가 생성한 52,000개의 영어 지시문-따르기 데이터를 바탕으로 훈련되었습니다. 이    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>데이터의 분포는 이전의 그림 1에서 보여주었습니다.                                                               \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 2 </span><span style=\"font-weight: bold\">LLaMA-GPT4-CN 모델</span>: 이 모델은 GPT-4가 생성한 52,000개의 중국어 지시문-따르기 데이터를 바탕으로 훈련되었습니다.  \n",
       "\n",
       "훈련 과정에서는 공정한 비교를 위해 Taori et al.(2023)의 훈련 스케줄을 따랐습니다. 이러한 모델들은 GPT-4가 생성한   \n",
       "데이터의 품질을 연구하고, 하나의 언어로 지시문 튜닝을 했을 때의 교차 언어 일반화 특성을 연구하는 데 사용됩니다.    \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "\n",
       "                                                 <span style=\"font-weight: bold; text-decoration: underline\">3.2 REWARD MODELS</span>                                                 \n",
       "\n",
       "이 섹션에서는 인간 피드백을 통한 강화 학습(RLHF, Reinforcement Learning from Human Feedback)의 핵심 요소인 보상    \n",
       "모델링에 대해 설명합니다. RLHF는 LLM의 행동을 인간의 선호와 맞춰 모델을 더욱 유용하게 만들려는 목적을 가지고       \n",
       "있습니다. 보상 모델링은 프롬프트와 응답이 주어졌을 때 스칼라 보상을 예측하는 회귀 과제로 문제를 구성합니다. 이렇게 \n",
       "하려면 대규모 비교 데이터가 필요한데, 두 가지 모델의 응답을 같은 프롬프트에 대해 비교하는 방식입니다. 하지만       \n",
       "Alpaca, Vicuna, Dolly 등의 기존 오픈소스 작업은 비교 데이터 레이블링 비용이 높아 RLHF를 포함하지 않습니다.         \n",
       "\n",
       "한편, 최근 연구는 GPT-4가 자신의 실수를 식별하고 수정하며, 응답 품질을 정확히 평가할 수 있음을 보여주었습니다.     \n",
       "따라서 RLHF 연구를 촉진하기 위해, 우리는 2절에서 설명한 바와 같이 GPT-4를 사용하여 비교 데이터를 생성했습니다.     \n",
       "\n",
       "데이터 품질 평가를 위해, 우리는 OPT 1.3B 기반으로 보상 모델을 훈련하여 서로 다른 응답들을 평가합니다. 각 비교      \n",
       "데이터 인스턴스는 한 프롬프트 $\\textbf{\\em x}$와 $K$개의 응답을 포함하고, GPT-4는 각 응답에 대해 1부터 10까지의    \n",
       "점수 $s$를 줍니다. 이 인스턴스에서 고유한 쌍 $C_{2}^{K}$를 구성할 수 있으며, 각 쌍은 $(y_{l}, y_{h})$로 표현됩니다.\n",
       "여기서 $y_{l}$의 점수는 $y_{h}$의 점수보다 낮습니다($s_{l} &lt; s_{h}$). 보상 모델 $r_{\\theta}$는 다음 목표로         \n",
       "훈련됩니다: $\\operatorname*{min}\\log(\\sigma(r_{\\pmb\\theta}(x,y_{h})-r_{\\pmb\\theta}(\\pmb x,y_{l})))$, 여기서        \n",
       "$\\sigma$는 시그모이드 함수입니다. 비교 데이터의 분포는 그림 2에 나타나 있습니다.                                   \n",
       "\n",
       "이 접근 방식은 보상 모델이 다양한 응답 품질을 평가하고 RLHF의 효과성을 높이는 데 도움을 줄 수 있도록               \n",
       "설계되었습니다.                                                                                                    \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "\n",
       "                                              <span style=\"font-weight: bold; text-decoration: underline\">4 EXPERIMENTAL RESULTS</span>                                               \n",
       "\n",
       "It looks like the content from the '4 EXPERIMENTAL RESULTS' section is missing from your request. If you can       \n",
       "provide the full text of that section, I would be happy to explain it in detail.                                   \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "\n",
       "                                                  <span style=\"font-weight: bold; text-decoration: underline\">4.1 BENCHMARKS</span>                                                   \n",
       "\n",
       "이 섹션에서는 LLM(대형 언어 모델)의 성능을 평가하기 위한 벤치마크를 소개하고, 인간 평가 방법과 그 결과를           \n",
       "설명합니다.                                                                                                        \n",
       "\n",
       "                                                     <span style=\"font-weight: bold\">벤치마크:</span>                                                     \n",
       "\n",
       "이 연구의 목표는 GPT-4 데이터를 기반으로 학습된 Self-Instruct 모델들이 새로운 지시문에 대해 얼마나 잘 반응하는지   \n",
       "평가하는 것입니다. 연구에서 사용한 데이터셋은 다음과 같습니다:                                                     \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">User-Oriented-Instructions- $252^{,2}$</span>: 이 데이터셋은 71개의 사용자 중심 애플리케이션(예: Grammarly,            \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>StackOverflow, Overleaf)을 기반으로 한 252개의 지시문을 포함합니다.                                             \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Vicuna-Instructions- $80^{3}$</span>: GPT-4가 생성한 80개의 도전적인 질문이 포함되어 있으며, 기본 모델들이 어려워하는  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>질문들입니다. 지식, 수학, Fermi 문제, 반사실적 시나리오, 롤플레잉, 일반, 코딩, 글쓰기, 상식 등 8개의 카테고리가 \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>있습니다.                                                                                                       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Unnatural Instructions</span>: 68,478개의 샘플로 구성된 이 데이터셋은 15개의 수작업 예제를 사용한 3-샷 맥락 학습으로   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>text-davinci-002가 합성한 것입니다.                                                                             \n",
       "\n",
       "                                                    <span style=\"font-weight: bold\">인간 평가:</span>                                                     \n",
       "\n",
       "모델의 정렬 품질을 평가하기 위해 Anthropic Askell et al.(2021)에서 제안한 정렬 기준을 따랐습니다. 이 기준은 AI     \n",
       "시스템이 인간의 가치에 얼마나 잘 정렬되어 있는지를 평가하는 데 사용됩니다. 기준은 다음과 같습니다:                 \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">도움됨</span>: 인간이 목표를 달성하는 데 도움이 되는지 여부를 평가합니다.                                              \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">정직성</span>: 정확한 정보를 제공하고 필요시 불확실성을 표현하여 인간 사용자를 오도하지 않는지를 평가합니다.           \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">무해성</span>: 인간에게 해를 끼치지 않는지를 평가합니다.                                                               \n",
       "\n",
       "Amazon Mechanical Turk를 통해 인간 평가를 수행했으며, 결과는 그림 3의 파이 차트에 나타나 있습니다.                 \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">첫 번째 비교</span>: GPT-4 데이터로 미세 조정된 LLaMA 모델과 GPT-3 데이터로 미세 조정된 LLaMA 모델(Stanford Alpaca     \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>모델)을 비교하여, \"도움됨\" 기준에서 GPT-4가 54.12%로 우세했으며, \"정직성\"과 \"무해성\"에서는 GPT-3(Alpaca)이 다소 \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>우세하나 비슷한 수준의 결과를 보였습니다.                                                                       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">두 번째 비교</span>: GPT-4 지시를 따라 튜닝된 LLaMA 모델과 원래의 GPT-4 모델을 비교했으며, 세 가지 기준 모두에서 유사한\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>성능을 보여줬습니다. 이는 GPT-4가 생성한 데이터로 학습함으로써 원래 GPT-4 수준의 성능을 달성할 수 있다는 것을   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>의미합니다.                                                                                                     \n",
       "\n",
       "                                             <span style=\"font-weight: bold\">추가 그림 설명 (그림 4):</span>                                              \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>GPT-4로 평가된 성능 비교를 보여줍니다. 각 막대는 두 모델 간의 평가 결과를 나타내며, 총 점수(최대 800점)와 상대  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>점수 비율(강력한 상대 모델 대비)이 보고됩니다.                                                                  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>LLaMA GPT4 모델의 응답을 우리 보상 모델로 평가한 결과를 나타내며, 전반적으로 ChatGPT 및 GPT-4와 비교하여 성능을 \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>평가했습니다.                                                                                                   \n",
       "\n",
       "이 연구는 GPT-4 데이터를 사용한 학습이 새로운 지시문 작업에서 뛰어난 성능을 낸다는 가능성을 시사합니다.            \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "\n",
       "                               <span style=\"font-weight: bold; text-decoration: underline\">4.3 COMPARISONS WITH SOTA USING AUTOMATIC EVALUATION</span>                                \n",
       "\n",
       "이 섹션에서는 자동 평가를 사용하여 LLaMA-GPT4 모델과 다른 최첨단(State-of-the-Art, SOTA) 모델들을 비교한 결과를    \n",
       "설명합니다.                                                                                                        \n",
       "\n",
       "                                                    <span style=\"font-weight: bold\">자동 평가:</span>                                                     \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">평가 방법</span>: GPT-4를 활용해 다양한 모델들이 80개의 새로운 질문에 대한 생성 응답의 품질을 자동 평가했습니다.       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>LLaMA-GPT4(7B)와 GPT-4의 응답을 수집하고, Vicuna(2023)에서 공개된 다른 채팅 봇들(LLaMA 13B, Alpaca 13B, Vicuna  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>13B, Bard, ChatGPT)의 응답을 사용했습니다. 두 모델 간의 응답 품질을 1에서 10까지의 점수로 평가했습니다.         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">결과</span>: LLaMA-GPT4를 두 가지 디코딩 결과로 평가했습니다. 첫째는 질문당 하나의 응답만을 생성하는 기본 디코딩 결과, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>둘째는 질문당 다섯 개의 응답을 생성한 후 보상 모델이 상위에서 하위로 순위 매긴 응답 그룹입니다. 이 그룹들은 기본\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>결과와 비교되었고, 평가 결과에서 피드백 데이터와 보상 모델의 유용성을 보여줍니다.                               \n",
       "\n",
       "                                                       <span style=\"font-weight: bold\">결과:</span>                                                       \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>LLaMA-GPT4는 text-davinci-003(즉, Alpaca)로 튜닝한 경우보다 높은 성능을 보여주었지만, 여전히 상업용 대형        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>챗봇(e.g., GPT-4)과 비교해서는 차이가 존재합니다.                                                               \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">중국어 평가</span>: 영어 응답을 중국어로 번역한 것이나, 중국어로 직접 질문하여 생성된 응답을 비교했을 때, 번역된 응답이\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>더 우수한 성능을 보여주었습니다. 이는 GPT-4가 영어 코퍼스에서 더 많이 훈련되어 영어 지시문을 따르는 능력이 더   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>강력하기 때문일 수 있습니다.                                                                                    \n",
       "\n",
       "                             <span style=\"font-weight: bold\">비정상 지시문(unnatural instructions)에서의 결과(그림 6):</span>                             \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>LLaMA-GPT4와 GPT-4, 그리고 Alpaca 모델을 비교했을 때, Alpaca가 다른 두 모델에 비해 평균 ROUGE-L 점수에서        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>우수하였습니다. 그러나 응답의 길이가 길어질수록 LLaMA-GPT4와 GPT-4가 더 높은 성능을 보여주기 시작했습니다. 이는 \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>더 창의적인 시나리오에서 이 모델들이 지시문을 더 잘 따를 수 있다는 것을 의미합니다.                             \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>각기 다른 응답 길이 하위 집합에 대해, LLaMA-GPT4는 GPT-4의 행동을 가깝게 따릅니다. 응답 길이가 짧을 때,         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>LLaMA-GPT4와 GPT-4는 단순한 정답을 포함하되, 응답을 더 대화형으로 만드는 추가적인 단어를 포함하기 때문에, 낮은  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>ROUGE-L 점수를 받을 가능성이 있습니다.                                                                          \n",
       "\n",
       "이 실험 결과는 LLaMA-GPT4가 GPT-4의 출력을 기반으로 학습했을 때, 상당한 성능 향상을 이룰 수 있음을 보여주며, 이는  \n",
       "혁신적인 지시문-따르기 LLM 개발에 있어서 유망한 방향임을 시사합니다.                                               \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "\n",
       "                                                  <span style=\"font-weight: bold; text-decoration: underline\">5 RELATED WORK</span>                                                   \n",
       "\n",
       "이 섹션에서는 지시문 튜닝과 오픈소스 노력이 관련된 연구에 대해 설명합니다.                                         \n",
       "\n",
       "                                         <span style=\"font-weight: bold\">지시문 튜닝(Instruction Tuning):</span>                                          \n",
       "\n",
       "지시문 튜닝은 NLP 분야에서 점점 주목받고 있는 연구 방향입니다. 기존 연구들은 크게 세 가지 요소인 지시문-따르기     \n",
       "데이터, 기본 언어 모델, 평가 벤치마크의 품질과 규모를 향상시키는 것을 목표로 합니다. 각 연구 그룹은 일반적으로     \n",
       "자체적인 개발 파이프라인을 유지합니다. 예를 들어, 플랜(FLAN)을 기반으로 지시문 미세 조정 언어 모델의 규모를        \n",
       "확대하는 연구(Chung et al., 2022)나, 점점 많은 프롬프트를 포함하는 PromptSource(공공 프롬프트 풀, P3)의 개발(Bach  \n",
       "et al., 2022) 등이 있습니다. T0는 P3에 기반한 다중 과제 프롬프트 훈련을 통해 개발된 일련의 모델(Sanh et al.,       \n",
       "2021)입니다. OPT 모델의 지시문 튜닝은 더 크고 포괄적인 벤치마크인 OPT-IML Bench를 사용하여 진행되었으며, 이는 FLAN,\n",
       "Super-NaturalInstructions(Wang et al., 2022b), UnifiedSKG(Xie et al., 2022)을 포함합니다.                          \n",
       "\n",
       "                                        <span style=\"font-weight: bold\">오픈소스 노력(Open-Source Efforts):</span>                                        \n",
       "\n",
       "ChatGPT의 폭넓은 능력 덕분에, 오픈소스 모델은 큰 관심을 받으며 인간의 가치에 맞는 일반 목적의 텍스트 기반          \n",
       "어시스턴트를 개발하는 데 기여하고 있습니다. 초기의 기초 LLM 노력에는 BLOOM(Scao et al., 2022), GPT-J(Wang &amp;        \n",
       "Komatsuzaki, 2021), GPT-NEO(Black et al., 2021), OPT(Zhang et al., 2022), LLaMA(Zhang et al., 2023)가 포함됩니다.  \n",
       "LLM을 대화 기반 어시스턴스에 맞추기 위해 Open-Assistant는 GPT-J를 기반으로 하고, Alpaca/Vicuna는 LLaMA를 기반으로  \n",
       "구축되었습니다. 또한, OpenFlamingo(Awadalla et al., 2023)와 LLaMA-Adapter(Zhang et al., 2023)는 LLaMA를 이미지     \n",
       "입력과 연결하여 오픈소스 멀티모달 LLM을 구축할 길을 마련했습니다.                                                  \n",
       "\n",
       "이러한 연구와 노력은 LLM이 더 다양하고 강력한 능력을 발휘할 수 있도록 하고, 오픈소스 환경에서의 발전을 촉진하는 데 \n",
       "그 목적이 있습니다.                                                                                                \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "                                                     \u001b[1;4mABSTRACT\u001b[0m                                                      \n",
       "\n",
       "이 연구에서는 대형 언어 모델(LLM)을 기계가 생성한 지시문 데이터를 사용하여 미세 조정(finetuning)하면 새로운        \n",
       "작업에서도 뛰어난 제로샷(Zero-shot) 성능을 발휘할 수 있다는 것을 이전 연구가 보여주었다고 설명합니다. 즉, 인간이   \n",
       "작성한 지시문이 필요 없게 됩니다. 본 논문은 GPT-4를 활용하여 LLM 미세 조정을 위한 지시문 데이터를 생성한 첫 시도를 \n",
       "소개합니다. 연구 결과, GPT-4가 생성한 5만 2천 개의 영어 및 중국어 지시문 데이터가 기존 최첨단 모델들이 생성한      \n",
       "지시문 데이터보다 새로운 작업에서 뛰어난 제로샷 성능을 보여주었습니다. 또한, 포괄적인 평가 및 보상 모델 훈련을 위해\n",
       "GPT-4로부터 피드백 및 비교 데이터를 수집하였습니다. 우리는 GPT-4를 사용하여 생성한 데이터와 코드베이스를 공개하고  \n",
       "있습니다.                                                                                                          \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "\n",
       "                                                   \u001b[1;4mINTRODUCTION\u001b[0m                                                    \n",
       "\n",
       "이 논문은 대형 언어 모델(LLM)의 뛰어난 일반화 능력을 설명하며, 이러한 모델들이 자연어 지시를 따르고 실제 세상에서의\n",
       "작업을 완료할 수 있도록 하는 방법을 탐구하고 있습니다. 그 방법 중 하나로 지시문 튜닝(instruction-tuning)이 있으며, \n",
       "이는 인간이 주석을 단 프롬프트와 피드백을 사용하여 모델을 다양한 작업에 맞게 미세 조정하거나, 수동 또는 자동으로   \n",
       "생성된 지시문을 추가한 공개 벤치마크와 데이터세트를 사용하는 방식으로 구현됩니다.                                  \n",
       "\n",
       "특히 Self-Instruct 튜닝은 최신의 지시문 튜닝된 교사 LLM이 생성한 데이터를 학습하여, LLM을 인간 의도에 맞추는       \n",
       "간단하고 효과적인 방법입니다. 이 연구 방향은 LLM의 제로샷 및 소수샷 일반화 능력을 향상시키는 효과적인 수단을 제공해\n",
       "왔습니다. 최근 ChatGPT와 GPT-4의 성공은 이러한 지시문 튜닝을 통해 오픈소스 LLM을 개선할 큰 기회를 제공합니다.      \n",
       "LLaMA는 이러한 오픈소스 LLM들 중 하나로, 상용 LLM인 GPT-3와 성능을 견줄 수 있습니다. LLaMA가 지시를 따를 수 있도록 \n",
       "하려면, 뛰어난 성능과 저비용의 Self-Instruct 튜닝이 빠르게 채택되고 있습니다.                                      \n",
       "\n",
       "본 논문에서 우리는 처음으로 GPT-4를 Self-Instruct 튜닝의 교사로 사용하는 방법을 제안합니다. 우리의 주요 기여는     \n",
       "다음과 같습니다:                                                                                                   \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1mGPT-4 데이터:\u001b[0m 우리는 영어와 중국어로 된 52K 개의 지시문 데이터셋과 세 개의 지시문 튜닝된 모델의 출력을 평가한   \n",
       "\u001b[1;33m   \u001b[0m피드백 데이터를 포함하여 GPT-4가 생성한 데이터를 공개합니다.                                                    \n",
       "\u001b[1;33m • \u001b[0m\u001b[1m모델 및 평가:\u001b[0m GPT-4가 생성한 데이터를 기반으로 지시문 튜닝된 LLaMA 모델과 보상 모델을 개발했습니다. 우리는 세   \n",
       "\u001b[1;33m   \u001b[0m가지 기준에 대한 인간 평가, GPT-4 피드백을 사용한 자동 평가, 그리고 비정형 지시문에 대한 ROUGE-L 점수를 사용하여\n",
       "\u001b[1;33m   \u001b[0m지시문 튜닝된 LLM의 품질을 평가합니다.                                                                          \n",
       "\n",
       "이 연구는 GPT-4로 생성된 데이터를 사용한 LLM 지시문 튜닝의 효과성을 확인하고, 일반 목적의 지시문을 따르는          \n",
       "에이전트를 구축하는 데 있어 실용적인 팁을 제안합니다.                                                              \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "\n",
       "                                                     \u001b[1;4m2 DATASET\u001b[0m                                                     \n",
       "\n",
       "이 논문의 '2 데이터셋' 섹션에서는 데이터 수집 과정과 GPT-4를 사용한 데이터 생성 방법에 대해 설명합니다.            \n",
       "\n",
       "데이터 수집: Alpaca 데이터셋(Taori et al., 2023)에서 수집된 52,000개의 고유한 지시문을 재사용합니다. 각 지시문은   \n",
       "모델이 수행해야 할 작업을 설명합니다. 지시문에는 작업에 대한 선택적 맥락이나 입력이 있을 수도 있고 없을 수도       \n",
       "있습니다. Alpaca 데이터셋에서는 GPT-3.5(text-davinci-003)를 사용해 출력을 생성했지만, 우리는 GPT-4(gpt-4)를        \n",
       "활용하여 출력을 생성합니다. GPT-4를 사용해 다음 네 가지 데이터셋을 생성합니다.                                     \n",
       "\n",
       "\u001b[1;33m 1 \u001b[0m\u001b[1m영어 지시문-답변 데이터:\u001b[0m Alpaca에서 수집한 52,000개의 지시문 각각에 대해 GPT-4가 영어로 답변을 제공합니다.      \n",
       "\u001b[1;33m   \u001b[0m자세한 내용은 알고리즘 1에 나와 있습니다. GPT-4와 Self-Instruct를 사용하여 자체 지시 세트를 구성하는 반복적     \n",
       "\u001b[1;33m   \u001b[0m과정을 향후 작업으로 남겨둡니다.                                                                                \n",
       "\u001b[1;33m 2 \u001b[0m\u001b[1m중국어 지시문-답변 데이터:\u001b[0m ChatGPT를 사용하여 52,000개의 지시문을 중국어로 번역하고, GPT-4가 중국어로 답변하게  \n",
       "\u001b[1;33m   \u001b[0m합니다. 이를 통해 LLaMA에 기반한 중국어 지시문-따르기 모델을 구축하고, 지시문 튜닝의 언어 간 일반화 능력을      \n",
       "\u001b[1;33m   \u001b[0m연구할 수 있습니다.                                                                                             \n",
       "\u001b[1;33m 3 \u001b[0m\u001b[1m비교 데이터:\u001b[0m GPT-4에게 자신의 응답을 1부터 10까지 평가하도록 요청합니다. 또한, GPT-4에게 GPT-4, GPT-3.5, 그리고 \n",
       "\u001b[1;33m   \u001b[0mOPT-IML을 포함한 세 모델의 응답을 비교하여 평가하도록 요청합니다. 이는 보상 모델을 훈련하는 데 사용됩니다.      \n",
       "\u001b[1;33m 4 \u001b[0m\u001b[1m비정상 지시문에 대한 답변:\u001b[0m 68,000개의 지시문-입력-출력 삼중 코어 데이터셋에 대해 GPT-4의 답변을 디코딩합니다. 이\n",
       "\u001b[1;33m   \u001b[0m하위 세트는 GPT-4와 우리 지시문 튜닝된 모델들 간의 차이를 계량화하는 데 사용됩니다.                             \n",
       "\n",
       "데이터 통계: 그림 1에서는 GPT-4와 GPT-3.5의 영어 출력 응답 세트를 비교합니다. 각 출력에 대해 동사 뿌리와 직접      \n",
       "목적어 명사를 추출하고, 각 출력 세트에서 고유한 동사-명사 쌍의 빈도를 계산합니다. 빈도가 10 이상인 동사-명사 쌍은  \n",
       "그림 1(a)와 (b)에 표시되며, 두 세트의 가장 빈번한 25개의 쌍을 그림 1(c)에 비교합니다. 출력 시퀀스 길이의 빈도      \n",
       "분포는 그림 1(d)에 비교되어 있습니다. GPT-4는 GPT-3.5보다 더 긴 시퀀스를 생성하는 경향이 있습니다. Alpaca의 GPT-3.5\n",
       "데이터는 반복적인 데이터 수집 과정을 통해 각 반복에서 유사한 지시문 인스턴스를 제거하여 더 긴 꼬리를 가진 출력     \n",
       "분포를 보입니다. 이는 현 시점의 일회성 데이터 생성 과정에서는 없는 특징입니다. 이러한 간단한 과정에도 불구하고,    \n",
       "GPT-4가 생성한 지시문-따르기 데이터는 이후 실험에서 더 나은 정렬 성능을 보여줍니다.                                \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "\n",
       "                                       \u001b[1;4m3 INSTRUCTION-TUNING LANGUAGE MODELS\u001b[0m                                        \n",
       "\n",
       "I apologize, but it seems that the '3 INSTRUCTION-TUNING LANGUAGE MODELS' section content is missing from your     \n",
       "request. If you can provide the full text of that section, I will be happy to explain it in detail.                \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "\n",
       "                                             \u001b[1;4m3.1 SELF-INSTRUCT TUNING\u001b[0m                                              \n",
       "\n",
       "이 섹션에서는 Self-Instruct 튜닝을 통해 언어 모델을 훈련하는 과정에 대해 설명합니다. 연구에서는 LLaMA 7B           \n",
       "체크포인트를 사용하여 두 가지 모델을 지도 학습 방식으로 미세 조정했습니다.                                         \n",
       "\n",
       "\u001b[1;33m 1 \u001b[0m\u001b[1mLLaMA-GPT4 모델\u001b[0m: 이 모델은 GPT-4가 생성한 52,000개의 영어 지시문-따르기 데이터를 바탕으로 훈련되었습니다. 이    \n",
       "\u001b[1;33m   \u001b[0m데이터의 분포는 이전의 그림 1에서 보여주었습니다.                                                               \n",
       "\u001b[1;33m 2 \u001b[0m\u001b[1mLLaMA-GPT4-CN 모델\u001b[0m: 이 모델은 GPT-4가 생성한 52,000개의 중국어 지시문-따르기 데이터를 바탕으로 훈련되었습니다.  \n",
       "\n",
       "훈련 과정에서는 공정한 비교를 위해 Taori et al.(2023)의 훈련 스케줄을 따랐습니다. 이러한 모델들은 GPT-4가 생성한   \n",
       "데이터의 품질을 연구하고, 하나의 언어로 지시문 튜닝을 했을 때의 교차 언어 일반화 특성을 연구하는 데 사용됩니다.    \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "\n",
       "                                                 \u001b[1;4m3.2 REWARD MODELS\u001b[0m                                                 \n",
       "\n",
       "이 섹션에서는 인간 피드백을 통한 강화 학습(RLHF, Reinforcement Learning from Human Feedback)의 핵심 요소인 보상    \n",
       "모델링에 대해 설명합니다. RLHF는 LLM의 행동을 인간의 선호와 맞춰 모델을 더욱 유용하게 만들려는 목적을 가지고       \n",
       "있습니다. 보상 모델링은 프롬프트와 응답이 주어졌을 때 스칼라 보상을 예측하는 회귀 과제로 문제를 구성합니다. 이렇게 \n",
       "하려면 대규모 비교 데이터가 필요한데, 두 가지 모델의 응답을 같은 프롬프트에 대해 비교하는 방식입니다. 하지만       \n",
       "Alpaca, Vicuna, Dolly 등의 기존 오픈소스 작업은 비교 데이터 레이블링 비용이 높아 RLHF를 포함하지 않습니다.         \n",
       "\n",
       "한편, 최근 연구는 GPT-4가 자신의 실수를 식별하고 수정하며, 응답 품질을 정확히 평가할 수 있음을 보여주었습니다.     \n",
       "따라서 RLHF 연구를 촉진하기 위해, 우리는 2절에서 설명한 바와 같이 GPT-4를 사용하여 비교 데이터를 생성했습니다.     \n",
       "\n",
       "데이터 품질 평가를 위해, 우리는 OPT 1.3B 기반으로 보상 모델을 훈련하여 서로 다른 응답들을 평가합니다. 각 비교      \n",
       "데이터 인스턴스는 한 프롬프트 $\\textbf{\\em x}$와 $K$개의 응답을 포함하고, GPT-4는 각 응답에 대해 1부터 10까지의    \n",
       "점수 $s$를 줍니다. 이 인스턴스에서 고유한 쌍 $C_{2}^{K}$를 구성할 수 있으며, 각 쌍은 $(y_{l}, y_{h})$로 표현됩니다.\n",
       "여기서 $y_{l}$의 점수는 $y_{h}$의 점수보다 낮습니다($s_{l} < s_{h}$). 보상 모델 $r_{\\theta}$는 다음 목표로         \n",
       "훈련됩니다: $\\operatorname*{min}\\log(\\sigma(r_{\\pmb\\theta}(x,y_{h})-r_{\\pmb\\theta}(\\pmb x,y_{l})))$, 여기서        \n",
       "$\\sigma$는 시그모이드 함수입니다. 비교 데이터의 분포는 그림 2에 나타나 있습니다.                                   \n",
       "\n",
       "이 접근 방식은 보상 모델이 다양한 응답 품질을 평가하고 RLHF의 효과성을 높이는 데 도움을 줄 수 있도록               \n",
       "설계되었습니다.                                                                                                    \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "\n",
       "                                              \u001b[1;4m4 EXPERIMENTAL RESULTS\u001b[0m                                               \n",
       "\n",
       "It looks like the content from the '4 EXPERIMENTAL RESULTS' section is missing from your request. If you can       \n",
       "provide the full text of that section, I would be happy to explain it in detail.                                   \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "\n",
       "                                                  \u001b[1;4m4.1 BENCHMARKS\u001b[0m                                                   \n",
       "\n",
       "이 섹션에서는 LLM(대형 언어 모델)의 성능을 평가하기 위한 벤치마크를 소개하고, 인간 평가 방법과 그 결과를           \n",
       "설명합니다.                                                                                                        \n",
       "\n",
       "                                                     \u001b[1m벤치마크:\u001b[0m                                                     \n",
       "\n",
       "이 연구의 목표는 GPT-4 데이터를 기반으로 학습된 Self-Instruct 모델들이 새로운 지시문에 대해 얼마나 잘 반응하는지   \n",
       "평가하는 것입니다. 연구에서 사용한 데이터셋은 다음과 같습니다:                                                     \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1mUser-Oriented-Instructions- $252^{,2}$\u001b[0m: 이 데이터셋은 71개의 사용자 중심 애플리케이션(예: Grammarly,            \n",
       "\u001b[1;33m   \u001b[0mStackOverflow, Overleaf)을 기반으로 한 252개의 지시문을 포함합니다.                                             \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mVicuna-Instructions- $80^{3}$\u001b[0m: GPT-4가 생성한 80개의 도전적인 질문이 포함되어 있으며, 기본 모델들이 어려워하는  \n",
       "\u001b[1;33m   \u001b[0m질문들입니다. 지식, 수학, Fermi 문제, 반사실적 시나리오, 롤플레잉, 일반, 코딩, 글쓰기, 상식 등 8개의 카테고리가 \n",
       "\u001b[1;33m   \u001b[0m있습니다.                                                                                                       \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mUnnatural Instructions\u001b[0m: 68,478개의 샘플로 구성된 이 데이터셋은 15개의 수작업 예제를 사용한 3-샷 맥락 학습으로   \n",
       "\u001b[1;33m   \u001b[0mtext-davinci-002가 합성한 것입니다.                                                                             \n",
       "\n",
       "                                                    \u001b[1m인간 평가:\u001b[0m                                                     \n",
       "\n",
       "모델의 정렬 품질을 평가하기 위해 Anthropic Askell et al.(2021)에서 제안한 정렬 기준을 따랐습니다. 이 기준은 AI     \n",
       "시스템이 인간의 가치에 얼마나 잘 정렬되어 있는지를 평가하는 데 사용됩니다. 기준은 다음과 같습니다:                 \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1m도움됨\u001b[0m: 인간이 목표를 달성하는 데 도움이 되는지 여부를 평가합니다.                                              \n",
       "\u001b[1;33m • \u001b[0m\u001b[1m정직성\u001b[0m: 정확한 정보를 제공하고 필요시 불확실성을 표현하여 인간 사용자를 오도하지 않는지를 평가합니다.           \n",
       "\u001b[1;33m • \u001b[0m\u001b[1m무해성\u001b[0m: 인간에게 해를 끼치지 않는지를 평가합니다.                                                               \n",
       "\n",
       "Amazon Mechanical Turk를 통해 인간 평가를 수행했으며, 결과는 그림 3의 파이 차트에 나타나 있습니다.                 \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1m첫 번째 비교\u001b[0m: GPT-4 데이터로 미세 조정된 LLaMA 모델과 GPT-3 데이터로 미세 조정된 LLaMA 모델(Stanford Alpaca     \n",
       "\u001b[1;33m   \u001b[0m모델)을 비교하여, \"도움됨\" 기준에서 GPT-4가 54.12%로 우세했으며, \"정직성\"과 \"무해성\"에서는 GPT-3(Alpaca)이 다소 \n",
       "\u001b[1;33m   \u001b[0m우세하나 비슷한 수준의 결과를 보였습니다.                                                                       \n",
       "\u001b[1;33m • \u001b[0m\u001b[1m두 번째 비교\u001b[0m: GPT-4 지시를 따라 튜닝된 LLaMA 모델과 원래의 GPT-4 모델을 비교했으며, 세 가지 기준 모두에서 유사한\n",
       "\u001b[1;33m   \u001b[0m성능을 보여줬습니다. 이는 GPT-4가 생성한 데이터로 학습함으로써 원래 GPT-4 수준의 성능을 달성할 수 있다는 것을   \n",
       "\u001b[1;33m   \u001b[0m의미합니다.                                                                                                     \n",
       "\n",
       "                                             \u001b[1m추가 그림 설명 (그림 4):\u001b[0m                                              \n",
       "\n",
       "\u001b[1;33m • \u001b[0mGPT-4로 평가된 성능 비교를 보여줍니다. 각 막대는 두 모델 간의 평가 결과를 나타내며, 총 점수(최대 800점)와 상대  \n",
       "\u001b[1;33m   \u001b[0m점수 비율(강력한 상대 모델 대비)이 보고됩니다.                                                                  \n",
       "\u001b[1;33m • \u001b[0mLLaMA GPT4 모델의 응답을 우리 보상 모델로 평가한 결과를 나타내며, 전반적으로 ChatGPT 및 GPT-4와 비교하여 성능을 \n",
       "\u001b[1;33m   \u001b[0m평가했습니다.                                                                                                   \n",
       "\n",
       "이 연구는 GPT-4 데이터를 사용한 학습이 새로운 지시문 작업에서 뛰어난 성능을 낸다는 가능성을 시사합니다.            \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "\n",
       "                               \u001b[1;4m4.3 COMPARISONS WITH SOTA USING AUTOMATIC EVALUATION\u001b[0m                                \n",
       "\n",
       "이 섹션에서는 자동 평가를 사용하여 LLaMA-GPT4 모델과 다른 최첨단(State-of-the-Art, SOTA) 모델들을 비교한 결과를    \n",
       "설명합니다.                                                                                                        \n",
       "\n",
       "                                                    \u001b[1m자동 평가:\u001b[0m                                                     \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1m평가 방법\u001b[0m: GPT-4를 활용해 다양한 모델들이 80개의 새로운 질문에 대한 생성 응답의 품질을 자동 평가했습니다.       \n",
       "\u001b[1;33m   \u001b[0mLLaMA-GPT4(7B)와 GPT-4의 응답을 수집하고, Vicuna(2023)에서 공개된 다른 채팅 봇들(LLaMA 13B, Alpaca 13B, Vicuna  \n",
       "\u001b[1;33m   \u001b[0m13B, Bard, ChatGPT)의 응답을 사용했습니다. 두 모델 간의 응답 품질을 1에서 10까지의 점수로 평가했습니다.         \n",
       "\u001b[1;33m • \u001b[0m\u001b[1m결과\u001b[0m: LLaMA-GPT4를 두 가지 디코딩 결과로 평가했습니다. 첫째는 질문당 하나의 응답만을 생성하는 기본 디코딩 결과, \n",
       "\u001b[1;33m   \u001b[0m둘째는 질문당 다섯 개의 응답을 생성한 후 보상 모델이 상위에서 하위로 순위 매긴 응답 그룹입니다. 이 그룹들은 기본\n",
       "\u001b[1;33m   \u001b[0m결과와 비교되었고, 평가 결과에서 피드백 데이터와 보상 모델의 유용성을 보여줍니다.                               \n",
       "\n",
       "                                                       \u001b[1m결과:\u001b[0m                                                       \n",
       "\n",
       "\u001b[1;33m • \u001b[0mLLaMA-GPT4는 text-davinci-003(즉, Alpaca)로 튜닝한 경우보다 높은 성능을 보여주었지만, 여전히 상업용 대형        \n",
       "\u001b[1;33m   \u001b[0m챗봇(e.g., GPT-4)과 비교해서는 차이가 존재합니다.                                                               \n",
       "\u001b[1;33m • \u001b[0m\u001b[1m중국어 평가\u001b[0m: 영어 응답을 중국어로 번역한 것이나, 중국어로 직접 질문하여 생성된 응답을 비교했을 때, 번역된 응답이\n",
       "\u001b[1;33m   \u001b[0m더 우수한 성능을 보여주었습니다. 이는 GPT-4가 영어 코퍼스에서 더 많이 훈련되어 영어 지시문을 따르는 능력이 더   \n",
       "\u001b[1;33m   \u001b[0m강력하기 때문일 수 있습니다.                                                                                    \n",
       "\n",
       "                             \u001b[1m비정상 지시문(unnatural instructions)에서의 결과(그림 6):\u001b[0m                             \n",
       "\n",
       "\u001b[1;33m • \u001b[0mLLaMA-GPT4와 GPT-4, 그리고 Alpaca 모델을 비교했을 때, Alpaca가 다른 두 모델에 비해 평균 ROUGE-L 점수에서        \n",
       "\u001b[1;33m   \u001b[0m우수하였습니다. 그러나 응답의 길이가 길어질수록 LLaMA-GPT4와 GPT-4가 더 높은 성능을 보여주기 시작했습니다. 이는 \n",
       "\u001b[1;33m   \u001b[0m더 창의적인 시나리오에서 이 모델들이 지시문을 더 잘 따를 수 있다는 것을 의미합니다.                             \n",
       "\u001b[1;33m • \u001b[0m각기 다른 응답 길이 하위 집합에 대해, LLaMA-GPT4는 GPT-4의 행동을 가깝게 따릅니다. 응답 길이가 짧을 때,         \n",
       "\u001b[1;33m   \u001b[0mLLaMA-GPT4와 GPT-4는 단순한 정답을 포함하되, 응답을 더 대화형으로 만드는 추가적인 단어를 포함하기 때문에, 낮은  \n",
       "\u001b[1;33m   \u001b[0mROUGE-L 점수를 받을 가능성이 있습니다.                                                                          \n",
       "\n",
       "이 실험 결과는 LLaMA-GPT4가 GPT-4의 출력을 기반으로 학습했을 때, 상당한 성능 향상을 이룰 수 있음을 보여주며, 이는  \n",
       "혁신적인 지시문-따르기 LLM 개발에 있어서 유망한 방향임을 시사합니다.                                               \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "\n",
       "                                                  \u001b[1;4m5 RELATED WORK\u001b[0m                                                   \n",
       "\n",
       "이 섹션에서는 지시문 튜닝과 오픈소스 노력이 관련된 연구에 대해 설명합니다.                                         \n",
       "\n",
       "                                         \u001b[1m지시문 튜닝(Instruction Tuning):\u001b[0m                                          \n",
       "\n",
       "지시문 튜닝은 NLP 분야에서 점점 주목받고 있는 연구 방향입니다. 기존 연구들은 크게 세 가지 요소인 지시문-따르기     \n",
       "데이터, 기본 언어 모델, 평가 벤치마크의 품질과 규모를 향상시키는 것을 목표로 합니다. 각 연구 그룹은 일반적으로     \n",
       "자체적인 개발 파이프라인을 유지합니다. 예를 들어, 플랜(FLAN)을 기반으로 지시문 미세 조정 언어 모델의 규모를        \n",
       "확대하는 연구(Chung et al., 2022)나, 점점 많은 프롬프트를 포함하는 PromptSource(공공 프롬프트 풀, P3)의 개발(Bach  \n",
       "et al., 2022) 등이 있습니다. T0는 P3에 기반한 다중 과제 프롬프트 훈련을 통해 개발된 일련의 모델(Sanh et al.,       \n",
       "2021)입니다. OPT 모델의 지시문 튜닝은 더 크고 포괄적인 벤치마크인 OPT-IML Bench를 사용하여 진행되었으며, 이는 FLAN,\n",
       "Super-NaturalInstructions(Wang et al., 2022b), UnifiedSKG(Xie et al., 2022)을 포함합니다.                          \n",
       "\n",
       "                                        \u001b[1m오픈소스 노력(Open-Source Efforts):\u001b[0m                                        \n",
       "\n",
       "ChatGPT의 폭넓은 능력 덕분에, 오픈소스 모델은 큰 관심을 받으며 인간의 가치에 맞는 일반 목적의 텍스트 기반          \n",
       "어시스턴트를 개발하는 데 기여하고 있습니다. 초기의 기초 LLM 노력에는 BLOOM(Scao et al., 2022), GPT-J(Wang &        \n",
       "Komatsuzaki, 2021), GPT-NEO(Black et al., 2021), OPT(Zhang et al., 2022), LLaMA(Zhang et al., 2023)가 포함됩니다.  \n",
       "LLM을 대화 기반 어시스턴스에 맞추기 위해 Open-Assistant는 GPT-J를 기반으로 하고, Alpaca/Vicuna는 LLaMA를 기반으로  \n",
       "구축되었습니다. 또한, OpenFlamingo(Awadalla et al., 2023)와 LLaMA-Adapter(Zhang et al., 2023)는 LLaMA를 이미지     \n",
       "입력과 연결하여 오픈소스 멀티모달 LLM을 구축할 길을 마련했습니다.                                                  \n",
       "\n",
       "이러한 연구와 노력은 LLM이 더 다양하고 강력한 능력을 발휘할 수 있도록 하고, 오픈소스 환경에서의 발전을 촉진하는 데 \n",
       "그 목적이 있습니다.                                                                                                \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich.console import Console\n",
    "from rich.markdown import Markdown\n",
    "from rich.panel import Panel\n",
    "from rich.syntax import Syntax\n",
    "from rich.table import Table\n",
    "from typing import Dict\n",
    "import os\n",
    "\n",
    "class MarkdownPrinter:\n",
    "    def __init__(self):\n",
    "        self.console = Console()\n",
    "        \n",
    "    def print_markdown_file(self, file_path: str):\n",
    "        \"\"\"마크다운 파일을 이쁘게 출력\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                markdown_content = f.read()\n",
    "            \n",
    "            # 마크다운 렌더링\n",
    "            md = Markdown(markdown_content)\n",
    "            \n",
    "            # 마크다운 내용 출력\n",
    "            self.console.print(md)\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.console.print(f\"[bold red]Error reading file: {str(e)}[/]\")\n",
    "            \n",
    "    def print_sections(self, sections: Dict[str, str]):\n",
    "        \"\"\"섹션별로 구분하여 출력\"\"\"\n",
    "        for section, content in sections.items():\n",
    "            # 섹션 제목\n",
    "            self.console.print(\"\\n\")\n",
    "            self.console.print(Panel(\n",
    "                f\"[bold cyan]{section}[/]\",\n",
    "                border_style=\"cyan\"\n",
    "            ))\n",
    "            \n",
    "            # 섹션 내용\n",
    "            md = Markdown(content)\n",
    "            self.console.print(md)\n",
    "            \n",
    "            # 구분선\n",
    "            self.console.print(\"[dim]\" + \"=\"*80 + \"[/]\")\n",
    "\n",
    "printer = MarkdownPrinter()\n",
    "\n",
    "# 마크다운 파일 출력\n",
    "printer.print_markdown_file(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "네, 맞습니다. ReST에서 새로운 데이터를 생성하는 과정은 원본 데이터셋을 샘플링하여 입력 시퀀스를 선택하고, 이를 기반으로 조건화된 새로운 출력 시퀀스를 생성하는 방식으로 이루어집니다. 여기서 \"조건화\"라는 것은 주어진 입력 시퀀스 \\(\\pmb{x}\\)에 대해 그 조건 하에서 가능한 출력 시퀀스 \\(\\pmb{y}\\)를 생성한다는 의미입니다.\n",
      "\n",
      "조건화는 확률적 모델링에서 매우 중요한 개념인데, 이는 주어진 입력이 어떤 특정한 상태나 값일 때의 출력 확률을 뜻합니다. 언어 모델링에서는 입력 시퀀스(또는 문맥)가 주어졌을 때 다음 가능한 출력 시퀀스를 생성하는 과정을 말합니다. 이 과정은 자동 회귀 모델을 사용하여 구현되며, 모델이 이미 학습한 확률 분포에 기반해 \\(\\pi_{\\theta}(\\pmb{y}|\\pmb{x})=\\prod_{t=1}^{T}\\pi_{\\theta}(y_{t}|\\pmb{y}_{1:t-1}, \\pmb{x})\\) 형태로 다음 토큰 \\(y_t\\)를 계산합니다.\n",
      "\n",
      "ReST에서는 이를 통해 현재 정책 \\(\\pi_{\\theta}\\)로 입력 시퀀스에 맞는 새로운 출력 시퀀스를 생성하고, 그 결과를 데이터셋에 추가하여 데이터를 확장시킵니다. 이러한 방식은 모델이 이미 알고 있는 문맥에 기반해 추가적인 학습 데이터를 생성할 수 있도록 하며, 이는 Grow 단계에서 원본 데이터셋의 샘플을 이용해 새로운 시퀀스를 생성하는 과정에서 핵심적인 역할을 합니다. 이는 Section 3의 내용을 바탕으로 설명한 것입니다.\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\" \n",
    "원본 데이터 셋을 샘플링해서 입력 시퀀스에 조건화를 한 후 새로운 데이터를 생성하는 것 같은데 이게 맞아? 그리고 조건화가 뭐임?\n",
    "\"\"\"\n",
    "\n",
    "qa = PaperQA(qa.conversation_history)\n",
    "response = qa.ask_question(question)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
