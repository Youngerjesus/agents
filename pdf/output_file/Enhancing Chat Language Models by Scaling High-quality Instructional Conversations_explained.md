
## Abstract

최근 ChatGPT와 같은 대화형 언어 모델을 구현하기 위해 지시 데이터에 대한 파인튜닝이 효과적인 방법으로 널리 검증되었습니다. 이러한 데이터의 다양성과 품질을 확대하는 것은 간단해 보이지만, 실제로 성능 향상에 큰 잠재력을 가지고 있습니다. 이 논문은 오픈 소스 모델의 성능을 더욱 향상시키는 것을 목표로 합니다.

우리는 먼저 UltraChat이라는 체계적으로 설계된 다양하고 정보가 풍부한 대규모 지시 대화 데이터셋을 제공합니다. 이 데이터셋은 인간의 질의를 포함하지 않고도 생성되었습니다. 우리의 목표는 사람이 AI 어시스턴트와 가질 수 있는 다양한 상호작용을 포괄하는 것이며, 이를 위해 반복적인 다중 턴 대화를 생성하는 종합적인 프레임워크를 활용합니다.

UltraChat은 150만 개의 고품질 다중 턴 대화를 포함하고 있으며, 광범위한 주제와 지시를 다룹니다. UltraChat에 대한 통계적 분석 결과, 규모, 평균 길이, 다양성, 일관성 등 다양한 핵심 지표에서 우수성이 입증되어 선도적인 오픈 소스 데이터셋으로서의 위치를 확고히 했습니다.

UltraChat을 기반으로 우리는 LLaMA 모델을 파인튜닝하여 강력한 대화형 모델인 UltraLLaMA를 개발했습니다. 평가 결과, UltraLLaMA는 이전에 최고 수준의 오픈 소스 모델로 인정받았던 Vicuna를 포함하여 다른 오픈 소스 모델들을 지속적으로 능가하는 성능을 보여주었습니다.

이 데이터셋과 모델은 공개적으로 출시될 예정입니다.

---

## 1 Introduction

최근 대형 언어 모델(LLM)은 다양한 언어 관련 과제에서 뛰어난 일반화 능력을 보여주고 있습니다(Han et al., 2021; Bommasani et al., 2021). 특히 ChatGPT(OpenAI, 2022)는 GPT-3(Brown et al., 2020)와 GPT-4(OpenAI, 2023)를 대화에 최적화한 버전으로서, 자연스럽고 상호작용적인 방식으로 응답을 이해하고 생성하는 데 뛰어나 사용자 경험을 한 단계 높였습니다.

ChatGPT의 등장은 일반적인 대화형 모델의 채택과 구현에 큰 변화를 가져왔습니다. Bard²와 Claude³와 같은 대형 기업이 개발한 경쟁 모델뿐만 아니라, 오픈 소스 커뮤니티도 AI 기술에 대한 접근을 민주화하기 위해 적극적으로 유사한 모델을 훈련하고 있습니다. 이와 관련된 주목할 만한 예로는 Alpaca(Taori et al., 2023a), Vicuna(Chiang et al., 2023), Koala(Geng et al., 2023), Baize(Xu et al., 2023), Belle(Ji et al., 2023) 등이 있으며, 이들은 유망한 성능을 보여주고 있습니다.

실험적 증거는 지시 파인튜닝(Wei et al., 2021; Sanh et al., 2021)을 통해 대화형 언어 모델을 효과적으로 훈련할 수 있음을 강하게 시사합니다. 또한 많은 데이터 효율적인(Zhou et al., 2023) 또는 컴퓨팅 효율적인(Hu et al., 2021; Ding et al., 2023) 방법이 적용될 수 있음을 나타냅니다. 그러나 이 논문은 다른 방식으로, 대화형 언어 모델의 "마지막 1마일"에 더 초점을 맞춥니다. 증거에 따르면 0에서 60까지 가는 여정은 쉽지만, 60에서 100으로 나아가는 것은 극도로 어려워집니다. 예를 들어, 연구자들은 작고 신중하게 큐레이션된 지시 세트를 사용하여 지시를 잘 따르는 모델을 훈련할 수 있음을 보여주었습니다. 그러나 이러한 접근법은 아직 현재 최고 수준의 오픈 소스 모델인 Vicuna의 성능을 뛰어넘는 모델을 만들어내지 못했습니다. ChatGPT나 GPT-4를 능가하는 것은 더 어려운 과제입니다.

이 논문에서는 훈련 과정에서 사용되는 데이터의 품질과 다양성이라는 가장 단순한 방법이 대화형 언어 모델의 성능을 더욱 향상시키는 데 중요한 역할을 한다고 믿습니다. 다시 말해, 더 높은 품질과 다양한 데이터를 활용하면 더 나은 결과를 얻을 수 있습니다. 이를 위해 우리는 UltraChat이라는 수백만 규모의 다중 턴 지시 대화 데이터를 제시하여 더 강력한 대화형 언어 모델의 구축을 촉진하고자 합니다.

UltraChat은 사람이 AI 어시스턴트와 가질 수 있는 다양한 상호작용의 폭을 포착하려는 원칙에 따라 설계되었습니다. 구체적으로, 우리는 질의응답이나 요약과 같은 특정 작업을 사용하여 데이터를 구성하지 않고, "세계에 대한 질문", "창작과 생성", "기존 자료에 대한 도움"의 세 가지 분야를 큐레이션합니다. 그런 다음 메타정보, 인컨텍스트 확장, 반복적 프롬프트를 활용하여 지시의 수를 확장합니다.

정보성이 높고 현실적인 다중 턴 대화를 생성하기 위해, 두 개의 별도 ChatGPT Turbo API를 사용하여 하나는 사용자 역할로 질의를 생성하고, 다른 하나는 응답을 생성합니다. 우리는 신중하게 설계된 프롬프트로 사용자 모델에게 인간 사용자의 행동을 모방하도록 지시하고, 이 두 개의 API를 반복적으로 호출합니다.

우리는 UltraChat을 기반으로 LLaMA-13B 모델을 파인튜닝하여 UltraLLaMA라는 모델을 만들었고, 이를 다양한 오픈 소스 모델들과 비교했습니다. 평가 결과, 우리의 모델은 다른 모델들을 지속적으로 능가하는 성능을 보였습니다.

**표 1: 다양한 오픈 소스 모델과 우리의 UltraChat으로 훈련한 모델의 평균 점수 (1-10). 점수는 GPT-4가 생성한 300개 이상의 질문으로 구성된 데이터셋을 사용하여 ChatGPT가 독립적으로 평가했습니다. 평가에 포함된 모델 중 Dolly-v2와 OpenAssistant는 12B 파라미터, MPT-Chat과 Alpaca는 7B 파라미터, 나머지 모델은 13B 파라미터를 가집니다. 평가 프롬프트는 부록 A에서 확인할 수 있습니다.**

| 모델                         | 점수            |
|------------------------------|-----------------|
| Dolly-v2 2 (Conover et al., 2023)  | 7.145 ± 2.773 |
| VMI MPT-Chat (Mosaic, 2023)  | 8.317 ± 2.117 |
| OpenAssistant (Kopf et al., 2023)  | 8.470 ± 1.505 |
| Baize (Xu et al., 2023)      | 8.566 ± 0.986 |
| Alpaca (Taori et al., 2023a) | 8.597 ± 1.292 |
| Koala (Geng et al., 2023)    | 8.881 ± 1.062 |
| Vicuna (Chiang et al., 2023) | 8.961 ± 0.718 |
| **UltraLLaMA (우리의 모델)**    | **9.023 ± 0.952** |

표 1에서 볼 수 있듯이, UltraLLaMA는 ChatGPT가 독립적으로 평가한 점수에서 최고 성능을 달성했습니다. 우리는 또한 ChatGPT에게 전체적인 성능이 더 높은 응답을 선택하도록 하는 선호도 연구를 수행했으며, 그 결과 우리 모델이 여전히 모든 오픈 소스 기준 모델들을 지속적으로 능가함을 확인했습니다.

---

## 2 Related Work

**2 관련 연구**

### 지시 튜닝(Instruction Tuning)

최근의 연구들은 대형 언어 모델(LLM)이 인간의 지시를 따르는 강력한 능력을 보여주고 있습니다. Wei et al. (2021)은 자연어 지시 템플릿으로 표현된 60개의 NLP 데이터셋을 T5(Raffel et al., 2020)에 파인튜닝하는 지시 튜닝을 선구적으로 제안하였습니다. 이렇게 파인튜닝된 모델은 지시를 이해하는 데 뛰어난 능력을 보였으며, 보지 못한 새로운 지시(작업)에도 잘 일반화되었습니다.

그 후, Longpre et al. (2023)은 이 설정을 1,836개의 작업으로 확장하여, 작업 수를 늘리는 것이 분포 밖 일반화(out-of-distribution generalization)에 이로운 점을 보여주었습니다. Wei et al. (2021) 또한 지시 튜닝의 성공이 데이터셋의 품질과 프롬프트의 설계에 달려 있음을 결론지었습니다.

튜닝된 모델의 동작을 더욱 제어하기 위해, Ouyang et al. (2022)와 Schulman et al. (2017)은 인간의 피드백이 주석된 데이터를 통해 직접 보상 모델을 학습한 다음, 강화 학습을 사용하여 모델의 행동을 인간의 선호에 맞추는 방법을 제안했습니다. 이 기술은 지시 튜닝과 결합되어 모델의 성능을 더욱 향상시킬 수 있으며, ChatGPT와 같은 LLM에 성공적으로 적용되었습니다.

### LLM을 활용한 데이터 증강

대규모의 인간이 주석한 지시와 그 응답을 수집하는 것은 시간과 노력이 많이 드는 작업입니다. 대신에, 비용 효율적이고 실용적인 접근법은 이미 파인튜닝된 LLM(예: ChatGPT, GPT-3.5)에서 샘플링하여 최고 품질의 데이터를 수집하는 것입니다. 최근에는 이러한 강력한 LLM을 데이터 증강을 위해 활용하는 데에 많은 관심이 모아지고 있습니다.

예를 들어, SelfInstruct(Wang et al., 2022) 기법을 사용하여 Alpaca(Taori et al., 2023b)는 Text-Davinci-003을 "디스틸링"하여 175개의 초기 작업(seed tasks)을 기반으로 52,000개의 고품질 지시-응답 쌍을 생성했습니다. 이 데이터셋으로 LLaMA(Touvron et al., 2023) 모델을 훈련한 후, 그 모델은 거의 Text-Davinci-003과 동등한 성능을 보였습니다.

Alpaca의 성공은 LLM을 활용한 데이터 증강에 대한 수많은 후속 연구를 촉진시켰습니다. 예로는 code-alpaca(Chaudhary, 2023), alpaca-cot(Si et al., 2023), GPT4ALL(Anand et al., 2023), ShareGPT(Domeccleston, 2023), Dolly-v2(Conover et al., 2023), BELLE(Ji et al., 2023), Vicuna(Chiang et al., 2023), Koala(Geng et al., 2023), Baize(Xu et al., 2023) 등이 있습니다. 연구 결과에 따르면 데이터 규모를 증가시키는 것이 모델 성능을 지속적으로 향상시킬 수 있다는 것이 밝혀졌습니다.

데이터 크기를 늘리는 것 외에도, 이러한 연구들은 더 나은 품질의 데이터를 수집하기 위한 프롬프트 엔지니어링 방식에서도 차이를 보입니다. 예를 들어, CAMEL(Li et al., 2023)은 LLM이 주어진 복잡한 작업을 해결하기 위한 다중 에이전트 역할극 환경을 설계하여, 실제 인간 대화를 시뮬레이션하는 115,000개의 지시-응답 쌍을 생성했습니다.

---

![UltraChat의 구성 과정](https://cdn-mineru.openxlab.org.cn/extract/f30ce1f4-043b-41ed-a2f2-b357b9bf524f/a0fb31efaf0fbdcf3dfbbfb87955c7df6081372c5204acbcd90b5a05c4ed0311.jpg)

**그림 1: UltraChat의 구성 과정. 세 가지 분야의 데이터는 서로 다른 메타 정보를 기반으로 도출됩니다.**

---

## 3 Design

**3 디자인(설계)**

대형 언어 모델(LLM)은 많은 시나리오에서 인간보다 더 뛰어난 주석자(annotator)로 인식되고 있습니다(Gilardi et al., 2023). 그러나 ChatGPT와 같은 LLM을 직접 사용하여 다중 턴 대화를 생성하면 만족스러운 결과를 얻을 수는 있지만, 정보성이 부족할 수 있습니다. 이는 이러한 모델이 정렬(alignment) 과정에서 인간 피드백을 활용한 강화 학습(RLHF)의 이점을 누릴 수 없기 때문입니다.

부록 B의 표 20은 동일한 시작 문장을 사용하여 직접 생성한 다중 턴 대화와 UltraChat의 사례를 비교하고 있습니다. 데이터의 품질을 보장하기 위해 두 가지 핵심 포인트를 도출할 수 있습니다:

1. **시작 문장(opening line)은 대화의 주제를 직접 결정합니다.** 시작 문장은 매우 다양해야 하며, 인간 사용자가 채팅 모델에게 요청할 수 있는 모든 작업을 포괄해야 합니다.

2. **사용자는 대화의 흐름(plot)을 결정하며, 출력은 현재 주제에 맞게 다양한 언어 스타일과 요청으로 맞춤화되어야 합니다.**

UltraChat은 방대한 범위의 지시문과 질의를 포함하도록 설계되었으며, 이는 세 가지 주요 분야로 구성됩니다:

- **세계에 대한 질문**: 일반 지식부터 전문 분야에 이르는 다양한 질문을 포함합니다.
- **창작 및 생성**: 스토리, 시, 에세이 등 창의적 콘텐츠의 생성에 대한 지시를 포함합니다.
- **기존 자료에 대한 지원**: 기존의 문서나 자료에 대한 요약, 해석, 분석 등의 도움을 제공합니다.

이러한 설계를 통해 UltraChat은 사용자가 AI 어시스턴트와 가질 수 있는 다양한 상호작용을 포괄하고, 더 풍부하고 유용한 대화 데이터셋을 제공합니다.

---

## 3.1 Principle

**3.1 원칙(Principle)**

다른 데이터셋들은 주로 질의응답, 재작성, 요약과 같은 특정 작업을 사용하여 데이터를 구성하는 경향이 있습니다. 그러나 우리의 스키마 설계는 인간이 AI 어시스턴트와 가질 수 있는 다양한 상호작용의 폭을 포착하려는 삼분법적 프레임워크에 기반을 두고 있습니다. 우리는 인간 사용자와 AI 어시스턴트 사이의 모든 상호작용이 정보를 얻는 것으로 간주될 수 있다고 믿습니다.

### 정보 접근(Information Access)

첫 번째 영역인 **"세계에 대한 질문"**은 세계에 존재하는 기존 정보에 대한 질의에 초점을 맞춥니다. 이는 인간-AI 상호작용의 핵심적인 측면으로, 사용자는 종종 AI 어시스턴트에게 자신의 질문에 대한 빠르고 정확한 답변을 기대합니다. 광범위한 주제를 포함함으로써, 데이터셋은 사용자의 다양한 정보 요구를 다루어 AI 어시스턴트가 관련 있고 포괄적인 응답을 제공할 수 있도록 합니다. 이 구성 요소는 사용자와 AI 어시스턴트 간의 효과적인 정보 교환을 촉진하며, 이는 모든 인간-AI 상호작용의 근간이 됩니다.

### 조건부 정보 생성(Conditional Information Creation)

두 번째 부분인 **"창작 및 글쓰기"**는 인간의 입력 조건을 기반으로 새로운 정보를 생성하는 것과 관련이 있습니다. 이 과정은 AI가 광범위한 지식과 패턴 인식 능력을 활용하여 사용자와 함께 창의적인 작업에 참여하는 능력을 반영합니다. 데이터셋의 이 부분은 창작 과정에서 AI 어시스턴트가 협력자로서의 역할을 인정하며, AI가 달성할 수 있는 한계를 넓히고 이메일 작성부터 이야기나 희곡 창작에 이르는 다양한 작업에서 사용자가 그 잠재력을 활용할 수 있도록 합니다.

### 정보 변환(Information Transformation)

세 번째 부분인 **"기존 자료에 대한 도움"**은 기존 정보의 수정에 초점을 맞춥니다. 이는 인간-AI 상호작용의 중요한 측면으로, AI 어시스턴트가 사용자의 입력을 능동적으로 처리하여 재작성, 이어쓰기, 요약 또는 추론 등 다양한 방식으로 변환할 수 있게 해줍니다. 데이터셋의 이 부분은 AI 어시스턴트가 정보를 조작하여 사용자의 요구에 더 잘 부응할 수 있도록 보장하며, 다양한 작업을 처리할 수 있는 다재다능하고 적응력 있는 도구로서의 기능을 수행합니다.

요약하면, 이 삼분법적 원칙은 인간과 AI 어시스턴트 사이의 가능한 상호작용을 포괄적으로 나타내기 위해 설계되었습니다. 이러한 설계를 기반으로 우리는 UltraChat을 만들어 인간 사용자와 AI 시스템 간의 의미 있고 효율적인 협업을 포착하고자 합니다.

---

## 4 Data Construction

**4 데이터 구축**

앞서 언급한 바와 같이, **UltraChat**은 세 가지 주요 분야로 구성되어 있으며, 각 분야는 고유한 도전을 가지고 있습니다. 우리의 주요 원칙은 데이터를 최대한 다양하게 만드는 것입니다. 데이터 다양성을 보장하기 위한 핵심 요소는 **시작 문장**의 다양성과 **사용자 응답 스타일**의 다양성을 확보하는 것입니다.

이 섹션에서는 **다양한 시작 문장**을 얻는 방법과 **사용자를 적절하게 프롬프트**하는 방법에 대한 구성과 설계에 초점을 맞춥니다. 각 분야별로 어떻게 이러한 다양성을 달성했는지에 대한 상세한 내용은 아래에서 설명할 것입니다.

UltraChat의 각 분야는 다음과 같은 특성과 도전을 가지고 있습니다:

1. **세계에 대한 질문**: 이 분야에서는 사용자가 AI 어시스턴트에게 세계에 대한 다양한 질문을 던지는 시나리오를 다룹니다. 도전은 광범위한 주제와 질문 형태를 포함하여 시작 문장의 다양성을 확보하는 것입니다.

2. **창작 및 생성**: 사용자가 창의적인 작업을 요청하는 이 분야에서는 스토리 작성, 시, 음악 가사 등 다양한 창작물의 생성을 포함합니다. 여기서는 사용자의 요구와 스타일이 매우 다양하기 때문에, 이를 반영한 시작 문장과 응답 스타일을 구축하는 것이 중요합니다.

3. **기존 자료에 대한 지원**: 이 분야는 사용자가 기존의 문서나 정보를 수정하거나 요약, 번역 등을 요청하는 시나리오를 포함합니다. 여기서는 다양한 형태의 자료와 사용자 요청을 처리할 수 있도록 데이터의 다양성을 확보해야 합니다.

이러한 분야별로 데이터 다양성을 확보하기 위해, 우리는 각 분야에 적합한 시작 문장 세트를 다양하게 생성하고, 사용자의 응답 스타일을 풍부하게 반영하도록 설계했습니다. 이를 통해 UltraChat은 사용자가 AI 어시스턴트와 가질 수 있는 다양하고 현실적인 대화를 포괄하는 것을 목표로 합니다.

---

## 4.1 Questions about the World

**4.1 세계에 대한 질문**

이 데이터 분야는 현실 세계에 존재하는 개념, 사물, 엔티티에 주로 초점을 맞추고 있습니다. 이 분야의 데이터를 수집하기 위해 우리는 두 가지 관점을 취했습니다: 하나는 주제와 개념에 중점을 두었고, 다른 하나는 실제 세계의 엔티티에 중점을 두었습니다.

먼저, 우리는 ChatGPT에게 일상 생활의 다양한 측면을 포괄하는 30개의 포괄적인 주제를 생성하도록 요청했습니다(표 3 참조). 그런 다음, 각 주제를 더 깊이 탐색하기 위해 30~50개의 하위 주제나 관련 개념을 생성했습니다. 마지막으로, 각 하위 주제나 개념에 대해 10개의 다른 질문을 생성하고, 원래 질문마다 추가로 10개의 질문을 더 생성하도록 ChatGPT에 요청했습니다.

또 다른 데이터 소스는 실제 세계의 사물로, 이는 위키데이터(Wikidata) 엔티티에서 가져왔습니다. 이러한 엔티티들은 위키백과(Wikipedia) 기사에서의 빈도수를 고려하여 더욱 정제되었으며, 특히 가장 자주 등장하는 10,000개의 엔티티에 집중했습니다. 각 엔티티에 대해 5개의 메타 질문을 만들고, 이어서 10개의 더 구체적인 질문과 20개의 확장된 질문을 생성했습니다. 이 확장된 질문들은 원래 질문과 어느 정도 유사성을 유지하면서도 다른 사물이나 주제를 탐색하는 것을 목표로 합니다.

대화를 구성하기 위해 이렇게 생성된 약 50만 개의 질문을 시작 문장으로 필터링하고 샘플링했습니다. 각 대화를 만들 때, 우리는 사용자 모델에게 신중하게 설계된 프롬프트를 제공하여 진행 중인 대화의 맥락을 고려하면서 간결하고 의미 있는 응답을 하도록 명시적으로 요청했습니다.

---

## 4.2 Creation and Generation

**4.2 창작과 생성**

이 분야는 사용자의 지시에 따라 다양한 텍스트 자료를 자동으로 생성하는 것에 초점을 맞추고 있습니다. 이러한 텍스트 자료는 **20가지 서로 다른 유형**으로 분류되며, 각 글쓰기 유형에 대해 다양한 지시문을 생성하기 위해 **ChatGPT 모델**을 활용했습니다.

먼저, 각 유형의 글쓰기에 대해 ChatGPT를 사용하여 다양한 지시문을 생성했습니다. 그런 다음 생성된 지시문의 약 **80%**를 다시 ChatGPT 모델에 입력하여 더 상세하고 구체적인 지시문을 생성했습니다. 이러한 지시문들은 **대화 생성의 시작 문장**으로 사용됩니다.

생성 과정 전반에 걸쳐, **사용자 프롬프트**는 대화의 주요 목적이 **글의 생성 및 개선**임을 지속적으로 강화합니다. 이는 사용자 모델의 행동이 집중되고 의도한 목적에 부합하도록 보장하기 위한 것입니다.

이를 통해 생성된 대화는 사용자와 AI 어시스턴트 간의 창의적인 상호작용을 잘 반영하며, 글쓰기와 창작에 대한 다양한 요청과 그에 대한 응답을 포함하게 됩니다. 이러한 접근 방식은 데이터셋의 다양성과 현실성을 높여, 대화형 언어 모델이 사용자와의 창의적이고 협력적인 상호작용을 더욱 잘 학습할 수 있도록 돕습니다.

---

## 4.3 Assistance on Existing Materials

**4.3 기존 자료에 대한 지원**

세 번째 분야는 **기존의 텍스트 자료**에 대한 다양한 작업을 포함합니다. 여기에는 **재작성, 번역, 요약, 질의응답** 등이 포함됩니다.

우리는 먼저 **C4 코퍼스⁶**에서 텍스트 조각을 수집했습니다. C4 코퍼스의 각 텍스트 조각은 원본 URL과 연결되어 있습니다. 텍스트의 내용과 스타일의 다양성을 보장하기 위해, 우리는 **표 3**에 나열된 **20가지 유형의 텍스트 자료**를 채택했습니다.

**표 3: UltraChat의 2번과 3번 분야 데이터를 생성하기 위해 사용된 20가지 텍스트 자료 유형**

| 텍스트 자료 유형                   | 텍스트 자료 유형                       | 텍스트 자료 유형                      |
|---------------------------------|-----------------------------------|---------------------------------|
| 기사 및 블로그 게시물             | 법률 문서 및 계약서                  | 시나리오                          |
| 마케팅 자료                      | 이메일                            | 요리법과 조리 지침                  |
| 제품 설명과 리뷰                  | 구직 지원 자료                      | 시                             |
| 언어 학습을 위한 스크립트           | 소셜 미디어 게시물                    | 과학 논문과 요약                    |
| 뉴스 기사                        | 프로그램과 코드                     | 이야기                           |
| 교육 자료                        | 기술 문서 및 보고서                  | 개인 에세이                       |
| 연설문 및 프레젠테이션              | 노래 가사                          |                                 |

이러한 텍스트 자료 유형을 기반으로, 우리는 C4 코퍼스에서 해당 유형의 텍스트를 수집했습니다. 각 텍스트 조각에 대해, 사용자가 기존 자료에 대한 도움을 요청하는 대화를 생성했습니다.

예를 들어, 사용자가 다음과 같은 요청을 할 수 있습니다:

- **재작성**: "이 문단을 더 간결하게 재작성해 주세요."
- **번역**: "이 텍스트를 스페인어로 번역해 주세요."
- **요약**: "이 기사에서 주요 요점을 요약해 주세요."
- **질의응답**: "이 텍스트에서 저자가 주장하는 바는 무엇인가요?"

대화를 생성할 때, 우리는 사용자 모델에게 신중하게 설계된 프롬프트를 제공하여, 대화의 맥락을 고려하면서 다양하고 현실적인 방식으로 응답하도록 유도했습니다.

이를 통해 생성된 대화는 기존 자료에 대한 다양한 작업 시나리오를 포괄하며, 대화형 언어 모델이 이러한 작업을 수행하는 데 필요한 능력을 학습할 수 있도록 돕습니다.

---

**참고:**

- **C4 코퍼스⁶**: "Colossal Clean Crawled Corpus"의 약자로, 웹에서 수집된 대규모의 깨끗한 텍스트 데이터셋입니다.
- **표 3**에 나열된 20가지 텍스트 자료 유형은 다양성과 현실성을 높이기 위해 선택되었습니다.

---

이러한 접근 방식을 통해 UltraChat은 사용자가 AI 어시스턴트에게 기존의 다양한 텍스트 자료에 대해 도움을 요청하는 현실적인 대화 데이터를 포함하게 됩니다. 이는 대화형 언어 모델의 성능을 향상시키는 데 중요한 역할을 합니다.

---

## Templates for concatenation

**템플릿을 통한 연결 (Templates for concatenation)**

이 부분에서는 **기존의 텍스트 자료**와 **생성된 지시문**을 결합하여 대화를 생성하는 방법에 대해 설명합니다. 이를 위해 우리는 **수동으로 설계된 템플릿**을 사용하였으며, 이는 **표 4**에 제시되어 있습니다.

---

**표 4: 기존 자료와 생성된 지시문을 결합하기 위한 수동 설계된 템플릿**

1. `{text}\n{instruction}`
2. `{text} {instruction}`
3. `{instruction} Answer according to: {text}`
4. `{text} Based on the passage above, {instruction}`
5. `{instruction}: {text}`
6. `Given the text: {text}\n{instruction}`
7. `{instruction}\nGenerate according to: {text}`

---

여기서 `{text}`는 기존의 텍스트 자료를, `{instruction}`은 해당 텍스트에 대한 사용자의 지시문을 나타냅니다. 이 템플릿들은 텍스트와 지시문을 다양한 방식으로 조합하여, 대화의 시작 문장으로서의 다양성과 자연스러움을 높이는 데 사용됩니다.

이전에 수집한 **C4 코퍼스**의 텍스트 조각들은 **표 3**에서 언급한 20가지 텍스트 자료 유형에 기반하여 수집되었습니다. 각 유형별로 관련된 키워드를 수동으로 선별하였고, 이러한 키워드를 해당 URL과 매칭하여 코퍼스 내의 텍스트를 분류하였습니다.

총 10,000개의 텍스트 조각을 수집한 후, 각 텍스트에 대해 **ChatGPT**를 활용하여 **5개의 서로 다른 지시문**을 생성하였습니다. 이렇게 생성된 지시문들은 앞서 제시한 템플릿과 결합하여, 총 **500,000개의 결합된 세트**를 만들었습니다.

이 결합된 세트들은 **생성된 대화의 시작 문장**으로 사용됩니다. 이러한 방식으로 대화의 다양성과 현실성을 높일 수 있었으며, 이는 **UltraChat** 데이터셋의 품질 향상에 기여하였습니다.

**요약하면**, 우리는 기존의 텍스트 자료와 생성된 지시문을 효과적으로 결합하기 위해 수동으로 설계된 여러 가지 템플릿을 사용하였습니다. 이러한 템플릿들은 텍스트와 지시문을 다양한 방식으로 연결하여 대화의 시작점을 만들고, 이를 통해 대화형 언어 모델이 풍부하고 현실적인 대화를 학습할 수 있도록 도왔습니다.

---

## 4.4 User Simulation and Refinement

**4.4 사용자 시뮬레이션과 정제**

자동 대화 생성을 성공적으로 수행하기 위해서는 **사용자 모델의 원하는 행동을 유지하는 것이 매우 중요**합니다. 관찰 결과, 사용자 모델에게 현재의 대화 이력만 제공하면 그 모델이 **AI 어시스턴트의 역할**을 수행하려는 경향이 나타났습니다. 이러한 **"역할 교환"** 상황은 다중 턴 대화의 **일관성에 큰 영향을 미칠 수 있습니다**.

이를 해결하기 위해, **대화 이력을 제공하는 것 외에도** 모델에게 다양한 **사용자 성격을 채택하도록 명시적으로 지시하는 프롬프트**를 포함시켰습니다. 특히 **섹터 2**에서는 모델에게 **대화의 주요 목적을 상기시키는 프롬프트**를 사용하여 **보다 자연스러운 대화 흐름**을 촉진하였습니다.

데이터 생성 과정이 완료된 후에는 **전체적인 데이터 품질을 보장하기 위해 추가적인 필터링 단계**를 수행하였습니다. **사용자 응답의 현실감을 높이기 위해**, 이후 모델이 생성한 출력에서 **"Thank you", "Thanks", "You're welcome"**과 같은 **지나치게 공손한 표현을 제외**하였습니다.

이러한 접근을 통해, **사용자 모델이 실제 사용자와 유사한 행동을 보일 수 있도록 하였으며**, 생성된 대화 데이터의 **일관성**과 **현실성**을 향상시켰습니다. 이는 최종적으로 **대화형 언어 모델의 성능 향상**에 기여합니다.

---

## 4.5 Data Analysis

**4.5 데이터 분석**

우리는 UltraChat과 다른 여러 지시 데이터셋에 대한 통계적 분석을 수행하였으며, 그 결과는 **표 5**에 나타나 있습니다. **UltraChat**은 규모 면에서 두드러지며, 공개적으로 이용 가능한 가장 큰 데이터셋 중 하나입니다. 또한, **평균 턴 수**와 **데이터 인스턴스당 평균 길이** 측면에서 가장 높은 값을 보입니다.

SODA도 높은 평균 라운드 수를 보여주지만, 주로 **지시적 내용보다는 개념적 대화**로 구성되어 있습니다. 추가로, SODA의 대화당 평균 토큰 수는 231.8인 반면, **UltraChat은 놀랍게도 1467.4 토큰**을 자랑합니다.

다양성을 평가하기 위해, 우리는 **어휘적 다양성(lexical diversity)**과 **주제 다양성(topic diversity)**을 측정하였습니다. **UltraChat은 어휘적 다양성 측면에서 이전 데이터셋들을 능가**합니다. 그러나 **주제 다양성** 측면에서는 UltraChat이 GPT4ALL에 비해 약간 부족하지만, 다른 데이터셋들보다 **여전히 크게 앞서고 있습니다**. 이는 각 데이터 인스턴스의 토큰 수가 상대적으로 많아, 각 대화의 데이터 임베딩이 정규화되기 때문일 수 있습니다(GPT4ALL 데이터셋은 단일 턴으로 구성됨).

**다중 라운드 대화의 일관성(coherence)**을 보장하기 위해, 우리는 일관성 평가도 수행하였습니다. 결과에 따르면 대부분의 데이터셋이 비교적 높은 일관성을 나타냅니다. 특히, **UltraChat**과 **Baize** 데이터는 일관성 측면에서 가장 높은 순위를 차지했습니다.

---

**표 5: 기존 지시 데이터셋의 통계**

| 데이터셋               | 대화 수 | 평균 턴 수 | 대화당 평균 길이<br>(토큰 수) | 발화당 평균 길이<br>(토큰 수) | 어휘적 다양성<br>(MTLD ↑) | 주제 다양성<br>(코사인 거리 ↑) | 일관성<br>(1-10점) | 사용자 시뮬레이션 |
|----------------------|---------|----------|--------------------------|-------------------------|----------------------|------------------------|---------------|---------------|
| Self-instruct        | 82,439  | 1        | 69.8                     | 29.2                    | 24.9                 | 0.733                  | -             | 아니요        |
| Stanford Alpaca      | 52,002  | 1        | 91.1                     | 64.5                    | 42.8                 | 0.727                  | -             | 아니요        |
| SODA                 | 1,486,869 | 3.6      | 231.8                    | 22.5                    | 38.6                 | 0.797                  | 8.48          | 아니요        |
| GPT-4-LLM            | 61,002  | 1        | 179.6                    | 142.9                   | 48.9                 | 0.721                  | -             | 아니요        |
| BELLE                | 1,436,679 | 1        | 102.3                    | 63.3                    | 35.9                 | 0.771                  | -             | 아니요        |
| Baize                | 210,311 | 3.1      | 293.9                    | 52.8                    | 67.1                 | 0.751                  | 9.06          | 예            |
| GPT4ALL              | 711,126 | 1        | 597.7                    | 318.9                   | 62.7                 | 0.692                  | -             | 아니요        |
| **UltraChat**        | **1,468,352** | **3.8**    | **1467.4**                | **309.3**               | **74.3**             | **0.702**              | **9.06**      | **예**        |

---

**표 5 설명:**

- **어휘적 다양성(Lexical Diversity)**: 각 발화의 MTLD 점수<sup>[1]</sup>를 계산하여 평균한 값으로, **LexicalRichness**<sup>[2]</sup>를 사용하여 측정하였습니다.
- **주제 다양성(Topic Diversity)**: 각 데이터셋에서 무작위로 10,000개의 샘플을 추출하여, OpenAI 임베딩 API를 사용하여 각 데이터 쌍의 코사인 거리를 평균하여 측정하였습니다.
- **일관성(Coherence)**: ChatGPT가 1-10 점수로 평가하였습니다.

---

**분석 결과 요약:**

- **규모와 길이**: UltraChat은 대화 수, 평균 턴 수, 대화당 평균 길이 모두에서 가장 높은 값을 보여줍니다. 이는 모델이 더욱 풍부하고 복잡한 대화 패턴을 학습할 수 있도록 도와줍니다.
- **어휘적 다양성**: UltraChat은 어휘 사용의 다양성 면에서 가장 높은 점수를 받았습니다. 이는 모델이 다양한 어휘와 표현을 학습하는 데 도움이 됩니다.
- **주제 다양성**: 주제 다양성 측면에서 UltraChat은 GPT4ALL에 약간 못 미치지만, 다른 데이터셋보다는 높은 다양성을 보입니다. 이는 모델이 다양한 주제에 대해 학습하고 응답할 수 있게 해줍니다.
- **일관성**: UltraChat과 Baize는 일관성 평가에서 가장 높은 점수를 받았습니다. 이는 대화의 흐름과 응답의 일관성이 우수하다는 것을 의미합니다.
- **사용자 시뮬레이션**: UltraChat은 사용자 시뮬레이션을 포함하여, 실제 사용자와의 상호작용을 더욱 잘 반영하고 있습니다.

**결론적으로**, 이러한 분석을 통해 UltraChat이 다른 지시 데이터셋에 비해 **규모**, **다양성**, **일관성** 면에서 우수함을 확인할 수 있습니다. 이는 대화형 언어 모델의 학습에 매우 중요한 요소이며, **UltraChat이 모델의 성능 향상에 크게 기여할 것**임을 시사합니다.

---

**주석:**

1. **MTLD (Measure of Textual Lexical Diversity)**: 어휘적 다양성을 측정하는 지표로, 텍스트에서 다양한 어휘의 사용 정도를 나타냅니다. (McCarthy and Jarvis, 2010)
2. **LexicalRichness**: 어휘적 풍부함과 다양성을 계산하기 위한 파이썬 라이브러리입니다.

---

## 4.6 UltraLLaMA

**4.6 UltraLLaMA**

우리는 **UltraChat** 데이터셋으로 **LLaMA-13B**(Touvron et al., 2023) 모델을 훈련하여, **향상된 버전인 UltraLLaMA**를 개발하였습니다. 이 모델의 **대화 맥락에 대한 이해 능력을 향상시키기 위해**, 각 대화를 더 작은 시퀀스로 분할하였으며, 각 시퀀스의 최대 길이를 **2048 토큰**으로 제한하였습니다.

훈련 과정에서, 우리는 **모델의 응답 부분에 대해서만 손실(loss)을 계산**하였습니다. 이러한 접근 방식은 모델이 대화의 이전 부분에서 **관련 정보에 접근할 수 있도록 보장하여**, 진행 중인 대화를 **더 종합적으로 이해**하게 합니다. 이전의 맥락을 통합함으로써, UltraLLaMA는 **맥락에 적합하고 일관성 있는 응답**을 생성할 수 있는 능력을 갖추게 되었습니다.

**모델의 파인튜닝(fine-tuning)**을 위해, 우리는 **표준 크로스 엔트로피 손실(cross-entropy loss)**을 사용하였습니다. 모델은 **128개의 A100 GPU**를 사용하여 훈련되었으며, 전체 배치 크기는 **512**입니다. 이러한 대규모 연산 자원을 활용함으로써, 모델은 방대한 데이터셋을 효과적으로 학습하고 성능을 향상시킬 수 있었습니다.

요약하면, UltraLLaMA는 UltraChat 데이터셋을 기반으로 LLaMA-13B 모델을 향상시킨 것으로, **대화의 맥락을 깊이 있게 이해하고, 보다 자연스럽고 일관성 있는 응답을 제공할 수 있는 대화형 언어 모델**입니다.

---

## 5 Evaluation

**5 평가(Evaluation)**

**대화 모델**이 생성한 **응답의 품질을 평가**하는 것은 상당한 **도전 과제**입니다. 특히 다양한 설정에서의 **잠재적 불안정성**을 고려할 때 더욱 그렇습니다. 기존에는 **전통적인 벤치마크**가 평가 목적으로 활용되어 왔지만, 최근에는 **ChatGPT**나 **GPT-4**와 같은 **고급 모델을 활용하는 현대적인 접근법**이 사용되고 있습니다. 우리의 예비 실험에서는 **인간 평가보다 더 신뢰할 만한 결과**를 제공한다는 것이 입증되었습니다.

### 우리의 평가 세트

우리의 평가 세트는 **Vicuna 벤치마크**와 **GPT-4**가 생성한 추가적인 **300개의 질문과 지시문**을 포함하고 있습니다. 이 질문과 지시문은 **상식**, **세계 지식**, **전문 지식**(특히 물리학과 생물학), **수학**, **응답 생성**, **글쓰기 작업** 등 다양한 주제를 다루고 있습니다. 또한 각 질문 세트는 **난이도 수준에 따라 추가로 분류**되었습니다. **표 6**에는 평가 세트의 몇 가지 예시가 나와 있습니다.

### Truthful QA

Sun et al.(2023)을 따라, 우리는 먼저 **TruthfulQA**를 사용하여 우리 모델과 기준 모델들의 **세계 지식**을 테스트하였습니다. **TruthfulQA 벤치마크**는 모델이 현실 세계와 관련된 **참된 진술**을 얼마나 잘 식별하는지를 평가합니다. 그 목적은 **잘못된 주장**을 생성하거나 **오정보**를 퍼뜨릴 위험을 파악하는 것입니다. 이 벤치마크는 다양한 스타일로 작성된 질문들로 구성되어 있으며, **38개의 서로 다른 카테고리**를 포함하고 있고, **도전적인 문제들**로 구성되어 있습니다. 평가에는 **다중 선택 과제(multiple-choice task)**와 **생성 과제(generation task)**의 두 가지가 포함됩니다.

---

**요약하면**, 우리의 평가에서는 다양한 주제와 난이도를 포함한 광범위한 질문 세트를 사용하였으며, 모델의 성능을 보다 정확하게 평가하기 위해 **최신의 고급 모델**을 활용한 평가 방법을 채택하였습니다. 또한 **TruthfulQA**와 같은 벤치마크를 통해 모델이 사실에 기반한 정확한 응답을 생성할 수 있는 능력을 검증하였습니다.

---

## 5.1 Baselines

**5.1 기준 모델(Baselines)**

우리의 모델인 UltraLLaMA의 성능을 평가하기 위해, 여러 기존의 오픈 소스 대화형 언어 모델들을 **기준 모델(baselines)**로 선정하여 비교를 수행했습니다. 주요 기준 모델들은 다음과 같습니다:

### Alpaca

**Alpaca**(Taori et al., 2023a)는 LLaMA(Touvron et al., 2023) 모델을 기반으로 하며, **52,000개의 지시 데이터 시연(demonstrations)**을 효과적으로 최적화한 **지시-따르기 언어 모델**입니다. 이 데이터는 Text-Davinci-003을 사용한 Self-Instruct 기법으로 생성되었습니다.

---

**표 6: 우리가 만든 평가 세트의 일부 예시**

| 유형                  | 예시                                                                                                                                                                                                                                                                          |
|----------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **상식 - 쉬움**         | 우리 행성의 주요 에너지원은 무엇인가요?                                                                                                                                                                                                                                                    |
| **상식 - 보통**         | 차량이 경적을 울리며 관찰자에게 다가오고 멀어질 때 들리는 **음조의 변화를 일으키는 현상**은 무엇인가요?                                                                                                                                                                                               |
| **세계 지식 - 쉬움**      | 화씨로 물의 **어는점**은 몇 도인가요?                                                                                                                                                                                                                                                       |
| **세계 지식 - 보통**      | **괴델의 불완전성 정리**는 무엇인가요?                                                                                                                                                                                                                                                      |
| **물리학 지식**          | **양자 얽힘**은 어떻게 작동하며, 정보 전송에 어떤 영향을 미치나요?                                                                                                                                                                                                                                    |
| **생물학 지식**          | 생물체에서 발견되는 **네 가지 주요 거대분자**는 무엇인가요?                                                                                                                                                                                                                                       |
| **수학**              | 함수 \( e^x \)의 **테일러 급수 전개**는 무엇인가요?                                                                                                                                                                                                                                       |
| **추론**              | 두 개의 양동이가 있습니다. 하나는 빨간 페인트, 다른 하나는 파란 페인트를 담고 있습니다. 빨간 양동이에서 한 컵을 떠서 파란 양동이에 붓고, 다시 파란 양동이에서 한 컵을 떠서 빨간 양동이에 부었습니다. 이때 어느 양동이에 다른 색의 페인트가 더 많이 들어 있을까요?                                                                                                |
| **글쓰기**             | 빛의 속도로 여행하는 **두 광자 사이의 대화**를 작성하세요.                                                                                                                                                                                                                                         |

---

### Vicuna-13B

**Vicuna**(Chiang et al., 2023)는 LLaMA를 기반으로 **ShareGPT<sup>8</sup>에서 수집한 사용자 공유 대화 데이터**로 파인튜닝한 오픈 소스 대화 모델입니다. GPT-4를 통한 자동 평가 결과, Vicuna는 ChatGPT의 응답 품질의 **90% 이상**을 달성할 수 있음을 보여주었습니다. 이후의 연구에서 Vicuna는 **최신의 오픈 소스 대화 모델**로 널리 인정받고 있습니다. 이는 **Chat Arena<sup>9</sup>**에서 분명히 드러나는데, 총 13,000개의 익명 투표 결과 Vicuna-13B의 품질 점수가 다른 오픈 소스 모델을 능가하는 것으로 나타났습니다.

### Koala-13B

앞의 두 기준 모델과 유사하게, **Koala**(Geng et al., 2023)는 **선택된 공개 대화**로 파인튜닝한 또 다른 **LLaMA 기반 모델**입니다. 기존의 오픈 평가에서 Koala의 성능은 Vicuna에 비해 약간 떨어지지만, 여전히 강력한 기준 모델로 남아 있습니다.

### Dolly-V2

**Dolly**(Conover et al., 2023)는 **Pythia**(Biderman et al., 2023) 모델을 기반으로 하며, **15,000개의 인간이 생성한 지시-따르기 데이터**를 활용합니다. 이 데이터는 **InstructGPT**(Ouyang et al., 2022)를 따라 구성되었으며, **브레인스토밍, 분류, 폐쇄형 QA, 생성, 정보 추출, 개방형 QA, 요약** 등의 항목을 포함합니다.

### OpenAssistant-12B

**OpenAssistant-12B**(Köpf et al., 2023) 역시 **Pythia 기반 모델**로, LLM의 **정렬(alignment) 과정의 민주화**를 시도합니다. 이 프로젝트는 **66,497개의 대화 트리**에 분포된 **161,443개의 메시지**로 구성된 대화 코퍼스를 수집하고, 이러한 **수동 주석된 데이터**로 모델을 훈련합니다.

우리의 평가에는 **ChatGPT**(OpenAI, 2022), **MPT**(Mosaic, 2023), **Baize**(Xu et al., 2023)와 같은 다른 대화형 언어 모델들도 포함되었습니다.

---

![UltraLLaMA와 다른 기준 모델의 응답 비교](https://cdn-mineru.openxlab.org.cn/extract/f30ce1f4-043b-41ed-a2f2-b357b9bf524f/8112178ef0439eeb0d2fba8e2861928b826b2aefd4d05cfbffaf5d2f13ce1340.jpg)

**그림 2: 선별된 평가 세트에서 UltraLLaMA와 다른 기준 모델의 응답 비교. 평가는 ChatGPT에 의해 수행되었습니다.**

---

> **참고:**
>
> 8. **ShareGPT**: 사용자들이 ChatGPT와 주고받은 대화를 공유하는 플랫폼입니다.
> 9. **Chat Arena**: 다양한 챗봇 간의 비교를 위해 만들어진 공개 플랫폼으로, 사용자 투표를 통해 모델의 품질을 평가합니다.

---

**요약하면**, 우리는 여러 오픈 소스 대화형 언어 모델들을 기준 모델로 선정하여 UltraLLaMA의 성능을 평가하였습니다. 이러한 기준 모델들은 각기 다른 데이터와 방법으로 훈련되었으며, 우리의 모델이 이들을 능가하거나 대등한 성능을 보이는지 확인하기 위한 중요한 비교 대상입니다.

---

## 5.2 Response Comparison

**5.2 응답 비교**

우리는 **우리 모델(UltraLLaMA)의 출력**을 각 기준 모델과 비교하기 위해 **ChatGPT**를 활용했습니다. 구체적으로, 각 질문에 대해 두 모델(우리 모델과 기준 모델)의 응답을 각각 입력하고, ChatGPT에게 각 응답에 대해 **1부터 10까지의 점수**를 매기고 그 점수에 대한 **이유를 제공**하도록 요청했습니다. 우리의 평가 프롬프트는 **정확성**을 다른 요소들(예: 정보량, 창의성 등)보다 우선시하도록 설계되었습니다.

또한, 우리는 **응답이 제공되는 순서가 평가 결과에 상당한 영향을 미친다**는 것을 발견했습니다. 이 문제를 해결하기 위해, 각 질문마다 응답의 순서를 **무작위로 결정**했습니다. 그렇게 함으로써 편향을 최소화하고 공정한 평가를 보장했습니다.

마지막으로, 우리는 각 기준 모델에 대해 **우리 모델과의 승/무/패 횟수**를 집계했으며, 그 결과는 **그림 2**에 제시되어 있습니다.

---

**표 7: 각 모델의 전체 점수와 세부 영역별 점수**

| 모델                     | Vicuna 세트 | 상식-쉬움 | 상식-보통 | 세계 지식-쉬움 | 세계 지식-어려움 | 물리학 지식 | 생물학 지식 | 수학 추론 | 추론 능력 | 글쓰기 | 전체 |
|------------------------|------------|-----------|-----------|---------------|----------------|------------|------------|----------|----------|-------|------|
| Dolly-12B              | 6.61       | 7.77      | 7.90      | 8.53          | 8.50           | 8.57       | 8.53       | 6.43     | 5.13     | 6.36  | 7.15 |
| MPT-7B                 | 8.38       | 8.17      | 9.07      | 8.30          | 8.87           | 8.57       | 8.87       | 8.80     | 7.53     | 7.76  | 8.32 |
| OpenAssistant-12B      | 8.40       | 8.97      | 8.70      | 9.57          | 8.23           | 8.67       | 8.80       | 8.80     | 8.17     | 7.81  | 8.47 |
| Baize-13B              | 8.36       | 9.03      | 8.87      | 9.37          | 8.97           | 8.83       | 8.93       | 8.50     | 8.57     | 7.90  | 8.57 |
| Alpaca-7B              | 8.05       | 9.50      | 8.83      | 9.67          | 9.17           | 8.60       | 8.80       | 9.10     | 7.80     | 8.16  | 8.60 |
| Koala-13B              | 8.60       | 9.53      | 8.93      | 9.77          | 9.23           | 9.10       | 9.33       | 8.90     | 8.70     | 8.34  | 8.88 |
| Vicuna-13B             | 8.63       | 9.53      | 9.03      | 9.63          | 9.27           | 9.00       | 9.27       | 9.10     | 9.10     | 8.51  | 8.96 |
| ChatGPT                | 8.79       | 9.77      | 9.07      | 9.77          | 9.30           | 9.07       | 9.27       | 9.37     | 9.63     | 8.63  | 9.12 |
| **UltraLLaMA-13B**     | **8.70**   | **9.70**  | **9.03**  | **9.90**      | **9.33**       | **9.17**   | **9.27**   | **9.27** | **8.87** | 8.51  | **9.02** |

*점수는 1에서 10까지이며, 평균 점수를 나타냅니다. **볼드체**는 최고 점수를, 밑줄은 두 번째로 높은 점수를 나타냅니다.*

---

**결과 분석**

- **전체 평균 점수**에서 UltraLLaMA-13B는 **9.02점**으로, Vicuna-13B의 **8.96점**과 ChatGPT의 **9.12점**에 근접한 높은 성능을 보였습니다.
- 세부 영역별로 보면, UltraLLaMA는 **세계 지식-쉬움**, **세계 지식-어려움**, **물리학 지식**, **생물학 지식**, **수학 추론** 등에서 **최고 점수**를 기록했습니다.
- 특히, **세계 지식-쉬움** 분야에서는 **9.90점**으로 가장 높은 점수를 받았으며, 이는 다른 모델들보다 우수한 성능을 나타냅니다.

**승률 분석**

- **그림 2**에서 볼 수 있듯이, UltraLLaMA는 모든 오픈 소스 모델에 비해 **우수한 성능**을 보였으며, **최대 85%의 승률**을 기록했습니다.
- Vicuna에 대해서도 **13% 더 높은 승률**을 보여, 현재까지 최고 수준의 오픈 소스 모델인 Vicuna를 **능가**했습니다.

---

**결론**

우리의 평가 결과, **UltraLLaMA는 다른 오픈 소스 모델들보다 전반적으로 우수한 성능**을 보였습니다. 이는 우리가 구축한 **UltraChat 데이터셋**의 **다양성**과 **품질**이 모델의 성능 향상에 큰 영향을 미쳤음을 시사합니다. 또한, 평가 방법에서 **응답 순서의 무작위화**를 통해 편향을 최소화하여 공정성을 높였습니다.

**결론적으로**, UltraLLaMA는 **최신의 오픈 소스 대화형 언어 모델들보다 뛰어난 성능**을 입증했으며, 이는 더 나은 대화형 AI 모델 개발에 중요한 기여를 합니다.

---

**참고 사항**

- **응답 순서의 영향**: 우리는 응답의 순서가 평가에 미치는 영향을 발견하고 이를 조정함으로써 평가의 신뢰성을 높였습니다.
- **평가 방법의 중요성**: ChatGPT를 활용한 자동 평가는 인간 평가보다 일관되고 정확한 결과를 제공할 수 있음을 확인했습니다.

---

**그림 2: UltraLLaMA와 다른 기준 모델들과의 응답 비교 결과**

![UltraLLaMA와 다른 기준 모델의 응답 비교 그래프](https://cdn-mineru.openxlab.org.cn/extract/f30ce1f4-043b-41ed-a2f2-b357b9bf524f/8112178ef0439eeb0d2fba8e2861928b826b2aefd4d05cfbffaf5d2f13ce1340.jpg)

*UltraLLaMA가 각 기준 모델에 대해 승리(WIN), 무승부(TIE), 패배(LOSE)를 얼마나 기록했는지 나타낸 그래프입니다. 평가에는 ChatGPT가 사용되었습니다.*

---

---

## 5.3 Independent Scoring

**5.3 독립적인 점수 평가**

**쌍(pair-wise) 비교의 불안정성**을 고려하여, 우리는 각 모델의 응답 품질을 기반으로 **ChatGPT를 활용하여 1부터 10까지의 점수를 할당하는 독립적인 점수 평가**를 수행하였습니다. **표 7**은 UltraLLaMA와 기준 모델들 간의 점수 비교를 보여줍니다. 특히, **우리의 모델은 전체 점수 측면에서 다른 오픈 소스 모델들보다 상당한 격차로 우수한 성능을 나타냈습니다**. 또한, UltraLLaMA는 평가 세트의 거의 모든 부문에서 **최고의 성능**을 달성하여 그 뛰어난 능력을 입증하였습니다.

이러한 세부 분석은 각 모델이 특정 유형의 질문과 지시문에 대해 어떤 성능을 보이는지에 대한 **통찰**을 제공합니다. 일반적으로, 모든 모델은 **상식 지식과 일반적인 세계 이해에 관한 더 쉬운 질문**에서 더 나은 성능을 보였습니다. 그러나 **추론과 창의적인 글쓰기**를 포함하는 더 복잡한 작업은 대부분의 모델에게 **도전적**인 것으로 나타났습니다.

흥미로운 점은, **70억 개의 파라미터**를 가진 **Alpaca**가 상식 및 세계 지식과 관련된 질문에서는 더 큰 모델들과 **비슷한 수준의 성능**을 보였지만, 더 **어려운 작업에서는 뒤처졌다**는 것입니다. 또한, **Pythia**(Biderman et al., 2023)를 기반으로 한 **Dolly**와 **OpenAssistant**는 크기가 비슷하거나 더 작은 **LLaMA 기반 모델**에 비해 **열등한 성능**을 보였습니다. 이 관찰은 **기반이 되는 백본 언어 모델**의 중요성을 강조합니다.

---

**표 8: TruthfulQA 벤치마크에서 다양한 모델의 정확도**

| 모델                                        | 정확도 |
|--------------------------------------------|-------|
| Alpaca-7B *(Taori et al., 2023b)*          | 0.43  |
| OpenAssistant-12B *(Köpf et al., 2023)*    | 0.50  |
| Koala-13B *(Geng et al., 2023)*            | 0.51  |
| Vicuna-13B *(Chiang et al., 2023)*         | 0.54  |
| **UltraLLaMA (우리의 모델)**                 | **0.54**  |

---

**TruthfulQA 벤치마크**에서 각 모델의 정확도를 측정한 결과, **UltraLLaMA**와 **Vicuna-13B**가 **0.54**의 정확도로 가장 높은 성능을 보였습니다. 이는 우리 모델이 사실에 기반한 질문에 대해 정확한 응답을 생성하는 능력이 우수하다는 것을 보여줍니다. 다른 모델들과 비교했을 때, UltraLLaMA의 성능은 특히 더 두드러지며, 이는 **데이터셋의 품질과 모델의 설계**가 성능에 큰 영향을 미친다는 것을 시사합니다.

---

## 5.4 TruthfulQA Results

**5.4 TruthfulQA 결과**

우리는 **TruthfulQA**의 **다중 선택 과제(multiple-choice task)**에서 모델들을 평가하였습니다. 이 과제에서 각 답변 후보에 대해 모델에게 그 답변이 **참(True)**인지 **거짓(False)**인지 판단하도록 요청하였습니다. 각 모델의 판단 정확도는 **표 8**에 제시되어 있습니다.

---

**표 8: TruthfulQA 벤치마크에서 다양한 모델의 정확도**

| 모델                                       | 정확도  |
|-------------------------------------------|-------|
| Alpaca-7B *(Taori et al., 2023b)*         | 0.43  |
| OpenAssistant-12B *(Köpf et al., 2023)*   | 0.50  |
| Koala-13B *(Geng et al., 2023)*           | 0.51  |
| Vicuna-13B *(Chiang et al., 2023)*        | 0.54  |
| **UltraLLaMA (우리의 모델)**                | **0.54**  |

---

결과를 보면, **진실 판단(Truth Judgment)**은 현재의 모델들에게 여전히 **도전적인 과제**임을 알 수 있습니다. 가장 정확도가 높은 모델조차도 **50%를 조금 넘는 수준**으로, 진실 여부를 판단하는 데 어려움을 겪고 있습니다. 이는 모델들이 현실 세계의 지식과 진술의 진위 여부를 정확하게 판단하는 데 한계가 있음을 나타냅니다.

**UltraLLaMA**는 **Vicuna**와 동등한 정확도인 **0.54**를 기록하였으며, 다른 기준 모델들인 **Koala**, **OpenAssistant**, **Alpaca**보다 **우수한 성능**을 보여주었습니다. 이는 우리의 모델이 사실에 기반한 질문에 대해 보다 정확하게 응답할 수 있는 능력을 갖추고 있음을 시사합니다.

그러나 전체적으로 보면, 진실 여부 판단 정확도가 **50% 정도**로 낮기 때문에, **진실 판단 능력을 향상시키기 위한 추가 연구와 개선**이 필요합니다. 특히, TruthfulQA 벤치마크는 모델들이 흔히 빠지기 쉬운 오류나 편향을 드러내기 위해 고안된 어려운 질문들로 구성되어 있어, 이 점수를 높이는 것은 대형 언어 모델의 **신뢰성과 정확성**을 향상시키는 데 중요한 과제입니다.

---

**요약하면**, **5.4 TruthfulQA 결과** 섹션에서는 TruthfulQA 벤치마크를 통해 모델들의 진실 판단 능력을 평가하였으며, 우리의 **UltraLLaMA**가 **Vicuna**와 함께 가장 높은 정확도를 달성하여 다른 오픈 소스 모델들을 앞섰음을 보여줍니다. 이는 **UltraChat** 데이터셋과 우리의 모델 설계가 모델의 **세계 지식과 진실성 판단 능력**을 향상시키는 데 기여했음을 나타냅니다. 그러나 여전히 진실 판단 정확도가 낮은 수준에 머물러 있으므로, 이 분야에서의 지속적인 연구와 개선이 필요함을 강조하고 있습니다.

---

## 5.5 The Impact of System Prompts

**5.5 시스템 프롬프트의 영향**

대형 언어 모델(LLM)의 **역할**과 **응답 스타일**을 지정하기 위해 **시스템 프롬프트**를 사용하는 것은 흔한 관행입니다. 우리의 평가에서, 시스템 프롬프트는 생성된 출력의 **응답 스타일에 상당한 영향을 미친다**는 것을 관찰했습니다.

구체적으로, 모델에게 "**도움이 되고 자세한**" 응답을 제공하도록 프롬프트하면, 모델은 더욱 관련성 높은 세부 정보를 생성하는 경향이 있습니다. 이는 응답의 **정보량**을 향상시켜 줍니다. 이러한 프롬프트는 결정론적인 질문의 **정확성**에 직접적인 영향을 미치지 않을 수 있지만, 추가적인 정보를 제공하여 응답의 **전체적인 품질을 향상**시킬 수 있습니다.

이러한 효과를 보여주기 위해 **표 9**를 참고할 수 있습니다. 여기서 두 개의 출력은 모두 정확하지만, **시스템 프롬프트**가 적용된 모델은 더 **정보가 풍부한 응답**을 제공합니다.

---

**표 9: UltraLLaMA의 시스템 프롬프트 사용 여부에 따른 응답 비교**

| 질문                           | 모나리자를 누가 그렸나요?                                                                                                                                                                 |
|--------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **시스템 프롬프트 없이:**       | 모나리자는 이탈리아 르네상스 예술가인 레오나르도 다 빈치가 그렸습니다.                                                                                                                                |
| **시스템 프롬프트와 함께:**     | 모나리자는 이탈리아 르네상스 예술가인 레오나르도 다 빈치의 작품입니다. 그는 이 작품을 16세기 초반인 1503년부터 1519년 사이에 그렸으며, 세계에서 가장 유명하고 상징적인 그림 중 하나로 널리 알려져 있습니다. 이 그림은 리사 게라르디니라는 여성의 초상화이며, 현재 프랑스 파리에 있는 루브르 박물관에 소장되어 있습니다. |

---

이 예시에서 볼 수 있듯이, **시스템 프롬프트를 사용한 경우** 모델은 더 **자세하고 풍부한 정보를 제공**합니다. 첫 번째 응답은 질문에 대한 정확한 답변을 제공하지만, 두 번째 응답은 **제작 시기**, **작품의 중요성**, **초상화의 대상**, **소장 장소** 등 추가적인 세부 정보를 포함하여 응답의 깊이와 품질을 높입니다.

따라서, **시스템 프롬프트**는 모델의 응답 스타일과 내용에 중요한 영향을 미칠 수 있으며, 특히 응답의 **정보성**을 향상시키는 데 유용합니다. 이는 모델을 실제 애플리케이션에 적용할 때, **원하는 응답 스타일에 맞게 모델의 출력을 조정**하는 데 도움을 줄 수 있습니다.

---

**요약하면**, 시스템 프롬프트를 통해 모델에게 특정한 역할이나 응답 방식을 지시하면, 모델의 출력을 원하는 방향으로 유도할 수 있습니다. 이는 사용자 경험을 개선하고, 모델이 제공하는 정보의 **품질과 유용성**을 높이는 데 중요한 요소입니다.

---
