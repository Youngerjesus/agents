
## ABSTRACT

이 연구에서는 대형 언어 모델(LLM)을 기계가 생성한 지시문 데이터를 사용하여 미세 조정(finetuning)하면 새로운 작업에서도 뛰어난 제로샷(Zero-shot) 성능을 발휘할 수 있다는 것을 이전 연구가 보여주었다고 설명합니다. 즉, 인간이 작성한 지시문이 필요 없게 됩니다. 본 논문은 GPT-4를 활용하여 LLM 미세 조정을 위한 지시문 데이터를 생성한 첫 시도를 소개합니다. 연구 결과, GPT-4가 생성한 5만 2천 개의 영어 및 중국어 지시문 데이터가 기존 최첨단 모델들이 생성한 지시문 데이터보다 새로운 작업에서 뛰어난 제로샷 성능을 보여주었습니다. 또한, 포괄적인 평가 및 보상 모델 훈련을 위해 GPT-4로부터 피드백 및 비교 데이터를 수집하였습니다. 우리는 GPT-4를 사용하여 생성한 데이터와 코드베이스를 공개하고 있습니다.

---

## INTRODUCTION

이 논문은 대형 언어 모델(LLM)의 뛰어난 일반화 능력을 설명하며, 이러한 모델들이 자연어 지시를 따르고 실제 세상에서의 작업을 완료할 수 있도록 하는 방법을 탐구하고 있습니다. 그 방법 중 하나로 지시문 튜닝(instruction-tuning)이 있으며, 이는 인간이 주석을 단 프롬프트와 피드백을 사용하여 모델을 다양한 작업에 맞게 미세 조정하거나, 수동 또는 자동으로 생성된 지시문을 추가한 공개 벤치마크와 데이터세트를 사용하는 방식으로 구현됩니다.

특히 Self-Instruct 튜닝은 최신의 지시문 튜닝된 교사 LLM이 생성한 데이터를 학습하여, LLM을 인간 의도에 맞추는 간단하고 효과적인 방법입니다. 이 연구 방향은 LLM의 제로샷 및 소수샷 일반화 능력을 향상시키는 효과적인 수단을 제공해 왔습니다. 최근 ChatGPT와 GPT-4의 성공은 이러한 지시문 튜닝을 통해 오픈소스 LLM을 개선할 큰 기회를 제공합니다. LLaMA는 이러한 오픈소스 LLM들 중 하나로, 상용 LLM인 GPT-3와 성능을 견줄 수 있습니다. LLaMA가 지시를 따를 수 있도록 하려면, 뛰어난 성능과 저비용의 Self-Instruct 튜닝이 빠르게 채택되고 있습니다.

본 논문에서 우리는 처음으로 GPT-4를 Self-Instruct 튜닝의 교사로 사용하는 방법을 제안합니다. 우리의 주요 기여는 다음과 같습니다:

- **GPT-4 데이터:** 우리는 영어와 중국어로 된 52K 개의 지시문 데이터셋과 세 개의 지시문 튜닝된 모델의 출력을 평가한 피드백 데이터를 포함하여 GPT-4가 생성한 데이터를 공개합니다.
- **모델 및 평가:** GPT-4가 생성한 데이터를 기반으로 지시문 튜닝된 LLaMA 모델과 보상 모델을 개발했습니다. 우리는 세 가지 기준에 대한 인간 평가, GPT-4 피드백을 사용한 자동 평가, 그리고 비정형 지시문에 대한 ROUGE-L 점수를 사용하여 지시문 튜닝된 LLM의 품질을 평가합니다.

이 연구는 GPT-4로 생성된 데이터를 사용한 LLM 지시문 튜닝의 효과성을 확인하고, 일반 목적의 지시문을 따르는 에이전트를 구축하는 데 있어 실용적인 팁을 제안합니다.

---

## 2 DATASET

이 논문의 '2 데이터셋' 섹션에서는 데이터 수집 과정과 GPT-4를 사용한 데이터 생성 방법에 대해 설명합니다.

데이터 수집: Alpaca 데이터셋(Taori et al., 2023)에서 수집된 52,000개의 고유한 지시문을 재사용합니다. 각 지시문은 모델이 수행해야 할 작업을 설명합니다. 지시문에는 작업에 대한 선택적 맥락이나 입력이 있을 수도 있고 없을 수도 있습니다. Alpaca 데이터셋에서는 GPT-3.5(text-davinci-003)를 사용해 출력을 생성했지만, 우리는 GPT-4(gpt-4)를 활용하여 출력을 생성합니다. GPT-4를 사용해 다음 네 가지 데이터셋을 생성합니다.

1. **영어 지시문-답변 데이터:** Alpaca에서 수집한 52,000개의 지시문 각각에 대해 GPT-4가 영어로 답변을 제공합니다. 자세한 내용은 알고리즘 1에 나와 있습니다. GPT-4와 Self-Instruct를 사용하여 자체 지시 세트를 구성하는 반복적 과정을 향후 작업으로 남겨둡니다.

2. **중국어 지시문-답변 데이터:** ChatGPT를 사용하여 52,000개의 지시문을 중국어로 번역하고, GPT-4가 중국어로 답변하게 합니다. 이를 통해 LLaMA에 기반한 중국어 지시문-따르기 모델을 구축하고, 지시문 튜닝의 언어 간 일반화 능력을 연구할 수 있습니다.

3. **비교 데이터:** GPT-4에게 자신의 응답을 1부터 10까지 평가하도록 요청합니다. 또한, GPT-4에게 GPT-4, GPT-3.5, 그리고 OPT-IML을 포함한 세 모델의 응답을 비교하여 평가하도록 요청합니다. 이는 보상 모델을 훈련하는 데 사용됩니다.

4. **비정상 지시문에 대한 답변:** 68,000개의 지시문-입력-출력 삼중 코어 데이터셋에 대해 GPT-4의 답변을 디코딩합니다. 이 하위 세트는 GPT-4와 우리 지시문 튜닝된 모델들 간의 차이를 계량화하는 데 사용됩니다.

데이터 통계: 그림 1에서는 GPT-4와 GPT-3.5의 영어 출력 응답 세트를 비교합니다. 각 출력에 대해 동사 뿌리와 직접 목적어 명사를 추출하고, 각 출력 세트에서 고유한 동사-명사 쌍의 빈도를 계산합니다. 빈도가 10 이상인 동사-명사 쌍은 그림 1(a)와 (b)에 표시되며, 두 세트의 가장 빈번한 25개의 쌍을 그림 1(c)에 비교합니다. 출력 시퀀스 길이의 빈도 분포는 그림 1(d)에 비교되어 있습니다. GPT-4는 GPT-3.5보다 더 긴 시퀀스를 생성하는 경향이 있습니다. Alpaca의 GPT-3.5 데이터는 반복적인 데이터 수집 과정을 통해 각 반복에서 유사한 지시문 인스턴스를 제거하여 더 긴 꼬리를 가진 출력 분포를 보입니다. 이는 현 시점의 일회성 데이터 생성 과정에서는 없는 특징입니다. 이러한 간단한 과정에도 불구하고, GPT-4가 생성한 지시문-따르기 데이터는 이후 실험에서 더 나은 정렬 성능을 보여줍니다.

---

## 3 INSTRUCTION-TUNING LANGUAGE MODELS

I apologize, but it seems that the '3 INSTRUCTION-TUNING LANGUAGE MODELS' section content is missing from your request. If you can provide the full text of that section, I will be happy to explain it in detail.

---

## 3.1 SELF-INSTRUCT TUNING

이 섹션에서는 Self-Instruct 튜닝을 통해 언어 모델을 훈련하는 과정에 대해 설명합니다. 연구에서는 LLaMA 7B 체크포인트를 사용하여 두 가지 모델을 지도 학습 방식으로 미세 조정했습니다.

1. **LLaMA-GPT4 모델**: 이 모델은 GPT-4가 생성한 52,000개의 영어 지시문-따르기 데이터를 바탕으로 훈련되었습니다. 이 데이터의 분포는 이전의 그림 1에서 보여주었습니다.

2. **LLaMA-GPT4-CN 모델**: 이 모델은 GPT-4가 생성한 52,000개의 중국어 지시문-따르기 데이터를 바탕으로 훈련되었습니다.

훈련 과정에서는 공정한 비교를 위해 Taori et al.(2023)의 훈련 스케줄을 따랐습니다. 이러한 모델들은 GPT-4가 생성한 데이터의 품질을 연구하고, 하나의 언어로 지시문 튜닝을 했을 때의 교차 언어 일반화 특성을 연구하는 데 사용됩니다.

---

## 3.2 REWARD MODELS

이 섹션에서는 인간 피드백을 통한 강화 학습(RLHF, Reinforcement Learning from Human Feedback)의 핵심 요소인 보상 모델링에 대해 설명합니다. RLHF는 LLM의 행동을 인간의 선호와 맞춰 모델을 더욱 유용하게 만들려는 목적을 가지고 있습니다. 보상 모델링은 프롬프트와 응답이 주어졌을 때 스칼라 보상을 예측하는 회귀 과제로 문제를 구성합니다. 이렇게 하려면 대규모 비교 데이터가 필요한데, 두 가지 모델의 응답을 같은 프롬프트에 대해 비교하는 방식입니다. 하지만 Alpaca, Vicuna, Dolly 등의 기존 오픈소스 작업은 비교 데이터 레이블링 비용이 높아 RLHF를 포함하지 않습니다.

한편, 최근 연구는 GPT-4가 자신의 실수를 식별하고 수정하며, 응답 품질을 정확히 평가할 수 있음을 보여주었습니다. 따라서 RLHF 연구를 촉진하기 위해, 우리는 2절에서 설명한 바와 같이 GPT-4를 사용하여 비교 데이터를 생성했습니다.

데이터 품질 평가를 위해, 우리는 OPT 1.3B 기반으로 보상 모델을 훈련하여 서로 다른 응답들을 평가합니다. 각 비교 데이터 인스턴스는 한 프롬프트 $\textbf{\em x}$와 $K$개의 응답을 포함하고, GPT-4는 각 응답에 대해 1부터 10까지의 점수 $s$를 줍니다. 이 인스턴스에서 고유한 쌍 $C_{2}^{K}$를 구성할 수 있으며, 각 쌍은 $(y_{l}, y_{h})$로 표현됩니다. 여기서 $y_{l}$의 점수는 $y_{h}$의 점수보다 낮습니다($s_{l} < s_{h}$). 보상 모델 $r_{\theta}$는 다음 목표로 훈련됩니다: $\operatorname*{min}\log(\sigma(r_{\pmb\theta}(x,y_{h})-r_{\pmb\theta}(\pmb x,y_{l})))$, 여기서 $\sigma$는 시그모이드 함수입니다. 비교 데이터의 분포는 그림 2에 나타나 있습니다. 

이 접근 방식은 보상 모델이 다양한 응답 품질을 평가하고 RLHF의 효과성을 높이는 데 도움을 줄 수 있도록 설계되었습니다.

---

## 4 EXPERIMENTAL RESULTS

It looks like the content from the '4 EXPERIMENTAL RESULTS' section is missing from your request. If you can provide the full text of that section, I would be happy to explain it in detail.

---

## 4.1 BENCHMARKS

이 섹션에서는 LLM(대형 언어 모델)의 성능을 평가하기 위한 벤치마크를 소개하고, 인간 평가 방법과 그 결과를 설명합니다.

### 벤치마크:
이 연구의 목표는 GPT-4 데이터를 기반으로 학습된 Self-Instruct 모델들이 새로운 지시문에 대해 얼마나 잘 반응하는지 평가하는 것입니다. 연구에서 사용한 데이터셋은 다음과 같습니다:

- **User-Oriented-Instructions- $252^{\,2}$**: 이 데이터셋은 71개의 사용자 중심 애플리케이션(예: Grammarly, StackOverflow, Overleaf)을 기반으로 한 252개의 지시문을 포함합니다.

- **Vicuna-Instructions- $80^{3}$**: GPT-4가 생성한 80개의 도전적인 질문이 포함되어 있으며, 기본 모델들이 어려워하는 질문들입니다. 지식, 수학, Fermi 문제, 반사실적 시나리오, 롤플레잉, 일반, 코딩, 글쓰기, 상식 등 8개의 카테고리가 있습니다.

- **Unnatural Instructions**: 68,478개의 샘플로 구성된 이 데이터셋은 15개의 수작업 예제를 사용한 3-샷 맥락 학습으로 text-davinci-002가 합성한 것입니다.

### 인간 평가:
모델의 정렬 품질을 평가하기 위해 Anthropic Askell et al.(2021)에서 제안한 정렬 기준을 따랐습니다. 이 기준은 AI 시스템이 인간의 가치에 얼마나 잘 정렬되어 있는지를 평가하는 데 사용됩니다. 기준은 다음과 같습니다:

- **도움됨**: 인간이 목표를 달성하는 데 도움이 되는지 여부를 평가합니다.

- **정직성**: 정확한 정보를 제공하고 필요시 불확실성을 표현하여 인간 사용자를 오도하지 않는지를 평가합니다.

- **무해성**: 인간에게 해를 끼치지 않는지를 평가합니다.

Amazon Mechanical Turk를 통해 인간 평가를 수행했으며, 결과는 그림 3의 파이 차트에 나타나 있습니다.

- **첫 번째 비교**: GPT-4 데이터로 미세 조정된 LLaMA 모델과 GPT-3 데이터로 미세 조정된 LLaMA 모델(Stanford Alpaca 모델)을 비교하여, "도움됨" 기준에서 GPT-4가 54.12%로 우세했으며, "정직성"과 "무해성"에서는 GPT-3(Alpaca)이 다소 우세하나 비슷한 수준의 결과를 보였습니다.

- **두 번째 비교**: GPT-4 지시를 따라 튜닝된 LLaMA 모델과 원래의 GPT-4 모델을 비교했으며, 세 가지 기준 모두에서 유사한 성능을 보여줬습니다. 이는 GPT-4가 생성한 데이터로 학습함으로써 원래 GPT-4 수준의 성능을 달성할 수 있다는 것을 의미합니다.

### 추가 그림 설명 (그림 4):
- GPT-4로 평가된 성능 비교를 보여줍니다. 각 막대는 두 모델 간의 평가 결과를 나타내며, 총 점수(최대 800점)와 상대 점수 비율(강력한 상대 모델 대비)이 보고됩니다.
- LLaMA GPT4 모델의 응답을 우리 보상 모델로 평가한 결과를 나타내며, 전반적으로 ChatGPT 및 GPT-4와 비교하여 성능을 평가했습니다.

이 연구는 GPT-4 데이터를 사용한 학습이 새로운 지시문 작업에서 뛰어난 성능을 낸다는 가능성을 시사합니다.

---

## 4.3 COMPARISONS WITH SOTA USING AUTOMATIC EVALUATION

이 섹션에서는 자동 평가를 사용하여 LLaMA-GPT4 모델과 다른 최첨단(State-of-the-Art, SOTA) 모델들을 비교한 결과를 설명합니다.

### 자동 평가:
- **평가 방법**: GPT-4를 활용해 다양한 모델들이 80개의 새로운 질문에 대한 생성 응답의 품질을 자동 평가했습니다. LLaMA-GPT4(7B)와 GPT-4의 응답을 수집하고, Vicuna(2023)에서 공개된 다른 채팅 봇들(LLaMA 13B, Alpaca 13B, Vicuna 13B, Bard, ChatGPT)의 응답을 사용했습니다. 두 모델 간의 응답 품질을 1에서 10까지의 점수로 평가했습니다.

- **결과**: LLaMA-GPT4를 두 가지 디코딩 결과로 평가했습니다. 첫째는 질문당 하나의 응답만을 생성하는 기본 디코딩 결과, 둘째는 질문당 다섯 개의 응답을 생성한 후 보상 모델이 상위에서 하위로 순위 매긴 응답 그룹입니다. 이 그룹들은 기본 결과와 비교되었고, 평가 결과에서 피드백 데이터와 보상 모델의 유용성을 보여줍니다.

### 결과:
- LLaMA-GPT4는 text-davinci-003(즉, Alpaca)로 튜닝한 경우보다 높은 성능을 보여주었지만, 여전히 상업용 대형 챗봇(e.g., GPT-4)과 비교해서는 차이가 존재합니다.
- **중국어 평가**: 영어 응답을 중국어로 번역한 것이나, 중국어로 직접 질문하여 생성된 응답을 비교했을 때, 번역된 응답이 더 우수한 성능을 보여주었습니다. 이는 GPT-4가 영어 코퍼스에서 더 많이 훈련되어 영어 지시문을 따르는 능력이 더 강력하기 때문일 수 있습니다.

### 비정상 지시문(unnatural instructions)에서의 결과(그림 6):
- LLaMA-GPT4와 GPT-4, 그리고 Alpaca 모델을 비교했을 때, Alpaca가 다른 두 모델에 비해 평균 ROUGE-L 점수에서 우수하였습니다. 그러나 응답의 길이가 길어질수록 LLaMA-GPT4와 GPT-4가 더 높은 성능을 보여주기 시작했습니다. 이는 더 창의적인 시나리오에서 이 모델들이 지시문을 더 잘 따를 수 있다는 것을 의미합니다.
- 각기 다른 응답 길이 하위 집합에 대해, LLaMA-GPT4는 GPT-4의 행동을 가깝게 따릅니다. 응답 길이가 짧을 때, LLaMA-GPT4와 GPT-4는 단순한 정답을 포함하되, 응답을 더 대화형으로 만드는 추가적인 단어를 포함하기 때문에, 낮은 ROUGE-L 점수를 받을 가능성이 있습니다.

이 실험 결과는 LLaMA-GPT4가 GPT-4의 출력을 기반으로 학습했을 때, 상당한 성능 향상을 이룰 수 있음을 보여주며, 이는 혁신적인 지시문-따르기 LLM 개발에 있어서 유망한 방향임을 시사합니다.

---

## 5 RELATED WORK

이 섹션에서는 지시문 튜닝과 오픈소스 노력이 관련된 연구에 대해 설명합니다.

### 지시문 튜닝(Instruction Tuning):
지시문 튜닝은 NLP 분야에서 점점 주목받고 있는 연구 방향입니다. 기존 연구들은 크게 세 가지 요소인 지시문-따르기 데이터, 기본 언어 모델, 평가 벤치마크의 품질과 규모를 향상시키는 것을 목표로 합니다. 각 연구 그룹은 일반적으로 자체적인 개발 파이프라인을 유지합니다. 예를 들어, 플랜(FLAN)을 기반으로 지시문 미세 조정 언어 모델의 규모를 확대하는 연구(Chung et al., 2022)나, 점점 많은 프롬프트를 포함하는 PromptSource(공공 프롬프트 풀, P3)의 개발(Bach et al., 2022) 등이 있습니다. T0는 P3에 기반한 다중 과제 프롬프트 훈련을 통해 개발된 일련의 모델(Sanh et al., 2021)입니다. OPT 모델의 지시문 튜닝은 더 크고 포괄적인 벤치마크인 OPT-IML Bench를 사용하여 진행되었으며, 이는 FLAN, Super-NaturalInstructions(Wang et al., 2022b), UnifiedSKG(Xie et al., 2022)을 포함합니다.

### 오픈소스 노력(Open-Source Efforts):
ChatGPT의 폭넓은 능력 덕분에, 오픈소스 모델은 큰 관심을 받으며 인간의 가치에 맞는 일반 목적의 텍스트 기반 어시스턴트를 개발하는 데 기여하고 있습니다. 초기의 기초 LLM 노력에는 BLOOM(Scao et al., 2022), GPT-J(Wang & Komatsuzaki, 2021), GPT-NEO(Black et al., 2021), OPT(Zhang et al., 2022), LLaMA(Zhang et al., 2023)가 포함됩니다. LLM을 대화 기반 어시스턴스에 맞추기 위해 Open-Assistant는 GPT-J를 기반으로 하고, Alpaca/Vicuna는 LLaMA를 기반으로 구축되었습니다. 또한, OpenFlamingo(Awadalla et al., 2023)와 LLaMA-Adapter(Zhang et al., 2023)는 LLaMA를 이미지 입력과 연결하여 오픈소스 멀티모달 LLM을 구축할 길을 마련했습니다.

이러한 연구와 노력은 LLM이 더 다양하고 강력한 능력을 발휘할 수 있도록 하고, 오픈소스 환경에서의 발전을 촉진하는 데 그 목적이 있습니다.

---
