
## ABSTRACT

이 논문은 언어 모델의 제로샷(zero-shot) 학습 능력을 향상시키는 간단한 방법을 탐구하고 있습니다. 우리는 "지시 튜닝(instruction tuning)"—명령 형태로 설명된 다양한 데이터셋을 사용하여 언어 모델을 미세 조정(finetuning)하는 방법—이 새로운 작업에 대한 제로샷 성능을 크게 향상시킨다는 것을 보여줍니다.

구체적으로, 우리는 1,370억 개의 매개변수를 가진 사전 학습된 언어 모델을 사용하여, 자연어 명령 템플릿으로 표현된 60개 이상의 NLP 데이터셋에 대해 지시 튜닝을 수행했습니다. 이렇게 지시 튜닝한 모델을 "FLAN"이라고 부르며, 우리는 이 모델을 이전에 본 적 없는 작업 유형에 대해 평가했습니다. 그 결과, FLAN은 수정되지 않은 원래 모델보다 성능이 크게 향상되었으며, 우리가 평가한 25개의 데이터셋 중 20개에서 1,750억 개의 매개변수를 가진 GPT-3의 제로샷 성능을 능가했습니다. 또한, FLAN은 ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, StoryCloze와 같은 데이터셋에서 GPT-3의 몇샷(few-shot) 성능을 큰 차이로 뛰어넘었습니다. 소거 연구(ablation studies)를 통해, 미세 조정에 사용된 데이터셋의 수, 모델의 규모, 그리고 자연어 명령이 지시 튜닝의 성공에 있어 핵심적인 요소임을 밝혀냈습니다.

![그림 1: 지시 튜닝과 FLAN의 개요](https://cdn-mineru.openxlab.org.cn/extract/29379f1e-bdc9-4a49-93e2-7e6e2fcf6c00/a0debe32e8b1db9df70cd1cedd03d6de204acde7fffe37aaff0db731a4497ddb.jpg)

**그림 1:** 위쪽은 지시 튜닝과 FLAN의 개요를 보여줍니다. 지시 튜닝은 사전 학습된 언어 모델을 명령 형태로 표현된 작업들의 혼합에 대해 미세 조정합니다. 추론 시에는 지시 튜닝 중에 본 적 없는 새로운 작업 유형에 대해 모델을 평가합니다. 아래쪽은 우리가 평가한 10개의 작업 유형 중, 지시 튜닝을 통해 성능이 크게 향상된 세 가지 새로운 작업 유형에서 FLAN의 제로샷 성능을 GPT-3의 제로샷 및 몇샷 성능과 비교한 결과를 보여줍니다.

- **자연어 추론(NLI) 데이터셋**: ANLI R1–R3, CB, RTE
- **독해 데이터셋**: BoolQ, MultiRC, OBQA
- **폐쇄형 질문 응답(Closed-book QA) 데이터셋**: ARC-easy, ARC-challenge, NQ, TriviaQA

\* 주요 기여자들. 저자별 기여 내용은 논문 끝부분에 명시되어 있습니다.

---

## 1 INTRODUCTION

우리의 이전 논의를 바탕으로, 논문의 '1 서론' 섹션을 설명해 드리겠습니다.

---

**1 서론**

대규모 언어 모델(예: GPT-3)(Brown et al., 2020)은 몇샷 학습(few-shot learning)을 놀라울 정도로 잘 수행하는 것으로 보여졌습니다. 그러나 제로샷 학습(zero-shot learning)에서는 그만큼 성공적이지 못합니다. 예를 들어, GPT-3의 제로샷 성능은 독해(reading comprehension), 질문 응답(question answering), 자연어 추론(natural language inference)과 같은 작업에서 몇샷 성능보다 훨씬 저조합니다. 그 이유 중 하나는, 몇샷 예시가 없으면 모델이 사전 학습 데이터의 형식과 유사하지 않은 프롬프트에서 좋은 성능을 내기가 어렵기 때문일 수 있습니다.

이 논문에서는 대규모 언어 모델의 제로샷 성능을 향상시키는 간단한 방법을 탐구하여, 보다 폭넓은 사용자들이 이러한 모델을 활용할 수 있도록 하고자 합니다. 우리의 직관은 NLP 작업들이 자연어 지시문을 통해 설명될 수 있다는 것입니다. 예를 들어 "이 영화 리뷰의 감정은 긍정적인가 부정적인가요?" 또는 "‘How are you’를 중국어로 번역하세요."와 같이 말입니다. 우리는 1,370억 개의 매개변수를 가진 사전 학습된 언어 모델을 사용하여, 자연어 지시문으로 표현된 60개 이상의 NLP 데이터셋의 혼합에 대해 **지시 튜닝(instruction tuning)**을 수행했습니다. 우리는 이렇게 지시 튜닝된 모델을 **FLAN(Finetuned Language Net)**이라고 부릅니다.

FLAN의 보지 못한 작업에 대한 제로샷 성능을 평가하기 위해, 우리는 NLP 데이터셋을 작업 유형별로 클러스터로 그룹화하고, FLAN을 지시 튜닝할 때 각 클러스터를 제외시켜 평가에 사용했습니다. 예를 들어, 그림 1에서 보여지는 것처럼, FLAN의 자연어 추론(NLI) 수행 능력을 평가하기 위해, 우리는 상식 추론, 번역, 감정 분석과 같은 다양한 다른 NLP 작업에 대해 모델을 지시 튜닝했습니다. 이 설정은 FLAN이 지시 튜닝 중에 어떤 자연어 추론 작업도 보지 않았음을 보장하며, 그 후 모델의 제로샷 자연어 추론 능력을 평가합니다.

우리의 평가 결과, FLAN은 기본 1,370억 매개변수 모델의 제로샷 성능을 크게 향상시키는 것으로 나타났습니다. FLAN의 제로샷 성능은 우리가 평가한 25개의 데이터셋 중 20개에서 1,750억 매개변수의 GPT-3의 제로샷 성능을 능가했으며, ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, StoryCloze와 같은 데이터셋에서는 GPT-3의 몇샷 성능을 큰 차이로 뛰어넘었습니다. 소거 실험(ablation studies)에서는 지시 튜닝에서 사용된 작업 클러스터의 수를 늘리면 보지 못한 작업에서의 성능이 향상되며, 충분한 모델 규모가 있을 때에만 지시 튜닝의 이점이 나타난다는 것을 발견했습니다.

**지시 튜닝**은 그림 2에서 보여지듯이, 사전 학습–미세 조정(pretrain–finetune)과 프롬프트(prompting) 패러다임의 매력적인 측면을 결합한 간단한 방법으로, 미세 조정을 통한 지도 학습(supervision)을 사용하여 추론 시의 텍스트 상호작용에 대한 언어 모델의 응답을 개선합니다. 우리의 실험 결과는 언어 모델이 지시문만으로 설명된 작업을 수행하는 데 있어 유망한 능력을 보여줍니다. FLAN에 사용된 지시 튜닝 데이터셋을 로드하는 소스 코드는 공개적으로 이용할 수 있으며, 다음에서 확인할 수 있습니다: https://github.com/google-research/flan.

![그림 2: 지시 튜닝과 사전 학습–미세 조정, 프롬프트의 비교](https://cdn-mineru.openxlab.org.cn/extract/29379f1e-bdc9-4a49-93e2-7e6e2fcf6c00/c203300098f4282debb6203d90f14936eaf33198df267b0e9f6e5de7fb168333.jpg)

**그림 2:** 지시 튜닝과 사전 학습–미세 조정, 그리고 프롬프트를 비교한 그림입니다.

---

**요약하면**, 이 논문에서는 대규모 언어 모델의 제로샷 학습 능력을 향상시키기 위한 **지시 튜닝(instruction tuning)** 방법을 제안하고 있습니다. 자연어 지시문을 통해 표현된 다양한 NLP 작업에 대해 모델을 미세 조정함으로써, 모델이 사전 학습 중에 보지 않은 새로운 작업에서도 높은 성능을 발휘할 수 있게 됩니다. 특히, 이 방법을 통해 개발된 FLAN 모델은 GPT-3보다 더 작은 규모임에도 불구하고 GPT-3의 제로샷 및 몇샷 성능을 능가하는 뛰어난 결과를 보여주었습니다. 이는 언어 모델이 자연어 지시만으로도 새로운 작업을 효과적으로 수행할 수 있음을 시사합니다.

---

## 2 FLAN: INSTRUCTION TUNING IMPROVES ZERO-SHOT LEARNING

사용자와의 이전 논의를 바탕으로, 논문의 '2 FLAN: 지시 튜닝이 제로샷 학습을 향상시킨다' 섹션을 설명해 드리겠습니다.

---

**2 FLAN: 지시 튜닝이 제로샷 학습을 향상시킨다**

**지시 튜닝(instruction tuning)**의 **동기(motivation)**는 언어 모델이 자연어 처리(NLP) **지시문(instructions)**에 응답하는 능력을 향상시키는 데 있습니다. 핵심 아이디어는 지시문을 통해 설명된 작업을 수행하도록 언어 모델을 **지도 학습(supervision)**을 통해 훈련하면, 모델이 지시문을 따르는 법을 배우게 되고, 이는 **보지 못한 새로운 작업(unseen tasks)**에서도 지시를 따라 수행할 수 있게 된다는 것입니다.

모델이 **보지 못한 작업에 대한 성능**을 평가하기 위해, 우리는 데이터셋을 **작업 유형별로 클러스터(cluster)**로 그룹화합니다. 그리고 지시 튜닝을 수행할 때, 각 작업 클러스터를 하나씩 제외시켜 해당 클러스터를 평가에 사용합니다. 예를 들어, 자연어 추론(NLI) 작업 클러스터를 평가하고자 할 때, NLI에 속한 모든 데이터셋을 지시 튜닝에서 제외하고, 나머지 작업들의 데이터셋에 대해 지시 튜닝을 진행합니다. 그런 다음, 제외된 NLI 클러스터에 대해 모델의 제로샷 성능을 평가합니다.

이러한 방법을 통해, 모델이 특정 작업 유형에 대한 직접적인 훈련 없이도 지시문만으로 새로운 작업을 수행할 수 있는지를 확인할 수 있습니다. 이는 모델의 지시 수행 능력이 일반화되어, 다양한 작업에 적용될 수 있음을 보여줍니다.

---

**설명 및 요약**

- **지시 튜닝의 목적**: 언어 모델이 다양한 NLP 지시문에 정확하고 효과적으로 응답하는 능력을 향상시키기 위함입니다.
  
- **핵심 아이디어**: 지시문을 통한 지도 학습으로 모델이 지시문을 따르는 방법을 배우면, 이후에 보지 못한 작업에서도 지시문만으로 작업을 수행할 수 있게 됩니다.

- **평가 방법**: 데이터셋을 작업 유형별로 클러스터링하고, 각 작업 클러스터를 지시 튜닝에서 제외한 후 해당 클러스터에 대한 모델의 제로샷 성능을 평가합니다. 이는 모델이 해당 작업 유형에 대한 직접적인 훈련 없이도 얼마나 잘 수행하는지를 측정합니다.

- **결과의 의미**: 모델이 지시 튜닝을 통해 지시문을 따르는 일반적인 능력을 획득하면, 새로운 작업에서도 지시문만으로 높은 성능을 발휘할 수 있습니다. 이는 제로샷 학습 능력을 크게 향상시켜, 모델의 활용 범위를 넓힐 수 있음을 나타냅니다.

---

**결론적으로**, 이 섹션에서는 지시 튜닝이 대규모 언어 모델의 제로샷 학습 능력을 향상시키는 방법과 그 평가 방법을 설명하고 있습니다. 지시 튜닝은 모델이 지시문을 이해하고 따르는 능력을 배양하여, 직접 훈련하지 않은 새로운 작업에서도 지시문만으로 효과적으로 작업을 수행할 수 있게 합니다. 이를 통해 언어 모델의 범용성과 활용 가능성을 크게 높일 수 있습니다.

---

## 2.1 TASKS & TEMPLATES

사용자와의 이전 논의를 바탕으로, 논문의 '2.1 작업과 템플릿' 섹션을 설명해 드리겠습니다.

---

**2.1 작업과 템플릿**

처음부터 다양한 작업을 포함하는 지시 튜닝 데이터셋을 만드는 것은 많은 자원이 소모되므로, 우리는 연구 커뮤니티에서 널리 사용되는 기존 데이터셋을 **지시 형식(instructional format)**으로 변환하는 방식을 택했습니다. 구체적으로, 우리는 Tensorflow Datasets에서 공개적으로 이용할 수 있는 62개의 텍스트 데이터셋을 모아 하나의 **단일 혼합 데이터셋(single mixture)**으로 결합했습니다. 이 데이터셋에는 **언어 이해(NLU)** 작업과 **언어 생성(NLG)** 작업이 모두 포함되어 있습니다.

**그림 3**은 이러한 데이터셋을 보여주고 있으며, 각 데이터셋은 12개의 **작업 클러스터(task clusters)** 중 하나로 분류되었습니다. 동일한 클러스터에 속한 데이터셋들은 동일한 작업 유형을 공유합니다. 각 데이터셋의 상세한 설명, 크기, 예시는 부록 G에 제공되어 있습니다.

![그림 3: 이 논문에서 사용된 데이터셋과 작업 클러스터 (파란색은 NLU 작업, 청록색은 NLG 작업)](https://cdn-mineru.openxlab.org.cn/extract/29379f1e-bdc9-4a49-93e2-7e6e2fcf6c00/fa3f0469c482710819b18bb8d13bd864d2939cdf9f8aaaa70ce546435dcd2914.jpg)

각 데이터셋마다, 우리는 해당 작업을 설명하는 **자연어 지시문(natural language instructions)**을 사용하여 **10개의 고유한 템플릿(unique templates)**을 수작업으로 작성했습니다. 이 10개의 템플릿 중 대부분은 원래 작업을 설명하지만, **다양성을 높이기 위해** 각 데이터셋마다 최대 3개의 템플릿은 작업을 "반대로 전환(turning the task around)"하는 것을 포함했습니다. 예를 들어, **감정 분류(sentiment classification)**의 경우, 영화 리뷰를 생성하도록 요청하는 템플릿을 포함했습니다.

그런 다음, 우리는 모든 데이터셋의 혼합물에 대해 사전 학습된 언어 모델을 지시 튜닝했습니다. 이때 각 데이터셋의 예제들은 해당 데이터셋의 템플릿 중 **무작위로 선택된 지시 템플릿**을 사용하여 형식화되었습니다. **그림 4**는 자연어 추론(NLI) 데이터셋에 대한 여러 지시 템플릿의 예를 보여줍니다.

![그림 4: 자연어 추론 작업을 설명하는 여러 지시 템플릿](https://cdn-mineru.openxlab.org.cn/extract/29379f1e-bdc9-4a49-93e2-7e6e2fcf6c00/75054b1f384f86c8d7c5a2817fb8edbf104680e229671fee1679a22d6523a502.jpg)

---

**요약하자면**, 이 섹션에서는 FLAN 모델의 지시 튜닝을 위해 어떻게 **작업(Task)**과 **지시 템플릿(Templates)**을 구성했는지 설명하고 있습니다.

- **데이터셋 수집**: 
  - **62개의 공개 데이터셋**을 Tensorflow Datasets에서 수집했습니다.
  - 데이터셋은 언어 이해(NLU)와 언어 생성(NLG) 작업을 모두 포함합니다.

- **작업 클러스터링**:
  - 데이터셋들을 **12개의 작업 클러스터**로 분류했습니다.
  - 동일한 클러스터에 속한 데이터셋들은 동일한 작업 유형(예: 번역, 요약, 감정 분석 등)을 공유합니다.

- **지시 템플릿 작성**:
  - 각 데이터셋마다 **10개의 고유한 자연어 지시 템플릿**을 수동으로 작성했습니다.
  - 대부분의 템플릿은 원래 작업을 설명하지만, 일부 템플릿은 **작업을 반대로 전환**하여 다양성을 높였습니다.
    - 예: 감정 분류 작업에서 리뷰의 감정을 판단하는 대신, 특정 감정에 맞는 리뷰를 작성하도록 요청하는 템플릿.

- **지시 튜닝 수행**:
  - 사전 학습된 언어 모델을 모든 데이터셋의 혼합물에 대해 지시 튜닝했습니다.
  - 각 데이터셋의 예제들은 해당 데이터셋의 템플릿 중 무작위로 선택된 템플릿을 사용하여 형식화되었습니다.

이를 통해 모델은 다양한 작업과 지시문에 노출되어, 지시문을 이해하고 따르는 일반적인 능력을 발전시킬 수 있었습니다. 특히, 작업을 반대로 전환한 템플릿은 모델의 생성 능력을 강화하고, 지시문 해석의 유연성을 높이는 데 기여했습니다.

---

**결론적으로**, 이 섹션에서는 FLAN 모델이 지시문 기반 학습을 통해 제로샷 성능을 향상시키기 위해, 어떻게 다양한 NLP 작업과 지시 템플릿을 준비하고 활용했는지를 상세히 설명하고 있습니다. 기존의 풍부한 데이터셋을 재구성하여 지시 튜닝에 활용함으로써, 모델의 학습 효율성을 높이고 자원 소모를 줄일 수 있었습니다. 이러한 접근은 언어 모델이 지시문만으로도 새로운 작업을 효과적으로 수행할 수 있게 하는 데 핵심적인 역할을 합니다.

---

## 2.2 EVALUATION SPLITS

사용자와의 이전 논의를 바탕으로, 논문의 '2.2 평가 분할(EVALUATION SPLITS)' 섹션을 설명해 드리겠습니다.

---

**2.2 평가 분할(EVALUATION SPLITS)**

우리는 FLAN 모델이 **지시 튜닝(instruction tuning)** 중에 보지 못한 작업에서 어떻게 성능을 발휘하는지에 관심이 있습니다. 따라서, **'보지 못한 작업(unseen task)'**을 어떻게 정의하는지가 매우 중요합니다.

일부 이전 연구에서는 **훈련 시 동일한 데이터셋이 포함되지 않도록** 하여 보지 못한 작업을 정의했습니다. 그러나 우리는 **그림 3**에서의 **작업 클러스터(task clusters)**를 활용하여, 더 **보수적인 정의**를 사용합니다.

이 논문에서, 우리는 어떤 데이터셋 **𝔻**가 평가 시에 보지 못한 것으로 간주하려면, **𝔻**가 속한 **모든 작업 클러스터의 데이터셋**이 지시 튜닝 중에 사용되지 않아야 한다고 규정합니다. 즉, 만약 데이터셋 **𝔻**가 **함의(entailment)** 작업에 속한다면, 지시 튜닝 중에 어떤 함의 데이터셋도 등장하지 않았어야 하며, 대신 다른 모든 작업 클러스터에 대해 지시 튜닝을 수행합니다.

따라서, **\(c\)개의 작업 클러스터**에서 FLAN의 제로샷 성능을 평가하기 위해, 우리는 **\(c\)개의 모델**을 지시 튜닝합니다. 각 모델은 평가를 위해 서로 다른 작업 클러스터를 제외하고 지시 튜닝을 수행합니다.

**주석 1**: **상식이 포함된 독해(read. comp. with commonsense)** 클러스터를 평가할 때는, **독해(reading comprehension)**와 **상식 추론(commonsense reasoning)** 클러스터 둘 다 지시 튜닝에서 제외했습니다. 반대로, 독해나 상식 추론을 평가할 때는, 지시 튜닝에서 상식이 포함된 독해 클러스터를 사용하지 않았습니다. 또한, **의역(paraphrase)** 클러스터를 자연어 추론(NLI) 작업을 평가할 때 지시 튜닝에서 제외했으며, 그 반대의 경우도 마찬가지였습니다.

---

**설명 및 요약**

- **보지 못한 작업의 정의**:
  - 우리가 보지 못한 작업으로 간주하는 데이터셋은, 그 데이터셋이 속한 작업 클러스터의 **어떤 데이터셋도** 지시 튜닝 중에 사용되지 않은 경우입니다.
  - 이는 작업 유형 전체를 모델이 보지 못하도록 함으로써, 모델이 진정으로 새로운 작업에 얼마나 일반화할 수 있는지를 평가합니다.

- **모델 평가 방법**:
  - 총 **\(c\)개의 작업 클러스터**가 있을 때, 각 클러스터에 대해 하나씩 **\(c\)개의 모델**을 지시 튜닝합니다.
  - 각 모델은 하나의 클러스터를 제외하고 나머지 클러스터의 데이터셋으로 지시 튜닝을 수행합니다.
  - 그런 다음, 제외된 클러스터의 데이터셋에 대해 모델의 제로샷 성능을 평가합니다.

- **특수한 고려사항**:
  - **주석 1**에서 언급된 것처럼, 일부 작업 클러스터는 내용이 겹치거나 유사성이 있을 수 있으므로, 평가의 정확성을 위해 관련 클러스터들을 함께 제외했습니다.
    - 예를 들어, **상식이 포함된 독해** 클러스터를 평가할 때는 **독해**와 **상식 추론** 클러스터 모두를 지시 튜닝에서 제외했습니다.
    - 마찬가지로, **자연어 추론(NLI)** 작업을 평가할 때는 **의역(paraphrase)** 클러스터를 지시 튜닝에서 제외하고, 그 반대의 경우도 동일하게 적용했습니다.

이러한 평가 방식을 통해, 우리는 FLAN 모델이 지시 튜닝 중에 보지 못한 **완전히 새로운 작업 유형**에서도 지시문만으로 얼마나 잘 작업을 수행할 수 있는지를 엄격하게 평가할 수 있습니다. 이는 모델의 제로샷 학습 능력과 지시 수행 능력의 일반화 범위를 측정하는 데 중요한 역할을 합니다.

---

**결론적으로**, 이 섹션에서는 FLAN 모델의 제로샷 성능을 정확하게 평가하기 위한 **평가 분할(evaluation splits)** 방법을 설명하고 있습니다. 작업 클러스터 전체를 지시 튜닝에서 제외함으로써, 모델이 새로운 작업 유형에 대해 지시문만으로 얼마나 잘 일반화할 수 있는지를 평가합니다. 이는 모델의 실질적인 제로샷 학습 능력을 측정하는 데 더 엄격하고 보수적인 접근 방법입니다.

---

## 2.3 CLASSIFICATION WITH OPTIONS

사용자와의 이전 논의를 바탕으로, 논문의 '2.3 분류에서의 선택지 사용' 섹션을 설명해 드리겠습니다.

---

**2.3 분류에서의 선택지 사용(CLASSIFICATION WITH OPTIONS)**

언어 모델에서 수행하는 작업의 출력 형태는 크게 두 가지로 나뉩니다:

1. **여러 개의 클래스 중 하나를 선택하는 분류 작업** (classification)
2. **자유로운 텍스트를 생성하는 생성 작업** (generation)

**FLAN**은 디코더 전용 언어 모델(decoder-only language model)의 지시 튜닝 버전이므로, 자연스럽게 **자유 텍스트 형태로 응답**합니다. 따라서 **생성 작업**의 경우 추가적인 수정 없이도 문제없이 수행할 수 있습니다.

그러나 **분류 작업**에 대해서는 추가적인 고려가 필요합니다. 이전 연구(Brown et al., 2020)에서는 **랭크 분류(rank classification)** 접근법을 사용했습니다. 예를 들어, 가능한 출력이 "예"와 "아니오" 두 가지인 경우, 모델은 이 두 가지 출력에 대한 확률을 계산하고, 더 높은 확률을 가진 것을 예측값으로 선택합니다.

하지만 이 방법에는 몇 가지 문제가 있습니다:

- **불완전성**: 이 절차는 논리적으로 타당하지만, **답변의 확률 분포가 원하는 방식으로 나오지 않을 수 있습니다**.
- **표현 방식의 다양성 문제**: 예를 들어, "예"를 표현하는 방법이 여러 가지 있는 경우(예: "네", "예요", "맞습니다" 등), "예"에 할당된 확률 질량(probability mass)이 분산되어 실제 "예"라는 단어에 할당되는 확률이 낮아질 수 있습니다.

이를 해결하기 위해, 우리는 **옵션 접미사(options suffix)**를 도입했습니다. 이는 **분류 작업의 끝에 'OPTIONS' 토큰**과 함께 해당 작업의 **출력 클래스 목록을 추가**하는 것입니다. 이를 통해 모델은 분류 작업에 응답할 때 어떤 선택지가 요구되는지를 명확히 알 수 있습니다.

**예시**:

- **자연어 추론(NLI)**이나 **상식 추론(commonsense reasoning)** 작업에서, 입력 문장의 끝에 'OPTIONS' 토큰과 가능한 답변 옵션들을 나열합니다. 예를 들어:

  ```
  질문: "두 문장이 서로 모순되는지 판단하세요."
  OPTIONS: "참", "거짓", "모름"
  ```

이렇게 하면 모델이 응답할 때 **주어진 옵션 중에서 선택**하도록 유도되며, 이전의 랭크 분류 방법에서 발생했던 확률 분포의 문제를 완화할 수 있습니다.

**그림 1**에서 이러한 옵션 사용의 예시를 확인할 수 있습니다. 이는 자연어 추론과 상식 추론 작업에서 옵션 접미사를 활용하여 모델이 원하는 방식으로 응답하도록 하는 방법을 보여줍니다.

---

**요약 및 추가 설명**

- **문제점 인식**:
  - 분류 작업에서 언어 모델이 자유 텍스트로 응답하면, 원하는 클래스 이외의 표현으로 답변할 수 있어 정확도가 떨어질 수 있습니다.
  - 기존의 랭크 분류 방법은 가능한 출력에 대한 확률을 비교하지만, 다양한 표현 방식 때문에 확률이 분산될 수 있습니다.

- **해결책**:
  - **옵션 접미사**를 사용하여 모델이 선택해야 하는 **정확한 출력 클래스들을 명시**합니다.
  - 이는 모델이 응답해야 할 **선택지를 명확히 알게 하여**, 분류 작업의 정확도를 향상시킵니다.

- **효과**:
  - 모델이 분류 작업에서 원하는 출력 중 하나를 정확히 선택하도록 도와줍니다.
  - 확률 분포의 문제를 줄이고, 모델의 응답 일관성을 높입니다.

---

**결론적으로**, 이 섹션에서는 분류 작업에서 언어 모델의 응답을 개선하기 위한 **옵션 접미사** 기법을 소개합니다. 이는 모델이 자유 텍스트로 응답하는 특성을 활용하면서도, 분류 작업에서는 **명시적인 선택지 제공**을 통해 정확한 예측을 가능하게 합니다. 이를 통해 FLAN 모델의 분류 작업 성능을 향상시킬 수 있었습니다.

---

## 2.4 TRAINING DETAILS

사용자와의 이전 논의를 바탕으로, 논문의 '2.4 훈련 세부사항(TRAINING DETAILS)' 섹션을 설명해 드리겠습니다.

---

**2.4 훈련 세부사항(TRAINING DETAILS)**

**모델 아키텍처와 사전 학습**

우리의 실험에서 우리는 **LaMDA-PT**라는 모델을 사용했습니다. 이 모델은 1,370억 개의 매개변수를 가진, 밀집(dense) 좌에서 우로 읽는(decoder-only) 트랜스포머 언어 모델입니다(Thoppilan et al., 2022). LaMDA-PT는 웹 문서(컴퓨터 코드가 포함된 문서도 포함), 대화 데이터(dialog data), 위키피디아(Wikipedia)로 구성된 컬렉션으로 사전 학습되었습니다. 이 데이터는 SentencePiece 라이브러리(Kudo & Richardson, 2018)를 사용하여 32,000개의 어휘(vocabulary)로 2.49조(terabyte) 개의 BPE 토큰으로 토크나이즈(tokenize)되었습니다. 사전 학습 데이터의 약 10%는 영어 이외의 언어로 구성되어 있습니다. 참고로, LaMDA-PT는 언어 모델 사전 학습만을 수행했으며(대화에 맞게 미세 조정된 LaMDA와는 달리), 대화에 대한 미세 조정은 포함되지 않았습니다.

**지시 튜닝 절차**

FLAN은 LaMDA-PT의 지시 튜닝(instruction-tuned) 버전입니다. 우리의 지시 튜닝 파이프라인은 모든 데이터셋을 혼합하고, 각 데이터셋에서 무작위로 샘플링합니다. 데이터셋의 크기가 다양하기 때문에 균형을 맞추기 위해, 각 데이터셋당 훈련 예제의 수를 최대 30,000개로 제한하고, 예제 수에 비례한 혼합 방식(examples-proportional mixing scheme)(Raffel et al., 2020)을 따르며, 최대 혼합 비율을 3,000으로 설정했습니다.⁽²⁾ 우리는 모든 모델을 학습률 3e-5를 가진 Adafactor 옵티마이저(Shazeer & Stern, 2018)를 사용하여, 배치 크기 8,192 토큰으로 총 30,000번의 그래디언트 스텝(gradient steps) 동안 미세 조정했습니다. 미세 조정에 사용된 입력 시퀀스 길이와 목표 시퀀스 길이는 각각 1024와 256입니다. 우리는 패킹(packing)(Raffel et al., 2020)을 사용하여 여러 훈련 예제를 하나의 시퀀스로 결합하고, 특별한 EOS 토큰을 사용하여 입력과 목표를 분리했습니다. 이 지시 튜닝 과정은 128코어의 TPUv3에서 약 60시간이 소요되었습니다. 모든 평가에서는 30,000 스텝 동안 훈련된 최종 체크포인트의 결과를 보고합니다.

---

⁽²⁾ 최대 혼합 비율(mixing rate maximum)을 3,000으로 설정했다는 의미입니다.

---

**요약 및 추가 설명**

- **모델 아키텍처**:
  - 사용된 모델은 **LaMDA-PT**로, 1,370억 개의 매개변수를 가진 디코더 전용 트랜스포머 언어 모델입니다.
  - 이 모델은 좌에서 우로 단방향으로 텍스트를 생성합니다.
  - LaMDA-PT는 대화를 위한 미세 조정 없이 언어 모델 사전 학습만을 수행했습니다.

- **사전 학습 데이터**:
  - 웹 문서(컴퓨터 코드 포함), 대화 데이터, 위키피디아 등의 다양한 소스로부터 수집된 텍스트를 사용했습니다.
  - 데이터는 SentencePiece를 사용하여 32,000개의 어휘로 토크나이즈되었으며, 총 2.49조 개의 BPE 토큰으로 구성되었습니다.
  - 약 10%의 데이터는 영어 이외의 언어로 구성되어 있습니다.

- **지시 튜닝 방법**:
  - 모든 데이터셋을 혼합하여 각 데이터셋에서 무작위로 샘플링했습니다.
  - 데이터셋 크기의 불균형을 조정하기 위해, 각 데이터셋당 훈련 예제의 수를 최대 30,000개로 제한했습니다.
  - **예제 수에 비례한 혼합 방식**을 적용하여, 데이터셋 간의 균형을 맞추었습니다(Raffel et al., 2020).
  - 혼합 비율의 최대값을 3,000으로 설정했습니다.
  - **Adafactor 옵티마이저**(Shazeer & Stern, 2018)를 사용하여, 학습률은 3×10<sup>−5</sup>로 설정했습니다.
  - 배치 크기는 8,192 토큰이며, 총 30,000번의 그래디언트 스텝 동안 미세 조정했습니다.
  - 입력 시퀀스 길이는 1024, 목표 시퀀스 길이는 256입니다.
  - **패킹(packing)** 기법을 사용하여 여러 훈련 예제를 하나의 시퀀스로 결합하였고, 입력과 목표는 **특수한 EOS 토큰**으로 구분했습니다.
  - 지시 튜닝은 128코어의 TPUv3에서 약 60시간이 소요되었습니다.

- **평가 방법**:
  - 모든 평가에서는 30,000 스텝 동안 훈련된 최종 체크포인트를 사용하여 결과를 보고했습니다.

---

**전체적으로**, 이 섹션에서는 FLAN 모델을 만들기 위한 **훈련 세부사항**을 설명하고 있습니다. 사전 학습된 LaMDA-PT 모델을 기반으로, 다양한 데이터셋을 균형 있게 혼합하여 지시 튜닝을 수행했습니다. 훈련 과정에서 모델의 효율성과 성능을 높이기 위해 데이터 전처리 및 학습 설정을 신중하게 조정했습니다. 이를 통해 FLAN 모델은 지시문에 따라 다양한 작업을 수행하는 능력을 갖추게 되었습니다.

---

## 3 RESULTS

사용자와의 이전 논의를 바탕으로, 논문의 '3 결과(RESULTS)' 섹션을 설명해 드리겠습니다.

---

**3 결과**

우리는 FLAN을 다양한 작업에서 평가했습니다: **자연어 추론(NLI)**, **독해(Reading Comprehension)**, **폐쇄형 질문 응답(Closed-book QA)**, **번역(Translation)**, **상식 추론(Commonsense Reasoning)**, **공지시어 분석(Coreference Resolution)**, **구조화된 입력을 텍스트로 변환(Struct-to-Text)** 등입니다.

$\S2.2$에서 설명한 대로, 우리는 데이터셋을 **작업 클러스터(task clusters)**로 그룹화하고, 각 클러스터를 평가를 위해 제외한 상태로 나머지 클러스터에 대해 FLAN을 지시 튜닝했습니다(즉, 각 평가 작업 클러스터는 다른 체크포인트를 사용합니다). 각 데이터셋에 대해, 우리는 해당 데이터셋의 모든 템플릿에 대한 성능의 평균을 평가했습니다. 이는 일반적인 자연어 지시문이 주어졌을 때의 기대 성능을 대변합니다. 또한, 일부 이전 연구(Brown et al., 2020)에서 개발 셋(dev set)을 사용하여 수동으로 프롬프트 엔지니어링을 수행할 수 있으므로, 각 데이터셋에 대해 개발 셋에서 최고의 성능을 보인 템플릿을 사용하여 테스트 셋 성능도 측정했습니다.

**비교를 위해**, 우리는 GPT-3와 동일한 프롬프트를 사용하여 LaMDA-PT의 제로샷과 몇샷 결과를 보고합니다(LaMDA-PT는 지시 튜닝 없이 자연어 지시문에 적합하지 않기 때문입니다). 이 베이스라인은 지시 튜닝이 얼마나 도움이 되는지를 직접적으로 보여줍니다. **지시 튜닝은 대부분의 데이터셋에서 LaMDA-PT의 성능을 크게 향상시켰습니다.**

또한, 우리는 해당 논문들에서 보고된 GPT-3 175B(Brown et al., 2020)와 GLaM 64B/64E(Du et al., 2021)의 제로샷 성능도 보여줍니다. 개발 셋에서 최고의 템플릿을 사용했을 때, 제로샷 FLAN은 우리가 평가한 25개의 데이터셋 중 20개에서 제로샷 GPT-3를 능가했으며, 10개의 데이터셋에서는 GPT-3의 몇샷 성능을 넘어섰습니다. 또한, 제로샷 FLAN은 19개의 사용 가능한 데이터셋 중 13개에서 제로샷 GLaM을 능가했으며, 11개 데이터셋에서는 1샷 GLaM보다도 뛰어난 성능을 보였습니다.

**전반적으로**, 지시 튜닝은 지시문으로 자연스럽게 표현되는 작업(NLI, QA, 번역, Struct-to-Text)에서 매우 효과적임을 관찰했습니다. 반면, 지시문이 크게 필요하지 않은 언어 모델링으로 직접 구성된 작업(예: 문장이나 단락을 완성하는 형태의 상식 추론이나 공지시어 분석 작업)에서는 그 효과가 덜했습니다. 자연어 추론, 독해, 폐쇄형 QA, 번역에 대한 결과를 **그림 5**에 요약하였으며, 아래에서 자세히 설명하겠습니다.

---

![그림 5: FLAN의 제로샷 성능을 LaMDA-PT 137B, GPT-3 175B, GLaM 64B/64E와 비교한 결과 (자연어 추론, 독해, 폐쇄형 QA, 번역). 각 작업당 최대 10개의 지시 템플릿의 평균 성능을 표시했습니다. 지도 학습된 모델은 T5, BERT 또는 번역 모델이며(부록의 표 2 및 표 1에 명시됨), FLAN의 성능은 각 작업별 최대 10개의 지시 템플릿의 평균입니다.](https://cdn-mineru.openxlab.org.cn/extract/29379f1e-bdc9-4a49-93e2-7e6e2fcf6c00/e1ae3b74a06fd742a1572e42f326be0c0b75a4dbae4a4bab9fe890dd40160889.jpg)

**그림 5:** FLAN의 제로샷 성능을 LaMDA-PT 137B, GPT-3 175B, GLaM 64B/64E와 비교한 결과입니다. 자연어 추론, 독해, 폐쇄형 QA, 번역 작업에 대한 성능이 포함되어 있습니다. FLAN의 성능은 각 작업별 최대 10개의 지시 템플릿의 평균입니다. 지도 학습된 모델은 T5, BERT 또는 번역 모델이며(부록의 표 2 및 표 1에 명시됨), 자세한 내용은 아래에서 설명합니다.

---

### **자연어 추론(NLI)**

다섯 개의 NLI 데이터셋에서, 모델은 주어진 전제(Premise)로부터 가설(Hypothesis)이 참인지 결정해야 합니다. **FLAN은 모든 베이스라인을 큰 차이로 능가했습니다.** Brown et al.(2020)이 언급한 바와 같이, GPT-3가 NLI에서 어려움을 겪는 이유 중 하나는 NLI 예제가 비지도 학습 데이터에서 자연스럽게 나타나지 않기 때문이며, 따라서 문장의 연속으로 어색하게 표현된다는 것입니다. FLAN에서는 NLI를 다음과 같은 보다 자연스러운 질문으로 표현했습니다: "**<전제>**가 **<가설>**를 뜻합니까?" 이를 통해 훨씬 높은 성능을 달성할 수 있었습니다.

### **독해(Reading Comprehension)**

독해 작업에서는 모델이 주어진 글(지문)에 대한 질문에 답해야 합니다. FLAN은 **MultiRC(Khashabi et al., 2018)**와 **OBQA(Mihaylov et al., 2018)**에서 베이스라인을 능가했습니다. **BoolQ(Clark et al., 2019a)**에서는 FLAN이 GPT-3를 큰 차이로 앞섰지만, LaMDA-PT 자체도 BoolQ에서 이미 높은 성능을 보였습니다.

### **폐쇄형 질문 응답(Closed-book QA)**

폐쇄형 QA에서는 모델이 답변을 포함한 특정 정보를 참조하지 않고 세상에 대한 질문에 답해야 합니다. FLAN은 네 개의 모든 데이터셋에서 GPT-3를 능가했습니다. GLaM과 비교했을 때, FLAN은 **ARC-easy**와 **ARC-challenge(Clark et al., 2018)**에서 더 나은 성능을 보였으며, **NQ(Lee et al., 2019; Kwiatkowski et al., 2019)**와 **TQA(Joshi et al., 2017)**에서는 약간 낮은 성능을 보였습니다.

### **번역(Translation)**

GPT-3와 마찬가지로, LaMDA-PT의 사전 학습 데이터는 약 90%가 영어이며, 모델을 기계 번역을 수행하도록 특별히 훈련하지는 않았지만 다른 언어의 텍스트도 일부 포함되어 있습니다. 우리는 GPT-3 논문에서 평가한 세 개의 데이터셋에 대해 FLAN의 기계 번역 성능을 평가했습니다: **WMT'14**의 프랑스어–영어(Bojar et al., 2014), **WMT'16**의 독일어–영어 및 루마니아어–영어(Bojar et al., 2016) 입니다.

GPT-3와 비교하여, FLAN은 모든 여섯 번의 평가에서 제로샷 GPT-3를 능가했지만, 대부분의 경우 GPT-3의 몇샷 성능보다는 낮은 성능을 보였습니다. GPT-3와 마찬가지로, FLAN은 영어로의 번역에서 강력한 성능을 보여주었으며, 지도 학습된 번역 베이스라인과도 비교할 만한 결과를 보였습니다. 그러나 영어에서 다른 언어로의 번역은 상대적으로 약했는데, 이는 FLAN이 영어 SentencePiece 토크나이저를 사용하고 사전 학습 데이터의 대부분이 영어인 것을 감안하면 예상할 수 있는 결과입니다.

---

### **추가 작업**

위에서 언급한 작업 클러스터에 대해 강력한 결과를 보았지만, 지시 튜닝의 한계 중 하나는 많은 언어 모델링 작업(예: 문장 완성으로 구성된 상식 추론이나 공지시어 분석 작업)에서 성능을 향상시키지 못한다는 것입니다. 일곱 개의 상식 추론 및 공지시어 분석 작업(부록의 표 2에서 확인 가능)에서, FLAN은 일곱 개 중 세 개의 작업에서만 LaMDA-PT를 능가했습니다. 이러한 부정적인 결과는 다운스트림 작업이 원래의 언어 모델링 사전 학습 목표와 동일한 경우(즉, 지시문이 거의 필요하지 않은 경우), 지시 튜닝이 유용하지 않음을 나타냅니다.

마지막으로, 우리는 감정 분석, 의역 탐지, 구조화된 입력을 텍스트로 변환하는 작업, 그리고 GPT-3 결과가 없는 추가 데이터셋에 대한 결과를 부록의 표 2와 표 1에 보고했습니다. 일반적으로, 제로샷 FLAN은 제로샷 LaMDA-PT를 능가했으며, LaMDA-PT의 몇샷 성능과 비교하여 동등하거나 더 나은 성능을 보였습니다.

---

**요약하면**, FLAN은 지시 튜닝을 통해 베이스 모델의 제로샷 성능을 크게 향상시켰습니다. 특히, 지시문으로 자연스럽게 표현될 수 있는 작업(NLI, QA, 번역 등)에서 뛰어난 성능을 보였습니다. 이는 지시 튜닝이 모델이 지시문을 이해하고 따르는 능력을 향상시켜, 새로운 작업에서도 높은 성능을 발휘할 수 있게 함을 보여줍니다. 그러나 언어 모델링 자체와 유사한 작업에서는 지시 튜닝의 효과가 미미했는데, 이는 지시문이 크게 필요하지 않기 때문입니다.

FLAN은 개발 셋에서 최고의 템플릿을 사용했을 때, GPT-3보다 작은 규모의 모델임에도 불구하고 제로샷에서 GPT-3를 능가했으며, 일부 작업에서는 GPT-3의 몇샷 성능보다도 우수한 결과를 보였습니다. 이는 지시 튜닝이 모델의 효율성과 성능을 향상시키는 강력한 방법임을 시사합니다.

---

**결론적으로**, 이 섹션에서는 FLAN의 평가 결과를 상세히 설명하고 있으며, 지시 튜닝이 대규모 언어 모델의 제로샷 학습 능력을 크게 향상시킨다는 것을 보여주고 있습니다. 다양한 작업에서의 성능 향상은 모델이 지시문을 이해하고 따르는 능력을 효과적으로 학습했음을 나타내며, 이는 언어 모델의 활용 범위를 넓히는 데 중요한 의미를 갖습니다.

---

## 4 ABLATION STUDIES & FURTHER ANALYSIS

사용자와의 이전 논의를 바탕으로, 논문의 '4 소거 연구 및 추가 분석(ABRATION STUDIES & FURTHER ANALYSIS)' 섹션을 설명해 드리겠습니다.

---

**4 소거 연구 및 추가 분석**

이 섹션에서는 FLAN 모델의 성능에 영향을 미치는 다양한 요소들을 분석하기 위해 여러 **소거 연구(ablation studies)**를 수행하고, 추가적인 실험과 분석을 통해 모델의 동작을 더 깊이 이해하고자 합니다.

### **4.1 데이터셋 수의 영향**

**지시 튜닝에 사용된 데이터셋의 수가 모델의 성능에 미치는 영향**을 조사했습니다. 이를 위해, 사용된 데이터셋의 수를 단계적으로 늘려가며 모델의 제로샷 성능 변화를 측정했습니다.

- **결과**: 지시 튜닝에 포함된 데이터셋의 수가 증가할수록 모델의 제로샷 성능이 꾸준히 향상되었습니다.
- **해석**: 다양한 작업과 지시문에 노출된 모델이 새로운 작업에서도 지시문만으로 효과적으로 대응할 수 있음을 시사합니다.

### **4.2 모델 규모의 영향**

**모델의 크기(매개변수의 수)가 지시 튜닝의 효과에 미치는 영향**을 분석했습니다. 작은 모델부터 대형 모델까지 다양한 규모의 모델에 대해 지시 튜닝을 수행했습니다.

- **결과**: 모델의 규모가 커질수록 지시 튜닝의 이점이 더욱 뚜렷하게 나타났습니다. 대형 모델은 지시 튜닝 후 제로샷 성능이 크게 향상되었지만, 작은 모델에서는 그 효과가 미미했습니다.
- **해석**: 대규모 언어 모델이 지시문을 이해하고 일반화하는 능력이 더 뛰어나며, 지시 튜닝의 혜택을 더 많이 받을 수 있음을 보여줍니다.

### **4.3 자연어 지시문의 중요성**

**지시 튜닝 시 자연어 지시문을 포함하는 것의 효과**를 평가하기 위해, 지시문이 포함된 경우와 포함되지 않은 경우를 비교했습니다.

- **결과**: 자연어 지시문을 포함한 모델이 그렇지 않은 모델에 비해 제로샷 성능이 크게 향상되었습니다.
- **해석**: 지시문 자체가 모델이 작업의 목적과 요구 사항을 이해하는 데 핵심적인 역할을 하며, 이를 통해 새로운 작업에서도 지시문만으로 높은 성능을 발휘할 수 있게 됩니다.

### **4.4 지시문의 다양성과 표현 방식**

**다양한 형태와 표현 방식의 지시문이 모델의 성능에 미치는 영향**을 분석했습니다.

- **결과**: 지시문의 다양성이 높을수록 모델이 새로운 지시문이나 표현 방식에도 유연하게 대응할 수 있었습니다.
- **해석**: 훈련 중에 다양한 지시문에 노출된 모델은 지시문의 표현 방식에 민감하지 않으며, 이는 현실 세계에서 사용자의 다양한 지시 요청에 대응하는 데 중요합니다.

### **4.5 추가 분석: 예시 수의 영향**

**몇샷 학습에서 제공되는 예시의 수가 모델의 성능에 미치는 영향**을 조사했습니다.

- **결과**: 지시 튜닝을 통해 모델이 지시문만으로도 높은 성능을 보이므로, 몇샷 학습에서 예시의 수를 줄여도 성능 저하가 크지 않았습니다.
- **해석**: 이는 지시 튜닝이 모델의 일반화 능력을 향상시켜, 추가적인 예시 없이도 작업을 수행할 수 있게 함을 의미합니다.

---

**요약하면**, 이 섹션에서는 FLAN 모델의 성능에 영향을 주는 주요 요소들을 심층적으로 분석하고 있습니다. **데이터셋의 수와 다양성**, **모델의 규모**, **자연어 지시문의 사용 및 표현 방식** 등이 지시 튜닝의 성공에 핵심적인 역할을 한다는 것을 발견했습니다. 이러한 분석을 통해, 모델의 제로샷 학습 능력을 극대화하고, 새로운 작업에서도 높은 성능을 발휘할 수 있는 방향성을 제시하고 있습니다.

---

**결론적으로**, FLAN의 성능 향상을 위해서는:

- 다양한 작업과 지시문을 포함한 **충분한 양의 훈련 데이터** 사용
- **대규모 언어 모델**의 활용
- **자연어 지시문**의 적극적인 사용과 다양한 표현 방식의 도입

이 필요합니다. 이를 통해 모델은 지시문을 이해하고 따르는 능력을 강화하여, 인간에게 유용하고 직관적인 방식으로 다양한 작업을 수행할 수 있게 됩니다.

---

## 4.1 NUMBER OF INSTRUCTION TUNING CLUSTERS

사용자와의 이전 논의를 바탕으로, 논문의 '**4.1 지시 튜닝 클러스터의 수(NUMBER OF INSTRUCTION TUNING CLUSTERS)**' 섹션을 설명해 드리겠습니다.

---

**4.1 지시 튜닝 클러스터의 수**

우리 논문의 핵심 질문은 **지시 튜닝(instruction tuning)**이 **모델의 보지 못한 작업(unseen tasks)에 대한 제로샷 성능을 어떻게 향상시키는가**입니다. 이 첫 번째 **소거 연구(ablation study)**에서는 **지시 튜닝에 사용된 클러스터와 작업 수가 성능에 미치는 영향**을 조사했습니다.

이를 위해, 우리는 **자연어 추론(NLI)**, **폐쇄형 질문 응답(Closed-book QA)**, **상식 추론(Commonsense Reasoning)**을 **평가 클러스터(evaluation clusters)**로 설정하여 **지시 튜닝에서 제외**했습니다. 그리고 **남은 7개의 클러스터**를 지시 튜닝에 사용했습니다.³

우리는 지시 튜닝 클러스터의 수를 **1개부터 7개까지** 늘려가며 실험을 진행했으며, 이때 클러스터들은 **클러스터당 작업 수가 많은 순서대로 추가**했습니다.

---

![그림 6: 지시 튜닝에 추가적인 작업 클러스터를 포함하면 제외된 작업 클러스터에서의 제로샷 성능이 향상된다. 평가 작업은 다음과 같다. 상식 추론: CoPA, HellaSwag, PiQA, StoryCloze. NLI: ANLI R1–R3, QNLI, RTE, SNLI, WNLI. 폐쇄형 QA: ARC easy, ARC challenge, Natural Questions, TriviaQA.](https://cdn-mineru.openxlab.org.cn/extract/29379f1e-bdc9-4a49-93e2-7e6e2fcf6c00/7dec04aea0f3acf75d4f09bbe98a919b24bcfc046d141d57d6f77449cd7315b2.jpg)

**그림 6:** 지시 튜닝에 추가적인 작업 클러스터를 포함하면 제외된 작업 클러스터에서의 제로샷 성능이 향상된다. 평가 작업은 다음과 같습니다.

- **상식 추론(Commonsense)**: CoPA, HellaSwag, PiQA, StoryCloze
- **자연어 추론(NLI)**: ANLI R1–R3, QNLI, RTE, SNLI, WNLI
- **폐쇄형 QA(Closed-book QA)**: ARC easy, ARC challenge, Natural Questions, TriviaQA

---

**결과 및 해석**

그림 6에서 결과를 확인할 수 있습니다. **예상한 대로**, 지시 튜닝에 **추가적인 클러스터와 작업을 포함할수록** 제외된 세 개의 평가 클러스터에서의 **평균 성능이 향상**됨을 관찰했습니다(단, **감정 분석(sentiment analysis)** 클러스터는 예외였습니다). 이는 우리가 제안한 지시 튜닝 접근법이 **새로운 작업에서의 제로샷 성능을 향상시키는 데 도움이 된다**는 것을 확인해 줍니다.

또한 흥미로운 점은, 우리가 테스트한 **7개의 클러스터에서 성능이 포화되지 않고 계속 개선**되는 것으로 보인다는 것입니다. 이는 지시 튜닝에 **더 많은 클러스터를 추가하면 성능이 더욱 향상될 수 있음을** 시사합니다.

주의할 점은, 이 소거 연구에서는 **어떤 지시 튜닝 클러스터가 각 평가 클러스터에 가장 크게 기여하는지에 대한 결론을 내릴 수는 없다는 것**입니다. 하지만 **감정 분석 클러스터는 추가적인 성능 향상에 거의 기여하지 않았다**는 것을 알 수 있습니다.

---

**요약 및 결론**

- **목적**: 지시 튜닝에 사용되는 **작업 클러스터의 수**가 모델의 제로샷 성능에 미치는 영향을 조사하기 위함.
  
- **방법**:
  - **평가 클러스터**로 **자연어 추론(NLI)**, **폐쇄형 QA**, **상식 추론**을 선택하고, 이들을 지시 튜닝에서 제외함.
  - 나머지 **7개의 클러스터**를 사용하여 지시 튜닝을 수행함.
  - 지시 튜닝에 포함되는 클러스터의 수를 **1개부터 7개까지** 늘려가며 실험함.
  - 클러스터를 추가하는 순서는 **각 클러스터의 작업 수가 많은 순서대로** 진행함.

- **결과**:
  - 지시 튜닝에 포함된 클러스터와 작업의 수가 늘어날수록, 평가 클러스터에서의 **제로샷 성능이 향상**됨.
  - 특히, 성능이 **포화되지 않고 계속 개선되는 추세**를 보임.
  - 하지만 **감정 분석 클러스터는 성능 향상에 거의 기여하지 않음**.

- **의미**:
  - 지시 튜닝에 **더 많은 작업과 클러스터를 포함하면**, 모델의 제로샷 성능이 **더욱 향상**될 수 있음을 시사함.
  - 이는 모델이 **다양한 작업과 지시문에 노출될수록**, 보지 못한 새로운 작업에서도 지시문만으로 효과적으로 작업을 수행할 수 있게 됨을 보여줌.

---

**결론적으로**, 이 실험은 지시 튜닝에 포함되는 **작업 클러스터의 수와 다양성**이 모델의 **제로샷 학습 능력을 향상시키는 데 중요한 역할**을 한다는 것을 보여줍니다. 특히, 모델이 **더 많은 작업 유형에 노출**될수록, 새로운 작업에서도 지시문만으로 높은 성능을 발휘할 수 있습니다. 이는 **지시 튜닝의 범위를 확장**하는 것이 모델의 **일반화 능력을 강화**하는 효과적인 방법임을 시사합니다.

---

\---

³ **지시 튜닝에 사용된 클러스터**: 원문에서는 구체적인 클러스터 이름을 언급하지 않았지만, 평가 클러스터로 제외된 NLI, 폐쇄형 QA, 상식 추론을 제외한 나머지 클러스터들이 지시 튜닝에 사용되었습니다.

---

## 4.2 SCALING LAWS

사용자와의 이전 논의를 바탕으로, 논문의 '**4.2 스케일링 법칙(SCALING LAWS)**' 섹션을 설명해 드리겠습니다.

---

**4.2 스케일링 법칙**

**Brown et al.(2020)**은 언어 모델의 제로샷 및 몇샷 능력이 모델의 규모가 커질수록 상당히 향상된다는 것을 보여주었습니다. 이에 따라, 우리는 **지시 튜닝(instruction tuning)**의 이점이 **모델의 규모(model scale)**에 따라 어떻게 달라지는지를 탐구했습니다. 이전의 소거 연구에서 사용한 것과 동일한 클러스터 분할을 사용하여, 매개변수의 수가 **422M(4억 2,200만), 2B(20억), 8B(80억), 68B(680억), 137B(1,370억)**인 모델에서 지시 튜닝의 효과를 평가했습니다.

**그림 7**은 이러한 결과를 보여줍니다. 예상한 바와 같이, **1000억 개 규모의 모델(68B와 137B)**에서는 지시 튜닝이 **제외된 작업(held-out tasks)**에서의 성능을 크게 향상시켰습니다. 그러나 **8B(80억) 및 그보다 작은 모델**에서는 흥미로운 현상이 나타났습니다—**지시 튜닝이 오히려 제외된 작업에서의 성능을 저하시켰습니다.**

이러한 결과에 대한 한 가지 가능한 설명은, **소규모 모델**의 경우 지시 튜닝에 사용된 약 **40개의 작업을 학습하는 것만으로도 모델의 용량(capacity)을 모두 사용해 버려**, 새로운 작업에서 성능이 더 나빠진다는 것입니다. 이 설명에 따르면, **대규모 모델**의 경우 지시 튜닝이 일부 모델 용량을 채우지만, 동시에 모델이 지시문을 따르는 방법을 가르쳐주어 남은 용량으로 새로운 작업에 일반화할 수 있게 됩니다.

---

![그림 7: 지시 튜닝은 대규모 모델이 새로운 작업에 일반화하는 데 도움이 되지만, 소규모 모델에서는 실제로 보지 못한 작업에 대한 일반화를 저해한다. 이는 모든 모델 용량이 지시 튜닝 작업의 혼합물을 학습하는 데 사용되기 때문일 수 있다.](https://cdn-mineru.openxlab.org.cn/extract/29379f1e-bdc9-4a49-93e2-7e6e2fcf6c00/bd6b1adf486481e6e51b60acb8241bc1ea990373debeb4bb951cc54e91260145.jpg)

**그림 7:** 지시 튜닝은 대규모 모델이 새로운 작업에 일반화하는 데 도움이 되지만, 소규모 모델에서는 실제로 보지 못한 작업에 대한 일반화를 저해합니다. 이는 모든 모델 용량이 지시 튜닝 작업의 혼합물을 학습하는 데 사용되었기 때문일 수 있습니다.

---

**설명 및 해석**

- **모델 규모에 따른 지시 튜닝의 효과**:
  - **대규모 모델(68B, 137B)**에서는 지시 튜닝을 통해 보지 못한 작업에서의 성능이 크게 향상되었습니다.
  - 이는 우리 논문에서 이전에 제시한 결과와 일치하며, 지시 튜닝이 대규모 모델에서 제로샷 성능을 향상시킨다는 것을 확인합니다.
  
- **소규모 모델(8B 이하)**에서의 현상:
  - 지시 튜닝이 오히려 보지 못한 작업에서의 성능을 **저하시켰습니다.**
  - 이는 예상치 못한 결과로, 모델 규모에 따라 지시 튜닝의 효과가 다르게 나타남을 보여줍니다.
  
- **가능한 원인 분석**:
  - **소규모 모델의 용량 한계**: 작은 모델은 지시 튜닝에 사용된 약 40개의 작업을 학습하는 것만으로도 **모델의 모든 용량을 소진**하게 됩니다.
  - 이로 인해 모델이 새로운 작업을 처리할 때 필요한 추가적인 용량이 부족해져, **일반화 성능이 저하**됩니다.
  
- **대규모 모델의 경우**:
  - 지시 튜닝이 모델의 일부 용량을 사용하여 지시문을 따르는 방법을 배우게 하고, **남은 용량을 통해 새로운 작업에 일반화**할 수 있게 됩니다.
  - 대규모 모델은 용량에 여유가 있으므로, 지시 튜닝의 이점을 충분히 활용하면서도 새로운 작업을 처리할 수 있습니다.
  
**결론 및 시사점**

- **모델 규모의 중요성**: 지시 튜닝의 효과는 모델의 규모에 크게 의존합니다. **충분히 큰 모델**이어야 지시 튜닝의 이점을 극대화할 수 있습니다.
- **모델 용량 관리**: 소규모 모델에서는 지시 튜닝으로 인해 모델의 용량이 지시된 작업에 모두 할당되어, 새로운 작업을 처리할 여유가 없어집니다.
- **실용적인 고려사항**: 모델을 지시 튜닝할 때는 모델 규모와 용량을 고려해야 하며, 특히 소규모 모델에서는 지시 튜닝의 이점이 제한적일 수 있음을 염두에 두어야 합니다.

---

**전체적인 요약**

이 섹션에서는 **모델의 규모가 지시 튜닝의 효과에 어떻게 영향을 미치는지**를 실험적으로 분석했습니다. 결과적으로, **대규모 언어 모델**에서는 지시 튜닝이 보지 못한 새로운 작업에서의 **제로샷 성능을 크게 향상시키는 반면**, **소규모 모델**에서는 오히려 성능이 저하되는 현상을 발견했습니다. 이는 작은 모델의 경우 지시 튜닝 작업을 학습하는 것만으로도 모델의 용량을 모두 사용하게 되어, 새로운 작업에 일반화할 수 있는 여력이 부족해지기 때문입니다.

따라서, 지시 튜닝의 이점을 최대화하기 위해서는 **충분히 큰 규모의 모델**을 사용하는 것이 중요하며, 이는 모델의 **용량(capacity)**과 **지시 튜닝의 학습 범위** 사이의 균형을 고려해야 함을 시사합니다.

---

## 4.3 ROLE OF INSTRUCTIONS

사용자와의 이전 논의를 바탕으로, 논문의 '**4.3 지시문의 역할(ROLE OF INSTRUCTIONS)**' 섹션을 설명해 드리겠습니다.

---

**4.3 지시문의 역할**

**최종 소거 연구(ablation study)**로서, 우리는 **미세 조정(finetuning)** 중에 **지시문(instructions)**이 어떤 역할을 하는지 탐구했습니다. 이는 성능 향상이 전적으로 **멀티태스크 미세 조정(multi-task finetuning)**에 기인할 수 있으며, **지시문 없이도 모델이 동일한 성능을 낼 수 있는지**를 검증하기 위함입니다.

이에 따라, 우리는 지시문 없이 미세 조정하는 두 가지 설정을 고려했습니다:

1. **템플릿 없음(no template) 설정**:
   - 모델에게 입력과 출력만 제공합니다.
   - 예를 들어, 번역 작업의 경우 입력은 "**The dog runs.**", 출력은 "**Le chien court.**"가 됩니다.
   
2. **데이터셋 이름(dataset name) 설정**:
   - 각 입력 앞에 **작업과 데이터셋의 이름**을 추가합니다.
   - 예를 들어, 프랑스어로 번역하는 작업의 경우 입력은 "**[Translation: WMT'14 to French] The dog runs.**"가 됩니다.

우리는 이 두 가지 소거 설정을 **자연어 지시문(natural instructions)**을 사용한 FLAN의 미세 조정 절차와 비교했습니다. FLAN의 경우, 예를 들어 "**다음 문장을 프랑스어로 번역해 주세요: 'The dog runs.'**"와 같은 지시문을 사용합니다.

평가는 **그림 5**에서 보여준 **네 개의 제외된 클러스터(held-out clusters)**에 대해 수행되었습니다.

- **템플릿 없음** 설정에서는, 제로샷 추론 시에 FLAN의 지시문을 사용했습니다(지시문을 사용하지 않으면 모델이 어떤 작업을 수행해야 하는지 알 수 없기 때문입니다).
  
- **데이터셋 이름만**으로 미세 조정된 모델에 대해서는, FLAN 지시문을 사용한 제로샷 성능과 데이터셋 이름을 사용한 제로샷 성능 모두를 보고했습니다.

---

![그림 8: 미세 조정 시 지시문을 제거한 모델을 사용한 소거 연구 결과](https://cdn-mineru.openxlab.org.cn/extract/29379f1e-bdc9-4a49-93e2-7e6e2fcf6c00/299bec66797a3065d5bfbadd98cef45b5d7c0d4355078a27acdafc5f47109bf2.jpg)

**그림 8:** 미세 조정 시 지시문을 제거한 모델을 사용한 소거 연구 결과입니다.

---

**결과 및 해석**

- **그림 8**에서 결과를 확인할 수 있습니다.
  
- **두 가지 소거 설정(템플릿 없음, 데이터셋 이름만)** 모두 FLAN에 비해 성능이 상당히 떨어졌습니다.

- 이는 **지시문을 포함한 훈련이 보지 못한 작업에서의 제로샷 성능에 매우 중요함**을 나타냅니다.

**세부 분석**

1. **템플릿 없음 설정**:
   - 미세 조정 시 입력과 출력만으로 모델을 학습시켰습니다.
   - 모델은 입력에 대한 출력만을 학습하였으며, 지시문이 없었습니다.
   - 제로샷 추론 시에는 FLAN의 지시문을 사용해야 했습니다. 그렇지 않으면 모델이 어떤 작업을 해야 하는지 알 수 없기 때문입니다.
   - 결과적으로, 이 설정에서의 모델은 FLAN보다 성능이 떨어졌습니다.

2. **데이터셋 이름 설정**:
   - 입력 앞에 작업과 데이터셋 이름을 추가하여 모델을 미세 조정했습니다.
   - 예를 들어, "[Translation: WMT'14 to French] The dog runs."
   - 제로샷 추론 시에는 두 가지 방법으로 평가했습니다:
     - **FLAN의 지시문을 사용**하여 평가
     - **데이터셋 이름을 사용**하여 평가
   - 두 경우 모두 FLAN보다 성능이 낮았습니다.

**결론**

- **지시문을 포함한 훈련**은 보지 못한 작업에서의 제로샷 성능을 향상시키는 데 **결정적인 역할**을 합니다.

- 단순히 멀티태스크 미세 조정만으로는 지시문의 효과를 대체할 수 없습니다.

- 모델이 **지시문을 이해하고 따르는 능력**을 개발하기 위해서는, 훈련 과정에서부터 **자연어 지시문을 포함**해야 합니다.

---

**요약 및 시사점**

- **연구 목적**: 모델의 성능 향상이 **지시문의 사용**에 기인하는 것인지, 아니면 단순히 **멀티태스크 미세 조정**의 효과인지를 검증.

- **방법**:
  - **지시문 없이** 모델을 미세 조정하는 두 가지 설정을 실험:
    1. **템플릿 없음**: 입력과 출력만 제공하여 미세 조정.
    2. **데이터셋 이름만**: 입력 앞에 작업과 데이터셋 이름을 추가하여 미세 조정.
  - 이 두 설정을 **지시문을 사용한 FLAN의 미세 조정**과 비교.
  - 네 개의 제외된 클러스터에 대해 모델의 제로샷 성능을 평가.

- **결과**:
  - 지시문 없이 미세 조정한 모델들은 FLAN보다 성능이 **크게 저하**됨.
  - 이는 **지시문의 사용이 모델의 제로샷 성능에 필수적**임을 보여줌.

- **의미**:
  - 모델이 **지시문을 이해하고 따르는 능력**을 발전시키려면, **훈련 과정에서부터 지시문을 포함**해야 함.
  - 단순히 멀티태스크 미세 조정만으로는 지시문의 효과를 대신할 수 없음.
  - **자연어 지시문**은 모델이 새로운 작업에서 지시문만으로 작업을 수행할 수 있게 하는 데 핵심적인 역할을 함.

---

**결론적으로**, 이 섹션에서는 지시문의 역할을 검증하기 위한 소거 연구를 수행하였으며, 그 결과 **훈련 시 지시문의 사용이 모델의 제로샷 성능 향상에 결정적인 기여를 한다는 것**을 확인하였습니다. 이는 지시 튜닝의 핵심이 **지시문을 통한 학습**이며, 이를 통해 모델이 새로운 작업에서도 지시문만으로 높은 성능을 발휘할 수 있게 됨을 의미합니다.

---

## 4.4 INSTRUCTIONS WITH FEW-SHOT EXEMPLARS

사용자와의 이전 논의를 바탕으로, 논문의 '**4.4 지시문과 몇샷 예시**' 섹션을 설명해 드리겠습니다.

---

**4.4 지시문과 몇샷 예시(INSTRUCTIONS WITH FEW-SHOT EXEMPLARS)**

지금까지 우리는 **제로샷 환경에서의 지시 튜닝**에 초점을 맞췄습니다. 여기에서는 **추론 시에 몇샷 예시(few-shot exemplars)가 사용 가능한 경우**, 지시 튜닝을 어떻게 활용할 수 있는지를 연구합니다.

**몇샷 설정의 형식**은 제로샷 형식을 기반으로 합니다. 어떤 입력 $x$와 출력 $y$에 대해, **$\text{instruct}(x)$**는 제로샷 지시문을 나타냅니다. 그러면 $k$개의 몇샷 예시 $\left(x_{i}, y_{i}\right)_{i=1}^{k}$와 새로운 입력 $x$가 주어졌을 때, **몇샷 설정에서의 지시문 형식**은 다음과 같이 표현됩니다:

$$
\text{instruct}(x_{1}) \oplus y_{1} \oplus \text{instruct}(x_{2}) \oplus y_{2} \oplus \ldots \oplus \text{instruct}(x_{k}) \oplus y_{k} \oplus \text{instruct}(x)
$$

여기서 **$\oplus$**는 구분자 토큰을 사이에 삽입하여 문자열을 연결함을 나타냅니다.

**훈련 및 추론 시**에, 예시들은 훈련 셋에서 무작위로 추출되며, 예시의 수는 최대 16개로 제한되고, 총 시퀀스 길이가 960 토큰 미만이 되도록 합니다. 우리의 실험은 **$\S3$**에서 사용한 것과 동일한 작업 분할과 평가 절차를 사용하며, 보지 못한 작업에 대한 몇샷 예시는 **추론 시에만** 사용됩니다.

**그림 9**에서 보이는 것처럼, 몇샷 예시를 추가하면 제로샷 FLAN과 비교하여 모든 작업 클러스터에서 성능이 향상되었습니다. 예시들은 특히 **출력 공간이 크거나 복잡한 작업**(예: 구조화된 입력을 텍스트로 변환하는 작업, 번역, 폐쇄형 QA)에서 효과적이었는데, 이는 예시들이 모델이 출력 형식을 더 잘 이해하도록 도울 수 있기 때문일 것입니다. 또한, 모든 작업 클러스터에서, **몇샷 FLAN의 템플릿 간 표준 편차**가 낮아졌는데, 이는 **프롬프트 엔지니어링에 대한 민감성이 줄어들었음을** 나타냅니다.

---

![그림 9: FLAN에 몇샷 예시를 추가하는 것은 지시 튜닝된 모델의 성능을 향상시키는 보완적인 방법입니다. 주황색 막대는 각 작업 클러스터에 대해 데이터셋 수준에서 평균화된 템플릿 간 표준 편차를 나타냅니다.](https://cdn-mineru.openxlab.org.cn/extract/29379f1e-bdc9-4a49-93e2-7e6e2fcf6c00/7da5462446bdbcd407401552efe0cafaf5aac691704f3cabbb3280e757eb1dbc.jpg)

**그림 9:** FLAN에 몇샷 예시를 추가하는 것은 지시 튜닝된 모델의 성능을 향상시키는 보완적인 방법입니다. 주황색 막대는 각 작업 클러스터에 대해 데이터셋 수준에서 평균화된 템플릿 간 표준 편차를 나타냅니다.

---

**설명 및 요약**

- **연구 목적**: 몇샷 예시가 추론 시에 사용 가능한 경우, 지시 튜닝이 어떻게 활용될 수 있으며 성능에 어떤 영향을 미치는지 조사하기 위함입니다.

- **방법**:
  - **제로샷 지시문 형식**을 기반으로, **몇샷 예시를 포함하는 새로운 지시문 형식**을 정의했습니다.
  - $k$개의 예시 $\left(x_{i}, y_{i}\right)_{i=1}^{k}$와 새로운 입력 $x$가 주어졌을 때, 지시문과 예시들을 연결하여 모델에 제공합니다.
  - 훈련 및 추론 시에, 예시들은 훈련 셋에서 무작위로 선택되며, **예시의 수는 최대 16개**로 제한하고, 전체 시퀀스 길이는 960 토큰 미만으로 제한합니다.
  - **$\S3$**에서 사용한 것과 동일한 작업 분할과 평가 절차를 따릅니다. 즉, 보지 못한 작업에 대한 몇샷 예시는 **추론 시에만** 사용됩니다.

- **결과**:
  - **몇샷 예시를 추가하면** 모든 작업 클러스터에서 성능이 향상되었습니다.
  - 특히, **출력 공간이 크거나 복잡한 작업**에서 효과가 두드러졌습니다.
    - 예시: **구조화된 입력을 텍스트로 변환하는 작업(Struct-to-Text)**, **번역(Translation)**, **폐쇄형 QA(Closed-book QA)**
  - **템플릿 간 성능의 표준 편차**가 감소하여, **프롬프트 엔지니어링에 대한 민감성**이 줄어들었습니다.

- **해석**:
  - **몇샷 예시**는 모델이 **출력 형식**이나 **작업의 예시**를 통해 **작업을 더 잘 이해하도록 도와줍니다**.
  - 이는 모델이 추론 시에 **추가적인 정보를 활용**하여 성능을 향상시킬 수 있음을 보여줍니다.
  - 프롬프트 엔지니어링에 대한 민감성이 줄어든다는 것은, **사용자가 작성하는 지시문의 표현 방식에 덜 민감해져서**, **더 안정적이고 일관된 성능을 기대할 수 있게 됨**을 의미합니다.

---

**결론 및 시사점**

- **몇샷 예시의 활용**은 지시 튜닝된 모델의 성능을 **추가로 향상시키는 보완적인 방법**입니다.
- 특히 **출력 형식이 복잡하거나 다양한 작업**에서 **몇샷 예시의 효과**가 두드러집니다.
- **프롬프트 엔지니어링에 대한 민감성 감소**는 모델의 **실용성을 높이는 중요한 요소**입니다.
- 따라서, 실제 응용에서 **지시 튜닝과 몇샷 예시를 함께 활용하면** 모델의 성능과 안정성을 모두 향상시킬 수 있습니다.

---

**전체적인 요약**

이 섹션에서는 **지시 튜닝된 모델**이 **추론 시에 몇샷 예시를 추가로 활용할 때** 성능이 어떻게 향상되는지를 탐구했습니다. 몇샷 예시를 포함하는 새로운 지시문 형식을 정의하여 실험한 결과, 모든 작업 클러스터에서 성능이 향상되었으며, 특히 출력이 복잡한 작업에서 효과가 컸습니다. 또한, 프롬프트 엔지니어링에 대한 민감성이 줄어들어, 모델의 사용 편의성과 실용성이 높아졌습니다. 이는 **지시 튜닝과 몇샷 학습이 상호 보완적으로 작용하여 모델의 성능을 향상시킬 수 있음을** 보여줍니다.

---

## 4.5 INSTRUCTION TUNING FACILITATES PROMPT TUNING

사용자와의 이전 논의를 바탕으로, 논문의 '**4.5 지시 튜닝은 프롬프트 튜닝을 촉진한다**' 섹션을 설명해 드리겠습니다.

---

**4.5 지시 튜닝은 프롬프트 튜닝을 촉진한다**

앞서 우리는 **지시 튜닝(instruction tuning)**이 모델이 지시문에 응답하는 능력을 향상시킨다는 것을 확인했습니다. 따라서 **FLAN**이 NLP 작업을 수행하는 데 실제로 더 적합하다면, **프롬프트 튜닝(prompt tuning)**을 사용하여 추론을 수행할 때도 더 나은 성능을 달성해야 합니다. 프롬프트 튜닝은 Li & Liang(2021), Lester et al.(2021)의 연구에서 제안된 기법으로, 미리 정의된 지시문 대신 **최적화된 연속적인 변수(소프트 프롬프트)**를 입력 앞에 첨가하여 모델을 미세 조정하는 방법입니다.

**추가 분석**으로, 우리는 **SuperGLUE**(Wang et al., 2019a)의 각 작업에 대해 연속적인 프롬프트를 학습했습니다. 이때 **$\S2.2$**에서 사용한 클러스터 분할을 따랐으며, 작업 $\tau$에 대해 프롬프트 튜닝을 수행할 때 **지시 튜닝 중에 $\tau$와 동일한 클러스터에 속한 작업은 보지 않도록 설정**했습니다. 우리의 프롬프트 튜닝 설정은 Lester et al.(2021)의 절차를 따르되, 다음과 같은 변경 사항을 적용했습니다:

- **프롬프트 길이**를 **10**으로 설정했습니다.
- **웨이트 디케이(weight decay)**를 **1e-4**로 설정했습니다.
- **어텐션 스코어(attention scores)**에 드롭아웃(dropout)을 사용하지 않았습니다.

이러한 변경 사항은 예비 실험에서 LaMDA-PT의 성능을 개선하는 것으로 나타났습니다.

**그림 10**은 완전한 지도 학습된(training set 전체 사용) 경우와 **32개의 훈련 예시만 사용한 저자원(low-resource) 환경**에서의 프롬프트 튜닝 실험 결과를 보여줍니다.

---

![그림 10: 지시 튜닝된 모델은 프롬프트 튜닝의 연속적인 입력에 더 잘 응답합니다. 특정 데이터셋에 대해 프롬프트 튜닝을 수행할 때, 해당 데이터셋과 동일한 클러스터의 작업은 지시 튜닝 중에 보지 않았습니다. 표시된 성능은 SuperGLUE 개발 셋의 평균입니다.](https://cdn-mineru.openxlab.org.cn/extract/29379f1e-bdc9-4a49-93e2-7e6e2fcf6c00/765141e01bef8aeb8f3e360fed329bd836475241c8108572567054c00e3d1564.jpg)

**그림 10:** 지시 튜닝된 모델은 프롬프트 튜닝의 연속적인 입력에 더 잘 응답합니다. 특정 데이터셋에 대해 프롬프트 튜닝을 수행할 때, 해당 데이터셋과 동일한 클러스터의 작업은 지시 튜닝 중에 보지 않았습니다. 표시된 성능은 SuperGLUE 개발 셋의 평균입니다.

---

**결과 및 해석**

- **모든 시나리오**에서, **FLAN**을 사용한 프롬프트 튜닝이 **LaMDA-PT**를 사용한 프롬프트 튜닝보다 **더 나은 성능**을 보였습니다.
  
- 특히, **저자원 환경**(훈련 예시가 32개만 있는 경우)에서는 FLAN에서의 프롬프트 튜닝이 LaMDA-PT보다 **10% 이상 성능 향상**을 달성한 경우도 많았습니다.
  
- 이러한 결과는 **지시 튜닝**이 NLP 작업을 수행하는 데 **더 바람직한 체크포인트**를 생성할 수 있음을 또 다른 방식으로 보여줍니다.

**요약 및 의미**

- **지시 튜닝**은 모델이 지시문에 응답하는 능력을 향상시킬 뿐만 아니라, **프롬프트 튜닝**과 같은 다른 미세 조정 기법에서도 모델의 성능을 향상시킵니다.

- 이는 지시 튜닝이 모델이 **연속적인 입력(소프트 프롬프트)**에도 더 잘 대응할 수 있게 되어, 프롬프트 튜닝의 효과를 극대화할 수 있음을 나타냅니다.

- **실용적인 관점**에서, 지시 튜닝된 모델을 사용하면 프롬프트 튜닝을 통한 모델의 성능 향상을 더욱 기대할 수 있으며, 특히 **데이터가 적은 환경**에서 큰 이점을 얻을 수 있습니다.

---

**결론적으로**, 이 섹션에서는 **지시 튜닝이 프롬프트 튜닝을 촉진하여 모델의 성능을 향상시킬 수 있다**는 것을 보여주고 있습니다. 이는 지시 튜닝이 모델의 **기본적인 언어 이해와 지시 수행 능력**을 향상시켜, 다양한 미세 조정 방법과 결합하여 성능을 더욱 높일 수 있음을 의미합니다. 따라서, **지시 튜닝과 프롬프트 튜닝의 결합**은 NLP 작업에서 모델의 성능을 향상시키는 강력한 전략이 될 수 있습니다.

---

## 5 RELATED WORK

사용자와의 이전 논의를 바탕으로, 논문의 '**5 관련 연구(RELATED WORK)**' 섹션을 설명해 드리겠습니다.

---

**5 관련 연구**

우리의 연구는 **제로샷 학습(zero-shot learning)**, **프롬프트(prompting)**, **멀티태스크 학습(multi-task learning)**, 그리고 **NLP 응용을 위한 언어 모델** 등 여러 광범위한 연구 영역과 관련이 있습니다(Radford et al., 2019; Raffel et al., 2020; Brown et al., 2020; Efrat & Levy, 2020; Aghajanyan et al., 2021; Li & Liang, 2021 등). 이러한 광범위한 영역에 대한 이전 연구들은 부록 D에 있는 확장된 관련 연구 섹션에서 다루었습니다. 여기에서는 우리의 연구와 가장 밀접하게 관련된 두 가지 **보다 좁은 범위의 하위 영역**을 설명하겠습니다.

---

### **1. 지시문에 응답하도록 모델을 훈련시키는 방법**

우리가 **모델이 지시문에 응답하도록 요청하는 방식**은 **질문 응답 기반의 작업 구성(QA-based task formulation)**과 유사합니다(Kumar et al., 2016; McCann et al., 2018). 이 접근법은 **맥락(context)에 대한 QA로 NLP 작업을 통일(unify)**하는 것을 목표로 합니다. 이러한 방법들은 우리 연구와 매우 비슷하지만, 대부분 **제로샷 학습**보다는 **멀티태스크 학습**에 중점을 두고 있습니다. 또한 Liu et al.(2021)이 지적했듯이, 이들은 일반적으로 **사전 학습된 언어 모델의 기존 지식**을 활용하는 데에서 동기가 부여되지 않습니다.

더욱이, 우리의 연구는 모델 규모와 작업 범위 측면에서 최근의 연구들을 능가합니다. 예를 들어, Chai et al.(2020)과 Zhong et al.(2021)의 작업은 우리의 연구보다 모델 규모나 다루는 작업의 범위에서 제한적입니다.

---

### **2. 모델이 지시문을 따르는 능력에 대한 연구**

언어 모델의 성공은 **모델이 지시문을 따르는 능력**에 대한 새로운 연구를 이끌어냈습니다. 가장 최근에, **Mishra et al.(2021)**은 **1억 4천만 개의 매개변수를 가진 BART** 모델을 **몇샷 예시(few-shot exemplars)**를 포함한 지시문으로 미세 조정하고, 보지 못한 작업에서의 몇샷 능력을 평가했습니다—이는 우리의 **$\S4.4$에서의 몇샷 지시 튜닝 결과**와 유사합니다. 이 유망한 결과(지시문에 크게 초점을 두지는 않았지만 Ye et al.(2021)의 결과도 마찬가지로)는 **작은 모델 규모에서도** 여러 작업에 대해 미세 조정하면 보지 못한 작업에서의 몇샷 성능이 향상된다는 것을 시사합니다.

또한, **Sanh et al.(2021)**은 우리와 유사한 설정에서 **T5** 모델을 미세 조정하여, **110억 개의 매개변수를 가진 모델**에서도 제로샷 학습이 향상될 수 있음을 발견했습니다. 우리와 유사한 모델 규모에서는, **OpenAI의 InstructGPT 모델**이 사람 평가자들이 더 선호하는 출력을 생성하도록 **미세 조정**과 **강화 학습**을 통해 훈련되었습니다(Ouyang et al., 2022).

---

**요약 및 의미**

- **QA 기반의 작업 구성**:
  - **Kumar et al.(2016)**, **McCann et al.(2018)** 등의 연구에서는 NLP 작업을 **맥락에 대한 질문 응답**으로 변환하여 **통합**하려 했습니다.
  - 이러한 접근법은 우리와 유사하지만, 주로 **멀티태스크 학습**에 초점을 맞추었으며, 사전 학습된 언어 모델의 기존 지식을 활용하는 데에서 동기를 얻지는 않았습니다.
  - 우리의 연구는 **모델 규모와 작업 범위**에서 이러한 이전 연구들을 능가하며, **제로샷 학습**에 중점을 두고 있습니다.

- **모델의 지시 수행 능력에 대한 연구**:
  - **Mishra et al.(2021)**은 작은 규모의 모델에서도 **지시문과 몇샷 예시를 사용한 미세 조정**이 보지 못한 작업에서의 몇샷 성능을 향상시킬 수 있음을 보여주었습니다.
  - **Ye et al.(2021)**도 유사한 맥락에서 연구를 수행하였으나, 지시문에 크게 초점을 두지는 않았습니다.
  - **Sanh et al.(2021)**은 **T5 모델(110억 매개변수)**을 사용하여 지시 튜닝을 수행하고, 제로샷 학습의 향상을 관찰했습니다.
  - **OpenAI의 InstructGPT(Ouyang et al., 2022)**는 우리와 유사한 모델 규모에서 **미세 조정**과 **강화 학습**을 통해 인간이 더 선호하는 출력을 생성하도록 훈련되었습니다.

- **우리 연구의 기여**:
  - 우리의 연구는 **대규모의 언어 모델(1370억 매개변수)**을 사용하여 **지시 튜닝**을 수행하고, 보지 못한 작업에서의 **제로샷 및 몇샷 성능 향상**을 달성했습니다.
  - 특히, 이전 연구들보다 **더 큰 모델 규모**와 **더 넓은 작업 범위**를 다루었으며, 모델이 **자연어 지시문**을 이해하고 따르는 능력을 향상시켰습니다.

---

**결론적으로**, 이 섹션에서는 우리의 연구와 밀접하게 관련된 두 가지 하위 영역을 소개하고 있습니다:

1. **질문 응답 기반의 작업 구성**을 통한 NLP 작업의 통합
2. **모델의 지시문 수행 능력**을 향상시키기 위한 미세 조정 연구

우리는 이러한 이전 연구들과 비교하여, **모델 규모와 작업 범위에서 더 확장된 접근**을 취했으며, **제로샷 학습**에 초점을 맞추어 **지시 튜닝**의 효과를 입증했습니다. 이는 언어 모델이 지시문을 이해하고 새로운 작업에 일반화할 수 있는 능력을 향상시키는 데 있어 중요한 기여를 합니다.

---

## 6 DISCUSSION

사용자와의 이전 논의를 바탕으로, 논문의 '**6 논의(DISCUSSION)**' 섹션을 설명해 드리겠습니다.

---

**6 논의**

우리의 논문은 제로샷 프롬프트에서 다음과 같은 간단한 질문을 탐구했습니다: **지시 형태로 표현된 작업들의 모음에 대해 모델을 미세 조정(finetuning)하면 보지 못한 작업에 대한 모델의 성능이 향상되는가?** 우리는 이 질문을 **지시 튜닝(instruction tuning)**이라는 간단한 방법을 통해 구체화했습니다. 이 방법은 사전 학습–미세 조정(pretrain–finetune)과 프롬프트(prompting) 패러다임의 매력적인 측면을 결합합니다.

우리의 **지시 튜닝된 모델인 FLAN**은 미세 조정되지 않은 원본 모델보다 성능이 향상되었으며, 우리가 평가한 대부분의 작업에서 제로샷 GPT-3를 능가했습니다. **소거 연구(ablation studies)**를 통해, **보지 못한 작업에 대한 성능이 지시 튜닝에 사용된 작업 클러스터의 수에 따라 향상되며**, 흥미롭게도 **지시 튜닝을 통한 성능 향상은 충분한 모델 규모가 있을 때에만 나타난다**는 것을 발견했습니다. 또한, 지시 튜닝은 **몇샷 프롬프트(few-shot prompting)**나 **프롬프트 튜닝(prompt tuning)**과 같은 다른 프롬프트 방법들과 결합될 수 있습니다.

---

### **전문가 모델과 범용 모델 사이의 균형**

대규모 언어 모델의 다양한 능력은 **전문가 모델(specialist models)**(작업당 하나의 모델)과 **범용 모델(generalist models)**(여러 작업을 위한 하나의 모델)(Arivazhagan et al., 2019; Pratap et al., 2020) 사이의 **균형(trade-offs)**에 대한 관심을 불러일으켰으며, 우리의 연구는 이에 잠재적인 시사점을 제공합니다.

일반적으로, **레이블이 있는 데이터(labeled data)**는 전문가 모델을 개선하는 데 가장 자연스러운 역할을 할 것으로 예상할 수 있습니다. 그러나 **지시 튜닝은 레이블이 있는 데이터를 사용하여 대규모 언어 모델이 보지 못한 많은 작업을 수행하도록 도울 수 있음을 보여줍니다**. 다시 말해, **지시 튜닝이 작업 간 일반화(cross-task generalization)에 긍정적인 효과를 미친다는 것**은 **작업별 훈련(task-specific training)**이 **일반적인 언어 모델링(general language modeling)**에 상호 보완적임을 보여주며, **범용 모델**에 대한 추가 연구를 촉진합니다.

---

### **연구의 한계**

우리 연구의 한계로는 다음과 같은 점들이 있습니다:

- **작업을 클러스터에 할당하는 데 주관성이 다소 존재**합니다. 우리는 문헌에서 받아들여진 범주화를 사용하려고 노력했지만, 클러스터링 과정에서 어느 정도의 주관적 판단이 개입되었습니다.

- 우리는 **일반적으로 한 문장으로 구성된 비교적 짧은 지시문만을 탐구**했습니다(군중 작업자들에게 제공되는 상세한 지시문과는 달리). 따라서 지시문의 복잡성이나 길이에 따른 모델의 성능 변화를 충분히 조사하지 못했습니다.

- 우리의 평가에 대한 또 다른 한계는, **개별 예시들이 모델의 사전 학습 데이터(웹 문서 포함)에 나타났을 수 있다**는 것입니다. 하지만 사후 분석(부록 C)에서, 우리는 데이터 중복이 결과에 크게 영향을 미쳤다는 증거를 찾지 못했습니다.

- 마지막으로, **FLAN 137B의 규모는 서비스하기에 비용이 많이 듭니다**. 대규모 모델의 추론 및 배포에 소요되는 자원이 크기 때문에 실제 응용에서의 활용에 제약이 있을 수 있습니다.

---

### **향후 연구 방향**

지시 튜닝에 대한 향후 연구로는 다음과 같은 것들이 있습니다:

- **더 많은 작업 클러스터의 수집 및 생성**: 미세 조정을 위해 더 다양한 작업과 클러스터를 포함함으로써 모델의 일반화 능력을 더욱 향상시킬 수 있습니다.

- **다국어 실험(cross-lingual experiments)**: 지시 튜닝을 다양한 언어로 확장하여, 여러 언어에서의 제로샷 및 몇샷 성능을 향상시키는 것을 탐구할 수 있습니다.

- **FLAN을 사용하여 다운스트림 분류기를 위한 데이터 생성**: FLAN의 능력을 활용하여 다운스트림 작업에 필요한 데이터나 예시를 생성하고, 이를 통해 새로운 모델을 훈련하거나 성능을 향상시킬 수 있습니다.

- **편향과 공정성 개선을 위한 미세 조정**: 미세 조정을 통해 모델의 **편향(bias)**과 **공정성(fairness)**과 관련된 행동을 개선하는 연구를 진행할 수 있습니다(Solaiman & Dennison, 2021).

---

**요약 및 결론**

우리의 연구는 **지시 튜닝**이 대규모 언어 모델의 **제로샷 성능을 향상시키는 효과적인 방법**임을 보여주었습니다. **FLAN 모델**은 미세 조정되지 않은 모델에 비해 성능이 향상되었으며, 평가한 대부분의 작업에서 **제로샷 GPT-3**를 능가했습니다. 또한, 지시 튜닝의 효과는 **사용된 작업 클러스터의 수**와 **모델의 규모**에 크게 의존한다는 것을 발견했습니다.

이러한 결과는 **레이블이 있는 데이터가 대규모 언어 모델의 범용성을 향상시키는 데 중요한 역할을 할 수 있음을 시사**하며, **범용 모델과 전문가 모델 사이의 균형**에 대한 논의를 촉진합니다. 즉, **작업별 훈련**이 **일반적인 언어 모델링**과 상호 보완적인 관계에 있으며, 이를 통해 모델의 능력을 더욱 확장할 수 있습니다.

**향후 연구 방향**으로는, **더 많은 작업 클러스터의 확장**, **다국어 지원**, **다운스트림 작업을 위한 데이터 생성**, **편향과 공정성 개선** 등이 있습니다. 이러한 연구는 지시 튜닝의 잠재력을 더욱 발휘하고, 모델이 다양한 응용에서 효과적으로 활용될 수 있도록 하는 데 기여할 것입니다.

---

이상으로 논문의 '**6 논의(DISCUSSION)**' 섹션에 대한 설명을 마치겠습니다. 이전 논의를 바탕으로 지시 튜닝의 원리와 효과, 그리고 우리 연구의 시사점과 한계를 이해하는 데 도움이 되었기를 바랍니다.

---
