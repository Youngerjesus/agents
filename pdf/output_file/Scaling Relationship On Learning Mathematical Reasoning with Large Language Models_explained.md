
## ABSTRACT

이 논문은 대형 언어 모델(LLM)이 수학적 추론을 수행하는 데 어려움을 겪고 있으며, 이러한 추론과 LLM 용량 간의 확장 관계가 충분히 탐구되지 않았다는 문제를 다루고 있습니다. 연구에서는 사전 학습 손실, 지도 학습 데이터 양, 그리고 증강 데이터 양이 지도 학습된 LLM의 추론 성능에 어떤 영향을 미치는지를 조사하였습니다. 연구 결과, 모델 성능을 예측하는 데 있어서 사전 학습 손실이 모델 매개변수 수보다 더 나은 지표라는 것을 발견하였습니다.

우리는 서로 다른 양의 지도 데이터를 사용하여 지도 미세 조정(SFT)을 적용하였고, 데이터 양과 모델 성능 사이에 로그 선형 관계가 있음을 실험적으로 확인하였습니다. 그리고 성능이 더 좋은 모델은 지도 데이터셋이 커질수록 성능 향상이 덜한다는 것도 발견하였습니다. 인간의 노력을 들이지 않고 모델 성능을 높이기 위해 더 많은 데이터 샘플을 증강하기 위해, 우리는 '거절 표본 추출 미세 조정(RFT)'을 제안합니다. RFT는 올바른 추론 경로를 생성 및 수집하여 증강된 미세 조정 데이터셋으로 사용하는 방식입니다. 더 다양한 추론 경로를 포함한 증강 샘플을 사용했을 때 RFT가 LLM의 수학적 추론 성능을 보다 향상시킨다는 것을 발견하였습니다. 또한, RFT는 성능이 낮은 LLM에 더 많은 개선을 가져온다는 것도 알아냈습니다. 더 나아가, 여러 모델의 거절 표본을 결합하여 LLaMA-7B 모델이 GSM8K에서 49.3%의 정확도를 달성하게 하였는데, 이는 기존 지도 미세 조정(SFT) 정확도인 35.9%를 크게 초과하는 결과입니다. 우리는 우리의 코드와 거절 표본 추출 증강 데이터를 다음 링크에서 공개합니다: https://github.com/OFA-Sys/gsm8k-ScRel.

---

## 1 INTRODUCTION

대형 언어 모델(LLMs)은 다양한 수학적 추론 작업에서 상당한 능력을 보여 왔습니다. 본 논문에서는 다양한 사전 학습된 LLM과 지도 학습 데이터셋을 기반으로 LLM의 수학적 추론 능력을 이해, 예측 및 개선하는 방법을 탐구합니다. 이러한 지식을 통해 LLM 개선이나 데이터셋 증강에 들이는 노력을 더 잘 결정할 수 있습니다.

최근 여러 연구에서는 모델의 추론 성능을 향상시키기 위해 다양한 프롬프트를 사용하거나, 여러 번의 추론을 결합/재배열하는 방법에 집중하고 있습니다. 그러나, 문맥 내 학습(ICL) 및 다중 추론 수행이 성능을 개선할 수는 있지만, 이는 계산 비용이 많이 들고 온라인 배포 환경에는 적합하지 않습니다. 따라서 본 연구에서는 온라인 배포에 더 가까운, 단 한 번의 추론만으로 이루어지는 지도 학습 LLM의 성능에 초점을 맞추었습니다.

본 연구에서는 사전 학습 손실, 지도 데이터 양, 증강 데이터 양 등 지도된 LLM의 수학적 추론 능력에 영향을 미치는 요인의 확장 관계를 실험적으로 조사합니다. 먼저, 우리는 지도 미세 조정(SFT)과 ICL의 성능을 분석했습니다. 사전 학습 손실은 주어진 범위 내에서 SFT 및 ICL 정확도와 대략적으로 음의 상관 관계를 가지며, 이는 사전 학습된 모델 크기나 토큰 수보다 나은 성능 지표임을 발견했습니다. 둘째로, SFT와 지도 데이터 양 간의 관계를 분석한 결과, 모델 성능이 로그 선형 관계를 가지며, 사전 학습된 모델이 더 나을수록 증가가 줄어든다는 것을 관찰했습니다. 셋째로, 모델 자체를 사용하여 더 많은 지도 데이터를 생성하고 추론 능력을 강화하려고 하며, 증강 데이터 양에 대한 확장 관계를 분석했습니다. 우리는 SFT 모델에서 거절 표본 추출을 적용하여 올바른 추론 경로를 증강 데이터셋으로 선택하고 이를 기반 LLM을 미세 조정하여 SFT보다 나은 성능을 달성하는 RFT로 명명하였습니다.

본 연구의 주요 발견은 다음과 같습니다:
- 사전 학습 손실이 작아질수록(즉, 사전 학습된 모델이 향상될수록) SFT 및 ICL의 모델 추론 성능이 일정 범위 내에서 선형적으로 증가합니다. SFT 성능은 ICL보다 느리게 향상됩니다.
- SFT는 지도 데이터 양이 증가함에 따라 로그 선형 방식으로 개선되며, 데이터 양 증가의 이점은 사전 학습된 모델이 나아질수록 줄어듭니다.
- RFT의 모델 성능은 서로 다른 추론 경로의 수가 증가할수록 향상됩니다. RFT 성능은 SFT보다 느리게 향상됩니다.
- 여러 모델의 거절 표본을 결합하면 RFT 성능이 더욱 향상됩니다. LLaMA-7B, LLaMA2-7B, LLaMA-13B, LLaMA2-13B 모델에서 각각 SFT 대비 49.3%, 50.3%, 52.1%, 55.4%의 정확도를 달성했습니다.

---

## 2 RELATED WORKS

이 논문은 대형 언어 모델(LLMs)을 사용한 수학적 추론 학습과 관련된 선행 연구를 소개하고 있습니다. 최근 연구에 따르면, LLM이 일정 규모를 넘어서면 추론 작업을 해결하는 새로운 능력이 나타날 수 있습니다. 이러한 추론 능력은 모델을 미세 조정하거나, 사례 기반 추론(few-shot prompting) 또는 비사례 기반 추론(zero-shot prompting)을 통해 이끌어낼 수 있습니다. 많은 연구가 수학적 단어 문제(MWP)의 추론 작업에 집중하고 있으며, 이러한 방법들은 다양한 수준의 MWP를 다루는 벤치마크에서 평가됩니다.

LLM의 수학적 추론 능력을 향상시키는 핵심 아이디어는 미세 조정 또는 추론 과정에서 다양한 샘플링된 추론 경로를 집계하는 것입니다. 일부 연구에서는 추론 중 올바른 결과를 선택하기 위해 추론 경로 검증기를 사용하거나 다수결 투표 방식을 통해 최종 결과를 도출합니다. 몇몇 연구는 거절 표본 추출 기법을 비롯해 다양한 샘플링된 추론 경로를 필터링하여 미세 조정 데이터 증강에 활용하였습니다. 거절 표본 추출은 간단하지만 효과적인 미세 조정 증강 기법으로, LLM의 인간 선호도에 맞춘 조정에도 사용됩니다.

또한, 일부 연구에서는 강화 학습 방법을 통해 LLM의 수학적 추론 능력을 향상시키는 방안을 탐구하였고, 결과 기반의 보상 모델링과 과정 기반의 보상 모델링 간의 차이를 논의했습니다. 후속 연구에서는 인간 주석을 통한 대규모 과정 기반 감독 신호를 수집하여, LLM이 과정 기반 보상 모델링에서 더 큰 이득을 얻을 수 있음을 검증했습니다. LLM의 추론 능력을 소형 언어 모델로 증류하는 연구도 존재합니다.

스케일링 법칙에 관한 연구는 모델의 크기가 증가할 때 성능 향상을 이해하고 예측하는 것이 중요합니다. 이전 연구들은 모델 매개변수의 수와 데이터 크기가 손실에 어떻게 기여하는지를 다루었습니다. 이 논문에서는 사전 학습 손실, 지도 데이터 양, 증강 데이터 양을 통해 대형 언어 모델의 수학적 단어 문제 학습에서의 확장 관계를 조사합니다.

---

## 3 THE FACTORS OF MATH REASONING ABILITY IN SUPERVISED LLM

이 논문의 목표는 지도 학습된 대형 언어 모델(LLMs)의 수학적 추론 성능을 이해하는 것입니다. 우리는 사전 학습된 LLM(기호로 $\rho$)이 지도 학습된 추론 데이터셋($\mathcal{D}$)에서 추론 능력을 학습하기를 기대합니다. 데이터셋 $\boldsymbol{D}$는 $\{q_{i},r_{i},a_{i}\}_{i}$의 형태로 정의되며, 여기서 $q$는 질문, $r$은 사고 사슬(chain-of-thought) 추론 경로, $a$는 숫자 형태의 답변을 나타냅니다.

이 데이터셋 $\mathcal{D}$를 사용하여 지도 미세 조정(SFT)을 수행하고, 그 결과로 SFT 모델 $\pi$를 얻습니다. 우리는 테스트 세트에서 $\pi$를 사용하여 탐욕적 해석(greedy decoding)으로 추론 경로와 답변을 생성하고, 정확도(즉, acc 또는 maj1 $@1$)를 우리의 평가 지표로 사용하여 그 결과를 보고합니다.

---

## 3.1 MODEL ACCURACY VS. PRE-TRAINING LOSS

이 논문에서는 모델의 매개변수 수가 추론 능력의 유일한 지표가 될 수 없다고 주장하며, LLaMA 모델이 GPT-3 모델보다 우수한 성능을 보였다는 것을 예로 들고 있습니다. LLMs가 서로 다른 아키텍처, 모델 매개변수, 사전 학습 토큰 수를 가지고 있음에도 불구하고, 사전 학습 손실이 수학적 추론 능력의 안정적인 성능 지표임을 발견하였습니다. 이에 따라, 우리는 모델 매개변수나 사전 학습 토큰 수 대신 사전 학습 손실을 사용하여 모델을 대표할 수 있다고 보았습니다.

연구에서는 GPT-3, LLaMA, LLaMA2, GPT-4 모델의 SFT(지도 미세 조정)와 ICL(8-shot) 성능을 분석했습니다. 이 모델들의 사전 학습 손실은 이들 논문에서 관찰되며, 서로 다른 사전 학습 데이터셋과 토크나이저에 해당하므로 엄격히 비교할 수는 없지만, 손실 간의 경향성은 유의미합니다. 연구에서는 GPT-3의 미세 조정 결과를 사용하였고, LLaMA와 LLaMA2는 GSM8K 훈련 세트에서 미세 조정하였습니다.

그림 2에서는 다음과 같은 결과를 확인할 수 있습니다:

- 사전 학습 손실은 주어진 손실 범위 내에서 SFT와 ICL 정확도와 대략적으로 음의 선형 관계를 가집니다.
- SFT는 ICL보다 꾸준히 더 나은 성능을 보이지만, 사전 학습 손실이 낮을수록 그 향상 폭이 줄어듭니다.

SFT와 ICL 정확도의 선형 관계는 주어진 범위 내에서만 작동할 수 있습니다. 그 이유는 (1) ICL의 기울기가 SFT보다 가파르지만, SFT 성능이 ICL 성능보다 더 커야 하기 때문이며, (2) 정확도는 1보다 클 수 없고 0보다 작을 수 없습니다. 이는 이론적으로 $\log(-acc)$ 대신 acc를 종속 변수로 사용해야 하지만, 사전 학습 손실과 acc 사이의 명백한 선형 관계를 발견하고 이를 종속 변수로 사용했습니다. LLaMA-2 7B(13B)는 LLaMA 7B(13B)의 계속 학습의 근사값으로 간주될 수 있으며, 매개변수 수를 변경하지 않고도 더 오랜 시간 학습하면서 ICL과 SFT 성능 모두 향상됩니다.

관찰 결과에 따르면, 추론 능력을 개선하는 한 가지 효과적인 방법은 더 낮은 사전 학습 손실을 가진 더 나은 기본 모델을 학습하는 것입니다. 사전 학습 손실이 낮은 모델은 미세 조정을 통해 향상 폭이 작을 수 있습니다. 이는 모델이 사전 학습 중 더 많은 추론 능력을 이미 획득하여 지도 데이터에서 제공할 수 있는 감독 신호가 적기 때문일 수 있습니다.

---

## 3.2 MODEL ACCURACY VS. SUPERVISED DATA COUNT

지도 미세 조정(SFT)은 LLM의 추론 능력을 향상시키지만, 지도 데이터 양이 모델의 성능 개선에 어떤 영향을 미치는지 분석하고자 합니다. 연구에서는 GSM8K 훈련 세트의 데이터를 $\{1, 1/2, 1/4, 1/8, 1/16, 1/32\}$ 양만큼 사용하여 LLaMA와 LLaMA2를 미세 조정하였습니다(자세한 내용은 부록 A.2 참조). 이 실험을 통해 지도 데이터가 더 많을 경우 모델 성능이 어떻게 변할지 외삽하려고 합니다.

그림 3에서는 다양한 지도 데이터 양으로 훈련한 결과를 나타내고 있습니다. 이 그림을 통해 다음과 같은 관찰을 할 수 있습니다:

- 모델 성능은 데이터 양에 대해 로그 선형 관계를 가집니다. 데이터 양이 두 배가 되면 성능은 한 단위 증가합니다.
- 더 나은 모델은 자체 ICL 성능을 능가하기 위해 더 많은 데이터 양이 필요합니다.
- 더 나은 모델은 지도 데이터 양이 두 배가 될 때 이점이 적습니다.

로그 선형 관계는 $\{1, 1/2, 1/4, 1/8\}$ 범위의 훈련 데이터 양에서 안정적입니다. 관찰에 따르면, 특히 성능이 떨어지는 모델의 경우 성능을 개선하기 위해 훈련 데이터세트를 확장하는 것이 간단합니다. 그러나 성능이 더 좋은 모델은 그 이점이 덜한데, 이는 더 나은 모델이 이미 사전 학습 중에 더 많은 추론 능력을 학습했기 때문이라는 점과 일치합니다.

---

## 3.3 MODEL ACCURACY VS. AUGMENTED DATA COUNT

수학적 추론 문제에 대한 레이블이 있는 데이터를 늘리는 것은 어려운 일이며, 특히 새로운 질문을 제안하는 것은 더욱 어렵습니다. 교육을 받은 학생은 하루에 수백 개의 수학 문제를 해결할 수 있지만, 다양한 교육적 수학 문제를 만드는 것은 어렵습니다. 따라서 기존 리소스를 활용하여 새로운 데이터를 증강하는 방향으로 초점을 전환하였습니다. 새로운 질의를 증강하거나 수정하는 방식(부록 D.1 및 D.2 참조)은 기본 SFT에 비해 별다른 성능 향상을 가져오지 못했습니다. 하지만 거절 표본 추출의 단순화된 버전이 새로운 추론 경로를 증강하는 효과적인 방법임을 발견했고, 이는 모델 성능을 향상시키는 데 기여했습니다. RFT(거절 표본 추출 미세 조정) 증강 데이터에서 가장 중요한 요소는 서로 다른 추론 경로의 수임을 확인했습니다.

연구 결과를 통해 다중 모델에서 거절 표본을 결합하여 LLaMA-7B 모델을 SFT의 35.9보다 향상된 49.3의 정확도로 미세 조정할 수 있었고, LLaMA-13B 모델은 SFT의 43.0보다 향상된 52.1의 정확도를 달성할 수 있었습니다.

RFT를 적용하여 $k=100$ 후보 추론 경로를 샘플링한 결과는 다음과 같습니다. 7B 및 13B 모델의 경우, RFT는 maj $@1$에서 약 5~6 포인트, maj $@100$에서 약 4 포인트의 성능 향상을 가져옵니다. 그러나 33B 모델에서는 SFT에 비해 성능 향상이 없었으며, 이는 주로 거절 표본 추출로 생성된 증강 샘플 때문입니다. 더 나은 모델이 질문당 더 많은 올바른 추론 경로를 생성할 수 있었지만, 이것이 학습셋 질문에서 다양한 경로를 생성하는 데는 어려움이 있음을 확인했습니다.

RFT의 성능을 이해하기 위해 $k$ 값을 다르게 적용한 결과, $k=3$에서는 SFT보다 2 포인트 향상되며, 대부분의 경우 $k$가 커질수록 성능이 향상되지만, $k$를 두 배로 늘릴 때 성능 이점은 감소하는 경향을 보입니다.

다양한 SFT 모델에서 얻은 추론 경로를 결합함으로써 수학적 추론 성능을 높일 수 있으며, 이는 RFT를 통해 더욱 향상된 성능을 제공했습니다. 특히, 다중 모델에서 집계한 데이터셋($\mathcal{D}_{\mathrm{U13B}}^{\prime}$ 및 $\mathcal{D}_{\mathrm{U33B}}^{\prime}$)을 사용하면 단일 모델로 학습한 데이터셋을 사용한 것보다 일관되게 더 나은 성능을 보였으며, 이러한 증강 데이터셋은 프리트레이닝의 격차를 메우기에 충분한 추론 감독을 제공합니다.

종합적으로, RFT는 LLM의 수학적 추론 성능을 향상시킬 수 있으며, 다양한 추론 경로를 제공함으로써 (특히 성능이 떨어지는) LLMs의 성능을 향상시키는 데 기여함을 확인했습니다.

---

## 4 DISCUSSION

Unfortunately, it seems that the content for "Section 4: DISCUSSION" is missing from your request. If you provide the specific content or points discussed in that section, I can help explain it based on our previous discussions. Please copy and paste the text from section 4 here, and I'll assist you with a detailed explanation.

---

## 4.1 DIFFERENT DISTRIBUTION OF REASONING PATHS

이번 섹션에서는 RFT(거절 표본 추출 미세 조정) 모델이 다양한 추론 경로를 학습하여 올바른 답에 도달하는지를 조사합니다. 이를 위해, LLaMA 및 LLaMA2 7B 및 13B 모델을 $\mathcal{D}_{\mathrm{U13B}}^{\prime}$ 데이터셋에서 미세 조정하였습니다. 추론 과정에서는 각 테스트 세트 질문에 대해 훈련된 모델에서 온도 0.7로 샘플링한 100개의 다른 추론 경로를 생성하였습니다. 각 질문에 대해, 올바른 답을 도출하는 100개의 샘플링된 추론 경로에서 나타나는 다른 계산 과정의 수를 계산하고, 테스트 세트 질문에 대한 히스토그램을 그렸습니다. 비교를 위해 자체 샘플링된 데이터셋에서의 SFT 및 RFT 모델(RFT $k=100$)도 포함했습니다.

결과는 그림 7에 나타나 있으며, RFT를 $\mathcal{D}_{\mathrm{U13B}}^{\prime}$에서 훈련한 모델들이 RFT $k=100$ 및 SFT로 훈련된 모델들보다 다양한 계산 과정을 통해 더 많은 질문을 해결할 수 있음을 보여줍니다. SFT 모델의 경우, 샘플링된 모든 추론 경로가 단일 계산 과정에만 해당하는 질문의 수가 많으며, SFT 모델은 하나의 질문에 대해 8개 이상의 다른 계산 과정을 거의 생성하지 못하는 것으로 나타났습니다. 이러한 분석은 교육 데이터에 다양한 추론 계산 경로가 포함되면 LLM이 수학 문제 해결을 위한 다양한 추론 논리를 찾을 수 있게 함을 보여줍니다.

표 3과 그림 7을 통해 RFT가 수학적 추론 성능을 향상시키며, 다양한 추론 경로를 제공함으로써 모델의 성능을 더욱 향상시킬 수 있음을 확인할 수 있습니다. 다양한 SFT 모델은 거절 표본 추출을 통해 서로 다른 계산 프로세스의 추론 경로를 제공할 수 있으며, 이는 RFT를 위한 더욱 다양한 교육 데이터를 제공합니다. 그러나 매개변수 크기가 큰 LLM은 학습 질문의 과적합으로 인해 다양한 추론 경로를 생성하는 데 어려움을 겪을 수 있습니다.

결론적으로, RFT는 SFT 모델의 거절 표본 추출을 통해 다양한 추론 경로를 제공함으로써 LLM의 수학적 추론 성능을 향상시킵니다. 이를 통해 SFT보다 우수한 성능을 달성할 수 있으며, 거절 표본 추출을 통해 다양한 모델에서 다양한 추론 경로를 결합함으로써 성능을 더욱 향상시킬 수 있습니다.

---

## 4.2 TOWARDS EXCELSIOR MATHEMATICAL REASONING

이번 섹션에서는 LLM의 수학적 추론 능력을 향상시킬 수 있는 두 가지 주요 요인에 대해 논의합니다. 첫 번째는 LLM을 사전 학습할 때 손실을 줄이는 것이고, 두 번째는 거절 표본 추출을 통한 미세 조정입니다. 다양한 실험을 통해, 이 두 가지 요인이 LLM의 수학적 추론 성능과 어떻게 확장 관계를 맺고 있는지를 실험적으로 검증했습니다. 지속 가능성을 고려하여, 두 요인에 따라 LLM의 수학적 성능을 추론하는 데 필요한 계산 자원을 추정하고, 성능을 더 효율적으로 개선하는 방법을 논의합니다.

Table 4에 따라 사전 학습, SFT, RFT 추론, 그리고 RFT의 FLOPs와 GPU 시간을 추정했습니다. SFT와 RFT의 시간 비용은 사전 학습에 비하면 미미한 수준이며, 언제든지 SFT와 RFT를 사용해 모델 성능을 개선할 수 있습니다. 하지만 성능을 더 향상시키기 위해서는 훨씬 많은 표본 수가 필요하고, 특정 수학적 추론 문제에 대한 서로 다른 추론 경로의 양은 상한선이 존재하기 때문에 RFT를 통해 성능을 더 끌어올리는 것은 어려울 수 있습니다.

본 논문의 결과에 따르면, 성능은 $\mathrm{RFT}>\mathrm{SFT}>\mathrm{ICL}$ 순으로 좋아지지만, 그 개선 속도는 $\mathrm{RFT<SFT<ICL}$ 순으로 느립니다. 만약 모든 것을 아우르는 언어 모델이 사전 학습 손실이 코퍼스의 무작위성과 같아서 $\mathrm{RFT}=\mathrm{SFT}=\mathrm{ICL}=100$일 수 있다면, 더 나은 언어 모델을 사전 학습할 때 모델의 성능은 여전히 $\mathrm{RFT}>\mathrm{SFT}>\mathrm{ICL}$ 순을 따르지만, 성능 격차는 점차 줄어들 것입니다. RFT 모델을 얻는 데 많은 노력이 들지 않으므로, 가장 중요한 것은 모델의 사전 학습 손실을 줄이는 것입니다. LLaMA-7B에서 LLaMA2-7B로 이동할 때, $4.2 \times 10^{22}$ FLOPs가 추가되어 사전 학습 손실이 0.05 감소하면서 RFT-U33B 설정에서 2.1의 개선이 이루어졌습니다. LLaMA-7B에서 LLaMA-13B로 이동할 때도 유사한 개선이 관찰됩니다. 사전 학습 손실을 최소화하는 데 많은 비용이 들긴 하지만, 더 나은 사전 학습이 다른 모든 작업에도 이점을 줄 수 있음을 믿고 있습니다.

---
