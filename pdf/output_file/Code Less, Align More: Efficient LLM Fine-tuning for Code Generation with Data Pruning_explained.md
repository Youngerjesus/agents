
## Abstract

이 논문은 코드 생성에 특화된 대형 언어 모델(LLM)에 대해 다루고 있습니다. 이전 연구에서는 합성 코드 생성을 통해 훈련 데이터의 양을 늘리면 뛰어난 성능을 발휘한다는 것을 보여주었습니다. 본 논문에서는 코드 LLM의 훈련 효율성을 높이기 위한 데이터 정제 방법을 탐구합니다. 다양한 클러스터링과 정제 지표를 결합하여 훈련 데이터를 선택적으로 줄이면서도 생성된 코드의 정확성과 기능성을 유지하는 기술을 제시합니다. 실험 결과 합성 훈련 데이터 생성에는 상당한 중복이 있다는 것을 발견했으며, 훈련 데이터의 단 10%만 사용해도 기준 성능을 대부분 유지할 수 있음을 보여주었습니다. 더불어, 훈련 데이터를 적당히 줄이는 것이 기준 결과의 일관된 개선을 가져온다는 것을 확인했습니다. 이러한 정제 전략은 계산 자원을 줄일 뿐만 아니라 코드 생성의 전반적인 품질을 향상시킵니다.

---

## 1 Introduction

이 논문에서 소개되는 대형 언어 모델(LLM)의 성능은 훈련 데이터 세트의 크기와 품질에 크게 의존합니다. 최첨단 코드 LLM 모델인 CodeAlpaca, WizardCoder, MagicCoder는 합성 코드 생성을 통해 감독 학습 데이터 세트를 대폭 확장하여 탁월한 성능을 달성했습니다. 이러한 합성 코드 생성 접근법에는 Self-Instruct, Evol-Instruct, OSS-Instruct 등의 기술이 개발되었습니다. 그러나 이러한 확장은 훈련 비용을 증가시키고 많은 계산 자원을 요구하여 비용이 많이 들고 접근성을 낮춥니다.

최적의 성능을 얻기 위해서는 크고 품질 높은 데이터 세트가 필요하지만, 최근에는 더 효율적인 미세 조정 방법에 대한 관심이 높아졌습니다. 새로 소개된 '표면적 정렬 가설'에 따르면, LLM의 지식 대부분은 사전 훈련에서 획득되며, 사람의 선호에 맞추기 위해서는 최소한의 조정 데이터만 필요합니다. 파라미터 효율적인 미세 조정(PEFT) 방법이나 훈련 중 데이터 샘플을 반복적으로 선택하는 능동 학습도 이러한 방향을 추진합니다.

데이터 선택 및 정제 방법 또한 문헌에서 많이 탐구되었습니다. 신중하게 데이터를 정제하면 전체 데이터 세트를 사용하는 것보다 나은 성능을 발휘할 수 있다는 증거가 있습니다. 그러나 이러한 방법들은 주로 계산 비용이 많이 듭니다.

이 논문에서는 코드 데이터 세트에 맞춘 효율적인 정제 전략의 격차를 메우고자 합니다. 대부분의 대규모 코드 데이터는 합성으로 생성되어 유사한 형태의 데이터가 많이 중복되며, 많은 지시-코드 쌍이 잡음이 많은 상태입니다. 효과적인 데이터 선택 및 정제를 통해 모델 성능을 향상시키는 것이 중요합니다.

우리는 확장 가능하고 효과적인 데이터 정제 방법을 제시하여 LLM에서 코드 생성을 향상했습니다. 우리 방법은 문제 지시와 코드 솔루션에 기반하여 데이터 샘플을 클러스터링하고 차원 축소를 적용하여 계산 부담을 줄입니다. 그런 다음 다양한 정제 척도를 사용해 각 클러스터에서 대표적인 부분 집합을 선택합니다. 대규모 데이터 세트와의 실험에서 우리의 방법은 훈련 데이터를 크게 줄이면서도 모델 성능을 유지하거나 개선할 수 있음을 보여줍니다. 주요 발견 사항은 다음과 같습니다:

- 우리는 대규모 합성 코드 세트에서의 정제를 최초로 연구하고, 비지도 학습 방법을 바탕으로 효율적이며 확장 가능한 정제 전략을 만듭니다.

- 합성 코드 데이터의 중복이 많으며, 전체 데이터의 10%만으로도 대부분의 기준 성능을 유지함을 발견했습니다.

- 데이터 세트를 적당히 정제함으로써 모델 성능이 꾸준히 향상될 수 있음을 확인했습니다.

- 세부적인 제어 실험을 통해 클러스터링 알고리즘이 중요하며, 정제 척도의 중요성은 상대적으로 낮음을 발견했습니다.

---

## 2 Related Work

이 절에서는 코드 생성에 대한 대형 언어 모델(LLM)의 발전을 2.1절에서 검토하고, 이후 2.2절에서 교육적 미세 조정에 대한 이전 연구를 살펴봅니다. 마지막으로, 2.3절에서는 데이터 선택 및 정제 방법에 관한 과거 연구를 논의합니다.

---

## 2.1 Large Language Models for Code Generation

코드 생성에 대한 대형 언어 모델(LLM)의 발전은 놀라울 정도로 크게 이루어졌습니다. CodeAlpaca 모델은 LLaMA 모델의 기능을 확장하며, Self-Instruct 기법을 통해 생성된 20,000개의 지시 데이터를 포함시켰습니다. 이 기법은 언어 모델을 자가 생성 지시에 맞추도록 합니다. CodeLlama는 LLaMA2에서부터 미세 조정하여 이 방법론을 더욱 발전시키며, 역시 Self-Instruct 기법으로 생성된 14,000개의 지시 데이터를 활용합니다.

WizardCoder는 Evol-Instruct 방법을 사용해 CodeAlpaca 데이터 세트를 더 발전시킵니다. 이 방법은 지시 데이터의 깊이와 폭 모두에서 데이터를 반복적으로 진화시킵니다. 반면에 MagicCoder는 OSS-Instruct 기법을 활용해 라벨이 없는 오픈 소스 코드 스니펫으로부터 지시 데이터를 생성합니다. 이 방식으로 StarCoder 데이터셋을 기반으로 한 75,000개의 샘플을 구성합니다.

---

## 2.2 Instructional Fine-tuning

언어 모델을 교육적 데이터 세트로 미세 조정하는 것은 모델 성능과 인간 선호 및 안전성과의 정렬에서 두드러진 개선을 제공하는 강력한 기법으로 떠올랐습니다. 다양한 교육적 작업을 탐구함으로써, [Wei et al., 2021]는 미세 조정을 통해 새로운 작업에 대한 제로샷 성능이 크게 향상됨을 보여주었습니다. 이를 기반으로, [Chung et al., 2024]는 작업 수와 모델 크기를 확장함으로써 다양한 모델 구조에서 상당한 성능 개선을 달성할 수 있음을 입증했습니다. [Peng et al., 2023]는 대형 언어 모델(LLM)을 활용하여 고품질의 지시 따름 데이터를 생성함으로써 새로운 작업에서의 제로샷 성능이 향상됨을 보여주면서 이 분야를 더욱 발전시켰습니다.

최근 연구 [Zhou et al., 2023]는 '표면적 정렬 가설'을 도입하며, LLM의 지식 대부분은 사전 훈련 중에 획득되며, 인간의 선호와의 정렬에는 최소한의 미세 조정 데이터가 충분하다고 제안했습니다. 이 연구는 단 1,000개의 고품질 지시 데이터 포인트만으로도 LLM 성능이 크게 향상됨을 입증했습니다. 이후 다양한 연구들이 일반 지시 따름을 위한 다양한 필터링 방법론을 통해 데이터세트 품질을 개선하는 데 집중하고 있습니다 [Xu et al., 2023b, Chen et al., 2024, Liu et al., 2023a].


---

## 2.3 Data Pruning for Efficient Training

효율적인 모델 훈련을 위해 정보량이 풍부한 샘플을 선택하는 다양한 데이터 정제 방법이 탐구되어 왔으며, 각 방법은 상황에 맞게 조정됩니다. 데이터 클러스터링은 데이터 정제에서 매우 효과적인 기법으로 널리 사용됩니다. TLDR [Wang et al., 2023]은 KMeans 클러스터링을 사용하여 유사한 데이터 포인트를 그룹화하고 각 클러스터에서 균일하게 샘플링하였습니다. 이 방법은 이미지-텍스트 매칭(ITM) 점수를 사용하여 적합한 비전-텍스트 쌍을 식별하므로, 샘플 선택에 대한 또 다른 관점을 제공합니다. DEFT [Das and Khetan, 2023]는 비감독 코어-셋 선택을 이용하여 클러스터링 기반의 데이터 효율적 LLM 미세 조정을 수행합니다. 이 접근법은 특히 텍스트 편집 응용 분야에서 데이터 효율성을 크게 향상시킵니다.

Hardness [Sorscher et al., 2022], Instruction Following Difficulty (IFD) [Li et al., 2023], SuperFiltering [Li et al., 2024]와 같은 지표는 학습하기 어렵거나 잊기 쉬운 "어려운" 샘플을 식별하는 데 초점을 맞추며, 각각의 데이터 샘플을 훈련하는 동안 추적합니다. 또한 LESS [Xia et al., 2024]와 TracIn [Pruthi et al., 2020]과 같은 샘플 영향 지표는 모델의 그래디언트와 개별 샘플의 영향을 모니터링하지만, 대형 모델과 데이터세트에서는 상당한 계산 부담이 발생할 수 있습니다. 외부 오라클의 품질 지표 [Chen et al., 2024; Liu et al., 2023a]는 ChatGPT와 같은 강력한 언어 모델을 활용하여 데이터 선택을 수행하지만, 외부 오라클을 사용하는 것은 비용 제약 때문에 항상 가능하지는 않습니다.

---

## 3 Methodology

이 논문의 목표는 높은 품질의 대표적인 데이터 샘플을 선택하여 이들을 훈련함으로써 전체 데이터 세트를 사용할 때와 동등하거나 더 나은 성능을 얻는 것입니다. 대규모 데이터세트로 LLM을 효과적으로 미세 조정하기 위한 효율적인 데이터 정제 개요는 그림 1에 설명되어 있습니다. 먼저, 임베딩 모델을 사용하여 지시-코드 쌍을 벡터 표현으로 투영합니다. 그런 다음, 계산 복잡성을 줄이기 위해 특징 표현의 차원을 낮추고, 클러스터링을 적용하여 유사한 데이터 샘플들을 식별하고 그룹화합니다. 마지막으로, 데이터 크기를 줄이기 위해 정제 지표를 적용합니다. 자세한 알고리즘은 Algorithm 1에 나와 있습니다.

코딩 데이터 세트를 다룰 때 두 가지 주요 선택 방향을 고려할 수 있습니다: 구문적(syntactical) 및 의미적(semantic)입니다. 구문적으로 다르지만 의미적으로 동등한 프로그램을 선택하거나 그 반대로 선택하는 것은 비효율적일 수 있습니다. 우리의 설계는 구문적 차이를 식별하는 데 중점을 둡니다. 프로그램 간의 의미적 차이를 감지하기 위해서는 일반적으로 퍼징(fuzzing) 기법이 필요하며 [Chen et al., 2018], 이는 더 큰 테스트 샘플을 만들고 프로그램을 실행하여 동작 기반으로 그룹화하는 작업을 포함합니다. 이러한 접근은 계산 비용을 줄이려는 우리의 목표와 상충됩니다. 따라서, 우리의 방법은 효율적이고 효과적인 데이터 선택을 달성하기 위해 구문적 분석에 중점을 둡니다.

---

## Algorithm 1 Data Pruning Algorithm

Algorithm 1은 데이터 정제 알고리즘의 주요 단계를 설명합니다. 이 알고리즘은 고품질의 데이터 샘플을 선택하여 훈련 데이터 크기를 줄이면서도 성능을 유지하거나 향상시키는 것을 목적으로 합니다. 

1. **초기화 단계**: 임베딩 모델과 압축 비율을 초기화합니다.

2. **임베딩 적용**: 주성분 분석(PCA)을 사용하여 데이터의 차원을 축소한 후, 이를 이용해 임베딩을 투영합니다.

3. **클러스터링**: 차원이 축소된 데이터(X)를 사용하여 클러스터링 알고리즘을 적용합니다. 이 단계에서는 유사한 데이터 샘플을 그룹화합니다.

4. **클러스터 처리**: 각 클러스터별로 다음의 단계를 수행합니다.

   - **정제 점수 계산**: 각 클러스터 내의 항목들을 대상으로 정제 지표를 사용하여 점수를 계산합니다.
   
   - **랜덤 선택**: 계산된 점수를 기반으로 항목들을 무작위로 선택합니다. 점수가 높을수록 선택될 확률이 높아집니다.
   
   - **클러스터 업데이트 및 선택 모음에 추가**: 선택된 항목들을 해당 클러스터와 최종 선택 목록에 업데이트합니다.

5. **결과 출력**: 최종적으로 선택된 데이터 샘플을 출력합니다. 

이 알고리즘은 데이터를 효율적으로 줄이면서도 정보가 풍부한 샘플을 유지하여 후기 훈련의 성능을 최적화하려고 합니다.

---

## 3.1 Dimension Reduction

3.1 절에서는 차원 축소 과정을 설명하고 있습니다. 각 지시-코드 쌍을 벡터 표현으로 변환하는 과정을 통해 클러스터링 및 데이터 정제 지표의 계산 효율성을 높이고자 합니다. 최근 연구에 따르면, 대형 언어 모델(LLM)의 임베딩 기반 거리 측정은 구문적 차이를 효과적으로 포착할 수 있습니다.

계산 복잡성을 줄이기 위해 우리는 주성분 분석(PCA)을 사용하여 벡터 표현의 차원을 축소합니다. LLM에서 추출한 표현은 종종 천 개 이상의 차원을 가지기 때문에, 차원 축소는 필수적입니다. 또한, 높은 차원 공간에서 여러 커널 방법을 사용하는 정제 지표가 차원의 저주에 의해 방해받지 않도록 합니다.

그림 1은 대규모 데이터 세트를 사용하여 LLM을 미세 조정하기 위한 효율적인 데이터 정제 과정을 보여줍니다. 첫 번째 단계에서는 지시 따름 데이터를 임베딩으로 변환하고 특징 표현의 차원을 줄입니다. 두 번째 단계에서는 클러스터링을 적용해 유사한 데이터 샘플을 식별하고 그룹화합니다. 마지막으로, 다양한 정제 지표를 적용하여 데이터 크기를 추가로 줄이는 과정을 진행합니다.

---

## 3.2 Clustering

3.2 절에서는 클러스터링 과정을 설명합니다. 클러스터링은 유사한 지시-코드 쌍을 그룹화하는 중요한 단계로, 다양한 대표 샘플을 선택하는 데 도움을 줍니다. 클러스터링을 수행하기 전에, 각 특징이 거리 계산에 동등하게 기여하도록 벡터 표현을 정규화합니다.

각 클러스터로부터 우리는 전체 데이터세트를 대표할 수 있는 지시-코드 쌍의 하위 집합을 샘플링합니다. 샘플링 전략은 다양한 정제 지표에 의해 결정됩니다. 이 과정은 데이터의 다양성과 대표성을 보장하면서도 효율적인 데이터 선택을 가능하게 합니다.

---

## 3.2.1 KMeans

3.2.1 절에서는 KMeans 알고리즘에 대해 설명합니다. KMeans 알고리즘은 데이터를 \( k \)개의 클러스터로 나누는 방법입니다. 클러스터 내부의 제곱합을 최소화하여 각 클러스터가 가능한 한 밀집하도록 보장합니다. KMeans의 주요 장점은 대규모 데이터세트를 처리할 때의 확장성과 효율성입니다. 즉, 이 알고리즘은 많은 양의 데이터를 클러스터링하는 데 적합하며, 처리 속도가 빠른 것이 특징입니다.

---

## 3.2.2 Agglomerative Clustering

3.2.2 절에서는 응집적(애글로머레이티브) 클러스터링에 대해 설명합니다. 이 방법은 연결 기준을 이용하여 중첩된 클러스터를 형성합니다. 응집적 클러스터링의 장점은 클러스터의 수를 사전에 지정할 필요가 없다는 것입니다. 이러한 유연성 덕분에 데이터 세트의 품질을 유지하면서도 보다 정교한 대표 샘플을 선택할 수 있습니다. 즉, 데이터 구조에 따라 동적으로 클러스터의 개수를 결정할 수 있어, 다양한 데이터 특성을 반영할 수 있습니다.

---

## 3.2.3 HDBSCAN

3.2.3 절에서는 HDBSCAN에 대해 설명합니다. HDBSCAN은 노이즈를 포함한 응용 프로그램의 밀도 기반 공간 클러스터링으로, 거리 척도로 측정된 고밀도 영역에 위치한 핵심 샘플을 기반으로 클러스터링을 수행합니다. 이 방법은 우리 디자인 가설과 잘 맞아떨어지는데, 이를 통해 가장 구문적으로 대표적인 데이터 샘플을 찾을 수 있기 때문입니다. 특히, HDBSCAN은 클러스터링 과정에서 핵심 샘플에 포함되지 않는 노이즈 샘플을 이상값으로 처리하여 제거합니다. 따라서 효율적으로 중요하고 대표적인 정보만을 추출할 수 있습니다.

---

## 3.3 Pruning Metrics

3.3 절에서는 정제 지표에 대해 설명합니다. 정제 지표를 선택하는 기준은 지속적으로 구문적 차이를 감지하고 가장 대표적인 샘플을 찾는 아이디어와 일치합니다. 이 절에서는 실험에서 탐구한 여러 정제 지표에 대해 설명합니다. 이러한 지표는 데이터 정제 과정에서 샘플의 중요성이나 대표성을 평가하는 데 사용되며, 결과적으로 구문적으로 다양한 데이터를 선택하는 데 기여합니다.

---

## 3.3.1 Diversity Metric

3.3.1 절에서는 다양성 지표(Diversity Metric)에 대해 설명합니다. 우리는 거리 기반의 지표를 사용하여 개별 샘플의 다양성 점수를 평가합니다. 다양성 점수는 다음과 같이 정의됩니다:

$$
d_{i}=\operatorname*{min}_{\mathbf{x}\in\mathcal{K}\backslash\{\mathbf{x}_{i}\}}\mathrm{dist}(\mathbf{x}_{i},\mathbf{x}),
$$  

여기서 \( x_{i} \)는 벡터 표현을 의미하고, \(\mathrm{dist}\)는 거리 함수, \(\mathcal{K}\)는 데이터 세트 클러스터 내 선택된 쿼리 집합을 나타냅니다. \( d_{i} \)는 샘플 \( x_{i} \)의 다양성 점수입니다. 임베딩의 점곱을 거리 함수로 사용하며, 이는 정제 이전에 임베딩을 정규화하기 때문입니다. 이 지표는 데이터 샘플 간의 다양성을 측정하여 보다 다양한 데이터 포인트를 선택하는 데 도움을 줍니다.

---

## 3.3.2 Density Metric

3.3.2 절에서는 밀도 지표(Density Metric)를 사용하는 방법을 설명합니다. 우리는 커널 밀도 추정(KDE)을 사용하여 특징 공간에서 샘플의 밀도를 측정합니다. KDE는 랜덤 변수의 확률 밀도 함수를 추정하는 기법입니다. 샘플 \(\mathbf{x}_{i}\)의 밀도 점수는 다음과 같이 계산됩니다:

$$
\rho(\mathbf{x}_{i})=\frac{1}{n h^{d}}\sum_{j=1}^{n}K\left(\frac{\mathbf{x}_{i}-\mathbf{x}_{j}}{h}\right),
$$  

여기서 \(K\)는 커널 함수(일반적으로 가우시안), \(h\)는 대역폭 파라미터, \(d\)는 특징 공간의 차원, \(n\)은 샘플의 총 수입니다. 커널 함수 \(K\)는 근처 점들이 밀도 추정에 미치는 영향을 측정합니다. 높은 밀도 점수는 샘플이 많은 유사한 인스턴스가 있는 영역에 위치함을 나타내며, 이는 다양성을 유지하는 데 덜 중요할 수 있음을 시사합니다.

표 1은 다양한 LLM이 HumanEval과 MBPP에서 가진 \(p a s s@1\) (\(\%\)) 성능 결과를 나타냅니다. 우리는 이전 연구 결과를 직접 사용했으며, HDBSCAN 클러스터링 알고리즘과 다양성 정제 지표(HDBSCAN-diversity)를 사용하여 결과를 보고합니다. 클러스터링 및 훈련의 무작위성을 고려하여, EvalPlus [Liu et al., 2023b]로 평가된 세 번의 실행에서 평균 결과를 보고합니다.

---

## 3.3.3 Random

3.3.3 절에서는 가장 기본적인 기준선인 무작위 선택(Random Selection)에 대해 설명합니다. 무작위 선택은 선택된 클러스터나 전체 훈련 데이터세트에서 데이터를 임의로 샘플링하여 교육 조정을 수행하는 방법입니다. 이 방법은 복잡한 정제 지표를 사용하는 것보다 단순하지만, 여전히 대표적인 성능 비교를 위한 기본적인 기준점으로 사용될 수 있습니다. 클러스터링을 사용하지 않고 전체 데이터세트를 대상으로 랜덤 샘플링을 할 수도 있습니다.

---

## 4 Experiments

4절에서는 실험에 대한 전반적인 내용을 설명합니다. 먼저, 4.1절에서 실험 설정에 대해 소개하고, 4.5절에서 주요 결과를 제시합니다. 이 절에서는 정제 방법이 전체 데이터 세트를 사용한 훈련에 비해 성능이 얼마나 개선되었는지를 네 개의 데이터세트, 특히 \( \mathbf{MBPP(+)} \)와 HumanEval \( (+) \)에서 강조합니다. 또한, 다양한 압축 비율에서의 \( \text{pass@1} \) 점수를 기준 방법들과 비교하여 설명합니다. 이를 통해 우리의 정제 방법이 데이터 효율성과 모델 성능을 어떻게 향상시키는지 확인하고자 합니다.

---

## 4.1 Setup

4.1절에서는 실험 설정에 대해 설명합니다. 우리는 실험의 기본 모델로 DeepSeek-Coder-Base 6.7B를 사용했는데, 이는 오픈 소스 모델 중에서 뛰어난 성능을 보였기 때문입니다. 모든 실험에서 주성분 분석(PCA) 알고리즘을 사용하여 데이터의 차원을 10으로 줄였습니다. 클러스터링 알고리즘과 훈련의 무작위성을 고려하여, 각 실험을 3회 반복하여 평균값과 표준 편차를 보고했습니다. 이를 통해 결과의 신뢰성과 일관성을 보장하고자 했습니다.

---

## 4.2 Training

4.2절에서는 훈련에 대한 내용을 다룹니다.

**데이터셋:** 우리는 두 개의 합성 코드 데이터셋을 훈련 데이터로 사용했습니다: Magicoder OSS-Instruct-75K와 Magicoder Evol-Instruct-110K. 이들을 결합하여 총 185K개의 데이터 항목을 대규모 데이터 세트로 사용했습니다.

**미세 조정:** 기본 모델을 미세 조정할 때 두 데이터 세트를 결합하고 셔플링하여 사용했습니다. 이는 원래 Magicoder 구현 방식과 다른데, 원래 구현에서는 OSS-Instruct 데이터로 2회의 에포크를 수행한 후 Evol-Instruct 데이터로 추가로 2회의 에포크를 수행합니다. 이 차이에도 불구하고, 우리의 전체 데이터세트 성능은 MagicoderS-DS 결과와 거의 유사합니다.

**훈련 과정:** 훈련은 파이토치의 분산 데이터 병렬(DDP) 모듈을 사용하여 16개의 NVIDIA A100-80GB GPU에서 수행되었습니다. 학습률은 5e-5로 설정하고, 15 번의 워밍업 단계와 선형 학습률 스케줄러를 사용했습니다. 옵티마이저로는 Adam을 사용했으며, 모든 파라미터 업데이트를 수행하고, 토큰 길이가 4096을 초과하는 경우는 잘라냈습니다. 데이터셋 크기가 원본 크기의 10% 이상일 때는 배치 크기를 512로, 작은 데이터 실험에서는 배치 크기를 32로 설정했습니다. 데이터셋 크기와 관계없이 2회의 에포크 동안 미세 조정을 진행했습니다.

**그림 2:** HDBSCAN-다양성과 클러스터링 없는 무작위 방법의 성능 비교를 다양한 벤치마크에서 보여줍니다. 우리의 전략은 다양한 데이터세트에서 기준 방법보다 큰 차이로 뛰어난 성능을 보였으며, 심지어 원본 데이터세트의 10% 크기일 때도 MBPP의 경우 전체 데이터세트와 비교해 성능이 더 좋거나 동등했습니다. $@1$ 성능 지표가 다양한 압축 비율에 대해 플로팅되어, 이 방법의 강력함과 효과를 증명합니다. HumanEval은 상대적으로 적은 문제 수로 인해 실험에서 큰 변동성을 보일 수 있습니다.

---

## 4.3 Evaluation

4.3 절에서는 평가 방법에 대해 설명합니다.

**데이터셋:** HumanEval과 MBPP는 코드 생성에 가장 널리 사용되는 벤치마크 중 두 가지입니다. 각각 164개와 1401개의 문제를 포함하고 있습니다. 이러한 벤치마크의 각 작업은 작업 설명(예: docstring)을 프롬프트로 사용하며, LLM이 생성한 코드의 정확성은 몇 가지 테스트 케이스로 확인됩니다. 그러나 이러한 벤치마크의 테스트는 충분하지 않을 수 있기 때문에, 보다 엄밀한 평가를 위해 EvalPlus에 의해 강화된 HumanEval+와 MBPP+를 사용하여 각각 80배와 35배 더 많은 테스트를 수행합니다.

**평가지표:** 이전 연구에 따라, 각 실험에서 우리는 편향되지 않은 \( \text{pass@k} \) 추정치를 사용하며 주로 \( \text{pass@1} \) 지표에 집중합니다:

$$
p a s s@k:=\mathbb{E}_{\mathrm{Problems}}\left[1-\frac{\binom{n-c}{k}}{\binom{n}{k}}\right].
$$  

**추론:** 우리는 EvalPlus의 추론 스크립트를 사용하여 후처리를 진행합니다. vLLM 프레임워크를 채택하고 모든 코드 생성을 위해 탐욕적 디코딩을 사용했습니다. 추론 엔진은 bf16 dtype, 텐서 병렬 크기 2, 최대 길이 4096으로 설정되었습니다. 이를 통해 코드 생성의 정확성을 다양한 테스트 케이스를 통해 엄밀하게 평가하도록 했습니다.

---

## 4.4 Implementation Details

4.4 절에서는 구현 세부사항에 대해 설명합니다.

모든 클러스터링 및 커널 밀도 추정 파라미터는 `sklearn`의 기본 설정을 따릅니다. 클러스터의 최적 수를 선택해야 하는 알고리즘(KMeans와 같은)의 경우, Elbow 방법을 사용하여 추가 클러스터를 추가해도 설명되는 분산이 크게 개선되지 않는 지점을 찾습니다. 정제 지표에 대해서는 Gaussian 커널 대역폭을 결정하기 위해 Scott's Rule을 적용하였고, 다양성 지표를 위해 데이터셋의 10%를 무작위로 선택하여 쿼리 세트(\(K\))로 사용했습니다.

실험에서는 PCA 차원 축소를 벤치마크 데이터셋에 맞춘 후, 지시 데이터에 투영을 적용했습니다. 우리는 OpenAI의 `text-embedding-ada-002` 임베딩 모델을 사용하여 데이터를 인코딩했습니다. 이러한 설정을 통해 데이터 처리 및 클러스터링을 효율적으로 수행했습니다.

---

## 4.5 Main Results

4.5절에서는 주요 결과를 설명합니다.

**표 1**: 이 표는 HumanEval과 MBPP 벤치마크에서 다양한 코드 LLM의 \( \text{pass@1} \) 결과를 보여줍니다. 결과는 탐욕적 디코딩을 통해 계산되었으며, 모든 결과는 HDBSCAN 클러스터링 알고리즘과 다양성 정제 지표(HDBSCAN-diversity)를 사용하여 보고되었습니다. 클러스터링과 훈련의 무작위성을 고려하여, 세 번의 실행에서 평균 결과를 보고했습니다. 특히, 훈련 데이터의 약간의 정제만으로도 HumanEval에서 최대 2.7%, MBPP에서 3.5%까지 성능이 향상될 수 있었습니다. 또한, 데이터세트의 10%만으로도 대부분의 벤치마크 정확도를 유지할 수 있으며, HumanEval에서 3.9%, MBPP에서 1.5%의 성능 저하만 있었습니다. 단지 1%의 데이터(약 700개의 샘플)만으로도, 우리의 방법은 경쟁력 있는 성능을 유지하며, 기본 모델에 비해 큰 성능 향상을 이뤘습니다. 이는 우리의 정제 전략의 효율성을 강조합니다.

**그림 3**: MBPP와 HumanEval 벤치마크에서 극도의 데이터 정제 조건 하에서의 성능 비교를 보여줍니다. MBPP의 \( \text{pass@1} \) 점수는 데이터의 1%만으로도 전체 데이터세트에 근접한 성능을 달성하며, 기본 모델에 비해 4.1%의 향상을 이뤘습니다. HumanEval 벤치마크에서는 데이터 1% 사용 시 성능이 저하되지만, 여전히 기본 모델보다 17.0% 더 높은 성능을 보여줍니다.

**그림 2**: MBPP, MBPP+, HumanEval, HumanEval+ 등 네 개의 데이터 세트에서의 우리의 정제 방법의 세부사항을 보여줍니다. 각 서브플롯은 HDBSCAN-다양성 방법과 무클러스터-랜덤 기준선의 \( \text{pass@1} \) 점수를 다양한 압축 비율에서 비교합니다. HDBSCAN-다양성 방법은 일관되게 무클러스터-랜덤 기준선을 능가합니다. 성능은 조금의 압축에서 개선되며, 10–20%에서 정점에 도달하고, 점차 감소합니다. 이는 HDBSCAN-다양성 방법의 견고함을 강조하며, 90% 압축에서도 전체 데이터세트보다 높은 \( \text{pass@1} \) 점수를 유지합니다.

전반적으로, 이 결과들은 데이터 정제 전략이 중요한 데이터 특성을 보존하고, 상당한 데이터 감소 하에서 모형 성능을 유지하는 데 효과적임을 보여줍니다. 이는 코딩 데이터세트 정제를 위한 우수한 선택임을 입증합니다.

---

## 5 Ablation Studies

5절에서는 우리의 연구에서 수행된 네 가지 소거 연구(ablation study)에 대해 설명합니다. 이 연구들은 데이터 정제의 효과에 미치는 (1) 클러스터링 알고리즘, (2) 정제 지표, (3) 차원 축소, (4) 벡터 표현 입력의 영향을 평가하는 데 초점을 맞추고 있습니다. 연구에서는 주로 MBPP 벤치마크에 초점을 두었는데, 이는 보다 안정적이고 일관된 결과를 제공하기 때문입니다.

**그림 4**: 이 그림은 MBPP 벤치마크에서 다양한 클러스터링 알고리즘과 훈련 데이터셋의 압축 비율에 따른 \( \text{pass@1} \) 점수를 비교합니다. HDBSCAN은 데이터셋의 90% 압축 비율에서도 전체 데이터셋에 비해 더 높은 \( \text{pass@1} \) 점수를 유지하는 강력한 견고성을 보여줍니다. 이는 HDBSCAN이 데이터 축소 상황에서도 성능을 효과적으로 유지할 수 있다는 것을 시사합니다.

---

## 5.1 Compare Clustering Algorithm

5.1 절에서는 클러스터링 알고리즘 비교에 대한 내용을 설명합니다. **그림 4**는 추가적인 정제 지표 없이 다양한 클러스터링 알고리즘을 적용한 결과를 보여줍니다. 평가된 알고리즘은 응집적 클러스터링, HDBSCAN, KMeans, 그리고 클러스터링을 적용하지 않은 기준선(nocluster)을 포함합니다.

결과에 따르면, 클러스터링 알고리즘은 일반적으로 클러스터링을 사용하지 않는 경우에 비해 성능을 개선하며, 특히 높은 압축 비율에서 두드러집니다. HDBSCAN은 일관되게 더 높은 \( \text{pass@1} \) 점수를 유지하여 중요한 데이터 특성을 보존하는 데 있어 견고함을 보여줍니다. KMeans와 응집적 클러스터링도 성능이 우수하지만, 변동성이 더 큽니다. 이러한 결과는 코딩 데이터세트의 데이터 효율성을 높이는 데 있어 클러스터링 알고리즘의 중요성을 강조합니다.

---

## 5.2 Compare Pruning Metrics

5.2 절에서는 다양한 정제 지표가 모델 성능에 미치는 영향을 비교합니다. HDBSCAN 클러스터링 알고리즘을 사용하여, 데이터 크기가 감소함에 따라 이러한 지표들이 성능에 어떤 영향을 미치는지를 평가합니다. **그림 5**에 그 결과가 나타나 있습니다.

결과에 따르면, 정제 지표의 효과는 압축 비율에 따라 다르게 나타납니다. 다양성 지표는 다른 지표에 비해 약간의 향상을 보이지만, 그 향상폭은 크지 않으며 10–40% 압축 비율에서만 효과적입니다. 이는 더 정교한 정제 지표가 일부 이점을 제공할 수 있지만, 그 영향이 제한적일 수 있으며 사용된 클러스터링 알고리즘에도 의존할 수 있음을 시사합니다. 이러한 결과는 복잡한 정제 지표가 항상 더 나은 결과를 보장하지 않는다는 것을 암시합니다.

---

## 5.3 Effect of PCA

5.3 절에서는 주성분 분석(PCA)의 적용이 KMeans 클러스터링 알고리즘과 밀도 지표 성능에 미치는 영향을 평가합니다. **표 2**에서는 50% 압축 비율에서의 결과가 나타나 있습니다.

PCA를 적용하면, MBPP에서는 \( \text{pass@1} \) 점수가 0.6% 미만으로 감소하고, HumanEval에서는 4.3% 정도의 중간 수준의 부정적 영향을 미치며 성능이 저하됩니다. 이러한 영향은 PCA 훈련에 사용된 MBPP와 HumanEval 데이터세트의 불균형 때문일 수 있습니다. HumanEval 데이터세트가 MBPP 데이터세트보다 훨씬 작기 때문에, HumanEval과 유사한 데이터에 대해 최적의 주성분 추출이 이루어지지 않습니다.

그럼에도 불구하고, 차원을 1536에서 10으로 줄이면 KMeans에서 약 12배의 속도 향상이 발생합니다. PCA 없이 HDBSCAN 클러스터링을 수행할 경우 4시간 내에 완료되지 않기 때문에, 해당 결과는 보고하지 않았습니다.

표 2는 PCA를 사용한 경우와 사용하지 않은 경우의 KMeans 클러스터링에서, 50% 압축 비율로의 \( \text{pass@1} \) 점수, 차원 및 데이터 정제 실행 시간(임베딩 및 훈련 제외)을 비교한 것입니다. 이는 PCA가 계산 효율성을 증가시키지만, 데이터셋의 특성에 따라 성능에 부정적인 영향을 미칠 수 있음을 시사합니다.

---

## 5.4 Embeddings for Instruction or Code

5.4 절에서는 임베딩 모델에 대한 입력이 성능에 미치는 영향을 조사합니다. **표 3**에서는 임베딩을 생성할 때 입력으로 지시만, 코드 솔루션만, 또는 둘 모두를 사용하는 것이 성능에 미치는 영향을 분석합니다.

결과에 따르면, 지시와 코드를 모두 임베딩 입력으로 사용하는 것이 각각 하나만 사용할 때보다 더 나은 성능을 발휘합니다. 지시만 사용하거나 코드만 사용할 때의 결과에는 큰 차이가 없습니다. 이는 지시와 코드 샘플이 종종 밀접하게 연결되지만, 데이터 정제 과정에서 둘 다에서 정보량이 풍부한 샘플을 선택하고 다양성을 유지하는 것이 중요하다는 것을 시사합니다.

**표 3**은 KMeans 클러스터링을 사용하여 50% 압축 비율로 수행한 다양한 임베딩 입력에 대한 \( \text{pass@1} \) 점수를 보여줍니다. 지시와 코드를 모두 사용하는 것이 약간의 이점을 제공합니다. 이는 임베딩 입력의 다양성이 데이터 정제의 효과를 높이는 데 기여할 수 있음을 나타냅니다.

---
