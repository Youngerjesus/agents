
## Abstract

이 논문은 대규모 언어 모델(LLM)을 가르칠 때, 범용 도메인에서의 지침을 따르는 데이터를 활용하면 큰 성공을 이룰 수 있음을 강조합니다. 하지만, 이러한 지침 데이터를 사람이 직접 만들어내는 것은 매우 많은 시간과 노동이 필요하며, 복잡한 지침을 인간이 제작하는 데 어려움을 겪을 수 있습니다. 이 논문에서는 인간 대신 LLM을 사용하여 다양한 복잡성 수준의 대량의 지침 데이터를 생성할 수 있는 방법을 제시합니다. 초기 지침의 집합을 시작점으로 하여, 제안된 'Evol-Instruct' 방법을 사용해 이 지침들을 단계적으로 더 복잡하게 다시 작성합니다. 그 후, 생성된 모든 지침 데이터를 LLaMA 모델을 미세 조정하는 데 혼합하여 사용합니다. 이렇게 나온 모델을 WizardLM이라고 부릅니다. 복잡성 균형을 맞춘 테스트 베드와 Vicuna의 테스트 세트에서 인간 평가 결과, Evol-Instruct를 사용하여 생성된 지침이 인간이 만든 것보다 우수하다는 것을 보여주었습니다. 높은 복잡성 부분에 대한 인간 평가 결과를 분석하여, 우리 WizardLM 모델의 출력물이 OpenAI의 ChatGPT 출력물보다 더 선호된다는 것을 증명했습니다. GPT-4 자동 평가에서 WizardLM은 29개 기술 중 17개에서 ChatGPT의 능력의 90% 이상을 달성했습니다. 비록 WizardLM이 몇몇 측면에서 여전히 ChatGPT에 뒤처지지만, AI에 의해 발전된 지침으로 미세 조정하는 것이 LLM을 향상시키는 유망한 방향임을 우리의 연구 결과가 시사합니다. 우리의 코드와 데이터는 https://github.com/nlpxucan/WizardLM에서 공개되어 있습니다.

---

## 1 Introduction

대규모 언어 모델(LLM)은 다양한 자연어 처리(NLP) 작업에 널리 사용되는 접근법이 되었습니다. LLM은 거대한 양의 텍스트 데이터를 기반으로 단어 예측을 수행하여 다양한 입력에 대해 일관되고 유창한 텍스트를 생성할 수 있도록 합니다. 그러나 사용자들이 지정한 지침이나 목표를 따르는 데에는 어려움을 겪어, 실제 상황에서의 유용성과 적용 가능성에 한계가 있습니다.

NLP 분야에서는 최근 LLM이 지침을 더 잘 따르고 도움이 되도록 훈련하려는 여러 시도가 있었습니다. 초기 시도는 다양한 NLP 과제들과 소량의 수작업으로 작성된 지침들을 기반으로 했습니다. 이러한 닫힌 도메인 지침은 두 가지 주된 한계가 있습니다. 첫째, NLP 데이터셋의 샘플이 몇 가지 공통 지침만을 공유하여 다양성이 부족하며, 둘째, 지침이 보통 번역이나 요약과 같이 단일 작업만 요청합니다. 그러나 실제 생활에서는 여러 가지 복잡한 작업을 요구하는 경우가 많습니다. 실제 사용자들이 생성한 범용 도메인 지침 데이터를 사용하여 OpenAI의 LLM (예: InstructGPT와 ChatGPT 4)은 큰 성공을 거두었습니다. 이러한 범용 도메인 지침은 LLM의 무한한 잠재력을 완전히 발휘하게 하고, 더 복잡하고 다양한 작업을 수행할 수 있게 합니다. 그러나 OpenAI와 같이 범용 도메인 지침 데이터를 인간이 생성할 경우 두 가지 문제에 직면하게 됩니다. 전체 주석 과정은 매우 비용이 많이 들고 시간 소모적이며, 인간이 생성한 지침의 난이도 분포는 쉬운 쪽으로 치우치는 경향이 있습니다. 주석자의 전문가 비율이 낮고, 복잡한 지침을 만드는 데 많은 정신적 노력이 필요하기 때문입니다. 따라서 이러한 문제를 기반으로 자동으로 범용 도메인 지침을 대량 생산할 수 있는 방법을 개발하는 것이 중요해집니다.

이 연구에서는 LLM을 활용하여 다양한 난이도의 범용 도메인 지침을 자동으로 대량 생산하여 LLM의 성능을 향상시키는 'Evol-Instruct'라는 새로운 방법을 소개합니다. Evol-Instruct는 간단한 초기 지침에서 시작하여 이를 점진적으로 더 복잡한 지침으로 발전시키는 방법입니다. 특정 프롬프트를 통해 LLM을 사용하여 이 여섯 가지 작업을 수행하고, 실패한 지침을 제거하는 'Elimination Evolving'을 사용하여 여러 번의 반복과정을 거칩니다.

Evol-Instruct의 유효성을 검증하기 위해, 우리는 진화시킨 지침을 사용하여 오픈 소스 LLaMA를 미세 조정하고, 그 성능을 Alpaca와 Vicuna와 같은 기존 최신 방법과 비교합니다. 연구 결과, Evol-Instruct의 지침이 사람에 의해 생성된 ShareGPT 지침보다 우수하며, 동일한 양의 Evol-Instruct 데이터를 사용하여 LLaMA 모델을 미세 조정할 때 Vicuna보다 성능이 높았습니다. 또한, 복잡한 지침에서는 WizardLM의 출력물이 ChatGPT보다 선호되었으며, 이는 Evol-Instruct가 LLM의 복잡한 지침 처리 능력을 크게 향상시킬 수 있음을 나타냅니다.

---

## 2 Related Work

이 섹션에서는 주로 두 가지 종류의 지침 미세 조정 작업에 대해 설명합니다: 닫힌 도메인과 열린 도메인 지침 미세 조정.

**닫힌 도메인 지침 미세 조정:** 초기의 지침-따르기 훈련 연구는 주로 언어 모델(LM)의 크로스 태스크 일반화에 중점을 두었습니다. 이러한 연구에서는 다양한 공공 NLP 데이터셋으로 LM을 미세 조정하고, 다른 NLP 작업 세트에 대해 평가했습니다. T5는 질문 응답, 문서 요약, 감정 분류와 같은 NLP 작업을 통합된 텍스트-투-텍스트 포맷으로 학습시키는 초기 시도를 했습니다. FLAN, ExT5, T0, KnowDA와 같은 연구들은 NLP 작업의 수를 약 100개로 늘렸으며, 각각의 작업에 대해 주의 깊게 설계된 지침을 사용했습니다. ZeroPrompt와 FLAN-T5 같은 연구는 작업의 수를 수천 개로 늘렸습니다. 이러한 연구들은 다양하게 설정된 NLP 작업 지침으로 LM을 미세 조정하는 것이 새로운 작업에서의 성능을 향상시킨다는 것을 일관되게 보여줍니다. 그러나, 이러한 닫힌 형식의 지침으로 훈련된 LLM은 실제 사용자 시나리오에서 종종 실패하곤 합니다.

**열린 도메인 지침 미세 조정:** 우리의 연구는 이 연구 라인에 속합니다. OpenAI는 많은 주석자들을 고용하여 다양한 형태와 풍부한 작업 유형의 인간-제작 지침을 작성했습니다. 이 데이터셋을 바탕으로 OpenAI는 GPT-3을 InstructGPT로 훈련하여 다양한 실제 사용자 지침을 처리할 수 있게 했고, 이는 ChatGPT의 성공으로 이어졌습니다. OpenAI의 이러한 뛰어난 연구는 공개되지 않았기 때문에, 알파카와 비쿠나는 오픈소스 LLM LLaMA를 기반으로 열린 도메인 지침 미세 조정을 적극적으로 탐구했습니다. 알파카는 제한된 수의 수작업으로 작성된 지침 시드 세트(예: 175개의 샘플)로부터 생성된 50k 지침 데이터셋을 사용했습니다. 비쿠나는 ShareGPT.com에서 수집된 ChatGPT와의 70k 사용자 공유 대화를 사용했습니다. 우리의 연구는 지침 미세 조정에 AI가 생성한 데이터를 사용한다는 점에서 InstructGPT와 비쿠나와 다릅니다. 알파카의 자체 지침 생성 방법과 달리, Evol-Instruct는 생성된 지침의 난이도와 복잡성 수준을 제어할 수 있습니다.

---

## 3 Approach

이 섹션에서는 제안된 Evol-Instruct 방법의 세부 사항을 자세히 설명합니다. Figure 2에서 볼 수 있듯이, 이 프로세스는 주로 두 가지 주요 구성 요소로 구성되어 있습니다: '지침 발전기(Instruction Evolver)'와 '지침 제거기(Instruction Eliminator)'입니다. 각 구성 요소의 세부 사항은 섹션 3.2에서 설명되며, 지침 미세 조정 방법은 섹션 3.3에서 설명될 것입니다.

- **지침 발전기(Instruction Evolver):** 이 구성 요소는 간단한 초기 지침을 기반으로 지침을 더 복잡하게 발전시키거나 새로운 지침을 생성하여 다양성을 높이는 역할을 합니다.

- **지침 제거기(Instruction Eliminator):** 이 구성 요소는 발전 과정에서 실패한 지침을 걸러내어 제거하는 역할을 합니다.

Evol-Instruct는 이러한 구성 요소들을 통해 다양한 난이도와 복잡성을 가진 대량의 지침 데이터를 자동으로 생성하는 효율적인 방법을 제공합니다.

---

## 3.1 Defnition of Instruction Data Evolution

이 섹션에서는 지침 데이터의 발전 과정을 정의합니다. 우리는 초기 지침 데이터셋 $D^{(0)}=\big(I_{k}^{(0)},R_{k}^{(0)}\big)_{1\leq k\leq N}$에서 발전을 시작합니다. 여기서 $I_{k}^{(0)}$는 $D^{(0)}$에서 $k$ 번째 지침을, $R_{k}^{(0)}$는 해당 지침에 대한 응답을 의미하며, $N$은 $D^{(0)}$에 포함된 샘플의 수를 나타냅니다.

각 발전 단계에서는 $D^{(t)}$에 있는 모든 $I^{(t)}$를 지침 발전 프롬프트를 LLM에 적용하여 $I^{(t+1)}$로 업그레이드하고, 진화된 $I^{t+\bar{1}}$에 대해 LLM을 사용하여 응답 $R^{\tilde{t}+1}$를 생성합니다. 이렇게 하여 우리는 발전된 지침 데이터셋 $D^{t+\overline{{1}}}$를 얻게 됩니다. 이러한 발전 과정을 $M$번 반복함으로써 순차적으로 $M$개의 발전된 데이터셋 $[D^{(1)}\cdot\cdot\cdot\bar{D}^{(M)}]$을 얻게 됩니다.

우리 연구는 지침 부분과 입력의 명확한 구분 없이 다양한 입력과 작업을 포함하는 개방형 도메인 지침 데이터에 초점을 맞춥니다. 이러한 접근 방식을 통해 다양한 난이도의 대량의 지침 데이터를 만들어낼 수 있습니다.

---

## 3.2 Automatic Instruction Data Evolution

이 섹션에서는 자동 지침 데이터 발전의 프로세스를 설명합니다. 세 가지 주요 단계로 이루어집니다: 1) 지침 발전, 2) 응답 생성, 3) 발전 실패한 지침을 필터링하는 '제거 발전'.

### 1. 지침 발전

**지침 발전**에서는 주어진 지침이 더 복잡하고 어려워질 수 있도록 LLM을 사용하여 발전시킵니다. 또한, 전혀 새로운 지침을 생성할 수도 있습니다. 이러한 과정을 통해 초기 지침 데이터셋으로부터 난이도를 높이고 다양성을 확장할 수 있습니다. 초기 지침 데이터셋 $D^{(0)}$로 지침 풀을 시작하고, 각 발전 단계에서 지침을 발전시킵니다. 성공적인 발전 지침은 풀에 추가되고 실패한 경우 다음 단계에서 성공적으로 발전할 수 있도록 그대로 남겨둡니다.

**지침 발전기(Instruction Evolver)**는 LLM을 사용하여 지침을 발전시키며, 두 가지 유형이 있습니다: 상세 발전(In-Depth Evolving)과 폭넓은 발전(In-Breadth Evolving).

- **상세 발전:** 지침을 더 복잡하고 어렵게 만들며, 5가지 방법(add constraints, deepening, concretizing, increase reasoning steps, complicate input)을 사용합니다. 핵심 프롬프트는 주어진 지침을 더 복잡한 버전으로 재작성하여, AI 시스템(예: ChatGPT, GPT4)이 더 처리하기 어렵게 만드는 것입니다. 이러한 발전은 학습 모델의 일반화 성능을 방해하지 않도록 점진적으로 이뤄져야 합니다.

- **폭넓은 발전:** 주어진 지침을 기반으로 완전히 새로운 지침을 생성하여 주제 및 기술의 커버리지와 전체 데이터셋의 다양성을 강화합니다.

### 2. 응답 생성

지침 발전과 동일한 LLM을 사용해 발전된 지침에 대한 응답을 생성합니다.

### 3. 제거 발전

발전 실패로 간주되는 네 가지 상황이 있습니다:

1. 발전된 지침이 원래 지침에 비해 정보이득을 제공하지 않는 경우.
2. 발전된 지침에 대해 LLM이 응답을 생성하기 어려운 경우. 생성된 응답에 'sorry'가 포함되고, 길이가 짧다면(80단어 미만) 이 경우를 포함합니다.
3. LLM이 생성한 응답이 구두점과 불용어만 포함할 때.
4. 발전된 지침에 발전 프롬프트에서 직접적으로 복사된 단어들이 포함되어 있을 때(예: 'given prompt', 'rewritten prompt' 등).

이러한 프로세스는 다양하고 복잡한 지침 데이터를 효과적으로 생성하고, 실패한 지침을 필터링하여 더 나은 LLM 학습을 가능하게 합니다.

---

## 3.3 Finetuning the LLM on the Evolved Instructions

이 섹션에서는 발전된 지침을 통해 LLM을 미세 조정하는 방법을 설명합니다. 모든 발전 과정이 완료되면, 초기 지침 데이터세트를 모든 에포크에서 얻어진 발전된 지침 데이터와 병합하고, 샘플을 무작위로 섞어 최종 미세 조정 데이터세트를 생성합니다. 이 과정을 통해 데이터세트 내에서 다양한 난이도 수준의 지침이 고르게 분포되도록 하여, 모델 미세 조정의 원활함을 최대화할 수 있습니다.

기존의 지침 조정 연구에서 사용된 복잡하거나 다양한 프롬프트 템플릿을 사용하지 않고, 오픈 도메인 지침을 처리할 수 있도록 하고자 했습니다. 지침을 "### Response:"와 단순히 연결하여 모델이 표준적인 지도 학습 방식으로 응답을 생성하도록 훈련합니다. 이렇게 함으로써 모델이 다양한 형태의 지침을 효과적으로 처리할 수 있도록 합니다.

---

## 4 Experiment

이 실험 섹션에서는 WizardLM, Alpaca, Vicuna, 그리고 ChatGPT의 성능을 Evol-Instruct 테스트세트와 Vicuna 테스트세트에서 평가합니다. 평가 방법은 자동 평가와 인간 평가 두 가지를 사용하여 각 모델의 성능을 비교합니다. 이러한 평가를 통해 각각의 모델이 다양한 지침과 복잡한 작업에 얼마나 잘 대응하는지를 분석합니다.

---

## 4.1 Baselines

이 섹션에서는 실험에서 비교할 기준 모델들에 대해 설명합니다:

1. **ChatGPT:** OpenAI에서 개발한 AI 챗봇으로, 사용자와 자연스럽고 매력적인 방식으로 상호작용할 수 있습니다. GPT-3.5와 GPT-4 같은 대규모 언어 모델을 기반으로 구축되었으며, 방대한 인터넷 텍스트 데이터를 학습했습니다. ChatGPT는 현재 가장 발전된 다재다능한 챗봇 중 하나이지만, 사실 정확성, 일관성, 안전성과 같은 몇 가지 한계와 도전 과제가 있습니다.

2. **Alpaca:** 스탠포드 대학교에서 개발한 오픈소스 지침-따르기 모델입니다. LLaMA를 기반으로 하며, OpenAI의 text-davinci-003 모델에서 생성된 52,000개의 지침-따르기 예제로 미세 조정되었습니다.

3. **Vicuna:** 사용자 쿼리에 자연스럽고 매력적인 응답을 생성할 수 있는 오픈소스 챗봇입니다. LLaMA를 기반으로 하며, 사람들이 ChatGPT 상호작용을 공유하는 사이트인 ShareGPT에서 수집된 70,000개의 사용자 공유 대화로 미세 조정되었습니다. 현재 가장 발전되고 다양한 운영이 가능한 오픈 지침-따르기 모델 중 하나입니다. FastChat 6의 7B 모델을 사용합니다.

이러한 기준 모델들을 WizardLM과 함께 테스트하여 성능을 비교하고 분석합니다.

---

## 4.2 Experiment detail

이 실험 세부사항 섹션에서는 데이터셋 구성 및 모델 훈련 과정에 대해 설명합니다.

데이터셋을 구성하기 위해, 최초 52K 개의 Alpaca 지침 데이터셋을 초기화한 후, 4회($M=4$)의 발전 과정을 반복하여 총 250K 개의 지침을 얻었습니다. 각 발전 단계에서 각 지침에 대해 6개의 발전 프롬프트(구체적 발전 다섯 개, 폭넓은 발전 하나) 중 하나를 균등한 확률로 무작위 선택해 사용했습니다. Azure OpenAI ChatGPT API를 사용하여 이 과정을 수행하였고, 마지막으로 ChatGPT를 활용하여 응답을 생성했습니다.

응답을 생성할 때, 우리는 온도 값을 1로 설정하고, 최대 토큰 수를 2048로 설정했습니다. 빈도 페널티는 0으로, top-p는 0.9로 설정했습니다. 데이터셋을 완전히 구축하기 위해 API를 총 $52\times4\times3=624\mathrm{K}$ 번 요청했습니다.

모델 초기화를 위해 사전 훈련된 LLaMA 7B 모델을 사용했습니다. Adam 옵티마이저를 초기 학습률로 $2\times10^{-5}$로 설정하고, 최대 토큰 수를 2048로, 각 GPU의 배치 크기는 8로 설정했습니다. 모델은 8개의 V100 GPU에서 Deepspeed Zero-3을 사용하여 70시간 동안 3 에포크 동안 훈련되었습니다.

공정한 비교를 위해, Alpaca의 원래 Davinci-003 응답을 ChatGPT 응답으로 대체했으며, 70K 개의 지침 부분집합을 샘플링하여 WizardLM을 훈련했습니다. 추론 단계에서는 WizardLM과 기준 모델 모두 동일한 설정에서 온도를 1로, top-p를 0.9로 설정하여 출력의 무작위성을 줄이고 좀 더 집중적이고 결정적인 출력을 보장했습니다. 또한, 빔 크기를 1로 설정하고, 최대 생성 길이를 2048로 설정했습니다.

---

## 4.3 Testset build

이 섹션에서는 Evol-Instruct 테스트세트의 구축 과정을 설명합니다. 우리는 다양한 출처에서 실제 인간의 지침을 수집하여 테스트세트를 구성하였습니다. 이 출처에는 온라인 오픈소스 프로젝트, 플랫폼, 포럼 등이 포함됩니다. 데이터를 분석한 결과, 29개의 독특한 기술을 식별했습니다. 이러한 기술은 코딩 생성 및 디버깅, 수학, 추론, 복잡한 포맷, 글쓰기, 다양한 학문 분야 등 인간의 주요 요구 사항을 나타냅니다. Figure 3a는 테스트세트의 예시와 기술의 분포를 보여줍니다.

우리의 테스트세트는 총 218개의 예시로 구성되어 있으며, 각 예시는 특정 기술에 대한 지침입니다. 우리의 테스트세트를 지침-따르기 모델을 평가하기 위한 벤치마크 데이터셋인 Vicuna의 테스트세트와 비교했습니다. 그 결과, Vicuna의 테스트세트는 80개의 예시와 9개의 기술로 구성되어 있어 훨씬 작고 다양성이 적다는 것을 발견했습니다.

Figure 3b는 테스트 데이터가 다른 예시들 간에 난이도와 복잡성이 어떻게 다양한지를 보여줍니다. 우리 테스트 데이터는 보다 균일한 분포를 가지며, 다양한 난이도와 복잡성을 가진 지침을 포함하고 있습니다. 반면, Vicuna와 Alpaca는 편향된 분포를 가지고 있어 주로 낮은 난이도와 복잡성을 가진 지침을 포함하고 있습니다. 이는 이 두 데이터셋이 더 복잡하고 요구가 많은 시나리오에서의 평가를 처리하기에 충분하지 않음을 나타냅니다.

---

## 4.4 Human evaluation

이 섹션에서는 WizardLM에 대한 인간 평가를 설명합니다. 우리는 Evol-Instruct 테스트세트를 사용하여 WizardLM과 기준 모델들 간의 블라인드 쌍별 비교를 수행했습니다. 구체적으로, 우리는 10명의 높은 학력을 지닌 평가자를 모집했습니다. 각 평가자에게 Alpaca, Vicuna-7b, WizardLM, ChatGPT로부터 생성된 네 가지 응답을 무작위로 섞어 출처를 숨긴 채 제시했습니다. 평가자들은 부록 H에 제시된 기준에 따라 어떤 응답이 더 나은지를 판단했고, 네 가지 응답을 1에서 5까지 순위로 매겼습니다(1이 최고이며, 경우에 따라 동점 허용). 승률을 추정하기 위해 각 모델 쌍 간의 승패 및 동점 빈도를 비교했습니다.

**주요 결과:** 실험 결과는 Figure 4에 보고되어 있습니다. WizardLM은 Alpaca와 Vicuna-7b에 비해 현저히 더 나은 결과를 보여주었으며, 이는 Evol-Instruct의 효과를 입증합니다.

**고난이도 기술에 대한 성능:** Figure 4c는 고난이도 지침(난이도 수준 $>=8$)에서, WizardLM의 응답이 인간 평가자에게 ChatGPT보다 더 많이 선호되는 경우가 많음을 보여줍니다.

이를 통해 WizardLM이 다양한 난이도에서 우수한 성능을 발휘함을 확인할 수 있으며, 특히 복잡하고 어려운 지침에서 두각을 나타냅니다.

---

## 4.5 GPT-4 automatic evaluation

이 섹션에서는 GPT-4에 기반한 자동 평가를 사용하여 챗봇 모델의 성능을 평가하는 방법에 대해 설명합니다. 우리는 Vicuna가 제안한 평가 프레임워크를 따르고, 동일한 GPT-4 하이퍼파라미터, 프롬프트 설정 및 평가 방법을 사용했습니다. 순서 편중을 줄이기 위해, WizardLM과 다른 모델의 위치를 쌍별 비교에서 번갈아가며 설정했습니다: 홀수 ID에서는 WizardLM이 첫 번째, 짝수 ID에서는 두 번째로 배치했습니다.

**주요 결과:** Figure 5a와 5b에서 보듯이, WizardLM은 Evol-Instruct 테스트세트에서 Alpaca-7B와 Vicuna-7B를 큰 차이로 능가했으며(각각 $6.2\%$와 $5.8\%$), Vicuna 테스트세트에서 Vicuna-7B와 비슷한 성능을 보였습니다.

**기술별 성능:** Figure 6에서 WizardLM과 ChatGPT의 Evol-Instruct 테스트세트 상의 기술 수준을 비교했습니다. 이 결과에 따르면, WizardLM은 평균적으로 ChatGPT의 $78\%$ 성능을 기록했으며, 17개의 기술에서는 거의 $90\%$ 이상의 능력을 보여주었습니다. 그러나 코드, 수학, 추론 시나리오에서는 어려움을 겪으며, ChatGPT와의 현저한 차이를 드러냈습니다.

**난이도별 성능:** Figure 5c에 나타난 바와 같이, WizardLM은 모든 난이도 수준에서 Vicuna를 능가했고 쉬운 및 어려운 기술에서는 Alpaca보다 우수한 성과를 보였으며, 어려운 기술에서는 ChatGPT의 거의 $88\%$ 능력에 도달했습니다. 이는 WizardLM이 복잡한 문제를 해결하고, 복잡한 데이터를 수집하는 데 있어 인간의 노력을 줄일 잠재력을 가지고 있음을 시사합니다.

**GPT-4와 인간 평가 간의 불일치:** 그러나 어려운 기술에서는 WizardLM이 ChatGPT에 비해 졌으며, 이는 이전의 인간 평가 결론과 상반됩니다. 주요 원인은 i) 인간이 깔끔하고 생동감 있는 형식을 선호하며 ii) 수동 주석 단계에서 사람들이 코드나 수학 문제에 대해 추가 점수를 주는 경향이 있어서, 만약 응답의 품질이 비슷하다면 컴파일이 가능하고 통과할 수 있는 문항들이 선호되기 때문입니다. 추가적인 증거는 부록 I의 사례 연구 섹션을 참조하십시오.

---

## 4.6 Discussion

이 섹션에서는 Evol-Instruct 방식에 의해 생성된 지침이 기존의 인간 작성 지침을 어떻게 초월했는지를 심층적으로 논의합니다.

**심층적 측면에서 인간 지침 초월:** 우리는 지침 발전 과정의 깊이를 연구하기 위해 ChatGPT를 활용하여 각 지침의 난이도와 복잡성을 평가했습니다. 사용된 프롬프트는 부록 E를 참조하십시오. Figure 7a와 7b는 Evol-Instruct로 생성된 지침이 ShareGPT 사용자들이 만든 지침보다 더 복잡하다는 것을 보여줍니다. 또한, 지침의 깊이는 발전 과정의 각 반복마다 현저히 증가합니다.

**폭넓은 측면에서 인간 지침 초월:** 지침의 의미적 폭을 조사하고자 하였습니다. 우리는 t-SNE 및 k-means 알고리즘을 사용하여 BERT 임베딩을 20개의 클러스터로 나눴습니다. 부록 F의 Figure 1은 이러한 클러스터들을 보여주며, 우리의 방법이 ShareGPT 및 Alpaca에 비해 우수한 분산을 보여주고 있음을 강조합니다. 이는 우리의 지침이 주제 다양성 측면에서 더 크다는 것을 의미합니다.

이러한 결과들은 Evol-Instruct가 지침의 복잡성과 주제 다양성 면에서 인간이 만들어낸 지침보다 우수할 수 있음을 나타냅니다.

---
