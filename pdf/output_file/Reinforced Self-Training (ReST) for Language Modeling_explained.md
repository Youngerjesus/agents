
## Abstract

이 논문에서는 인간의 피드백을 통해 강화 학습(RL)을 수행하여 대형 언어 모델(LLM)의 출력을 인간의 선호에 맞춰 개선할 수 있는 방법에 대해 설명합니다. 저자들은 'Reinforced Self-Training (ReST)'라는 간단한 알고리즘을 제안합니다. 이는 배치 강화 학습(RL)에서 아이디어를 얻은 것으로, LLM을 인간의 선호에 맞추도록 도와줍니다. 초기 LLM 정책이 주어지면, ReST는 그 정책으로부터 샘플을 생성하고, 이 샘플들을 오프라인 강화 학습 알고리즘을 사용하여 LLM 정책을 개선하는 데 활용합니다. ReST는 트레이닝 데이터셋을 오프라인으로 생성하기 때문에 데이터 재사용이 가능하여, 일반적인 온라인 RLHF 방법보다 효율적입니다. 비록 ReST가 모든 생성 학습 설정에 적용될 수 있는 일반적인 접근법이지만, 이 연구에서는 특히 기계 번역 분야에 적용된 사례에 중점을 둡니다. 연구 결과에 따르면, ReST는 계산과 샘플 효율성 측면에서 자동화된 메트릭과 인간 평가 기준으로 측정할 때 번역 품질을 상당히 향상시킬 수 있음을 보여줍니다. 

주요 키워드: 오프라인 강화 학습, 인간의 피드백으로부터의 강화 학습, 언어 모델, 자연어 처리, 기계 번역

---

## 1. Introduction

이 논문의 도입부에서는 대형 언어 모델(LLM)의 성능과 그 한계를 설명합니다. LLM은 고품질의 텍스트를 생성하고 다양한 언어 작업을 해결하는 데 뛰어난 능력을 보여주었지만, 이러한 모델이 생성하는 텍스트가 인간의 선호와 반드시 일치하지 않는다는 문제가 있습니다. 또한, 잘못된 정렬로 인해 안전하지 않은 내용이 생성될 위험도 있습니다. 이 문제를 해결하기 위해 인간의 피드백을 통해 강화 학습(RLHF)을 수행하여 LLM을 정렬하는 방법이 사용됩니다. 일반적으로 이는 보상 모델을 학습하고 이를 기반으로 LLM을 강화 학습 목표로 미세 조정하는 방식으로 이루어집니다.

그러나 기존 온라인 RL 방법(예: PPO, A2C)은 새로운 샘플 흐름을 지속적으로 처리해야 하므로 계산 비용이 높고, 잘못된 보상을 생성할 위험이 있습니다. 반면 오프라인 RL 방법은 고정된 데이터셋에서 학습하므로 더 효율적이고 이러한 위험이 적지만, 데이터셋의 품질에 크게 의존합니다.

저자들은 언어 모델 정렬 문제를 성장형 배치 RL 문제로 보고, Reinforced Self-Training (ReST)라는 방법을 제안합니다. 이 방법은 두 개의 루프, Grow와 Improve로 구성되어 있습니다. Grow 단계에서는 최신 정책으로부터 샘플링하여 데이터셋을 확장하고, Improve 단계에서는 이 확장된 데이터셋을 사용해 정책을 미세 조정합니다. 이는 데이터셋 생성 비용을 줄이고, 새로운 정책에서 샘플링된 데이터를 사용하므로 초기 데이터셋의 품질에 의존하지 않습니다. 또한, Grow와 Improve 단계가 독립적으로 수행되기 때문에 데이터 품질을 점검하고 정렬 문제를 진단하기 용이합니다.

ReST는 간단하고 안정적이며 설정해야 할 하이퍼파라미터가 적어 실용적입니다. 논문에서는 ReST의 자세한 내용을 3장에서 설명하고, 4장에서는 기계 번역 벤치마크에 대한 실험 결과를 제시합니다. 특히, ReST는 기계 번역 작업에서 높은 품질의 번역을 생성하는 데 유리하다는 것을 보여줍니다.

---

## 2. Preliminaries

이 논문의 두 번째 섹션에서는 조건부 언어 모델의 기본 원리에 대해 설명하고 있습니다. 조건부 언어 모델은 주어진 문맥(또는 원본 입력) \(\pmb{x}=(x_{1}, x_{2}, \ldots, x_{L})\)에 대해 출력 시퀀스 \(\pmb{y}=(y_{1}, y_{2}, \ldots, y_{T})\)를 생성합니다. 여기서 각 토큰 \(x_{l}, y_{t}\)는 미리 정해진 어휘에 속합니다. 자동 회귀 모델에서는 정책 \(\pi\)가 조건부 확률 분포 \(\theta\)로 매개변수화됩니다. 이 확률 분포는 다음과 같이 표현됩니다:

$$
\pi_{\boldsymbol{\theta}}(\boldsymbol{y}\mid\boldsymbol{x})=\prod_{t=1}^{T}\pi_{\boldsymbol{\theta}}(y_{t}\mid\boldsymbol{y}_{1:t-1},\boldsymbol{x}),
$$  

여기서 \(y_{1:0}=\emptyset\)와 \(y_{1:t-1}=(y_{1}, y_{2}, \ldots, y_{t-1})\)로 정의됩니다.

데이터 분포 \(p(\pmb{x}, \pmb{y})=p(\pmb{x})p(\pmb{y}|\pmb{x})\)에서 샘플을 모아 데이터셋 \(\mathcal{D}\)를 구성합니다:

$$
\mathcal{D}=\{\mathbf{\Phi}(x^{i},y^{i})|_{i=1}^{N}\;\mathrm{~such~that~}\;x^{i}\sim p(\mathbf{x}),\;y^{i}\sim p(\mathbf{y}|\mathbf{x}=x^{i})\ \}.
$$  

이 데이터셋을 사용하여 감독 정책은 음의 로그 가능도(NLL) 손실을 최소화하는 방식으로 학습됩니다:

$$
\mathcal{L}_{\mathrm{NLL}}(\theta)=-\mathbb{E}_{(\boldsymbol{x},\boldsymbol{y})\sim\mathcal{D}}\left[\sum_{t=1}^{T}\log\pi_{\theta}(y_{t}\mid\boldsymbol{y}_{1:t-1},\boldsymbol{x})\right].
$$  

이 손실 함수로 학습된 모델은 강화 학습 문헌에서 행동 복제(Behavioral Cloning, BC)로 불립니다. 이는 주어진 데이터셋의 예시를 그대로 따라 하는 방식을 의미합니다.

---

## 3. Reinforced Self-Training (ReST)

이 섹션에서는 인간의 선호에 맞춰 언어 모델의 출력을 정렬하는 RLHF 알고리즘인 Reinforced Self-Training (ReST)을 소개합니다. 인간의 선호는 학습된 보상 함수로 모델링됩니다(자세한 내용은 부록 A.4 참고). 조건부 언어 모델링을 위한 기저 마르코프 결정 과정에서 상태는 부분 시퀀스이고, 행동은 생성된 토큰입니다(부록 A.1 참고).

ReST 알고리즘은 일반적인 강화 학습 파이프라인의 데이터셋 성장과 정책 개선 단계를 별도의 오프라인 단계로 구분합니다(그림 1 및 2 참고). 먼저, 초기 모델 \(\pi_{\theta}(y|x)\)을 학습하여 주어진 시퀀스 쌍 데이터셋 \(\mathcal{D}\)에서 입력 시퀀스 \(\pmb{x}\)를 출력 시퀀스 \(\pmb{y}\)에 맵핑합니다. 이때, 음의 로그 가능도(NLL) 손실을 사용합니다. 다음으로, Grow 단계에서는 모델로부터 샘플을 생성하여 초기 트레이닝 데이터셋을 보강하는 새로운 데이터셋 \(\mathcal{D}_{g}\)를 만듭니다:

$$
\mathcal{D}_{g}=\left\{\begin{array}{l l}{(\boldsymbol{x}^{i},\boldsymbol{y}^{i})|_{i=1}^{N_{g}}\;\mathrm{~such~that~}\;\boldsymbol{x}^{i}\sim\mathcal{D},\;\boldsymbol{y}^{i}\sim\pi_{\theta}(\boldsymbol{y}|\boldsymbol{x}^{i})\;\right\}\cup\mathcal{D}.}\end{array}
$$  

여기서 컨디셔닝 입력은 원본 데이터셋 \(\pmb{x}^{i}\sim\mathcal{D}\)에서 다시 샘플링되지만, 만약 \(p(\pmb{x})\)에 접근할 수 있다면 직접 샘플링할 수도 있습니다. 예를 들어, 텍스트 설명으로부터 이미지를 생성하는 모델에서는 텍스트 입력의 분포를 언어 모델 \(p(x)\)로부터 샘플링할 수 있습니다.

이어지는 Improve 단계에서는 \(\mathcal{D}_{g}\)를 사용해 정책 \(\pi_{\theta}\)를 미세 조정합니다. 이때, 정책이 지나치게 바뀌지 않도록 원본 데이터셋을 훈련에 포함시킵니다. 아래에서 Grow와 Improve 단계의 세부 사항을 설명합니다.

---

## Grow

이 섹션에서는 Reinforced Self-Training (ReST) 방법의 'Grow' 단계에 대해 설명합니다. 이 단계는 강화 학습에서의 행위 또는 데이터 생성 단계에 해당됩니다. 현재 정책 \(\pi_{\theta}\)로부터 많은 출력 시퀀스를 샘플링하여 보강된 궤적 데이터셋 \(\mathcal{D}_{g}\)를 만듭니다. 여기서 \(\mathbf{y}\;\sim\;\pi_{\theta}(\mathbf{y}|\pmb{x})\)는 \(x\,\sim\,{\mathcal{D}}\)일 때 수행됩니다. 이렇게 생성된 새로운 시퀀스 데이터셋은 보상 함수 \(R(x,y)\)로 평가됩니다.

이후, 보상 값이 설정된 임계값을 초과하는 데이터 포인트는 정책을 업데이트하는 데 사용됩니다. 정책이 개선되면, 더 나은 품질의 샘플을 포함하는 새로운 데이터셋을 다시 생성할 수 있습니다(그림 2의 하단 부분 참조). 이는 데이터 품질을 단계적으로 향상시킬 수 있는 방법입니다.

---

## Improve

이 섹션에서는 'Improve' 단계, 즉 강화 학습 용어로 정책 개선 또는 사용 단계에 대해 설명합니다. 이 단계의 목표는 새로운 데이터셋 \(\mathcal{D}_{g}\)를 사용하여 정책 \(\pi_{\theta}\)를 미세 조정하는 것입니다. 먼저, 일정 임계값 \(\tau\)보다 높은 보상을 받는 샘플만 포함하는 필터링 함수를 정의합니다:

$$
F({\pmb x},{\pmb y};\tau)=\mathbb{1}_{R({\pmb x},{\pmb y})>\tau}.
$$  

이 필터링 함수는 환경의 동적 특성에 따라 높은 분산을 가진 결과를 선호하는 최적이 아닌 행동을 학습할 수도 있습니다. 그러나, 이 연구에서는 언어 모델링과 번역 작업을 결정론적 강화 학습 문제로 설정하여 이러한 문제를 최소화합니다.

다음으로, 현재의 최선 정책을 미세 조정합니다. 데이터셋이 고정된 상태에서 하나의 Improve 단계로 수행되는 행동 복제(BC)와 같은 표준 모방 학습 접근법과 달리, ReST는 Grow 단계를 추가하여 모델이 원본 데이터셋에서 새로운 출력 시퀀스(잠재적 번역)를 수집할 수 있게 합니다.

Improve 단계를 반복하면서, 필터링 임계값을 점차 높여갑니다: \(\tau_{1}<\cdot\cdot\cdot<\tau_{N-1}<\tau_{N}\). 이렇게 임계값을 높이는 필터링은 데이터의 품질을 높이지만 데이터 세트의 크기는 줄어듭니다. LLM이 작은 데이터셋에 빠르게 과적합되기 때문에, 낮은 학습률로 이전 정책에서 새로운 정책으로 미세 조정합니다. 각 정책을 고품질 데이터에 연속적으로 미세 조정하면 고정된 데이터셋 \(\mathcal{D}_{g}\)에서도 정책이 개선됩니다.

이후 각 Grow 단계 후에 여러 번의 Improve 단계가 수행되어, 데이터셋 생성 비용이 여러 Improve 단계에 걸쳐 상쇄됩니다. Algorithm 1은 여러 데이터셋 성장 및 정책 개선 단계를 포함하는 전체 ReST 알고리즘의 과정을 설명합니다.

이 접근법은 자가 학습(self-training)과 유사하며, 모델이 데이터 분포 \(p(y|x)\)에서 너무 멀어지는 것을 방지하여 모델 붕괴를 피합니다. 다음 섹션에서는 손실 함수, 필터링 함수 및 임계값, 언어 정책에 의해 샘플링된 합성 데이터가 결과적인 정책 \(\pi_{\theta}\)의 성능에 미치는 영향을 탐구합니다.

---

## 4. Experiments and analysis

이 섹션에서는 ReST의 실험 및 분석 결과를 공유합니다. ReST 알고리즘의 성능을 평가하기 위해 기계 번역을 테스트베드로 선택했습니다. 이는 조건부 언어 모델링의 중요한 응용 분야로, Metric X, BLEURT, COMET와 같은 평가 모델들이 존재합니다. 실험은 IWSLT 2014, WMT 2020, 웹 도메인이라는 내부 벤치마크 데이터셋에서 수행되었습니다.

1. **실험 설계**: 각 데이터셋에는 소스 언어의 문장과 그에 대응하는 인간 번역이 포함되어 있으며, 각 데이터셋에 대해 다른 언어 쌍을 선택하여 결과의 일반성을 테스트했습니다. 평가를 위해 별도의 검증 및 테스트 세트를 준비했습니다.

2. **향상(Improve) 단계의 효과**: ReST의 여러 향상 단계를 통해 보상 모델 점수가 향상되는지를 평가했습니다. 각 반복적인 향상 단계가 세 개의 데이터셋 전반에 걸쳐 번역 모델의 성능을 크게 개선시킵니다.

3. **더 많은 성장(Grow) 단계의 효과**: 추가적인 성장 단계를 통해 성능이 더 개선되는지를 테스트했습니다. 추가 성장 단계가 IWSLT 2014 및 웹 도메인 데이터셋에서 성능을 더욱 개선하였습니다.

4. **감독 학습 대비 ReST 성능**: ReST가 감독 학습에 비해 성능을 개선하는지를 확인하기 위해, 여러 종류의 손실 함수와 성장 및 향상 단계의 수를 조합한 ReST 변형들로 실험을 진행했습니다. 그 결과, 모든 ReST 변형이 감독 학습보다 뛰어난 성능을 보였습니다.

5. **단일 ReST 단계에서의 최적 손실**: BC 손실이 다른 손실 함수보다 우수한 성능을 보였습니다. 이는 보상 필터링 단계를 통해 보상 모델을 고려하여 ReST의 성능을 개선합니다.

6. **온라인 강화 학습과의 비교**: ReST는 PPO와 같은 online RL 알고리즘과 비교했을 때, 멀티플 향상 단계를 통해 온라인 RL보다 더 높은 보상을 달성했습니다. 또, BLEU 점수의 급격한 하락 없이 보상 모델의 점수를 개선할 수 있었습니다.

7. **Inference 단계에서의 Best-of-N 샘플링의 영향**: Best-of-N 샘플링 기법이 적용될 때, ReST의 성능이 더욱 개선되었습니다. 특히, Best-of-N 샘플링이 다양한 ReST 변형에서 유사한 수준으로 유익함을 보였습니다.

8. **인간 선호 점수**: 인간 평가자들이 매긴 점수를 통해 ReST가 인간 번역 평가에서도 BC를 초과하는지를 평가했습니다. 모든 ReST 변형이 인간 평가에서 BC를 능가하는 것으로 나타났으나, 학습된 보상 모델 성능과는 약간의 차이를 보였습니다. 이는 보상 모델이 인간의 선호도를 완벽히 반영하지 못하기 때문으로 해석됩니다.

결론적으로, ReST는 기계 번역 작업에서 감독 학습보다 뛰어난 성능을 보였으며, 인간 평가에서도 긍정적인 결과를 얻었습니다. 그러나, 보상 모델의 일반화 성능을 개선할 필요가 있으며, 이는 Interim 단계에서 최신 정책의 인간 주석 데이터를 사용하여 보상 모델을 미세 조정함으로써 해결할 수 있습니다.

---

## 5. Related works

이 섹션에서는 ReST와 관련된 기존 연구들을 소개합니다.

1. **자기 학습(Self-training)**: 이는 레이블이 없는 데이터를 활용하여 모델을 개선하는 반지도 학습 접근법으로, 다양한 작업에 성공적으로 적용되었습니다. ReST의 Improve 단계는 자기 학습과 유사하지만, ReST의 Grow 단계는 강화 학습을 위한 탐색 데이터를 생성하는 점이 차별점입니다.

2. **전문가 반복(Expert Iteration, EI)**: EI는 정책 반복 접근법으로 RL 문제를 계획과 일반화라는 두 부분으로 나눕니다. EI와 ReST 모두 데이터를 생성하여 정책을 학습하지만, ReST는 계획 메커니즘이 필요 없고, 향상 단계가 반복되어 데이터를 보다 효과적으로 활용할 수 있습니다.

3. **언어 모델을 이용한 추론**: Zelikman et al. 및 Uesato et al.의 연구는 정당화를 사용하여 모델을 미세 조정하는 방법을 제안했습니다. ReST는 오프라인 RL 손실을 사용하여 다양한 필터링 메커니즘과 함께 연속 값의 보상을 다룰 수 있는 특징이 있습니다.

4. **반복 학습(Iterated Learning, IL)**: IL은 다른 에이전트의 행동을 보고 배우는 과정입니다. 이 접근법은 다중 에이전트 설정에서 작동하기 때문에 ReST와 다릅니다.

5. **자기 모방 학습(Self Imitation Learning, SIL)**: SIL은 높은 보상 궤적만을 학습하는 오프라인 정책-비평가 알고리즘입니다. ReST는 기본 RL 알고리즘에 무관하며, 가치 함수 없이도 작동할 수 있다는 점에서 SIL과 차별화됩니다.

6. **보상 순위 미세 조정(Reward Ranked Fine-Tuning, RAFT)**: RAFT는 한 번의 향상 단계와 고정된 양분 필터링 임계값을 사용하여 ReST와 유사하지만, ReST는 여러 향상 단계로 더 나은 성능 개선을 이룰 수 있습니다.

ReST는 이들 연구와 비교해 계산 효율적이며, 탐색 데이터와 보상을 활용할 수 있는 유일한 접근법으로 평가되었습니다.

---

## 6. Discussion

이 논문에서는 ReST라는 알고리즘을 제안했습니다. 이는 간단하고 조정해야 할 하이퍼파라미터가 적으며, 다양한 Grow와 Improve 단계 디자인과 호환될 수 있는 유연성을 가지고 있습니다. 우리는 기계 번역 작업에서 ReST의 성능을 연구했으며, 이는 이 작업에 대해 확립된 보상 모델이 존재하기 때문입니다. 실험에서는 여러 오프라인 강화 학습 손실 함수를 테스트했으나, 표준 지도 학습(BC)이 보상 모델 점수를 개선하는 데 가장 효과적임을 발견했습니다.

향상 단계에서 점진적으로 필터링 임계값을 높이며 NLL 훈련을 여러 단계로 수행하면 모델의 보상 점수가 지속적으로 개선되었습니다. 그러나 보상 모델 점수의 향상이 인간의 선호를 반드시 반영하지는 않으며, 보상 모델은 단지 인간 선호의 대리 변수일 뿐입니다. 결과적으로, 인간 평가 점수를 고려할 때 한 번의 Grow 단계를 거치는 것이 가장 좋았습니다. 보상을 계속해서 증가시키기 위해 여러 Grow 단계를 진행할 수 있지만, 이는 보상 모델에 대한 과적합 위험을 증가시킬 수 있습니다. 이를 해결하기 위해서는 Bai et al. (2022) 및 Glaese et al. (2022)를 참조하여 인간 선호로 주석된 \(\mathsf{D}_{g}\)의 부분집합에서 보상 모델을 미세 조정할 수 있습니다. 이 부분은 향후 연구 과제로 남겨두었습니다.

간단한 BC 손실은 보상 모델 점수와의 정렬 측면에서 여러 오프라인 강화 학습 손실보다 여전히 우수합니다. 그러나 BC는 보상 모델에 과적합될 수 있으며, 이는 강화 학습에서 가치 함수 학습이 드문 보상, 크레딧 할당 문제, 하이퍼파라미터 민감도 및 제한적인 Grow 단계 탐색 때문에 어려운 것과 관련이 있습니다. ReST는 Grow 단계에서 MCTS와 같은 더 나은 RL 탐색 전략을 통해 이점을 얻을 수 있습니다. 생성된 데이터를 활용하는 능력은 상태-행위 공간의 보다 광범위한 탐색 및 더 나은 일반화를 초래할 수 있습니다.

결론적으로, ReST는 일반적이고 효율적인 접근법입니다. 인간 선호의 강력한 보상 모델이 존재하고 모델에서 대규모 샘플을 생성할 수 있을 때 적용 가능합니다. 따라서 요약, 턴 기반 대화, 기타 생성적 오디오 및 비디오 모델과 같은 여러 언어 도메인 작업에 적용될 수 있습니다. 다양한 향후 탐색 및 응용 분야와 함께, ReST는 RLHF를 위한 유용한 배치 성장형 RL 방법론이라고 생각합니다.

감사의 글에서는 프로젝트의 초기 구상 단계에서 입력을 제공하고 코드베이스를 설정하는 데 기여한 Google과 Google DeepMind의 기계 번역 팀 구성원들과 논의를 해준 Matt Hofman, Bobak Shahriari, Taylan Cemgil, Chris Dyer에게 감사를 표했습니다. 또한, 논문의 초안에 피드백을 제공한 Bilal Piot와 프로젝트에서 사용된 다양한 프레임워크, 특히 DeepMind JAX 에코시스템과 Launchpad를 담당한 분들께도 감사를 전했습니다.

---
