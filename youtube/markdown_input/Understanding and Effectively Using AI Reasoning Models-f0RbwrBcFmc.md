## 새로운 추론 모델에 대한 개요
Hey, this is Lance from Langsham. You've probably heard a lot about the new reasoning models from OpenAI, such as `01` and `03`. I want to talk about these by reviewing some of my favorite new videos and blogs I've seen on this topic.

### 현재의 스케일링 패러다임: 다음 단어 예측
But first, let's just start with the current scaling paradigm that we've been in for a number of years, which is **next word prediction**. So Jason Wei has a great talk on this, and he kind of frames nicely why has next word prediction worked so well. And he basically explains it as next word prediction is a **multitask learning problem**. When you ask an LM to predict the next word or the next token in a sentence, it learns a lot of things at once. It learns grammar. It learns world knowledge, sentiment, translation, spatial reasoning, math. So this simple learning objective is extremely powerful, and there's lots of nice papers and talks on this.

### 스케일링과 등장(emergence) 개념
But that just sets the stage. Now we've scaled this over roughly seven orders of magnitude. Jason also touches on this, and this is well covered in the Kaplan et al. paper from 2020. But with respect to model size, data set size, and trained compute, the overall capability of models trained just with this kind of simple objective has gotten better. Now there's some interesting points here related to the concept of **emergence**. Certain capabilities appear to be unlocked at certain scales, like for example, GPT-2 and 3 were poor at math, 4 kind of unlocked greater math capability. But overall, you can kind of think about the capability increasing in a relatively predictable fashion with respect to size of model, data set, and trained compute. So that's kind of the paradigm we've been in.

### 다음 단어 예측의 한계
Now here's kind of the catch, and Jason kind of lays this out nicely. Next word prediction is kind of like **system one thinking**. It's fast and intuitive. But some next words are really hard to predict. So for example, a challenging math problem, a challenging reasoning problem. The problem is these models use the same amount of compute to solve easy and hard problems. That's kind of the overall bottleneck with this paradigm.

### Chain of Thought 프롬프팅
Now a workaround that we're all probably familiar with at this point is this notion of **chain of thought prompting**. This came out around 2022, and Jason Wei has a nice paper on it. Of course, you prompt an alum to think step by step. But what's really happening here? So Nathan Lambert has a nice video on this, and I thought it was really interesting. You're kind of trying to enforce **system two thinking**, which is conscious and effortful. So for example, if you do a math problem yourself, you perform a bunch of intermediate steps. Those are kind of storing intermediate variables for yourself, which then you utilize to generate the final solution. And chain of thought is kind of like forcing the model to produce those intermediates as tokens along its trajectory towards a solution. So it's like by telling it to think step by step, you're actually having it produce work in the form of tokens, and those tokens are kind of storing intermediate information that the model is using to form a final answer. So it's kind of like a hack to force the model from system one thinking to system two thinking. So I kind of like the way that Jason lays that out, and Nathan explains what's happening when you actually do CFT.

## 새로운 스케일링 패러다임: Chain of Thought 기반 강화 학습
Now this new scaling paradigm, which you see with these reasoning models, is basically scaling reinforcement learning on a chain of thought. So what's actually happening here? There's a nice blog post from OpenAI, a nice video from Nathan, but really the summary is that you have some training data which contains explicitly correct answers. Now this is important, okay? Coding problems, math problems that are verifiably correct, okay? You have a model that can sometimes generate correct solutions, and you have a grader that can verify whether or not a model output is correct or not, okay? And if it's correct, you give the model a reward. Now this is where the reinforcement learning thing comes in. You have some policy that will nudge weight so it's more likely to produce high reward outputs. Now what you do is when you train this on this data set that has many explicitly correct answers for every problem, you basically have the model produce a bunch of different trajectories, and you grade them all, and you reward the correct ones. And over time, in training, you do lots of forward passes, okay? But you kind of tune or nudge the model to favor chain of thought or trajectories that result in correct answers. That's the overall intuition, and I think more will come out on this, but this is kind of my distillation from reading, of course, a blog post and some nice work by Nathan Lambert to explain a little bit more detail. And here's a nice kind of schematic of what's going on. You have your training data. You have a policy to nudge your weights. You have some verifiable reward, and you're basically running your model over the training data. You're doing lots of forward passes. You're rewarding the correct chains of thoughts that get you to correct answers that are verifiable with your grader. That's the big idea.

### 새로운 스케일링 법칙의 중요성
Now why is this exciting? Well, it represents a new scaling law. So some really nice videos from Noam Brown, Jason Way on this. If you look at the recent results from `01` and now `03`, just dropped right before the new year, they're obviously extremely strong. Okay, so this is exciting. Now another way to think about this is I really like this slide from David Raine from NeurIPS. Benchmarks are getting saturated more and more quickly. This is a cool visualization showing how long it takes for a benchmark to get saturated. It used to be in 2012 it would take like eight years, right? Now it takes like a year for GPQA, which is a new benchmark made by David Raine, which was Google-proof QA. Those question answers that are not easily Googled. And basically we're seeing new state-of-the-art reasoning models are basically saturating on benchmarks very quickly. So it's exciting. We're early in the scaling curve. That's the big idea here.

## 01 모델에 대한 오해와 올바른 사용법
Now here's where things are really interesting. There's actually been a lot of confusion about `01` models with some people saying, oh, these actually are really bad. Okay. Now Ben Hylak and Swicks put out a really nice post on LatentSpace, and it really helps to clarify when you work with these reasoning models, you should not think about them as chat models, and you shouldn't prompt them as such. There's this nice visualization of the anatomy of no one prompt where really what you're doing is you're giving it an explicit goal. You don't tell it how to think. You tell it what you want. You give it a return format, warnings, and just dump all your work. A lot of people have shown that this style of prompting works really well with `01`. So unlike chat models, chat models you try to tell it how to think. You're a researcher. You think step by step. With these models, you don't do that. You give it what you want, and you give it as much context as possible. So that's really the context on how to prompt these. Again, focus on the what, not the how. Don't tell it to do a particular style of reasoning. Just give it what you want. That's the big point here.

### 01 모델 사용 예시
Now let me show you usage very quickly. First, there's a few models available through the API, `01`, `01mini`. One thing I'll note, mini does not support system messages. That's just a small thing to note. Now parameters. So with `01`, not with `01mini`, you can provide these values of reasoning effort for low, medium, high. This just tunes the amount of reasoning the model will do. Basically, faster or slower responses, fewer and more tokens accordingly. So I'm in a notebook now. I've just pip installed LangChain OpenAI, import. Now I'm just using Chat OpenAI, model `01`. I'll set reasoning level to medium. Now note how I prompt this. I tell it what I want. I want an educational report on cause of mitigation for high cholesterol. So this is just an example prompt. I tell it how I want it to produce the output and give it just a dump of stuff I'm interested in. Run this. Cool. So that ran. We'll look at the trace in a minute, but I'll just show you here in Markdown. Here's kind of the output. So you get a really nice kind of well laid out report on the topic of interest, right? So this is really cool and it's quite exhaustive. Pretty nice. Open up the trace and I just want to show you indeed a one ran. Now here's what's interesting, right? The latency is going to be higher than you see with the chat model. It took 27 seconds. Okay, fair enough. And again, here was my prompt. As you can see, we laid that out and here is the report output. So it's quite exhaustive. Higher latency, high quality, lots of reasoning goes into producing the report.

### 구조화된 출력 및 도구 호출
So the `01` models work with structured outputs, which is a very popular use case. So let me just show you how to do that. So we basically define our LM previously, call with structured outputs, this very nice helper method, pass in a schema. In this case, I just have a pedantic schema. That's pretty cool. And I go ahead and run that and we get a structured object out. Pretty cool. Now people are going to be very interested in this as well. Of course, tool calling works with these models. So again, I just can call LM.bindtools. And basically, in this case, I pass in a multiply tool and I make a request that's related to the tool. The reasoning model decides to call the tool and I get a tool call out. There we go. So those are really the core things you're going to want. Very high quality reasoning and report generation, for example. Structured outputs, tool calling. With those primitives, you can do a huge amount, of course.

## 주요 사용 사례
Let me talk a little bit about use cases. So here's some examples that I've seen and they're actually covered pretty nicely in Swix's blog posts, I think are really interesting to think about for these models.

### 코딩
So coding is obvious. They're extremely strong at coding. But what types of coding problems? So I think McKay's done some really neat work on this. So what we've heard and seen quite a bit is these models are very strong at one-shotting entire files or sets of files. So again, McKay has some nice workflows and tutorials on this, how you basically give a one, a overall problem, give it the opportunity to produce or end or edit a large set of files and it can do that often in one shot. So coding is obviously a smash use case for these models and they're trained obviously on very hard coding problems. They perform very well on SWE Bench, which is a popular coding benchmark. So coding is obvious and a very interesting application area for these models.

### 계획 및 에이전트
Now another one is this notion of planning and agents. So in a lot of cases, we've seen agents or agentic workflows that use some kind of pre-planning step up front, which kind of lays out a set of follow-on steps that may be executed by smaller LLMs or by an overall workflow. So there's a blog post from Unify that actually showcases this using line graphing. But I think this general point about using these for upfront planning and workflows or agents is obviously a natural fit.

### 정보 반영 및 데이터 분석
Another interesting area, so Nat Freeman kind of laid out kind of a prompt, what kind of O1 outputs are seen or interesting. And I think kind of reflection over sources of information, it could be meeting notes, it could be documents is interesting. You know, this user followed up with, you know, what's the most important thing no one's paying attention to? Pipe all your meeting transcripts in and kind of be surprised or amazed. And so I think that's kind of a generally interesting area for these models is kind of like deep reflection over some large sets of context. It could be meeting notes, it could be documents, it could be papers. Now data analysis. Similarly, I've seen a lot of people report on utilizing these models for analyzing even things like blood tests. Very good for kind of medical diagnosis or medical reasoning. Now, of course, people may not want to actually share private medical data with an API, totally understandable. Some people do. But anyway, I think kind of medical analysis or diagnostics is interesting area or other areas of data analysis are obviously very strong for these reasoning models.

### 연구 및 보고서 생성
So another one is kind of research and report generation. So we've seen deep research from Google come out. It's really interesting. I think doing deep research kind of with O1 as your own kind of workflow is obviously a very nice use case for O1. Ben mentioned in his blog post that LLM is judged. And so basically there's a lot of interest in using LLMs as evaluators. These reasoning models could be very strong for that particular use case. And so I think in any kind of workflow where you have like an evaluation step, either it's online or, for example, offline, these models can be very strong.

### 뉴스피드에 대한 인지적 레이어
And finally, I'll just make a note of kind of cognitive layer over newsfeeds. I think this is similar to the reasoning point above, but I've actually seen a few specific examples of this. So Eric Siarla mentioned O1 trend finder. This is pretty neat. It's using Firecrawl for actually content scraping and passing that to O1 to monitor, notify you of trends on social media. I have a number of apps that do similar things, and I think it's a really nice use case. And I think using O1 is really cool for this. Swicks mentioned in one of the podcasts that he's actually using O1 for AI news as well. And so that's another good example of kind of a cognitive layer over newsfeeds, isolating relevant information and servicing it to you.

## 결론: 채팅 모델 vs 추론 모델
So maybe just to recap briefly, chat models and reasoning models are pretty different. Different scaling paradigms. So chat models scale using next token prediction. Reasoning models are scaling using RL over chain of thought. The reasoning types are different. So chat models you can think of as system one, fast, intuitive. Reasoning models more like system two, slow, effortful. Now how do you work with a model? What do you actually tell it? With chat models, we often told it how to think. Think step by step. Think as an engineer. Reasoning models, don't tell it how to think. Tell it what you want. Here's the output I want. Okay. Interaction mode. Chat models. Chat. Interactive. It's pulling context from the user over the course of a chat. Reasoning models. They are going off and wormholing on something. You don't want really necessarily to interact with them in a chat format. You want to give it a deeper task and have it just go churn. Better for research and planning. Really good for things like ambient or in the background style agent. So those are the workflows. I think about more here. If you look back at our use cases, things like trend finder, again, this can run in the background. LLM is judged when it is running offline. It's kind of in the background. Deep research. Again, in the background. Run that research process for 30 seconds, one minute. A lot of these you'll see are kind of use cases that don't demand low latency. They're things that can run in the background over longer periods of time to produce kind of deeper, effortful outputs. So again, I think overall, really exciting paradigm. We're early in this trend. It's absolutely worth, if you have applications already using, for example, OpenAI, you can slot in O1 or, of course, other LLMs, slot in O1 and give it a try if they fit kind of any of the use cases mentioned here. And so if you have kind of good experiences or further thoughts, we'd love to hear comments below. And thank you very much for listening.