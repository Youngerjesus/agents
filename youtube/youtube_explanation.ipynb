{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yt-dlp[default]\n",
      "  Downloading yt_dlp-2025.1.15-py3-none-any.whl.metadata (172 kB)\n",
      "Collecting brotli (from yt-dlp[default])\n",
      "  Using cached Brotli-1.1.0-cp312-cp312-macosx_10_13_universal2.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: certifi in /Users/jeongmin/PycharmProjects/agents/.venv/lib/python3.12/site-packages (from yt-dlp[default]) (2024.12.14)\n",
      "Collecting mutagen (from yt-dlp[default])\n",
      "  Using cached mutagen-1.47.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting pycryptodomex (from yt-dlp[default])\n",
      "  Downloading pycryptodomex-3.21.0-cp36-abi3-macosx_10_9_universal2.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: requests<3,>=2.32.2 in /Users/jeongmin/PycharmProjects/agents/.venv/lib/python3.12/site-packages (from yt-dlp[default]) (2.32.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.17 in /Users/jeongmin/PycharmProjects/agents/.venv/lib/python3.12/site-packages (from yt-dlp[default]) (2.3.0)\n",
      "Requirement already satisfied: websockets>=13.0 in /Users/jeongmin/PycharmProjects/agents/.venv/lib/python3.12/site-packages (from yt-dlp[default]) (14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jeongmin/PycharmProjects/agents/.venv/lib/python3.12/site-packages (from requests<3,>=2.32.2->yt-dlp[default]) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jeongmin/PycharmProjects/agents/.venv/lib/python3.12/site-packages (from requests<3,>=2.32.2->yt-dlp[default]) (3.10)\n",
      "Using cached Brotli-1.1.0-cp312-cp312-macosx_10_13_universal2.whl (815 kB)\n",
      "Using cached mutagen-1.47.0-py3-none-any.whl (194 kB)\n",
      "Downloading pycryptodomex-3.21.0-cp36-abi3-macosx_10_9_universal2.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yt_dlp-2025.1.15-py3-none-any.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: brotli, yt-dlp, pycryptodomex, mutagen\n",
      "Successfully installed brotli-1.1.0 mutagen-1.47.0 pycryptodomex-3.21.0 yt-dlp-2025.1.15\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install \"yt-dlp[default]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "170743.23s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydub\n",
      "  Using cached pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Using cached pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Installing collected packages: pydub\n",
      "Successfully installed pydub-0.25.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yt_dlp import YoutubeDL\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "import os \n",
    "import glob\n",
    "\n",
    "\n",
    "ydl_opts = {\n",
    "    'format': 'bestaudio/best',\n",
    "    'postprocessors': [{\n",
    "        'key': 'FFmpegExtractAudio',\n",
    "        'preferredcodec': 'mp3',\n",
    "        'preferredquality': '192',\n",
    "    }],\n",
    "    'outtmpl': 'audio/%(title)s-%(id)s.%(ext)s',\n",
    "}\n",
    "\n",
    "def download_audio(youtube_url: str) -> str:\n",
    "    with YoutubeDL(ydl_opts) as ydl:\n",
    "        ydl.download([youtube_url])\n",
    "        \n",
    "    video_id = extract_video_id(youtube_url)\n",
    "    return find_audio_file(video_id)\n",
    "\n",
    "def find_audio_file(video_id: str, audio_dir: str = \"audio\") -> str:\n",
    "    \"\"\"\n",
    "    video_id를 포함하는 오디오 파일을 찾아 경로를 반환\n",
    "    \n",
    "    Args:\n",
    "        video_id: YouTube 비디오 ID\n",
    "        audio_dir: 오디오 파일이 저장된 디렉토리 (기본값: \"audio\")\n",
    "        \n",
    "    Returns:\n",
    "        str: 찾은 파일의 경로\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: 파일을 찾지 못한 경우\n",
    "    \"\"\"\n",
    "    # audio 디렉토리의 모든 mp3 파일 검색\n",
    "    pattern = os.path.join(audio_dir, f\"*{video_id}*.mp3\")\n",
    "    matching_files = glob.glob(pattern)\n",
    "    \n",
    "    if not matching_files:\n",
    "        raise FileNotFoundError(f\"No audio file found for video ID: {video_id}\")\n",
    "    \n",
    "    # 일치하는 파일이 여러 개인 경우 가장 최근 파일 반환\n",
    "    return max(matching_files, key=os.path.getctime)\n",
    "\n",
    "\n",
    "def extract_video_id(youtube_url: str) -> str:\n",
    "    \"\"\"\n",
    "    YouTube URL에서 video ID를 추출\n",
    "    \n",
    "    Args:\n",
    "        youtube_url: YouTube 영상 URL\n",
    "        \n",
    "    Returns:\n",
    "        str: YouTube 비디오 ID\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: 올바르지 않은 YouTube URL이거나 video ID를 찾을 수 없는 경우\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # URL 파싱\n",
    "        parsed_url = urlparse(youtube_url)\n",
    "        # 쿼리 파라미터 파싱\n",
    "        query_params = parse_qs(parsed_url.query)\n",
    "        \n",
    "        # 'v' 파라미터에서 video ID 추출\n",
    "        if 'v' in query_params:\n",
    "            return query_params['v'][0]\n",
    "        \n",
    "        raise ValueError(\"Could not extract video ID from URL\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Invalid YouTube URL: {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "import tempfile\n",
    "\n",
    "class AudioTranscriber:\n",
    "    def __init__(self):\n",
    "        load_dotenv()\n",
    "        self.client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "        self.chunk_duration = 5 * 60 * 1000  # 명시적으로 클래스 속성으로 정의\n",
    "    \n",
    "    def transcribe_audio(self, audio_path: str, language: str = None) -> str:\n",
    "        \"\"\"\n",
    "        음성 파일을 텍스트로 변환\n",
    "        \n",
    "        Args:\n",
    "            audio_path: 음성 파일 경로\n",
    "            language: 음성의 언어 (예: 'ko', 'en', None). None일 경우 자동 감지\n",
    "        \n",
    "        Returns:\n",
    "            str: 변환된 텍스트\n",
    "        \"\"\"\n",
    "        try:\n",
    "            audio_file = Path(audio_path)\n",
    "            if not audio_file.exists():\n",
    "                raise FileNotFoundError(f\"Audio file not found: {audio_path}\")\n",
    "            \n",
    "            # 오디오 파일을 청크로 분할\n",
    "            chunks = self.split_audio(audio_path)\n",
    "            transcribed_texts = []\n",
    "            \n",
    "            # 각 청크를 순차적으로 처리\n",
    "            for i, chunk_path in enumerate(chunks):\n",
    "                print(f\"Processing chunk {i+1}/{len(chunks)}...\")\n",
    "                text = self.transcribe_chunk(chunk_path, language)\n",
    "                if text:\n",
    "                    transcribed_texts.append(text)\n",
    "                    \n",
    "            # 임시 파일들 정리\n",
    "            for chunk_path in chunks:\n",
    "                try:\n",
    "                    os.remove(chunk_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error removing temporary file {chunk_path}: {str(e)}\")\n",
    "            try:\n",
    "                os.rmdir(os.path.dirname(chunks[0]))  # 임시 디렉토리 제거\n",
    "            except Exception as e:\n",
    "                print(f\"Error removing temporary directory: {str(e)}\")\n",
    "            \n",
    "            # 모든 텍스트 합치기\n",
    "            return \" \".join(transcribed_texts)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during transcription: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    \n",
    "    def split_audio(self, audio_path: str) -> list:\n",
    "            \"\"\"\n",
    "            오디오 파일을 5분 단위로 분할\n",
    "            \n",
    "            Args:\n",
    "                audio_path: 오디오 파일 경로\n",
    "                \n",
    "            Returns:\n",
    "                list: 임시 파일 경로 리스트\n",
    "            \"\"\"\n",
    "            audio = AudioSegment.from_file(audio_path)\n",
    "            chunks = []\n",
    "            \n",
    "            # 임시 디렉토리 생성\n",
    "            temp_dir = tempfile.mkdtemp()\n",
    "            \n",
    "            for i in range(0, len(audio), self.chunk_duration):\n",
    "                chunk = audio[i:i + self.chunk_duration]\n",
    "                chunk_path = os.path.join(temp_dir, f\"chunk_{i}.mp3\")\n",
    "                chunk.export(chunk_path, format=\"mp3\")\n",
    "                chunks.append(chunk_path)\n",
    "                \n",
    "            return chunks\n",
    "        \n",
    "    def transcribe_chunk(self, chunk_path: str, language: str = None) -> str:\n",
    "        \"\"\"\n",
    "        단일 오디오 청크를 텍스트로 변환\n",
    "        \n",
    "        Args:\n",
    "            chunk_path: 청크 파일 경로\n",
    "            language: 음성의 언어\n",
    "            \n",
    "        Returns:\n",
    "            str: 변환된 텍스트\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(chunk_path, \"rb\") as file:\n",
    "                response = self.client.audio.transcriptions.create(\n",
    "                    model=\"whisper-1\",\n",
    "                    file=file,\n",
    "                    language=language,\n",
    "                    response_format=\"text\"\n",
    "                )\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(f\"Error transcribing chunk {chunk_path}: {str(e)}\")\n",
    "            raise e\n",
    "\n",
    "    def save_text(self, text: str, file_name: str, output_dir: str = \"audio_text\") -> None:\n",
    "        \"\"\"\n",
    "        텍스트를 파일로 저장\n",
    "        \n",
    "        Args:\n",
    "            text: 저장할 텍스트\n",
    "            output_path: 저장할 파일 경로\n",
    "        \"\"\"\n",
    "        with open(os.path.join(output_dir, file_name), 'w', encoding='utf-8') as f:\n",
    "            f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os \n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "api_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "client = OpenAI(api_key=api_key, base_url=\"https://api.deepseek.com\")\n",
    "model_name = \"deepseek-chat\"\n",
    "\n",
    "def get_completion_openai(messages):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=messages,\n",
    "        temperature=0.3, \n",
    "        max_tokens=8192, \n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def get_completion(messages, model_name=model_name):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=messages,\n",
    "        temperature=0.3, \n",
    "        max_tokens=8192, \n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def get_completion_json_output(messages, model_name=model_name):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=messages,\n",
    "        temperature=0.3, \n",
    "        max_tokens=8192, \n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "    return json.loads(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextStructurer:    \n",
    "    def structure_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        텍스트를 마크다운 형식으로 구조화\n",
    "        \n",
    "        Args:\n",
    "            text: 변환할 텍스트\n",
    "            \n",
    "        Returns:\n",
    "            str: 마크다운 형식으로 구조화된 텍스트\n",
    "        \"\"\"\n",
    "        prompt = \"\"\"\n",
    "        다음 텍스트를 분석하여 주요 섹션별로 나누고 마크다운 형식으로 구조화해주세요.\n",
    "        \n",
    "        다음 규칙을 따라주세요:\n",
    "        1. 주요 섹션은 ## (h2)로 시작\n",
    "        2. 하위 섹션은 ### (h3)로 시작\n",
    "        3. 중요한 포인트는 볼드체(**) 사용\n",
    "        4. 리스트 항목은 적절히 구분하여 표시\n",
    "        5. 핵심 개념이나 용어는 `코드 형식`으로 강조\n",
    "        \n",
    "        원본 텍스트:\n",
    "        {text}\n",
    "        \n",
    "        마크다운 형식으로 구조화된 결과를 제공해주세요. \n",
    "        \n",
    "        ```markdown 없이 구조화된 마크다운 형식으로 텍스트를 제공하시면 됩니다. 모든 내용을 빠짐없이 구조화해주세요.: \n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = get_completion([\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert at structuring text into well-organized markdown sections.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt.format(text=text)}\n",
    "            ])\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during text structuring: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def save_markdown(self, file_name: str, markdown_text: str, output_dir: str = \"markdown_input\") -> None:\n",
    "        \"\"\"\n",
    "        마크다운 텍스트를 파일로 저장\n",
    "        \n",
    "        Args:\n",
    "            file_name: 저장할 파일 이름\n",
    "            markdown_text: 저장할 마크다운 텍스트\n",
    "            output_dir: 저장할 디렉토리 (기본값: \"markdown_input\")\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(os.path.join(output_dir, file_name), 'w', encoding='utf-8') as f:\n",
    "                f.write(markdown_text)\n",
    "            print(f\"Markdown saved to: {os.path.join(output_dir, file_name)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving markdown: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "class TwoStepTextStructurer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def extract_sections(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        1단계: 긴 텍스트에 대해 주요 섹션을 뽑아주세요.\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        아래 긴 텍스트를 주요 섹션 별로 나눠주고 이 섹션은 어떤 것인지 간단하게 설명해주세요.\n",
    "        \n",
    "        어떠한 주요 섹션이라도 놓치지 말고 모두 뽑아주세요. \n",
    "        \n",
    "        텍스트:\n",
    "        {text}\n",
    "        \n",
    "        Example Output 은 다음과 같이 json 형식으로 제공해주세요. \n",
    "        {{\n",
    "            \"sections\": [\n",
    "                {{\n",
    "                    \"title\": \"section1\",\n",
    "                    \"description\": \"section1 description\"\n",
    "                }},\n",
    "                {{\n",
    "                    \"title\": \"section2\",\n",
    "                    \"description\": \"section2 description\"\n",
    "                }},\n",
    "                {{\n",
    "                    \"title\": \"section3\",\n",
    "                    \"description\": \"section3 description\"\n",
    "                }},\n",
    "                ...\n",
    "            ],\n",
    "        }}\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = get_completion_json_output([\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert at analyzing text and organizing it into key sections, breaking down the content into well-structured segments.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ])\n",
    "            return response[\"sections\"]\n",
    "        except Exception as e:\n",
    "            print(f\"Error during extract sections: {str(e)}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def structure_text_by_sections(self, text: str, sections: List[Dict[str, str]]) -> str:\n",
    "        \"\"\"\n",
    "        2단계: 1단계 요약에서 뽑아낸 섹션 정보를 토대로,\n",
    "        텍스트를 섹션 단위로 분할하여 각각 마크다운으로 구조화.\n",
    "        최종 구조화된 텍스트를 합쳐서 반환.\n",
    "        \"\"\"\n",
    "        \n",
    "        # 섹션별 텍스트를 추출하기 (필요 시 chunking)\n",
    "        # 실제로는 find_section_text, chunk_text 등의 함수를 구현해야 함\n",
    "        # 여기서는 일단 원본 전체 텍스트를 그대로 쓰는 예시\n",
    "        structured_sections = []\n",
    "        \n",
    "        for section in sections:\n",
    "            title = section[\"title\"]\n",
    "            description = section[\"description\"]\n",
    "            # 프롬프트 준비\n",
    "            prompt = f\"\"\"\n",
    "            원본 텍스트에서 섹션'{title}'에 해당하는 내용을 뽑아주세요.\n",
    "            \n",
    "            섹션 설명인 '{description}'을 참고해서 관련된 내용을 모두 빠짐없이 뽑아주시면 됩니다. \n",
    "            \n",
    "            다음은 섹션 '{title}'에 해당하는 원본 텍스트입니다:\n",
    "            \n",
    "            {text}\n",
    "            \n",
    "            섹션에 해당하는 내용을 뽑아올 때 이 텍스트를 마크다운 형식으로 구조화해주세요.\n",
    "            규칙:\n",
    "            1. 섹션 제목은 ## (h2)\n",
    "            2. 하위 섹션은 ### (h3)\n",
    "            3. 중요한 포인트는 **볼드체**\n",
    "            4. 리스트 항목은 - 또는 * 사용\n",
    "            5. 코드나 핵심 용어는 `백틱`으로 감싸 강조\n",
    "            6. ```markdown 없이 구조화된 마크다운 형식으로 텍스트를 제공하시면 됩니다. 모든 내용을 빠짐없이 구조화해주세요.: \n",
    "            \"\"\"\n",
    "            \n",
    "            try:\n",
    "                response = get_completion([\n",
    "                    {\"role\": \"system\", \"content\": \"You are an expert at structuring text into markdown.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ])\n",
    "                \n",
    "                # 섹션별 구조화된 결과를 리스트에 담는다\n",
    "                structured_sections.append(f\"{response}\\n\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error during section structuring: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # 모든 섹션 구조화 결과를 하나의 문자열로 합치기\n",
    "        final_result = \"\\n\".join(structured_sections)\n",
    "        return final_result\n",
    "    \n",
    "    def run_two_step_structure(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        사용 예시 메서드:\n",
    "        1) 섹션 추출 -> 2) 섹션별 마크다운 구조화\n",
    "        \"\"\"\n",
    "        sections = self.extract_sections(text)\n",
    "        structured_result = self.structure_text_by_sections(text, sections)\n",
    "        return structured_result\n",
    "    \n",
    "    \n",
    "    def save_markdown(self, file_name: str, markdown_text: str, output_dir: str = \"markdown_input\") -> None:\n",
    "        \"\"\"\n",
    "        마크다운 텍스트를 파일로 저장\n",
    "        \n",
    "        Args:\n",
    "            file_name: 저장할 파일 이름\n",
    "            markdown_text: 저장할 마크다운 텍스트\n",
    "            output_dir: 저장할 디렉토리 (기본값: \"markdown_input\")\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(os.path.join(output_dir, file_name), 'w', encoding='utf-8') as f:\n",
    "                f.write(markdown_text)\n",
    "            print(f\"Markdown saved to: {os.path.join(output_dir, file_name)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving markdown: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Sections: [{'title': 'YC Spring Batch Application', 'description': 'Information about the application deadline for the YC Spring Batch, the benefits of being accepted, and a call to action to apply.'}, {'title': 'Scaling of Large Language Models', 'description': \"Discussion on the trend of increasing size and intelligence of large language models (LLMs), the strategy of scaling, and the comparison to Moore's law.\"}, {'title': 'GPT-2 and GPT-3 Releases', 'description': 'Overview of the release of GPT-2 and its successor GPT-3 by OpenAI, highlighting the significant increase in size and capabilities.'}, {'title': 'Scaling Laws for Neural Language Models', 'description': 'Introduction to the influential paper on scaling laws for neural language models by Jared Kaplan, Sam McCandlish, and colleagues, and its impact on the field.'}, {'title': 'Ingredients for Training AI Models', 'description': \"Explanation of the three main ingredients for training AI models: the model itself, the data it's trained on, and the compute power used.\"}, {'title': 'Chinchilla and Optimal Model Training', 'description': \"Discussion on Google DeepMind's research on scaling laws, the concept of optimal model size and training data, and the introduction of Chinchilla as a milestone.\"}, {'title': 'Debate on the Limits of Scaling Laws', 'description': 'Exploration of the ongoing debate within the AI community about the potential limits of scaling laws, including concerns about plateauing capabilities and data availability.'}, {'title': 'New Frontiers in Scaling: Reasoning Models', 'description': \"Introduction to OpenAI's new class of reasoning models, such as O1 and O3, and the potential for a new paradigm in scaling LLMs through test-time compute.\"}, {'title': 'Future of AI and Scaling Across Modalities', 'description': 'Reflection on the future of AI, the importance of large-language models in the pursuit of artificial general intelligence, and the potential for scaling in other modalities like image diffusion and robotics.'}]\n",
      "\n",
      "Title: YC Spring Batch Application\n",
      "\n",
      "Response: ## YC Spring Batch Application\n",
      "\n",
      "### Application Deadline\n",
      "- **The deadline to apply for the first YC Spring Batch is February 11th.**\n",
      "\n",
      "### Benefits of Being Accepted\n",
      "- If you're accepted, you'll receive **$500,000 in investment**.\n",
      "- You'll also gain **access to the best startup community in the world**.\n",
      "\n",
      "### Call to Action\n",
      "- **Apply now** and come build the future with us.\n",
      "\n",
      "---\n",
      "\n",
      "The rest of the text discusses advancements in AI and scaling laws, which are not directly related to the YC Spring Batch Application section. Therefore, they have been excluded from this markdown structure.\n",
      "\n",
      "Title: Scaling of Large Language Models\n",
      "\n",
      "Response: ## Scaling of Large Language Models\n",
      "\n",
      "### The Trend of Increasing Size and Intelligence\n",
      "- **Large language models (LLMs) are getting bigger and smarter.** Over the past few years, AI labs have adopted a winning strategy: **scaling**. This involves increasing the number of parameters, the amount of data, and the compute power used to train these models.\n",
      "- **The scaling trend has led to consistent improvements in model performance**, similar to how Moore's law predicted the doubling of computing power every 18 months. In AI, performance has been doubling approximately every six months.\n",
      "- **OpenAI's GPT-3**, released in 2020, marked a significant milestone. It was **over 100 times larger than its predecessor, GPT-2**, and demonstrated unprecedented capabilities.\n",
      "\n",
      "### The Strategy of Scaling\n",
      "- **Scaling involves three key ingredients**:\n",
      "  1. **Model size**: Larger models have more parameters, which are internal values of the neural network that are adjusted during training.\n",
      "  2. **Data**: Models are trained on more data, measured in tokens (words or parts of words).\n",
      "  3. **Compute**: Training larger models requires more computing power, often involving more GPUs and energy.\n",
      "- **The Scaling Laws for Neural Language Models**, published by Jared Kaplan, Sam McCandlish, and colleagues at OpenAI in January 2020, revealed that **increasing all three factors (parameters, data, and compute) leads to a smooth, consistent improvement in model performance**.\n",
      "- **Performance depends more on scale than on the algorithm**, a principle that has been confirmed across various types of models, including text-to-image, image-to-text, and even math models.\n",
      "\n",
      "### The Scaling Hypothesis\n",
      "- **Gwern**, an anonymous researcher, was one of the first to articulate the **Scaling Hypothesis**: **Scale up the size, data, and compute, and intelligence will emerge**.\n",
      "- This hypothesis suggests that **intelligence might simply be the result of applying a lot of compute to a lot of data and parameters**.\n",
      "- Gwern's work helped bring Scaling Laws into the mainstream, turning them into a foundational principle for AI development.\n",
      "\n",
      "### Challenges and Limitations of Scaling\n",
      "- **Recent debates** within the AI community question whether **the era of scaling laws is coming to an end**.\n",
      "- **Some argue that as models grow larger and more expensive, their capabilities have started to plateau**.\n",
      "- **Diminishing returns** and **failed training runs** have been reported, with some speculating that **the lack of high-quality data** is becoming a major bottleneck.\n",
      "- **Chinchilla**, a model released by Google DeepMind in 2022, demonstrated that **training models on more data can lead to better performance than simply increasing model size**. Chinchilla, which was **less than half the size of GPT-3 but trained on four times more data**, outperformed larger models.\n",
      "\n",
      "### The Future of Scaling\n",
      "- **OpenAI's new class of reasoning models**, such as **O1 and O3**, hint at a potential new direction for scaling. These models leverage **test-time compute**, allowing them to think through complex problems for longer periods, which improves performance.\n",
      "- **O3**, the successor to O1, has set new benchmarks in various fields, from software engineering to PhD-level science questions, suggesting that **scaling test-time compute could unlock new capabilities**.\n",
      "- **Scaling pre-training may have plateaued**, but **scaling test-time compute** could open up a **new paradigm for scaling laws**, potentially leading to **artificial general intelligence (AGI)**.\n",
      "\n",
      "### Scaling Beyond LLMs\n",
      "- **The principles of scaling** are not limited to LLMs. They also apply to **image diffusion models, protein folding, chemical models, and even world models for robotics**.\n",
      "- **While LLMs may be in the mid-game of scaling**, other modalities are still in the early stages, indicating that **there is still much to explore and achieve in the field of AI scaling**.\n",
      "\n",
      "---\n",
      "\n",
      "This structured markdown captures the key points and discussions from the section on **Scaling of Large Language Models**, including the trends, strategies, challenges, and future directions of scaling in AI.\n",
      "\n",
      "Title: GPT-2 and GPT-3 Releases\n",
      "\n",
      "Response: ## GPT-2 and GPT-3 Releases\n",
      "\n",
      "### Overview of GPT-2 and GPT-3\n",
      "- **GPT-2** was released by OpenAI in **November 2019** with **1.5 billion parameters**, marking a significant milestone in the size of language models.\n",
      "- **GPT-3**, released in the **summer of 2020**, was **over 100 times larger** than GPT-2, with **175 billion parameters**. This model demonstrated unprecedented capabilities and usability, solidifying the era of **scaling laws** in AI.\n",
      "\n",
      "### The Era of Scaling Laws\n",
      "- Before GPT-3, the benefits of increasing model size, data, and compute were uncertain. There was no guarantee that a **100x larger model** would perform **100x better**.\n",
      "- In **January 2020**, OpenAI researchers Jared Kaplan, Sam McCandlish, and colleagues published the influential paper **\"Scaling Laws for Neural Language Models\"**, which revealed that **scaling up parameters, data, and compute** led to consistent improvements in model performance, following a **power law**.\n",
      "- **Performance** was found to depend more on **scale** than on the **algorithm** itself.\n",
      "\n",
      "### Scaling Beyond GPT-3\n",
      "- OpenAI's research confirmed that **Scaling Laws** applied to other types of models, including **text-to-image**, **image-to-text**, and even **mathematical models**.\n",
      "- In **2022**, **Google DeepMind** released research that added a critical insight: **training data** was just as important as model size. They found that previous models like GPT-3 were **under-trained**, meaning they hadn't been trained on enough data to fully realize their potential.\n",
      "- DeepMind's **Chinchilla** model, which was **less than half the size of GPT-3** but trained on **four times more data**, outperformed larger models, demonstrating the importance of **data scaling**.\n",
      "\n",
      "### Challenges to Scaling Laws\n",
      "- Recently, there has been debate within the AI community about whether **scaling laws** have reached their limits. Some argue that as models grow larger and more expensive, **capabilities have started to plateau**.\n",
      "- Concerns have been raised about **diminishing returns**, **failed training runs**, and the **lack of high-quality data** to train new models.\n",
      "- There is speculation that the AI community may soon **run out of data**, which could hinder further scaling.\n",
      "\n",
      "### A New Frontier: Reasoning Models\n",
      "- OpenAI has introduced a new class of **reasoning models**, such as **O1** and its successor **O3**, which focus on **test-time compute** rather than just scaling model size.\n",
      "- **O3** has demonstrated remarkable performance, surpassing benchmarks in **software engineering**, **math**, and **PhD-level science questions**.\n",
      "- Researchers believe that by **scaling test-time compute**, models can achieve **artificial general intelligence (AGI)** by allowing them to **think for longer** and solve increasingly complex problems.\n",
      "\n",
      "### The Future of Scaling\n",
      "- While **pre-training scaling** may have plateaued, **test-time compute scaling** opens up a new paradigm for AI development.\n",
      "- These principles of scaling are not limited to language models but also apply to **image diffusion models**, **protein folding**, **chemical models**, and **robotics**.\n",
      "- The AI community is still in the **early stages** of scaling other modalities, indicating that there is much more to explore and achieve.\n",
      "\n",
      "Title: Scaling Laws for Neural Language Models\n",
      "\n",
      "Response: ## Scaling Laws for Neural Language Models\n",
      "\n",
      "### Introduction to Scaling Laws\n",
      "- **Large language models (LLMs)** are getting bigger and smarter.\n",
      "- AI labs have adopted a strategy of **scaling**: more parameters, more data, more compute.\n",
      "- Similar to **Moore's Law**, AI performance has been doubling every six months.\n",
      "- The era of scaling laws began with the release of **GPT-3**, which was over 100 times larger than its predecessor, **GPT-2**.\n",
      "\n",
      "### The Influential Paper by Jared Kaplan and Sam McCandlish\n",
      "- In January 2020, Jared Kaplan, Sam McCandlish, and colleagues at OpenAI released the influential paper **\"Scaling Laws for Neural Language Models\"**.\n",
      "- The paper revealed that **scaling up parameters, data, and compute** leads to consistent improvements in model performance, following a **power law**.\n",
      "- **Performance** depends more on **scale** than on the algorithm.\n",
      "\n",
      "### Key Ingredients for Training AI Models\n",
      "- **Three main ingredients** for training AI models:\n",
      "  1. **Model**: Larger models have more parameters.\n",
      "  2. **Data**: Measured in tokens (words or parts of words).\n",
      "  3. **Compute**: More GPUs running for longer periods, using more energy.\n",
      "\n",
      "### Impact of Scaling Laws on AI Development\n",
      "- OpenAI's research confirmed that **Scaling Laws** apply to other types of models, such as **text-to-image**, **image-to-text**, and even **math**.\n",
      "- The **Scaling Hypothesis**, popularized by the anonymous researcher **Gwern**, suggested that **intelligence** emerges from scaling up size, data, and compute.\n",
      "- **Gwern's post** brought Scaling Laws into the mainstream, turning them into a foundational principle for AI development.\n",
      "\n",
      "### Google DeepMind's Contribution to Scaling Laws\n",
      "- In 2022, **Google DeepMind** released research that added an important piece to the Scaling Laws puzzle.\n",
      "- Their research suggested that previous LLMs, like **GPT-3**, were **under-trained**.\n",
      "- They trained **Chinchilla**, an LLM less than half the size of GPT-3 but with four times more data, which outperformed larger models.\n",
      "- **Chinchilla Scaling Laws** emphasized the importance of **optimal data training** alongside model size.\n",
      "\n",
      "### The Future of Scaling Laws\n",
      "- Recent debates within the AI community question whether we've reached the **limits of scaling laws**.\n",
      "- Some argue that as models get bigger and more expensive, **capabilities have started to plateau**.\n",
      "- **Rumors** of failed training runs and diminishing returns have surfaced.\n",
      "- **Lack of high-quality data** has become a potential bottleneck for further scaling.\n",
      "\n",
      "### New Frontiers in Scaling\n",
      "- OpenAI's new class of **reasoning models**, like **O1** and **O3**, hints at a new direction for scaling.\n",
      "- **O3** made headlines by smashing benchmarks in software engineering, math, and PhD-level science questions.\n",
      "- Researchers are shifting focus from **scaling model size** to **scaling test-time compute**, allowing models to think longer and solve harder problems.\n",
      "- This new paradigm may unlock **capabilities** previously thought impossible.\n",
      "\n",
      "### Scaling Beyond Language Models\n",
      "- **Scaling principles** apply to other models, such as **image diffusion models**, **protein folding**, and **chemical models**.\n",
      "- **World models** for robotics, like those used in **self-driving**, also benefit from scaling.\n",
      "- While large-language models may be in the **mid-game**, scaling for other modalities is still in the **early game**.\n",
      "\n",
      "### Conclusion\n",
      "- The future of AI may not just be about **bigger models**, but about **new paradigms** of scaling, such as **test-time compute**.\n",
      "- The hunt for **artificial general intelligence (AGI)** continues, with scaling laws playing a key role.\n",
      "- **Buckle up**—AI development is far from over.\n",
      "\n",
      "Title: Ingredients for Training AI Models\n",
      "\n",
      "Response: ## Ingredients for Training AI Models\n",
      "\n",
      "### The Three Main Ingredients\n",
      "- **The model itself**: Larger models have more parameters, which are the internal values of the neural net that are tweaked and trained to make predictions.\n",
      "- **The data it's trained on**: These models are typically trained on much more data, measured in tokens (often words or parts of words for LLMs).\n",
      "- **The compute power used**: Training larger models requires more computing power, meaning more GPUs running for longer periods and consuming more energy.\n",
      "\n",
      "### Scaling Laws for Neural Language Models\n",
      "- **Scaling Laws**: The influential paper by Jared Kaplan, Sam McCandlish, and their colleagues at OpenAI revealed that increasing parameters, data, and compute results in a smooth, consistent improvement in model performance, following a power law.\n",
      "- **Performance depends on scale**: Performance improvement depends more on scale than on the algorithm.\n",
      "\n",
      "### Confirmation of Scaling Laws\n",
      "- **OpenAI's research**: Later research confirmed that Scaling Laws apply to other types of models, such as text-to-image, image-to-text, and even math models.\n",
      "- **Google DeepMind's research**: In 2022, Google DeepMind added an important insight: it's not just about making models bigger but also ensuring they are trained on enough data.\n",
      "\n",
      "### Chinchilla Scaling Laws\n",
      "- **Chinchilla model**: A model less than half the size of GPT-3 but trained with four times more data outperformed larger models.\n",
      "- **Optimal training**: The research suggested that previous LLMs like GPT-3 were under-trained, and optimal performance requires both larger models and sufficient data.\n",
      "\n",
      "### Debate on Scaling Limits\n",
      "- **Plateauing capabilities**: Some argue that as models grow larger and more expensive, their capabilities are starting to plateau.\n",
      "- **Data bottleneck**: The lack of high-quality data for training new models has become a major bottleneck.\n",
      "\n",
      "### New Frontiers in Scaling\n",
      "- **OpenAI's reasoning models**: Models like O1 and O3 leverage test-time compute, allowing them to think longer and solve harder problems.\n",
      "- **New paradigm**: Instead of scaling up model size, researchers may focus on scaling the amount of compute available during the model's chain of thought.\n",
      "\n",
      "### Future of Scaling\n",
      "- **Artificial General Intelligence (AGI)**: Scaling principles may unlock capabilities leading to AGI.\n",
      "- **Other modalities**: Scaling laws also apply to image diffusion models, protein folding, chemical models, and world models for robotics.\n",
      "\n",
      "**Conclusion**: While scaling pre-training may have plateaued, new paradigms like test-time compute scaling could open up entirely new possibilities for AI development.\n",
      "\n",
      "Title: Chinchilla and Optimal Model Training\n",
      "\n",
      "Response: ## Chinchilla and Optimal Model Training\n",
      "\n",
      "### Introduction to Scaling Laws\n",
      "- **Large language models (LLMs)** are growing in size and intelligence.\n",
      "- AI labs have adopted a strategy of **scaling**: increasing parameters, data, and compute power.\n",
      "- Historically, performance doubled every 18 months (similar to Moore's Law), but with AI, this has accelerated to every six months.\n",
      "- Questions arise: Is the era of scaling ending, or are we at the beginning of a new paradigm?\n",
      "\n",
      "### The Emergence of GPT-3\n",
      "- In November 2019, OpenAI released **GPT-2** with 1.5 billion parameters.\n",
      "- By summer 2020, **GPT-3** was released, which was **100 times larger** than GPT-2.\n",
      "- GPT-3 marked the arrival of **scaling laws**, demonstrating that larger models could be more useful and usable.\n",
      "\n",
      "### The Scaling Laws Paper\n",
      "- In January 2020, Jared Kaplan, Sam McCandlish, and colleagues at OpenAI published the **Scaling Laws for Neural Language Models** paper.\n",
      "- The paper revealed that **increasing parameters, data, and compute** leads to consistent improvements in model performance, following a **power law**.\n",
      "- Performance depends more on **scale** than on the algorithm.\n",
      "\n",
      "### Scaling Beyond OpenAI\n",
      "- The **Scaling Hypothesis** was popularized by the anonymous researcher **Gwern**, who argued that intelligence emerges from scaling up size, data, and compute.\n",
      "- In 2022, **Google DeepMind** expanded on this research, emphasizing the importance of **optimal model size and training data** for a given compute budget.\n",
      "\n",
      "### The Chinchilla Breakthrough\n",
      "- Google DeepMind trained over 400 models of different sizes with varying amounts of data.\n",
      "- They discovered that previous LLMs, like **GPT-3**, were **under-trained**—large but not trained on enough data.\n",
      "- **Chinchilla**, an LLM less than half the size of GPT-3 but trained on **four times more data**, outperformed larger models.\n",
      "- This introduced the **Chinchilla Scaling Laws**, highlighting the importance of **data quantity** in model training.\n",
      "\n",
      "### The Future of Scaling Laws\n",
      "- Recent debates question whether scaling laws have reached their limits.\n",
      "- Some argue that as models grow larger and more expensive, **capabilities plateau**.\n",
      "- Concerns about **diminishing returns** and the **lack of high-quality data** have emerged.\n",
      "- OpenAI's new class of **reasoning models** (e.g., **O1** and **O3**) suggests a potential new direction: **scaling test-time compute** instead of model size.\n",
      "- **O3** has demonstrated significant improvements, hinting at a new paradigm for scaling laws.\n",
      "\n",
      "### Scaling Beyond LLMs\n",
      "- Scaling principles apply to other models, such as **image diffusion models**, **protein folding**, and **chemical models**.\n",
      "- While LLMs may be in the **mid-game**, other modalities are still in the **early game** of scaling.\n",
      "\n",
      "### Conclusion\n",
      "- The future of AI may involve **new frontiers** in scaling, such as **test-time compute** and **reasoning models**.\n",
      "- The journey to **artificial general intelligence** continues, with scaling laws playing a pivotal role.\n",
      "\n",
      "Title: Debate on the Limits of Scaling Laws\n",
      "\n",
      "Response: ## Debate on the Limits of Scaling Laws\n",
      "\n",
      "### Introduction to Scaling Laws\n",
      "- **Large language models (LLMs)** are getting bigger and smarter.\n",
      "- AI labs have adopted a strategy of **scaling**: more parameters, more data, and more compute power.\n",
      "- Historically, performance has doubled every six months, similar to **Moore's Law**.\n",
      "- However, there is a growing debate about whether this trend can continue indefinitely.\n",
      "\n",
      "### The Era of Scaling Laws\n",
      "- **OpenAI** released **GPT-2** in 2019 with 1.5 billion parameters, followed by **GPT-3** in 2020, which was over 100 times larger.\n",
      "- The **Scaling Laws for Neural Language Models** paper by Jared Kaplan, Sam McCandlish, and colleagues at OpenAI in January 2020 revealed that increasing parameters, data, and compute power leads to consistent improvements in model performance.\n",
      "- These scaling laws were later confirmed to apply to other types of models, such as **text-to-image**, **image-to-text**, and even **math models**.\n",
      "\n",
      "### The Scaling Hypothesis\n",
      "- The anonymous researcher **Gwern** was one of the first to propose the **Scaling Hypothesis**: scale up size, data, and compute, and intelligence will emerge.\n",
      "- This hypothesis became a foundational principle for AI development.\n",
      "\n",
      "### Google DeepMind's Contribution\n",
      "- In 2022, **Google DeepMind** released research showing that **training models on enough data** is as important as increasing model size.\n",
      "- They trained **Chinchilla**, a model less than half the size of GPT-3 but with four times more data, which outperformed larger models.\n",
      "- This led to the **Chinchilla Scaling Laws**, emphasizing the importance of data in model training.\n",
      "\n",
      "### The Debate on Scaling Limits\n",
      "- **Recent debates** within the AI community question whether we have reached the limits of scaling laws.\n",
      "- Some argue that as models grow larger and more expensive, **capabilities have started to plateau**.\n",
      "- There are concerns about **diminishing returns** and **failed training runs** in major labs.\n",
      "- Another issue is the **lack of high-quality data** for training new models, which could become a bottleneck.\n",
      "\n",
      "### The Future of Scaling\n",
      "- **OpenAI** has introduced a new class of **reasoning models**, such as **O1** and **O3**, which focus on **test-time compute** rather than just increasing model size.\n",
      "- **O3** has shown significant improvements in performance across various benchmarks, suggesting a new paradigm for scaling laws.\n",
      "- Researchers may shift focus to **scaling compute during inference** (test-time compute) rather than pre-training, potentially unlocking new capabilities.\n",
      "\n",
      "### Conclusion\n",
      "- While **scaling pre-training** may have plateaued, **test-time compute** offers a new frontier for scaling laws.\n",
      "- The principles of scaling apply not only to LLMs but also to other models like **image diffusion**, **protein folding**, and **robotics**.\n",
      "- The AI community is still in the early stages of exploring scaling for other modalities, indicating that the journey is far from over.\n",
      "\n",
      "Title: New Frontiers in Scaling: Reasoning Models\n",
      "\n",
      "Response: ## New Frontiers in Scaling: Reasoning Models\n",
      "\n",
      "### Introduction to OpenAI's New Class of Reasoning Models\n",
      "\n",
      "OpenAI has introduced a new class of reasoning models, such as `O1` and `O3`, which represent a potential new paradigm in scaling large language models (LLMs) through **test-time compute**. These models are designed to think through complex problems using their own **chain of thought**, and the longer they are allowed to think, the better they perform.\n",
      "\n",
      "### The Evolution of Scaling Laws\n",
      "\n",
      "- **Scaling Laws for Neural Language Models**: In January 2020, OpenAI researchers Jared Kaplan, Sam McCandlish, and their colleagues published the influential *Scaling Laws for Neural Language Models* paper. This paper revealed that by increasing **parameters**, **data**, and **compute**, model performance improves consistently according to a power law.\n",
      "  \n",
      "- **Chinchilla Scaling Laws**: In 2022, Google DeepMind introduced the **Chinchilla Scaling Laws**, which emphasized the importance of training models with sufficient data. Chinchilla, a model smaller than GPT-3 but trained on four times more data, outperformed larger models, demonstrating that **optimal model performance** depends on both model size and data volume.\n",
      "\n",
      "### The Plateau of Traditional Scaling\n",
      "\n",
      "- **Debate on Scaling Limits**: Recently, there has been significant debate within the AI community about whether traditional scaling laws have reached their limits. Some argue that as models grow larger and more expensive, their **capabilities have started to plateau**.\n",
      "  \n",
      "- **Data Bottleneck**: A major concern is the potential **lack of high-quality data** to train new models. Researchers speculate that we may be nearing the point where we **run out of data** to continue scaling curves.\n",
      "\n",
      "### The Emergence of Reasoning Models\n",
      "\n",
      "- **O1 and O3 Models**: OpenAI's reasoning models, such as `O1` and `O3`, represent a new direction in scaling. These models leverage **test-time compute** to enhance their problem-solving abilities. The longer these models are allowed to think, the better they perform, suggesting a shift from scaling **pre-training** to scaling **test-time compute**.\n",
      "  \n",
      "- **O3's Breakthrough**: The release of `O3` marked a significant leap in AI capabilities. It surpassed benchmarks in **software engineering**, **math**, and **PhD-level science questions**, demonstrating that this new paradigm of scaling could unlock previously unimaginable capabilities.\n",
      "\n",
      "### The Future of Scaling\n",
      "\n",
      "- **Test-Time Compute as a New Paradigm**: Instead of focusing solely on increasing model size during training, researchers are now exploring the potential of scaling **test-time compute**. This approach allows models like `O1` and `O3` to **leverage more compute on the fly**, enabling them to tackle increasingly complex problems.\n",
      "  \n",
      "- **Potential for Artificial General Intelligence (AGI)**: OpenAI researchers believe that this trajectory could lead to **artificial general intelligence**. By scaling test-time compute, models may achieve levels of intelligence that were previously thought impossible.\n",
      "\n",
      "### Scaling Beyond LLMs\n",
      "\n",
      "- **Other Modalities**: The principles of scaling are not limited to LLMs. They also apply to **image diffusion models**, **protein folding**, **chemical models**, and even **world models for robotics** (e.g., self-driving cars). While LLMs may be in the **mid-game**, other modalities are still in the **early game** of scaling.\n",
      "\n",
      "---\n",
      "\n",
      "This new frontier in scaling, driven by reasoning models and test-time compute, promises to revolutionize AI development and potentially unlock capabilities that were once considered out of reach.\n",
      "\n",
      "Title: Future of AI and Scaling Across Modalities\n",
      "\n",
      "Response: ## Future of AI and Scaling Across Modalities\n",
      "\n",
      "### The Era of Scaling Laws\n",
      "\n",
      "- **Large language models (LLMs)** are getting bigger and smarter. Over the past few years, AI labs have adopted a strategy of **scaling**: more parameters, more data, and more compute power.\n",
      "- **Scaling** has led to consistent improvements in model performance, with performance doubling approximately every six months.\n",
      "- In November 2019, OpenAI released **GPT-2**, a model with 1.5 billion parameters. By the next summer, they released **GPT-3**, which was over 100 times larger than GPT-2, marking the arrival of the **era of scaling laws**.\n",
      "- Before GPT-3, it was unclear whether increasing model size would lead to proportional improvements. The **Scaling Laws for Neural Language Models** paper, published in January 2020 by Jared Kaplan, Sam McCandlish, and colleagues at OpenAI, revealed that **performance depends more on scale than on the algorithm**.\n",
      "\n",
      "### The Ingredients of Scaling\n",
      "\n",
      "- Training AI models involves three main ingredients:\n",
      "  1. **Model size**: Larger models have more parameters, which are internal values of the neural network that are tweaked during training.\n",
      "  2. **Data**: Models are trained on more data, measured in tokens (words or parts of words).\n",
      "  3. **Compute power**: Training larger models requires more GPUs, longer training times, and more energy.\n",
      "- The **Scaling Laws** paper showed that increasing all three ingredients—parameters, data, and compute—leads to a smooth, consistent improvement in model performance, following a **power law**.\n",
      "\n",
      "### Scaling Beyond Language Models\n",
      "\n",
      "- OpenAI's research confirmed that **Scaling Laws** apply to other types of models, including **text-to-image**, **image-to-text**, and even **math models**.\n",
      "- In 2022, **Google DeepMind** released research that added a crucial insight: **training models on enough data** is just as important as increasing model size. They found that previous LLMs, like GPT-3, were **under-trained**.\n",
      "- **Chinchilla**, a model less than half the size of GPT-3 but trained on four times more data, outperformed larger models. This discovery led to the **Chinchilla Scaling Laws**, emphasizing the importance of data in model training.\n",
      "\n",
      "### The Limits of Scaling\n",
      "\n",
      "- Recently, there has been debate within the AI community about whether **scaling laws** have reached their limits. Some argue that as models grow larger and more expensive, **capabilities have started to plateau**.\n",
      "- **Rumors** of failed training runs and diminishing returns have emerged, and some speculate that the lack of **high-quality data** is becoming a bottleneck.\n",
      "- **Practical issues** such as running out of data could hinder further scaling. However, researchers believe that new strategies, like **test-time compute**, could open up new frontiers for scaling.\n",
      "\n",
      "### A New Frontier: Test-Time Compute\n",
      "\n",
      "- OpenAI's **O1** and **O3** models represent a new direction in scaling. These models use **chain-of-thought reasoning**, allowing them to think through complex problems for longer periods.\n",
      "- **O3**, the successor to O1, has achieved groundbreaking results in areas like **software engineering**, **math**, and **PhD-level science questions**, surpassing previous state-of-the-art benchmarks.\n",
      "- Instead of scaling up model size during training, researchers are now focusing on **scaling test-time compute**, allowing models to leverage more compute power dynamically for harder problems.\n",
      "- This new paradigm could unlock **capabilities previously thought impossible** and may even be a step toward **artificial general intelligence (AGI)**.\n",
      "\n",
      "### Scaling Across Modalities\n",
      "\n",
      "- **Large-language models** are a key component in the pursuit of **AGI**, but the principles of scaling also apply to other modalities:\n",
      "  - **Image diffusion models**\n",
      "  - **Protein folding and chemical models**\n",
      "  - **World models for robotics** (e.g., self-driving cars)\n",
      "- While LLMs may be in the **mid-game** of scaling, other modalities are still in the **early game**, suggesting significant potential for future advancements.\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "- The future of AI lies in **scaling across modalities**, with **large-language models** leading the way. However, new strategies like **test-time compute** and **Chinchilla Scaling Laws** are reshaping how we approach scaling, potentially unlocking new levels of intelligence and capabilities. **Buckle up**—the journey is just beginning.\n",
      "\n",
      "Markdown saved to: markdown_input/test.md\n"
     ]
    }
   ],
   "source": [
    "text = \"The deadline to apply for the first YC Spring Batch is February 11th. If you're accepted, you'll receive $500,000 in investment, plus access to the best startup community in the world. So apply now and come build the future with us. Large language models are getting bigger, much bigger. They're also getting smarter. Over the past few years, AI labs have hit on what feels like a winning strategy. Scaling. More parameters, more data, more compute. Keep scaling the models and they keep improving. You know, just like Moore's law, we saw the doubling in performance every 18 months. With AI, we have now started to see the doubling every six months or so. But could that be coming to an end? Is the era of scaling finally over? Or are we standing right at the beginning of a brand new scaling paradigm, one that promises to revolutionize AI forever? In November of 2019, OpenAI released GPT-2, its largest ever model with one and a half billion parameters. The next summer, they released its successor, GPT-3, which was something we'd never seen before. Not only was GPT-3 far more useful and usable, it was also much bigger, over 100 times bigger than GPT-2. The era of scaling laws had arrived. Before GPT-3, LLMs were already getting bigger. But it was still anyone's guess whether or not that extra size, data and compute would be worth it. There was no guarantee that making your model 100 times bigger would also make it 100 times better. What if they started to run into diminishing returns? It wasn't until January of 2020, when Jared Kaplan, Sam McCandlish, and their colleagues at OpenAI released the influential Scaling Laws for Neural Language Models paper, that the field started to take notice. Think of training AI models like a recipe. You have three main ingredients. The model itself, the data it's trained on, and the compute power used to train it. Larger models have more parameters. These are the internal values of the neural net that are tweaked and trained in order to make predictions. These models are also typically trained on much more data, measured in tokens, which for LLMs are often words or parts of words. Finally, training these larger models requires computing power, which means more GPUs running for longer, using more and more energy. What the Scaling Laws paper revealed was that by cranking up all three, the parameters, the data, and the compute, the result was a smooth, consistent improvement in model performance in the form of a power law. Performance, it turned out, depends much more on scale than on the algorithm. Later in the year, more research from OpenAI confirmed that these Scaling Laws worked for other kinds of models too. On text-to-image, image-to-text, and even math, the same Scaling Laws were there. But back in early 2020, LLM Scaling Laws were pretty much unknown outside of OpenAI. That is, except for one person. The anonymous researcher and writer, Gwern, was one of the first people to hone in on what he called the Scaling Hypothesis. Scale up the size, the data, and the compute, and watch intelligence emerge. Maybe intelligence really is just, like, a lot of compute applied to a lot of data, applied to a lot of parameters. Maybe Moravec and Lag and Kurzweil were right. Gwern's post brought Scaling Laws into the mainstream. And over time, what started as a quiet observation quickly turned into a foundational principle for AI development. But OpenAI's research was just a part of the picture. In 2022, Google DeepMind released their own research on Scaling Laws, and they added an important missing piece. It turned out that it's not just about making models bigger, it's also about making sure you train them on enough data. Researchers were looking to find the most optimal model size and training data for a given compute budget. So they trained over 400 models of different sizes with different amounts of data. And what they found was surprising. Their research suggested that previous LLMs, like GPT-3, were actually under-trained. These models were huge, but they hadn't been trained on enough text to fully realize their potential. To test this, they trained Chinchilla, an LLM less than half the size of GPT-3, but with four times more data. And it won. Chinchilla was far better than models double, even triple, its size. These so-called Chinchilla Scaling Laws meant that training the optimal model wasn't just about making the model larger, but also about having enough data to feed it. Chinchilla was a huge milestone on the road to training the frontier AI models we have today, like GPT-4.0, CLOD 3.5 SONNET, and others. Labs learned they could trust in the scaling laws and get reliably better and better models. So the future of AI is just bigger and bigger models forever, right? Well, recently, there's been plenty of debate within the AI community about whether or not we've finally reached the limits of scaling laws. Some argued that as the latest generation of models have gotten bigger and more expensive, capabilities have started to plateau. There's a lot of debate, in fact, just in the last multiple weeks, though, have we hit the wall with scaling laws? The current generation of LLM models are roughly, you know, a few companies have converged at the top. But I think they're all working on our next versions, too. We're increasing GPUs at the same, like, rate, but we're not getting the intelligence improvements at all. Meanwhile, rumors have leaked out of major labs about failed training runs and diminishing returns. Others have speculated that the lack of high-quality data to train new models has also become a major bottleneck. One practical issue we could have is we could run out of data. For various reasons, I think that's not going to happen. But, you know, if you look at it very, very naively, we're not that far from running out of data. And so it's like, we just don't have the data to continue the scaling curves. So if the old scaling laws are beginning to lose their edge, what comes next? What if there were a new frontier for scaling from a brand new kind of model? OpenAI's new class of reasoning models hints at a potential new direction. In a previous video, we explained how O1 learns to think through complex problems using its own chain of thought. And OpenAI researchers found that the longer O1 was able to think, the better it performed. It wasn't immediately clear how well this strategy would continue to scale up. But now, with the recent release of its successor, O3, the sky seems to be the limit for this new paradigm of scaling LLMs. O3 made headlines when it was announced, as it smashed benchmarks that were previously considered far out of reach for AI. From software engineering to math to PhD-level science questions, O3 easily surpasses the old, state-of-the-art results. O3 isn't just a small improvement on its predecessors. It's a huge leap. And OpenAI researchers say they have every reason to believe this trajectory will continue. It may even be on a path to artificial general intelligence. Instead of continuing to scale up the model size during training, it seems likely that researchers will shift focus to scaling the amount of compute available to the model for its chain of thought, also called test-time compute. By letting models think for longer, LLMs like O1 and O3 can leverage more compute on the fly, scaling up their intelligence when it's needed for harder and harder problems. Scaling pre-training may have plateaued. But by training test-time compute, OpenAI may have just opened up an entirely new paradigm for scaling laws, potentially unlocking capabilities we never thought possible. Large-language models are a key piece of the hunt to artificial general intelligence. These same principles of scaling appear to hold for other models too. Image diffusion models, protein folding, and chemical models. Even world models for robotics. Like for self-driving. One thing is clear. It might be mid-game for large-language models, but we are clearly still in the early game for scaling other modalities. Buckle up.\"\n",
    "text_structurer = TwoStepTextStructurer()\n",
    "structured_text = text_structurer.run_two_step_structure(text)\n",
    "text_structurer.save_markdown(\"test.md\", structured_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import re\n",
    "\n",
    "class MarkdownParser:\n",
    "    def __init__(self, max_header_level: int = 2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            max_header_level: 별도 섹션으로 분리할 최대 헤더 레벨 (기본값: 2, ## 까지만 분리)\n",
    "        \"\"\"\n",
    "        self.max_header_level = max_header_level\n",
    "    \n",
    "\n",
    "    def parse_markdown(self, markdown_text: str) -> Dict[str, str]:\n",
    "        \"\"\"마크다운 텍스트를 파싱하여 섹션별로 분리\"\"\"\n",
    "        sections = {}\n",
    "        current_content = []\n",
    "        current_title = None\n",
    "        \n",
    "        for line in markdown_text.split('\\n'):\n",
    "            if line.startswith('#') and ' ' in line:\n",
    "                # 헤더 레벨 확인\n",
    "                level = len(line) - len(line.lstrip('#'))\n",
    "                \n",
    "                # max_header_level 이하의 헤더만 새로운 섹션으로 처리\n",
    "                if level <= self.max_header_level:\n",
    "                    # 이전 섹션 저장\n",
    "                    if current_title and current_content:\n",
    "                        sections[current_title] = '\\n'.join(current_content).strip()\n",
    "                    \n",
    "                    # 새로운 섹션 시작\n",
    "                    current_title = line.lstrip('#').strip()\n",
    "                    current_content = []\n",
    "                else:\n",
    "                    # 상위 레벨 헤더는 내용에 포함\n",
    "                    current_content.append(line)\n",
    "            else:\n",
    "                if current_title is None:\n",
    "                    continue  # 첫 헤더 이전의 내용은 무시\n",
    "                current_content.append(line)\n",
    "        \n",
    "        # 마지막 섹션 저장\n",
    "        if current_title and current_content:\n",
    "            sections[current_title] = '\\n'.join(current_content).strip()\n",
    "        \n",
    "        return sections\n",
    "    \n",
    "    def save_markdown(self, file_name: str, markdown_text: str, output_dir: str = \"markdown_input\") -> None:\n",
    "        \"\"\"\n",
    "        마크다운 텍스트를 파일로 저장\n",
    "        \"\"\"\n",
    "        with open(os.path.join(output_dir, file_name), 'w', encoding='utf-8') as f:\n",
    "            f.write(markdown_text)\n",
    "        print(f\"Markdown saved to: {os.path.join(output_dir, file_name)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: YC Spring Batch Application\n",
      "\n",
      "Content: ### Application Deadline\n",
      "- **The deadline to apply for the first YC Spring Batch is February 11th.**\n",
      "\n",
      "### Benefits of Being Accepted\n",
      "- If you're accepted, you'll receive **$500,000 in investment**.\n",
      "- You'll also gain **access to the best startup community in the world**.\n",
      "\n",
      "### Call to Action\n",
      "- **Apply now** and come build the future with us.\n",
      "\n",
      "---\n",
      "\n",
      "The rest of the text discusses advancements in AI and scaling laws, which are not directly related to the YC Spring Batch Application section. Therefore, they have been excluded from this markdown structure.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Title: Scaling of Large Language Models\n",
      "\n",
      "Content: ### The Trend of Increasing Size and Intelligence\n",
      "- **Large language models (LLMs) are getting bigger and smarter.** Over the past few years, AI labs have adopted a winning strategy: **scaling**. This involves increasing the number of parameters, the amount of data, and the compute power used to train these models.\n",
      "- **The scaling trend has led to consistent improvements in model performance**, similar to how Moore's law predicted the doubling of computing power every 18 months. In AI, performance has been doubling approximately every six months.\n",
      "- **OpenAI's GPT-3**, released in 2020, marked a significant milestone. It was **over 100 times larger than its predecessor, GPT-2**, and demonstrated unprecedented capabilities.\n",
      "\n",
      "### The Strategy of Scaling\n",
      "- **Scaling involves three key ingredients**:\n",
      "  1. **Model size**: Larger models have more parameters, which are internal values of the neural network that are adjusted during training.\n",
      "  2. **Data**: Models are trained on more data, measured in tokens (words or parts of words).\n",
      "  3. **Compute**: Training larger models requires more computing power, often involving more GPUs and energy.\n",
      "- **The Scaling Laws for Neural Language Models**, published by Jared Kaplan, Sam McCandlish, and colleagues at OpenAI in January 2020, revealed that **increasing all three factors (parameters, data, and compute) leads to a smooth, consistent improvement in model performance**.\n",
      "- **Performance depends more on scale than on the algorithm**, a principle that has been confirmed across various types of models, including text-to-image, image-to-text, and even math models.\n",
      "\n",
      "### The Scaling Hypothesis\n",
      "- **Gwern**, an anonymous researcher, was one of the first to articulate the **Scaling Hypothesis**: **Scale up the size, data, and compute, and intelligence will emerge**.\n",
      "- This hypothesis suggests that **intelligence might simply be the result of applying a lot of compute to a lot of data and parameters**.\n",
      "- Gwern's work helped bring Scaling Laws into the mainstream, turning them into a foundational principle for AI development.\n",
      "\n",
      "### Challenges and Limitations of Scaling\n",
      "- **Recent debates** within the AI community question whether **the era of scaling laws is coming to an end**.\n",
      "- **Some argue that as models grow larger and more expensive, their capabilities have started to plateau**.\n",
      "- **Diminishing returns** and **failed training runs** have been reported, with some speculating that **the lack of high-quality data** is becoming a major bottleneck.\n",
      "- **Chinchilla**, a model released by Google DeepMind in 2022, demonstrated that **training models on more data can lead to better performance than simply increasing model size**. Chinchilla, which was **less than half the size of GPT-3 but trained on four times more data**, outperformed larger models.\n",
      "\n",
      "### The Future of Scaling\n",
      "- **OpenAI's new class of reasoning models**, such as **O1 and O3**, hint at a potential new direction for scaling. These models leverage **test-time compute**, allowing them to think through complex problems for longer periods, which improves performance.\n",
      "- **O3**, the successor to O1, has set new benchmarks in various fields, from software engineering to PhD-level science questions, suggesting that **scaling test-time compute could unlock new capabilities**.\n",
      "- **Scaling pre-training may have plateaued**, but **scaling test-time compute** could open up a **new paradigm for scaling laws**, potentially leading to **artificial general intelligence (AGI)**.\n",
      "\n",
      "### Scaling Beyond LLMs\n",
      "- **The principles of scaling** are not limited to LLMs. They also apply to **image diffusion models, protein folding, chemical models, and even world models for robotics**.\n",
      "- **While LLMs may be in the mid-game of scaling**, other modalities are still in the early stages, indicating that **there is still much to explore and achieve in the field of AI scaling**.\n",
      "\n",
      "---\n",
      "\n",
      "This structured markdown captures the key points and discussions from the section on **Scaling of Large Language Models**, including the trends, strategies, challenges, and future directions of scaling in AI.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Title: GPT-2 and GPT-3 Releases\n",
      "\n",
      "Content: ### Overview of GPT-2 and GPT-3\n",
      "- **GPT-2** was released by OpenAI in **November 2019** with **1.5 billion parameters**, marking a significant milestone in the size of language models.\n",
      "- **GPT-3**, released in the **summer of 2020**, was **over 100 times larger** than GPT-2, with **175 billion parameters**. This model demonstrated unprecedented capabilities and usability, solidifying the era of **scaling laws** in AI.\n",
      "\n",
      "### The Era of Scaling Laws\n",
      "- Before GPT-3, the benefits of increasing model size, data, and compute were uncertain. There was no guarantee that a **100x larger model** would perform **100x better**.\n",
      "- In **January 2020**, OpenAI researchers Jared Kaplan, Sam McCandlish, and colleagues published the influential paper **\"Scaling Laws for Neural Language Models\"**, which revealed that **scaling up parameters, data, and compute** led to consistent improvements in model performance, following a **power law**.\n",
      "- **Performance** was found to depend more on **scale** than on the **algorithm** itself.\n",
      "\n",
      "### Scaling Beyond GPT-3\n",
      "- OpenAI's research confirmed that **Scaling Laws** applied to other types of models, including **text-to-image**, **image-to-text**, and even **mathematical models**.\n",
      "- In **2022**, **Google DeepMind** released research that added a critical insight: **training data** was just as important as model size. They found that previous models like GPT-3 were **under-trained**, meaning they hadn't been trained on enough data to fully realize their potential.\n",
      "- DeepMind's **Chinchilla** model, which was **less than half the size of GPT-3** but trained on **four times more data**, outperformed larger models, demonstrating the importance of **data scaling**.\n",
      "\n",
      "### Challenges to Scaling Laws\n",
      "- Recently, there has been debate within the AI community about whether **scaling laws** have reached their limits. Some argue that as models grow larger and more expensive, **capabilities have started to plateau**.\n",
      "- Concerns have been raised about **diminishing returns**, **failed training runs**, and the **lack of high-quality data** to train new models.\n",
      "- There is speculation that the AI community may soon **run out of data**, which could hinder further scaling.\n",
      "\n",
      "### A New Frontier: Reasoning Models\n",
      "- OpenAI has introduced a new class of **reasoning models**, such as **O1** and its successor **O3**, which focus on **test-time compute** rather than just scaling model size.\n",
      "- **O3** has demonstrated remarkable performance, surpassing benchmarks in **software engineering**, **math**, and **PhD-level science questions**.\n",
      "- Researchers believe that by **scaling test-time compute**, models can achieve **artificial general intelligence (AGI)** by allowing them to **think for longer** and solve increasingly complex problems.\n",
      "\n",
      "### The Future of Scaling\n",
      "- While **pre-training scaling** may have plateaued, **test-time compute scaling** opens up a new paradigm for AI development.\n",
      "- These principles of scaling are not limited to language models but also apply to **image diffusion models**, **protein folding**, **chemical models**, and **robotics**.\n",
      "- The AI community is still in the **early stages** of scaling other modalities, indicating that there is much more to explore and achieve.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Title: Scaling Laws for Neural Language Models\n",
      "\n",
      "Content: ### Introduction to Scaling Laws\n",
      "- **Large language models (LLMs)** are getting bigger and smarter.\n",
      "- AI labs have adopted a strategy of **scaling**: more parameters, more data, more compute.\n",
      "- Similar to **Moore's Law**, AI performance has been doubling every six months.\n",
      "- The era of scaling laws began with the release of **GPT-3**, which was over 100 times larger than its predecessor, **GPT-2**.\n",
      "\n",
      "### The Influential Paper by Jared Kaplan and Sam McCandlish\n",
      "- In January 2020, Jared Kaplan, Sam McCandlish, and colleagues at OpenAI released the influential paper **\"Scaling Laws for Neural Language Models\"**.\n",
      "- The paper revealed that **scaling up parameters, data, and compute** leads to consistent improvements in model performance, following a **power law**.\n",
      "- **Performance** depends more on **scale** than on the algorithm.\n",
      "\n",
      "### Key Ingredients for Training AI Models\n",
      "- **Three main ingredients** for training AI models:\n",
      "  1. **Model**: Larger models have more parameters.\n",
      "  2. **Data**: Measured in tokens (words or parts of words).\n",
      "  3. **Compute**: More GPUs running for longer periods, using more energy.\n",
      "\n",
      "### Impact of Scaling Laws on AI Development\n",
      "- OpenAI's research confirmed that **Scaling Laws** apply to other types of models, such as **text-to-image**, **image-to-text**, and even **math**.\n",
      "- The **Scaling Hypothesis**, popularized by the anonymous researcher **Gwern**, suggested that **intelligence** emerges from scaling up size, data, and compute.\n",
      "- **Gwern's post** brought Scaling Laws into the mainstream, turning them into a foundational principle for AI development.\n",
      "\n",
      "### Google DeepMind's Contribution to Scaling Laws\n",
      "- In 2022, **Google DeepMind** released research that added an important piece to the Scaling Laws puzzle.\n",
      "- Their research suggested that previous LLMs, like **GPT-3**, were **under-trained**.\n",
      "- They trained **Chinchilla**, an LLM less than half the size of GPT-3 but with four times more data, which outperformed larger models.\n",
      "- **Chinchilla Scaling Laws** emphasized the importance of **optimal data training** alongside model size.\n",
      "\n",
      "### The Future of Scaling Laws\n",
      "- Recent debates within the AI community question whether we've reached the **limits of scaling laws**.\n",
      "- Some argue that as models get bigger and more expensive, **capabilities have started to plateau**.\n",
      "- **Rumors** of failed training runs and diminishing returns have surfaced.\n",
      "- **Lack of high-quality data** has become a potential bottleneck for further scaling.\n",
      "\n",
      "### New Frontiers in Scaling\n",
      "- OpenAI's new class of **reasoning models**, like **O1** and **O3**, hints at a new direction for scaling.\n",
      "- **O3** made headlines by smashing benchmarks in software engineering, math, and PhD-level science questions.\n",
      "- Researchers are shifting focus from **scaling model size** to **scaling test-time compute**, allowing models to think longer and solve harder problems.\n",
      "- This new paradigm may unlock **capabilities** previously thought impossible.\n",
      "\n",
      "### Scaling Beyond Language Models\n",
      "- **Scaling principles** apply to other models, such as **image diffusion models**, **protein folding**, and **chemical models**.\n",
      "- **World models** for robotics, like those used in **self-driving**, also benefit from scaling.\n",
      "- While large-language models may be in the **mid-game**, scaling for other modalities is still in the **early game**.\n",
      "\n",
      "### Conclusion\n",
      "- The future of AI may not just be about **bigger models**, but about **new paradigms** of scaling, such as **test-time compute**.\n",
      "- The hunt for **artificial general intelligence (AGI)** continues, with scaling laws playing a key role.\n",
      "- **Buckle up**—AI development is far from over.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Title: Ingredients for Training AI Models\n",
      "\n",
      "Content: ### The Three Main Ingredients\n",
      "- **The model itself**: Larger models have more parameters, which are the internal values of the neural net that are tweaked and trained to make predictions.\n",
      "- **The data it's trained on**: These models are typically trained on much more data, measured in tokens (often words or parts of words for LLMs).\n",
      "- **The compute power used**: Training larger models requires more computing power, meaning more GPUs running for longer periods and consuming more energy.\n",
      "\n",
      "### Scaling Laws for Neural Language Models\n",
      "- **Scaling Laws**: The influential paper by Jared Kaplan, Sam McCandlish, and their colleagues at OpenAI revealed that increasing parameters, data, and compute results in a smooth, consistent improvement in model performance, following a power law.\n",
      "- **Performance depends on scale**: Performance improvement depends more on scale than on the algorithm.\n",
      "\n",
      "### Confirmation of Scaling Laws\n",
      "- **OpenAI's research**: Later research confirmed that Scaling Laws apply to other types of models, such as text-to-image, image-to-text, and even math models.\n",
      "- **Google DeepMind's research**: In 2022, Google DeepMind added an important insight: it's not just about making models bigger but also ensuring they are trained on enough data.\n",
      "\n",
      "### Chinchilla Scaling Laws\n",
      "- **Chinchilla model**: A model less than half the size of GPT-3 but trained with four times more data outperformed larger models.\n",
      "- **Optimal training**: The research suggested that previous LLMs like GPT-3 were under-trained, and optimal performance requires both larger models and sufficient data.\n",
      "\n",
      "### Debate on Scaling Limits\n",
      "- **Plateauing capabilities**: Some argue that as models grow larger and more expensive, their capabilities are starting to plateau.\n",
      "- **Data bottleneck**: The lack of high-quality data for training new models has become a major bottleneck.\n",
      "\n",
      "### New Frontiers in Scaling\n",
      "- **OpenAI's reasoning models**: Models like O1 and O3 leverage test-time compute, allowing them to think longer and solve harder problems.\n",
      "- **New paradigm**: Instead of scaling up model size, researchers may focus on scaling the amount of compute available during the model's chain of thought.\n",
      "\n",
      "### Future of Scaling\n",
      "- **Artificial General Intelligence (AGI)**: Scaling principles may unlock capabilities leading to AGI.\n",
      "- **Other modalities**: Scaling laws also apply to image diffusion models, protein folding, chemical models, and world models for robotics.\n",
      "\n",
      "**Conclusion**: While scaling pre-training may have plateaued, new paradigms like test-time compute scaling could open up entirely new possibilities for AI development.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Title: Chinchilla and Optimal Model Training\n",
      "\n",
      "Content: ### Introduction to Scaling Laws\n",
      "- **Large language models (LLMs)** are growing in size and intelligence.\n",
      "- AI labs have adopted a strategy of **scaling**: increasing parameters, data, and compute power.\n",
      "- Historically, performance doubled every 18 months (similar to Moore's Law), but with AI, this has accelerated to every six months.\n",
      "- Questions arise: Is the era of scaling ending, or are we at the beginning of a new paradigm?\n",
      "\n",
      "### The Emergence of GPT-3\n",
      "- In November 2019, OpenAI released **GPT-2** with 1.5 billion parameters.\n",
      "- By summer 2020, **GPT-3** was released, which was **100 times larger** than GPT-2.\n",
      "- GPT-3 marked the arrival of **scaling laws**, demonstrating that larger models could be more useful and usable.\n",
      "\n",
      "### The Scaling Laws Paper\n",
      "- In January 2020, Jared Kaplan, Sam McCandlish, and colleagues at OpenAI published the **Scaling Laws for Neural Language Models** paper.\n",
      "- The paper revealed that **increasing parameters, data, and compute** leads to consistent improvements in model performance, following a **power law**.\n",
      "- Performance depends more on **scale** than on the algorithm.\n",
      "\n",
      "### Scaling Beyond OpenAI\n",
      "- The **Scaling Hypothesis** was popularized by the anonymous researcher **Gwern**, who argued that intelligence emerges from scaling up size, data, and compute.\n",
      "- In 2022, **Google DeepMind** expanded on this research, emphasizing the importance of **optimal model size and training data** for a given compute budget.\n",
      "\n",
      "### The Chinchilla Breakthrough\n",
      "- Google DeepMind trained over 400 models of different sizes with varying amounts of data.\n",
      "- They discovered that previous LLMs, like **GPT-3**, were **under-trained**—large but not trained on enough data.\n",
      "- **Chinchilla**, an LLM less than half the size of GPT-3 but trained on **four times more data**, outperformed larger models.\n",
      "- This introduced the **Chinchilla Scaling Laws**, highlighting the importance of **data quantity** in model training.\n",
      "\n",
      "### The Future of Scaling Laws\n",
      "- Recent debates question whether scaling laws have reached their limits.\n",
      "- Some argue that as models grow larger and more expensive, **capabilities plateau**.\n",
      "- Concerns about **diminishing returns** and the **lack of high-quality data** have emerged.\n",
      "- OpenAI's new class of **reasoning models** (e.g., **O1** and **O3**) suggests a potential new direction: **scaling test-time compute** instead of model size.\n",
      "- **O3** has demonstrated significant improvements, hinting at a new paradigm for scaling laws.\n",
      "\n",
      "### Scaling Beyond LLMs\n",
      "- Scaling principles apply to other models, such as **image diffusion models**, **protein folding**, and **chemical models**.\n",
      "- While LLMs may be in the **mid-game**, other modalities are still in the **early game** of scaling.\n",
      "\n",
      "### Conclusion\n",
      "- The future of AI may involve **new frontiers** in scaling, such as **test-time compute** and **reasoning models**.\n",
      "- The journey to **artificial general intelligence** continues, with scaling laws playing a pivotal role.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Title: Debate on the Limits of Scaling Laws\n",
      "\n",
      "Content: ### Introduction to Scaling Laws\n",
      "- **Large language models (LLMs)** are getting bigger and smarter.\n",
      "- AI labs have adopted a strategy of **scaling**: more parameters, more data, and more compute power.\n",
      "- Historically, performance has doubled every six months, similar to **Moore's Law**.\n",
      "- However, there is a growing debate about whether this trend can continue indefinitely.\n",
      "\n",
      "### The Era of Scaling Laws\n",
      "- **OpenAI** released **GPT-2** in 2019 with 1.5 billion parameters, followed by **GPT-3** in 2020, which was over 100 times larger.\n",
      "- The **Scaling Laws for Neural Language Models** paper by Jared Kaplan, Sam McCandlish, and colleagues at OpenAI in January 2020 revealed that increasing parameters, data, and compute power leads to consistent improvements in model performance.\n",
      "- These scaling laws were later confirmed to apply to other types of models, such as **text-to-image**, **image-to-text**, and even **math models**.\n",
      "\n",
      "### The Scaling Hypothesis\n",
      "- The anonymous researcher **Gwern** was one of the first to propose the **Scaling Hypothesis**: scale up size, data, and compute, and intelligence will emerge.\n",
      "- This hypothesis became a foundational principle for AI development.\n",
      "\n",
      "### Google DeepMind's Contribution\n",
      "- In 2022, **Google DeepMind** released research showing that **training models on enough data** is as important as increasing model size.\n",
      "- They trained **Chinchilla**, a model less than half the size of GPT-3 but with four times more data, which outperformed larger models.\n",
      "- This led to the **Chinchilla Scaling Laws**, emphasizing the importance of data in model training.\n",
      "\n",
      "### The Debate on Scaling Limits\n",
      "- **Recent debates** within the AI community question whether we have reached the limits of scaling laws.\n",
      "- Some argue that as models grow larger and more expensive, **capabilities have started to plateau**.\n",
      "- There are concerns about **diminishing returns** and **failed training runs** in major labs.\n",
      "- Another issue is the **lack of high-quality data** for training new models, which could become a bottleneck.\n",
      "\n",
      "### The Future of Scaling\n",
      "- **OpenAI** has introduced a new class of **reasoning models**, such as **O1** and **O3**, which focus on **test-time compute** rather than just increasing model size.\n",
      "- **O3** has shown significant improvements in performance across various benchmarks, suggesting a new paradigm for scaling laws.\n",
      "- Researchers may shift focus to **scaling compute during inference** (test-time compute) rather than pre-training, potentially unlocking new capabilities.\n",
      "\n",
      "### Conclusion\n",
      "- While **scaling pre-training** may have plateaued, **test-time compute** offers a new frontier for scaling laws.\n",
      "- The principles of scaling apply not only to LLMs but also to other models like **image diffusion**, **protein folding**, and **robotics**.\n",
      "- The AI community is still in the early stages of exploring scaling for other modalities, indicating that the journey is far from over.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Title: New Frontiers in Scaling: Reasoning Models\n",
      "\n",
      "Content: ### Introduction to OpenAI's New Class of Reasoning Models\n",
      "\n",
      "OpenAI has introduced a new class of reasoning models, such as `O1` and `O3`, which represent a potential new paradigm in scaling large language models (LLMs) through **test-time compute**. These models are designed to think through complex problems using their own **chain of thought**, and the longer they are allowed to think, the better they perform.\n",
      "\n",
      "### The Evolution of Scaling Laws\n",
      "\n",
      "- **Scaling Laws for Neural Language Models**: In January 2020, OpenAI researchers Jared Kaplan, Sam McCandlish, and their colleagues published the influential *Scaling Laws for Neural Language Models* paper. This paper revealed that by increasing **parameters**, **data**, and **compute**, model performance improves consistently according to a power law.\n",
      "  \n",
      "- **Chinchilla Scaling Laws**: In 2022, Google DeepMind introduced the **Chinchilla Scaling Laws**, which emphasized the importance of training models with sufficient data. Chinchilla, a model smaller than GPT-3 but trained on four times more data, outperformed larger models, demonstrating that **optimal model performance** depends on both model size and data volume.\n",
      "\n",
      "### The Plateau of Traditional Scaling\n",
      "\n",
      "- **Debate on Scaling Limits**: Recently, there has been significant debate within the AI community about whether traditional scaling laws have reached their limits. Some argue that as models grow larger and more expensive, their **capabilities have started to plateau**.\n",
      "  \n",
      "- **Data Bottleneck**: A major concern is the potential **lack of high-quality data** to train new models. Researchers speculate that we may be nearing the point where we **run out of data** to continue scaling curves.\n",
      "\n",
      "### The Emergence of Reasoning Models\n",
      "\n",
      "- **O1 and O3 Models**: OpenAI's reasoning models, such as `O1` and `O3`, represent a new direction in scaling. These models leverage **test-time compute** to enhance their problem-solving abilities. The longer these models are allowed to think, the better they perform, suggesting a shift from scaling **pre-training** to scaling **test-time compute**.\n",
      "  \n",
      "- **O3's Breakthrough**: The release of `O3` marked a significant leap in AI capabilities. It surpassed benchmarks in **software engineering**, **math**, and **PhD-level science questions**, demonstrating that this new paradigm of scaling could unlock previously unimaginable capabilities.\n",
      "\n",
      "### The Future of Scaling\n",
      "\n",
      "- **Test-Time Compute as a New Paradigm**: Instead of focusing solely on increasing model size during training, researchers are now exploring the potential of scaling **test-time compute**. This approach allows models like `O1` and `O3` to **leverage more compute on the fly**, enabling them to tackle increasingly complex problems.\n",
      "  \n",
      "- **Potential for Artificial General Intelligence (AGI)**: OpenAI researchers believe that this trajectory could lead to **artificial general intelligence**. By scaling test-time compute, models may achieve levels of intelligence that were previously thought impossible.\n",
      "\n",
      "### Scaling Beyond LLMs\n",
      "\n",
      "- **Other Modalities**: The principles of scaling are not limited to LLMs. They also apply to **image diffusion models**, **protein folding**, **chemical models**, and even **world models for robotics** (e.g., self-driving cars). While LLMs may be in the **mid-game**, other modalities are still in the **early game** of scaling.\n",
      "\n",
      "---\n",
      "\n",
      "This new frontier in scaling, driven by reasoning models and test-time compute, promises to revolutionize AI development and potentially unlock capabilities that were once considered out of reach.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Title: Future of AI and Scaling Across Modalities\n",
      "\n",
      "Content: ### The Era of Scaling Laws\n",
      "\n",
      "- **Large language models (LLMs)** are getting bigger and smarter. Over the past few years, AI labs have adopted a strategy of **scaling**: more parameters, more data, and more compute power.\n",
      "- **Scaling** has led to consistent improvements in model performance, with performance doubling approximately every six months.\n",
      "- In November 2019, OpenAI released **GPT-2**, a model with 1.5 billion parameters. By the next summer, they released **GPT-3**, which was over 100 times larger than GPT-2, marking the arrival of the **era of scaling laws**.\n",
      "- Before GPT-3, it was unclear whether increasing model size would lead to proportional improvements. The **Scaling Laws for Neural Language Models** paper, published in January 2020 by Jared Kaplan, Sam McCandlish, and colleagues at OpenAI, revealed that **performance depends more on scale than on the algorithm**.\n",
      "\n",
      "### The Ingredients of Scaling\n",
      "\n",
      "- Training AI models involves three main ingredients:\n",
      "  1. **Model size**: Larger models have more parameters, which are internal values of the neural network that are tweaked during training.\n",
      "  2. **Data**: Models are trained on more data, measured in tokens (words or parts of words).\n",
      "  3. **Compute power**: Training larger models requires more GPUs, longer training times, and more energy.\n",
      "- The **Scaling Laws** paper showed that increasing all three ingredients—parameters, data, and compute—leads to a smooth, consistent improvement in model performance, following a **power law**.\n",
      "\n",
      "### Scaling Beyond Language Models\n",
      "\n",
      "- OpenAI's research confirmed that **Scaling Laws** apply to other types of models, including **text-to-image**, **image-to-text**, and even **math models**.\n",
      "- In 2022, **Google DeepMind** released research that added a crucial insight: **training models on enough data** is just as important as increasing model size. They found that previous LLMs, like GPT-3, were **under-trained**.\n",
      "- **Chinchilla**, a model less than half the size of GPT-3 but trained on four times more data, outperformed larger models. This discovery led to the **Chinchilla Scaling Laws**, emphasizing the importance of data in model training.\n",
      "\n",
      "### The Limits of Scaling\n",
      "\n",
      "- Recently, there has been debate within the AI community about whether **scaling laws** have reached their limits. Some argue that as models grow larger and more expensive, **capabilities have started to plateau**.\n",
      "- **Rumors** of failed training runs and diminishing returns have emerged, and some speculate that the lack of **high-quality data** is becoming a bottleneck.\n",
      "- **Practical issues** such as running out of data could hinder further scaling. However, researchers believe that new strategies, like **test-time compute**, could open up new frontiers for scaling.\n",
      "\n",
      "### A New Frontier: Test-Time Compute\n",
      "\n",
      "- OpenAI's **O1** and **O3** models represent a new direction in scaling. These models use **chain-of-thought reasoning**, allowing them to think through complex problems for longer periods.\n",
      "- **O3**, the successor to O1, has achieved groundbreaking results in areas like **software engineering**, **math**, and **PhD-level science questions**, surpassing previous state-of-the-art benchmarks.\n",
      "- Instead of scaling up model size during training, researchers are now focusing on **scaling test-time compute**, allowing models to leverage more compute power dynamically for harder problems.\n",
      "- This new paradigm could unlock **capabilities previously thought impossible** and may even be a step toward **artificial general intelligence (AGI)**.\n",
      "\n",
      "### Scaling Across Modalities\n",
      "\n",
      "- **Large-language models** are a key component in the pursuit of **AGI**, but the principles of scaling also apply to other modalities:\n",
      "  - **Image diffusion models**\n",
      "  - **Protein folding and chemical models**\n",
      "  - **World models for robotics** (e.g., self-driving cars)\n",
      "- While LLMs may be in the **mid-game** of scaling, other modalities are still in the **early game**, suggesting significant potential for future advancements.\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "- The future of AI lies in **scaling across modalities**, with **large-language models** leading the way. However, new strategies like **test-time compute** and **Chinchilla Scaling Laws** are reshaping how we approach scaling, potentially unlocking new levels of intelligence and capabilities. **Buckle up**—the journey is just beginning.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "markdown_parser = MarkdownParser()\n",
    "sections = markdown_parser.parse_markdown(structured_text)\n",
    "\n",
    "for title, content in sections.items():\n",
    "    print(f\"Title: {title}\\n\")\n",
    "    print(f\"Content: {content}\\n\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "class TextExplainer:\n",
    "    def __init__(self):\n",
    "        self.conversation_history = []\n",
    "        self.delay = 1\n",
    "        \n",
    "    def explain_section(self, section_title: str, section_content: str, is_first: bool = False) -> str:\n",
    "        \"\"\"\n",
    "        섹션의 내용을 설명하는 함수\n",
    "        \n",
    "        Args:\n",
    "            section_title: 섹션 제목\n",
    "            section_content: 섹션 내용\n",
    "            is_first: 첫 번째 섹션인지 여부\n",
    "            \n",
    "        Returns:\n",
    "            str: 섹션에 대한 설명\n",
    "        \"\"\"\n",
    "        if is_first:\n",
    "            prompt = f\"\"\"다음 섹션 '{section_title}'의 내용을 명확하고 자세하게 설명해주세요.\n",
    "            \n",
    "            섹션 내용:\n",
    "            {section_content}\"\"\"\n",
    "        else:\n",
    "            prompt = f\"\"\"이전 설명을 바탕으로, 다음 섹션 '{section_title}'의 내용을 설명해주세요.\n",
    "            \n",
    "            섹션 내용:\n",
    "            {section_content}\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # 이전 대화 내용을 포함하여 컨텍스트 유지\n",
    "            self.conversation_history.append({\"role\": \"user\", \"content\": prompt})\n",
    "            \n",
    "            response = get_completion(self.conversation_history, model_name=\"deepseek-reasoner\")\n",
    "            \n",
    "            # 대화 히스토리 업데이트\n",
    "            self.conversation_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "            \n",
    "            # API 호출 간 딜레이\n",
    "            time.sleep(self.delay)\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error explaining section {section_title}: {str(e)}\")\n",
    "            return f\"Error: Failed to explain section {section_title}\"\n",
    "\n",
    "    def explain_text(self, sections: Dict[str, str]) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        텍스트의 각 섹션을 순차적으로 설명\n",
    "        \n",
    "        Args:\n",
    "            sections: 섹션 제목과 내용을 매핑한 딕셔너리\n",
    "            \n",
    "        Returns:\n",
    "            섹션 제목과 설명을 매핑한 딕셔너리\n",
    "        \"\"\"\n",
    "        explanations = {}\n",
    "        \n",
    "        print(\"\\nProcessing sections:\")\n",
    "        for i, (title, content) in tqdm(enumerate(sections.items()), desc=\"Explaining sections\"):\n",
    "            print(f\"\\nProcessing: {title}\")\n",
    "            explanation = self.explain_section(title, content, is_first=(i==0))\n",
    "            explanations[title] = explanation\n",
    "            \n",
    "        return explanations\n",
    "    \n",
    "    def get_conversation_history(self):\n",
    "        \"\"\"대화 히스토리 반환\"\"\"\n",
    "        return self.conversation_history\n",
    "    \n",
    "    def save_explanations(self, explanations: Dict[str, str], file_name: str, output_dir: str = \"explanation_output\") -> str:\n",
    "        \"\"\"설명을 파일로 저장\"\"\"\n",
    "        with open(os.path.join(output_dir, file_name), 'w', encoding='utf-8') as f:\n",
    "            for title, explanation in explanations.items():\n",
    "                f.write(f\"## {title}\\n\\n{explanation}\")\n",
    "                f.write(\"\\n\\n---\\n\")\n",
    "        print(f\"Explanations saved to: {os.path.join(output_dir, file_name)}\")\n",
    "        \n",
    "        return os.path.join(output_dir, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Dict\n",
    "import time\n",
    "\n",
    "class TextQA:\n",
    "    def __init__(self, context: Optional[List[Dict[str, str]]] = None):\n",
    "        self.conversation_history = context or []\n",
    "        self.delay = 1\n",
    "        \n",
    "    def ask_question(self, question: str) -> str:\n",
    "        \"\"\"텍스트에 대한 질문에 답변\"\"\"\n",
    "        try:\n",
    "            prompt = f\"\"\"Based on the text we discussed, please answer the following question in Korean. \n",
    "            Be specific and cite relevant sections when possible.\n",
    "\n",
    "            Question: {question}\"\"\"\n",
    "            \n",
    "            self.conversation_history.append({\"role\": \"user\", \"content\": prompt})\n",
    "            response = get_completion(self.conversation_history)\n",
    "            self.conversation_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "            \n",
    "            time.sleep(self.delay)\n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question: {str(e)}\")\n",
    "            return f\"Error: Failed to process question\"\n",
    "    \n",
    "    def view_conversation_history(self, start_idx: int = 0, end_idx: Optional[int] = None) -> None:\n",
    "        \"\"\"대화 내역을 출력\n",
    "        \n",
    "        Args:\n",
    "            start_idx: 시작 인덱스 (기본값: 0)\n",
    "            end_idx: 종료 인덱스 (기본값: None, None일 경우 끝까지 출력)\n",
    "        \"\"\"\n",
    "        conversations = [\n",
    "            msg for msg in self.conversation_history \n",
    "            if not msg[\"content\"].startswith(\"Here's the text summary:\")\n",
    "        ]\n",
    "        \n",
    "        end_idx = end_idx if end_idx is not None else len(conversations)\n",
    "        \n",
    "        print(\"\\n=== 대화 내역 ===\\n\")\n",
    "        for i, msg in enumerate(conversations[start_idx:end_idx], start=start_idx):\n",
    "            role = msg[\"role\"].upper()\n",
    "            if role == \"ASSISTANT\":\n",
    "                print(f\"\\n🤖 Assistant ({i}):\\n{msg['content']}\\n\")\n",
    "                print(\"-\" * 80)\n",
    "            elif role == \"USER\":\n",
    "                print(f\"\\n👤 User ({i}):\\n{msg['content']}\\n\")\n",
    "                print(\"-\" * 80)\n",
    "    \n",
    "    def get_last_n_conversations(self, n: int = 1) -> None:\n",
    "        \"\"\"최근 n개의 대화 내역을 출력\n",
    "        \n",
    "        Args:\n",
    "            n: 출력할 최근 대화 개수 (기본값: 1)\n",
    "        \"\"\"\n",
    "        conversations = [\n",
    "            msg for msg in self.conversation_history \n",
    "            if not msg[\"content\"].startswith(\"Here's the text summary:\")\n",
    "        ]\n",
    "        start_idx = max(0, len(conversations) - n)\n",
    "        self.view_conversation_history(start_idx)\n",
    "        \n",
    "    def get_conversation_history(self) -> List[Dict[str, str]]:\n",
    "        \"\"\"전체 대화 기록 반환\"\"\"\n",
    "        return self.conversation_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.console import Console\n",
    "from rich.markdown import Markdown\n",
    "from rich.panel import Panel\n",
    "from rich.syntax import Syntax\n",
    "from rich.table import Table\n",
    "from typing import Dict\n",
    "import os\n",
    "\n",
    "class MarkdownPrinter:\n",
    "    def __init__(self):\n",
    "        self.console = Console()\n",
    "        \n",
    "    def print_markdown_file(self, file_path: str):\n",
    "        \"\"\"마크다운 파일을 이쁘게 출력\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                markdown_content = f.read()\n",
    "            \n",
    "            # 마크다운 렌더링\n",
    "            md = Markdown(markdown_content)\n",
    "            \n",
    "            # 마크다운 내용 출력\n",
    "            self.console.print(md)\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.console.print(f\"[bold red]Error reading file: {str(e)}[/]\")\n",
    "            \n",
    "    def print_sections(self, sections: Dict[str, str]):\n",
    "        \"\"\"섹션별로 구분하여 출력\"\"\"\n",
    "        for section, content in sections.items():\n",
    "            # 섹션 제목\n",
    "            self.console.print(\"\\n\")\n",
    "            self.console.print(Panel(\n",
    "                f\"[bold cyan]{section}[/]\",\n",
    "                border_style=\"cyan\"\n",
    "            ))\n",
    "            \n",
    "            # 섹션 내용\n",
    "            md = Markdown(content)\n",
    "            self.console.print(md)\n",
    "            \n",
    "            # 구분선\n",
    "            self.console.print(\"[dim]\" + \"=\"*80 + \"[/]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_text(youtube_url: str) -> str:\n",
    "    file_path = download_audio(youtube_url)\n",
    "    file_name = extract_file_name(file_path)\n",
    "    \n",
    "    audio_transcriber = AudioTranscriber()\n",
    "    text = audio_transcriber.transcribe_audio(file_path)\n",
    "    audio_transcriber.save_text(text, file_name)\n",
    "    \n",
    "    if (len(text) > 1000):\n",
    "        text_structurer = TwoStepTextStructurer()\n",
    "        structured_text = text_structurer.run_two_step_structure(text)\n",
    "        text_structurer.save_markdown(file_name, structured_text)\n",
    "    else: \n",
    "        text_structurer = TextStructurer()\n",
    "        structured_text = text_structurer.run_text_structure(text)\n",
    "        text_structurer.save_markdown(file_name, structured_text)\n",
    "    \n",
    "    markdown_parser = MarkdownParser()\n",
    "    sections = markdown_parser.parse_markdown(structured_text)\n",
    "    markdown_parser.save_markdown(file_name, structured_text)\n",
    "    \n",
    "    explainer = TextExplainer()\n",
    "    explanations = explainer.explain_text(sections)\n",
    "    explanation_path = explainer.save_explanations(explanations, file_name)\n",
    "    \n",
    "    qa = TextQA(context=explainer.get_conversation_history())\n",
    "    return explanation_path, qa\n",
    "\n",
    "def extract_file_name(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    파일 경로에서 확장자와 디렉토리 경로를 제거하고 파일 이름만 반환\n",
    "    \n",
    "    Args:\n",
    "        file_path: 파일의 전체 경로\n",
    "        \n",
    "    Returns:\n",
    "        str: 확장자와 경로가 제거된 파일 이름\n",
    "        \n",
    "    Example:\n",
    "        \"audio/My Video Title-abc123.mp3\" -> \"My Video Title-abc123\"\n",
    "    \"\"\"\n",
    "    # 파일 이름과 확장자 분리 (디렉토리 경로 제거)\n",
    "    base_name = os.path.basename(file_path)\n",
    "    # 확장자 제거\n",
    "    file_name = os.path.splitext(base_name)[0]\n",
    "    return file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=z0wt2pe_LZM\n",
      "[youtube] z0wt2pe_LZM: Downloading webpage\n",
      "[youtube] z0wt2pe_LZM: Downloading tv player API JSON\n",
      "[youtube] z0wt2pe_LZM: Downloading ios player API JSON\n",
      "[youtube] z0wt2pe_LZM: Downloading player 0f7c1eff\n",
      "[youtube] z0wt2pe_LZM: Downloading m3u8 information\n",
      "[info] z0wt2pe_LZM: Downloading 1 format(s): 251\n",
      "[download] Destination: audio/2024： The Year the GPT Wrapper Myth Proved Wrong-z0wt2pe_LZM.webm\n",
      "[download] 100% of   33.03MiB in 00:00:44 at 759.44KiB/s   \n",
      "[ExtractAudio] Destination: audio/2024： The Year the GPT Wrapper Myth Proved Wrong-z0wt2pe_LZM.mp3\n",
      "Deleting original file audio/2024： The Year the GPT Wrapper Myth Proved Wrong-z0wt2pe_LZM.webm (pass -k to keep)\n",
      "Processing chunk 1/8...\n",
      "Processing chunk 2/8...\n",
      "Processing chunk 3/8...\n",
      "Processing chunk 4/8...\n",
      "Processing chunk 5/8...\n",
      "Processing chunk 6/8...\n",
      "Processing chunk 7/8...\n",
      "Processing chunk 8/8...\n",
      "Error during section structuring: Expecting value: line 1 column 1 (char 0)\n",
      "Error during section structuring: Expecting value: line 1 column 1 (char 0)\n",
      "Error during section structuring: Expecting value: line 1 column 1 (char 0)\n",
      "Error during section structuring: Expecting value: line 1 column 1 (char 0)\n",
      "Error during section structuring: Expecting value: line 1 column 1 (char 0)\n",
      "Error during section structuring: Expecting value: line 1 column 1 (char 0)\n",
      "Error during section structuring: Expecting value: line 1 column 1 (char 0)\n",
      "Error during section structuring: Expecting value: line 1 column 1 (char 0)\n",
      "Error during section structuring: Expecting value: line 1 column 1 (char 0)\n",
      "Error during section structuring: Expecting value: line 1 column 1 (char 0)\n",
      "Error during section structuring: Expecting value: line 1 column 1 (char 0)\n",
      "Markdown saved to: markdown_input/2024： The Year the GPT Wrapper Myth Proved Wrong-z0wt2pe_LZM\n",
      "Markdown saved to: markdown_input/2024： The Year the GPT Wrapper Myth Proved Wrong-z0wt2pe_LZM\n",
      "\n",
      "Processing sections:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanations saved to: explanation_output/2024： The Year the GPT Wrapper Myth Proved Wrong-z0wt2pe_LZM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "explanation_path, qa = process_text(\"https://www.youtube.com/watch?v=z0wt2pe_LZM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "                                               <span style=\"font-weight: bold; text-decoration: underline\">SDG Process Overview</span>                                                \n",
       "\n",
       "                                             <span style=\"font-weight: bold\">SDG Process Overview 설명</span>                                             \n",
       "\n",
       "                                               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">1. SDG 프로세스 개요</span>                                                \n",
       "\n",
       "<span style=\"font-weight: bold\">목적</span>: 대규모 고품질 합성 데이터 생성 및 정제 프로세스를 체계화하여 AI 모델 훈련에 최적화된 데이터셋을 구축합니다.  \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">LLM NIM 시작</span>: 프로세스는 <span style=\"font-weight: bold\">LLM NIM</span>(Large Language Model NVIDIA Inference Microservice)을 활용해 원시 데이터를     \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>생성하는 단계로 시작됩니다.                                                                                     \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>LLM NIM은 NVIDIA의 최적화된 추론 서비스로, 효율적인 데이터 생성을 위한 기반 역할을 합니다.                   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">프롬프트 제공</span>: 다양한 유형의 프롬프트(질문-답변, 코드 생성, 대화 시나리오 등)를 LLM NIM에 입력하여 대량의 원시  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>데이터를 생성합니다.                                                                                            \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>예: \"기후 변화의 영향은 무엇인가요?\"와 같은 프롬프트로 시작해 관련 답변을 확장합니다.                        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">피드백 루프 유지</span>: 데이터 품질을 지속적으로 개선하기 위해 생성 단계와 품질 검증 단계를 순환적으로 연결합니다.    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>생성된 데이터는 즉시 평가 모델과 필터로 전달되어 품질이 검증되며, 이를 통해 후속 생성 단계의 프롬프트가      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>개선됩니다.                                                                                                  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">품질 보증</span>: 여러 LLM, <span style=\"font-weight: bold\">보상 모델</span>(Reward Model), 에이전트를 활용해 데이터의 정확성, 일관성, 유용성을 검증합니다.   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>보상 모델은 사전 정의된 기준(예: 논리적 일관성, 사실 정확성)에 따라 데이터에 점수를 부여합니다.              \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">필터링</span>: 품질 검증을 통과하지 못한 데이터는 제거되고, 고품질 데이터만 선별되어 <span style=\"font-weight: bold\">미세 조정(Fine-Tuning)</span> 또는 **사전\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>훈련(Pre-Training)**에 사용됩니다.                                                                              \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">반복적 실행</span>: 목표 데이터셋 크기 또는 토큰 수에 도달할 때까지 1~3단계(생성→검증→필터링)를 반복합니다.            \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "                                              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">2. NVIDIA의 SDG 솔루션</span>                                               \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">사전 구축 파이프라인</span>: NVIDIA는 <span style=\"font-weight: bold\">Nemotron 4340B</span> 모델 훈련에 사용된 검증된 파이프라인을 제공합니다.                \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>이 파이프라인은 합성 데이터 생성부터 필터링까지의 전 과정을 자동화합니다.                                    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">다양한 데이터 유형 지원</span>:                                                                                        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">질문-답변 쌍</span>: 교육, 기술 지원 시나리오에 적합합니다.                                                         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">맵 프롬프트</span>: 작업 절차 또는 프로세스 매핑을 위한 데이터 생성.                                                \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">작문 프롬프트</span>: 창의적인 텍스트(이야기, 논평) 생성.                                                           \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">코드 프롬프트</span>: 프로그래밍 문제 및 솔루션 생성.                                                               \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">대화 데이터</span>: 챗봇 훈련을 위한 인간-기계 상호작용 시뮬레이션.                                                 \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">커스터마이징 가능한 프레임워크</span>:                                                                                 \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>사용자 정의 모델 통합: 자체 LLM을 데이터 생성기 또는 품평가로 활용 가능합니다.                               \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>확장성: 분산 컴퓨팅을 통해 수십억 토큰 규모의 데이터 생성이 가능합니다.                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>NVIDIA 필터링 기술 적용: 사전 구축된 필터 또는 사용자 정의 필터를 결합해 데이터를 정제합니다.                \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "                                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">3. 피드백 루프 관리</span>                                                \n",
       "\n",
       "NVIDIA는 피드백 루프를 통해 데이터 품질을 지속적으로 개선하는 시스템을 관리합니다.                                 \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">구성 요소</span>:                                                                                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    1 </span><span style=\"font-weight: bold\">LLM NIM</span>: 초기 원시 데이터 생성.                                                                              \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    2 </span><span style=\"font-weight: bold\">보상 모델(NIM)</span>: 데이터에 대한 품질 점수 부여(예: 1~5점).                                                     \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    3 </span><span style=\"font-weight: bold\">LLM Judge</span>: 보상 모델의 결과를 보완해 주관적 품질(예: 창의성, 윤리적 적합성)을 평가합니다.                    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    4 </span><span style=\"font-weight: bold\">다중 필터</span>: 점수 기반 필터링, 키워드 차단, 중복 제거 등을 계층적으로 적용합니다.                              \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">파이프라인 오케스트레이션</span>:                                                                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">Curator 도구</span>: 사용자가 피드백 루프 단계를 유연하게 조합해 맞춤형 워크플로우를 설계할 수 있습니다.            \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>예: 생성 → [보상 모델 평가 → LLM Judge 검토 → 동적 필터링] → 재생성                                          \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "                                                   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">4. 핵심 장점</span>                                                    \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">고품질 데이터 확보</span>: 다중 검증 계층을 통해 유효하지 않거나 편향된 데이터가 제거됩니다.                           \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">확장성</span>: 클라우드 네이티브 아키텍처로 대규모 데이터 생성 시 리소스를 탄력적으로 확장할 수 있습니다.              \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">유연성</span>:                                                                                                         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>사전 훈련된 모델 사용 또는 커스텀 모델/필터 추가 가능.                                                       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>특정 산업(의료, 금융)의 규정 준수를 위한 맞춤형 필터 설계 지원.                                              \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "                                          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">5. 예시: 피드백 루프 작동 방식</span>                                           \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">질문</span>: \"NVIDIA가 피드백 루프를 관리한다고 했는데, 구체적 절차는 어떻게 되나요?\"                                  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">답변</span>:                                                                                                           \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    1 </span><span style=\"font-weight: bold\">LLM NIM</span>이 \"기후 변화 대응 정책\" 관련 텍스트를 생성합니다.                                                    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    2 </span><span style=\"font-weight: bold\">보상 모델</span>이 생성된 텍스트의 사실 정확성(예: IPCC 보고서 기준)을 평가합니다.                                  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    3 </span><span style=\"font-weight: bold\">LLM Judge</span>가 텍스트의 가독성과 논리적 구조를 추가 검토합니다.                                                 \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    4 </span><span style=\"font-weight: bold\">필터</span>가 80점 미만 데이터를 제거하고, 남은 데이터는 훈련용 데이터셋에 병합됩니다.                              \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    5 </span>부족한 데이터 양을 충족할 때까지 이 과정을 반복합니다.                                                       \n",
       "\n",
       "이 프로세스를 통해 NVIDIA는 실제 환경과 유사한 고품질 합성 데이터를 효율적으로 생성하며, 사용자는 도메인 특화      \n",
       "데이터셋을 빠르게 구축할 수 있습니다.                                                                              \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "\n",
       "                                        <span style=\"font-weight: bold; text-decoration: underline\">Synthetic Data Generation Offerings</span>                                        \n",
       "\n",
       "                                         <span style=\"font-weight: bold\">합성 데이터 생성(SDG) 솔루션 설명</span>                                         \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "                                               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">1. SDG 프로세스 개요</span>                                                \n",
       "\n",
       "<span style=\"font-weight: bold\">목적</span>: LLM NIM을 시작점으로 고품질 합성 데이터를 생성하고, 반복적 피드백 루프를 통해 품질을 보장합니다.             \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">시작 단계</span>:                                                                                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">LLM NIM</span>을 사용해 초기 원시 데이터 생성.                                                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>예: \"기후 변화 대책\" 관련 프롬프트 입력 → 다양한 답변 생성.                                                  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">피드백 루프 구조</span>:                                                                                               \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    1 </span><span style=\"font-weight: bold\">데이터 생성</span>: LLM NIM이 대량의 원시 데이터 생성.                                                              \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    2 </span><span style=\"font-weight: bold\">품질 평가</span>: 보상 모델(Reward Model), 에이전트, 다른 LLM을 활용해 데이터의 정확성/유용성 검증.                 \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">       • </span>보상 모델: 사전 정의된 기준(예: 사실 일치도)에 따라 1~10점으로 점수화.                                    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    3 </span><span style=\"font-weight: bold\">필터링</span>: 특정 점수 이상의 데이터만 선별, 미세 조정(Fine-Tuning) 등에 활용.                                    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    4 </span><span style=\"font-weight: bold\">반복</span>: 목표 데이터 크기 도달 시까지 1~3단계 반복.                                                             \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "                                            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">2. NVIDIA의 핵심 제공 기능</span>                                             \n",
       "\n",
       "                                             <span style=\"font-weight: bold; text-decoration: underline\">가. 사전 구축 파이프라인</span>                                              \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">검증된 스타터 파이프라인</span>:                                                                                       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">Nemotron 4340B</span> 모델 훈련에 실제 사용된 인프라 제공.                                                          \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>데이터 생성 → 품질 평가 → 필터링까지의 전 과정 자동화.                                                       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>사용 예: 대화형 AI 훈련을 위해 10억 토큰의 대화 데이터 생성.                                                 \n",
       "\n",
       "                                               <span style=\"font-weight: bold; text-decoration: underline\">나. 지원 데이터 유형</span>                                                \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">다양한 데이터 생성 가능</span>:                                                                                        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>                                                                                                                \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span> <span style=\"font-weight: bold\"> 데이터 유형   </span> <span style=\"font-weight: bold\"> 설명                          </span> <span style=\"font-weight: bold\"> 사용 사례             </span>                                        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━                                        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>  질문-답변 쌍    특정 주제에 대한 Q&amp;A            교육용 AI, FAQ 시스템                                         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>  작문 프롬프트   창의적 글쓰기 유도              콘텐츠 생성 툴                                                \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>  코드 프롬프트   프로그래밍 문제/해결안          코드 자동 완성 모델                                           \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>  대화 데이터     인간-기계 상호작용 시뮬레이션   챗봇 훈련                                                     \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>                                                                                                                \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">모듈식 설계</span>: 레고 블록 조립처럼 데이터 유형을 유연하게 결합 가능.                                               \n",
       "\n",
       "                                               <span style=\"font-weight: bold; text-decoration: underline\">다. 커스터마이징 기능</span>                                               \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">자체 모델 통합</span>:                                                                                                 \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>데이터 생성기 또는 품평가로 사용자 소유 LLM 활용 가능.                                                       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>예: 금융 전문 LLM을 이용해 투자 보고서 생성 데이터셋 구축.                                                   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">OpenAI API 호환</span>: 표준 사양을 따르는 모든 LLM과 연동 가능.                                                       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">대규모 확장</span>: 분산 처리로 수십억 토큰 수준의 데이터 생성 지원.                                                   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">고급 필터링</span>:                                                                                                    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>NVIDIA 필터 + 사용자 정의 필터 조합 가능 (예: 업계별 규정 준수 필터).                                        \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "                                             <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">3. 피드백 루프 관리 체계</span>                                              \n",
       "\n",
       "                                                 <span style=\"font-weight: bold; text-decoration: underline\">가. NVIDIA의 역할</span>                                                 \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">3단계 품질 관리 시스템</span>:                                                                                         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    1 </span><span style=\"font-weight: bold\">LLM NIM</span>: 초기 데이터 생성.                                                                                   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    2 </span><span style=\"font-weight: bold\">보상 모델 NIM</span>: 객관적 품질 지표(정확도, 일관성) 평가.                                                        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    3 </span><span style=\"font-weight: bold\">다중 필터</span>:                                                                                                   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">       • </span>점수 기반 필터(예: 7점 미만 데이터 제거).                                                                 \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">       • </span>중복 데이터/민감 정보 제거 필터.                                                                          \n",
       "\n",
       "                                           <span style=\"font-weight: bold; text-decoration: underline\">나. 파이프라인 오케스트레이션</span>                                           \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Curator 도구 활용</span>:                                                                                              \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>사용자가 드래그 앤 드롭으로 워크플로우 설계.                                                                 \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>예시 파이프라인:                                                                                             \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span><span style=\"background-color: #272822\">                                                                                                             </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span><span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">pipeline </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> Curator()</span><span style=\"background-color: #272822\">                                                                                        </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span><span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">pipeline</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">add_step(LLM_NIM(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"생성 모델\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">))</span><span style=\"background-color: #272822\">                                                                     </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span><span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">pipeline</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">add_step(RewardModel_NIM(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"품질 평가\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">))</span><span style=\"background-color: #272822\">                                                             </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span><span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">pipeline</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">add_step(CustomFilter(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"금융용어 필터\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">))</span><span style=\"background-color: #272822\">                                                            </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span><span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">pipeline</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">run(iterations</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">5</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                                                  </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span><span style=\"background-color: #272822\">                                                                                                             </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>독립 실행 가능: 클라우드 또는 온프레미스 환경에서 배포.                                                      \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "                                                  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">4. 핵심 경쟁력</span>                                                   \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">품질 보장</span>:                                                                                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>생성된 데이터 100만 건 중 92%가 미세 조정에 직접 활용 가능 (NVIDIA 내부 테스트 기준).                        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">초대규모 확장성</span>:                                                                                                \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>분당 500만 토큰 처리 가능한 분산 아키텍처.                                                                   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">유연한 구성</span>:                                                                                                    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>엔터프라이즈 요구사항 반영 가능:                                                                             \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">       • </span>의료 데이터 생성 시 HIPAA 규정 필터 추가.                                                                 \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">       • </span>게임 대화 데이터에 욕설 필터 적용.                                                                        \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "                                               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">5. 사용 시나리오 예시</span>                                               \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">문제 상황</span>: 법률 AI 훈련을 위한 판례 데이터 부족.                                                                \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">SDG 적용 과정</span>:                                                                                                  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    1 </span>LLM NIM에 \"계약 위반 사례\" 관련 프롬프트 입력 → 50만 건의 가상 판례 생성.                                    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    2 </span>보상 모델이 법적 논리 일관성 평가 → 상위 30만 건 선별.                                                       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    3 </span>법률 전문가용 커스텀 필터로 추가 정제 → 최종 25만 건 확보.                                                   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    4 </span>목표량 달성 시까지 프로세스 반복.                                                                            \n",
       "\n",
       "이를 통해 기존 수작업 데이터 수집 대비 70% 시간 단축 효과를 달성할 수 있습니다. NVIDIA의 SDG 솔루션은 업계별       \n",
       "특수성을 반영한 고품질 데이터셋 구축을 가속화합니다.                                                               \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "\n",
       "                                             <span style=\"font-weight: bold; text-decoration: underline\">Custom Model Integration</span>                                              \n",
       "\n",
       "                                  <span style=\"font-weight: bold\">맞춤형 모델 통합(Custom Model Integration) 설명</span>                                  \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "                                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">1. 개요</span>                                                      \n",
       "\n",
       "<span style=\"font-weight: bold\">목적</span>: 사용자 소유의 AI 모델을 NVIDIA SDG 프레임워크에 통합해 도메인 특화 합성 데이터를 생성하고 품질을 관리합니다. \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">주요 기능</span>:                                                                                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">자체 모델 사용</span>: 데이터 생성 및 품질 평가 단계에 사용자 정의 모델 적용 가능.                                  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">OpenAI API 호환</span>: OpenAI 사양을 준수하는 모든 모델과 즉시 연동.                                               \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">NVIDIA 인프라 활용</span>: 대규모 데이터 생성 및 필터링을 위한 NVIDIA의 확장 가능한 플랫폼 사용.                    \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "                                                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">2. 프로세스 흐름</span>                                                  \n",
       "\n",
       "                                                  <span style=\"font-weight: bold; text-decoration: underline\">가. 단계별 절차</span>                                                  \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 1 </span><span style=\"font-weight: bold\">LLM NIM 시작</span>:                                                                                                   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>사용자 정의 프롬프트 세트를 LLM NIM에 입력.                                                                  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>예: 의료 진단 리포트 생성을 위한 프롬프트 500종 구성 → 초기 원시 데이터 100만 건 생성.                       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 2 </span><span style=\"font-weight: bold\">피드백 루프 운영</span>:                                                                                               \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">품질 평가 계층화</span>:                                                                                            \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">       • </span>1차: 사용자 소유 <span style=\"font-weight: bold\">보상 모델</span>이 정확도 평가 (예: 의학 용어 정합성).                                          \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">       • </span>2차: <span style=\"font-weight: bold\">LLM Judge</span>가 문맥 일관성 분석.                                                                        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">       • </span>3차: NVIDIA 필터 적용 (중복 제거, 민감 정보 마스킹).                                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 3 </span><span style=\"font-weight: bold\">반복 실행</span>:                                                                                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>목표 데이터 규모(예: 10억 토큰) 도달 시까지 생성→평가→필터링 사이클 반복.                                    \n",
       "\n",
       "                                                 <span style=\"font-weight: bold; text-decoration: underline\">나. 핵심 메커니즘</span>                                                 \n",
       "\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Curator 도구를 이용한 커스텀 파이프라인 예시</span><span style=\"background-color: #272822\">                                                                    </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">pipeline </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> Curator()</span><span style=\"background-color: #272822\">                                                                                              </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">pipeline</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">add_step(CustomLLM_NIM(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"의료 전문 생성 모델\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">))  </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># 사용자 모델</span><span style=\"background-color: #272822\">                                            </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">pipeline</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">add_step(RewardModel_NIM(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"진단 정확도 평가기\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">))  </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># 사용자 보상 모델</span><span style=\"background-color: #272822\">                                      </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">pipeline</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">add_step(NVIDIA_Filter(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"HIPAA 규정 필터\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">))       </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># NVIDIA 필터</span><span style=\"background-color: #272822\">                                           </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">pipeline</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">run(target_tokens</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">1e9</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                                                   </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "                                               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">3. NVIDIA의 제공 기능</span>                                               \n",
       "\n",
       "                                             <span style=\"font-weight: bold; text-decoration: underline\">가. 사전 구축 파이프라인</span>                                              \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Nemotron 4340B 검증 파이프라인</span>:                                                                                 \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>4340억 파라미터 모델 훈련에 실제 사용된 인프라.                                                              \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>즉시 사용 가능한 템플릿 제공 (생성→평가→필터링 자동화).                                                      \n",
       "\n",
       "                                               <span style=\"font-weight: bold; text-decoration: underline\">나. 데이터 유형 확장</span>                                                \n",
       "\n",
       "                                                        \n",
       " <span style=\"font-weight: bold\"> 데이터 유형   </span> <span style=\"font-weight: bold\"> 커스텀 모델 적용 사례                </span> \n",
       " ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \n",
       "  질문-답변 쌍    금융 상품 FAQ 생성용 BERT 기반 모델   \n",
       "  코드 프롬프트   GitHub 코드 학습 전용 코드 생성 모델  \n",
       "  대화 데이터     고객센터 음성 대화 시뮬레이션 모델    \n",
       "                                                        \n",
       "\n",
       "                                                   <span style=\"font-weight: bold; text-decoration: underline\">다. 통합 조건</span>                                                   \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">OpenAI API 표준 준수 필수</span>:                                                                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">/v1/completions</span>, <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">/v1/chat/completions</span> 엔드포인트 지원 모델만 통합 가능.                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">확장성 보장</span>:                                                                                                    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>Kubernetes 기반 분산 처리로 시간당 최대 2TB 데이터 처리 지원.                                                \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "                                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">4. 피드백 루프 관리</span>                                                \n",
       "\n",
       "                                               <span style=\"font-weight: bold; text-decoration: underline\">가. NVIDIA 관리 항목</span>                                                \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">품질 평가 인프라</span>:                                                                                               \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>자동화된 병렬 평가 시스템 (동시 100개 모델 실행 가능).                                                       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">필터링 기술 패키지</span>:                                                                                             \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>기본 필터: 중복 제거, NSFW 콘텐츠 탐지.                                                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>확장 필터: 산업별 규정 준수 필터 추가 가능.                                                                  \n",
       "\n",
       "                                               <span style=\"font-weight: bold; text-decoration: underline\">나. 사용자 제어 권한</span>                                                \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">품질 기준 커스터마이징</span>:                                                                                         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>점수 임계값 조정 (예: 의료 데이터는 90점 이상만 허용).                                                       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>평가 가중치 설정 (정확도 60%, 창의성 40% 등).                                                                \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "                                                   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">5. 핵심 장점</span>                                                    \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">도메인 특화 데이터 생성</span>:                                                                                        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>예: 법률 문서 생성 시 <span style=\"font-weight: bold\">로스쿨 훈련 LLM</span> + <span style=\"font-weight: bold\">판례 분석 필터</span> 조합.                                                 \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">비용 효율성</span>:                                                                                                    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>기존 데이터 라벨링 비용 대비 60% 절감 (NVIDIA 사례 기준).                                                    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">규정 준수</span>:                                                                                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>GDPR, HIPAA 등 필터 템플릿 제공으로 법적 리스크 감소.                                                        \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">6. 적용 사례: 제약회사의 약물 상호작용 데이터 생성</span>                                 \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">문제</span>: 희귀 질환 약물 상호작용 데이터 부족.                                                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">해결</span>:                                                                                                           \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    1 </span><span style=\"font-weight: bold\">커스텀 모델 통합</span>:                                                                                            \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">       • </span>생성: 약학 논문 학습 전용 LLM.                                                                            \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">       • </span>평가: FDA 가이드라인 기반 보상 모델.                                                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    2 </span><span style=\"font-weight: bold\">파이프라인 실행</span>:                                                                                             \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">       • </span>200만 건 생성 → 필터링 후 150만 건 유효 데이터 확보.                                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    3 </span><span style=\"font-weight: bold\">결과</span>:                                                                                                        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">       • </span>기존 연구 데이터 대비 12배 빠른 데이터셋 구축.                                                            \n",
       "\n",
       "NVIDIA의 커스텀 모델 통합 기능은 업계별 특수 요구사항을 충족하는 고품질 데이터 생성 생태계를 제공합니다.           \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "\n",
       "                                        <span style=\"font-weight: bold; text-decoration: underline\">Feedback Loop Management by NVIDIA</span>                                         \n",
       "\n",
       "                                        <span style=\"font-weight: bold\">NVIDIA의 피드백 루프 관리 체계 설명</span>                                        \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "                                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">1. 피드백 루프 프로세스 개요</span>                                            \n",
       "\n",
       "<span style=\"font-weight: bold\">목적</span>: 생성된 합성 데이터의 품질을 단계적으로 개선하며 AI 모델 훈련에 최적화된 데이터셋을 구축합니다.               \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">SDG 다이어그램</span>:                                                                                                 \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span><span style=\"background-color: #272822\">                                                                                                                </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span><span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">graph LR</span><span style=\"background-color: #272822\">                                                                                                       </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span><span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">  A[LLM NIM] --&gt; B[원시 데이터 생성]</span><span style=\"background-color: #272822\">                                                                           </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span><span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">  B --&gt; C[품질 평가 계층]</span><span style=\"background-color: #272822\">                                                                                      </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span><span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">  C --&gt; D{품질 통과?}</span><span style=\"background-color: #272822\">                                                                                          </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span><span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">  D --&gt;|Yes| E[고품질 데이터 저장]</span><span style=\"background-color: #272822\">                                                                             </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span><span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">  D --&gt;|No| F[재생성/필터링]</span><span style=\"background-color: #272822\">                                                                                   </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span><span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">  E --&gt; G[목표 달성?]</span><span style=\"background-color: #272822\">                                                                                          </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span><span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">  G --&gt;|No| A</span><span style=\"background-color: #272822\">                                                                                                  </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span><span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">  G --&gt;|Yes| H[프로세스 종료]</span><span style=\"background-color: #272822\">                                                                                  </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span><span style=\"background-color: #272822\">                                                                                                                </span>\n",
       "\n",
       "                                                 <span style=\"font-weight: bold; text-decoration: underline\">단계별 상세 절차</span>                                                  \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 1 </span><span style=\"font-weight: bold\">데이터 생성 (LLM NIM)</span>:                                                                                          \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>초기 입력: 500~10,000개의 시드 프롬프트 (예: \"암 진단 보고서 작성 가이드라인\").                              \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>출력: 프롬프트당 50~100개의 변형 데이터 생성 → 총 25만~100만 건의 원시 데이터.                               \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 2 </span><span style=\"font-weight: bold\">품질 평가 계층</span>:                                                                                                 \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">1차 검증 (보상 모델)</span>:                                                                                        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">       • </span>객관적 지표 평가 (정확도, 문법 오류 수).                                                                  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">       • </span>예: 의료 데이터 → ICD-11 코드 일치 여부 확인.                                                             \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">2차 검증 (LLM Judge)</span>:                                                                                        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">       • </span>주관적 품질 평가 (맥락 일관성, 창의성).                                                                   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">       • </span>예: \"이 대화 데이터가 자연스러운 인간 상호작용을 반영하는가?\"                                             \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">3차 필터링</span>:                                                                                                  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">       • </span>기술적 필터: 중복 데이터 제거, 토큰 길이 제한.                                                            \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">       • </span>규정 준수 필터: GDPR/CCPA 기준 개인정보 마스킹.                                                           \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 3 </span><span style=\"font-weight: bold\">반복 실행</span>:                                                                                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>1사이클당 평균 24~72시간 소요 (데이터 규모에 따라 가변).                                                     \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>종료 조건: 10억 토큰 달성 또는 사용자 정의 품질 임계값 충족.                                                 \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "                                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">2. 파이프라인 핵심 구성 요소</span>                                            \n",
       "\n",
       "                                             <span style=\"font-weight: bold; text-decoration: underline\">가. 사전 구축 파이프라인</span>                                              \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Nemotron 4340B 훈련 검증 파이프라인</span>:                                                                            \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>434B 파라미터 모델 학습에 실제 사용된 구조.                                                                  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>기본 성능:                                                                                                   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>                                                                                                             \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span> <span style=\"font-weight: bold\"> 항목           </span> <span style=\"font-weight: bold\"> 성능                          </span>                                                            \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━                                                            \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>  처리량           분당 120만 토큰                                                                           \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>  평균 품질 점수   8.2/10 (NVIDIA 내부 벤치마크)                                                             \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>                                                                                                             \n",
       "\n",
       "                                               <span style=\"font-weight: bold; text-decoration: underline\">나. 데이터 생성 유형</span>                                                \n",
       "\n",
       "                                                                          \n",
       " <span style=\"font-weight: bold\"> 데이터 유형   </span> <span style=\"font-weight: bold\"> 생성 메커니즘                </span> <span style=\"font-weight: bold\"> 품질 검증 도구          </span> \n",
       " ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \n",
       "  질문-답변       다중 프롬프트 변형 기반 확장   Fact-Check LLM           \n",
       "  코드 프롬프트   GitHub 레포지토리 패턴 학습    Code Compiler Agent      \n",
       "  대화 데이터     역할극 시나리오 생성           대화 흐름 시뮬레이션 툴  \n",
       "                                                                          \n",
       "\n",
       "                                               <span style=\"font-weight: bold; text-decoration: underline\">다. 커스터마이징 기능</span>                                               \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">호환 조건</span>:                                                                                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>OpenAI API 엔드포인트 필수 지원 (e.g., <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">/v1/completions</span>).                                                     \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>최소 사양: 16GB VRAM GPU에서 구동 가능한 모델.                                                               \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">통합 예시</span>:                                                                                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span><span style=\"background-color: #272822\">                                                                                                                </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span><span style=\"background-color: #272822\"> </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># 사용자 정의 법률 모델 통합</span><span style=\"background-color: #272822\">                                                                                   </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span><span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">legal_llm </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> CustomModel(</span><span style=\"background-color: #272822\">                                                                                       </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span><span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">  endpoint</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"http://legal-llm/api/v1/completions\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">,</span><span style=\"background-color: #272822\">                                                              </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span><span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">  api_key</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"YOUR_KEY\"</span><span style=\"background-color: #272822\">                                                                                           </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span><span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                                                                              </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span><span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">nvidia_pipeline</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">replace_generator(legal_llm)</span><span style=\"background-color: #272822\">                                                                   </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span><span style=\"background-color: #272822\">                                                                                                                </span>\n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "                                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">3. NVIDIA의 피드백 루프 관리</span>                                            \n",
       "\n",
       "                                                <span style=\"font-weight: bold; text-decoration: underline\">가. 핵심 관리 요소</span>                                                 \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">3단계 품질 게이트</span>:                                                                                              \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    1 </span><span style=\"font-weight: bold\">정량적 평가</span>: 보상 모델 점수 7.5/10 이상 필수.                                                                \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    2 </span><span style=\"font-weight: bold\">정성적 평가</span>: 3명의 가상 LLM Judge 합의 도출.                                                                 \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    3 </span><span style=\"font-weight: bold\">규정 검증</span>: 업계별 규정 준수 자동 감지 시스템.                                                                \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">자동화 도구</span>:                                                                                                    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">품질 대시보드</span>: 실시간 데이터 분포 모니터링.                                                                  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span><span style=\"background-color: #272822\">                                                                                                                </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span><span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">dashboard </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> QualityMonitor()</span><span style=\"background-color: #272822\">                                                                                   </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span><span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">dashboard</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">track_metric(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"정확도\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, threshold</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">0.85</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                               </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span><span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">dashboard</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">track_metric(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"창의성\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, threshold</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">0.7</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                                </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span><span style=\"background-color: #272822\">                                                                                                                </span>\n",
       "\n",
       "                                           <span style=\"font-weight: bold; text-decoration: underline\">나. 파이프라인 오케스트레이션</span>                                           \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Curator 도구 활용 시나리오</span>:                                                                                     \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    1 </span>워크플로우 설계: 드래그 앤 드롭 인터페이스.                                                                  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    2 </span>리소스 할당: GPU 클러스터 자동 확장 설정.                                                                    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    3 </span>실행:                                                                                                        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span><span style=\"background-color: #272822\">                                                                                                             </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span><span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">curator run --pipeline medical_sdg.yaml --gpus </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">32</span><span style=\"background-color: #272822\">                                                           </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span><span style=\"background-color: #272822\">                                                                                                             </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    4 </span>모니터링: 실시간 로그 및 품질 지표 추적.                                                                     \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "                                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">4. 핵심 경쟁력 요약</span>                                                \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">품질 보장 메커니즘</span>:                                                                                             \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>98% 이상의 데이터가 HELM(Holistic Evaluation of Language Models) 기준 통과.                                  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">초확장 아키텍처</span>:                                                                                                \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>512개 GPU 클러스터에서 일일 1조 토큰 처리 가능.                                                              \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">유연한 통합</span>:                                                                                                    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>150+ 개의 오픈소스 LLM 사전 지원 (Llama-3, Mistral 등).                                                      \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "                                    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">5. 실제 적용 사례: 금융 리스크 보고서 생성</span>                                     \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">문제</span>: EU MIFID II 규정 준수 리포트 데이터 부족.                                                                 \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">솔루션</span>:                                                                                                         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    1 </span><span style=\"font-weight: bold\">커스텀 모델 통합</span>:                                                                                            \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">       • </span>생성: BloombergGPT 파인튠 모델.                                                                           \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">       • </span>평가: EU 규정 템플릿 매칭 시스템.                                                                         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    2 </span><span style=\"font-weight: bold\">피드백 루프 운영</span>:                                                                                            \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">       • </span>5사이클 실행 → 450만 건 생성 → 320만 건 최종 데이터셋 확보.                                               \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    3 </span><span style=\"font-weight: bold\">결과</span>:                                                                                                        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">       • </span>수동 검수 대비 오류율 68% 감소 (1.2% → 0.38%).                                                            \n",
       "\n",
       "NVIDIA의 피드백 루프 관리는 산업별 도메인 지식과 기술 인프라를 결합해 데이터 생성 프로세스를 혁신합니다. 사용자는  \n",
       "복잡한 품질 관리 부담 없이 고품질 데이터 생성에 집중할 수 있습니다.                                                \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "                                               \u001b[1;4mSDG Process Overview\u001b[0m                                                \n",
       "\n",
       "                                             \u001b[1mSDG Process Overview 설명\u001b[0m                                             \n",
       "\n",
       "                                               \u001b[1;2m1. SDG 프로세스 개요\u001b[0m                                                \n",
       "\n",
       "\u001b[1m목적\u001b[0m: 대규모 고품질 합성 데이터 생성 및 정제 프로세스를 체계화하여 AI 모델 훈련에 최적화된 데이터셋을 구축합니다.  \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1mLLM NIM 시작\u001b[0m: 프로세스는 \u001b[1mLLM NIM\u001b[0m(Large Language Model NVIDIA Inference Microservice)을 활용해 원시 데이터를     \n",
       "\u001b[1;33m   \u001b[0m생성하는 단계로 시작됩니다.                                                                                     \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0mLLM NIM은 NVIDIA의 최적화된 추론 서비스로, 효율적인 데이터 생성을 위한 기반 역할을 합니다.                   \n",
       "\u001b[1;33m • \u001b[0m\u001b[1m프롬프트 제공\u001b[0m: 다양한 유형의 프롬프트(질문-답변, 코드 생성, 대화 시나리오 등)를 LLM NIM에 입력하여 대량의 원시  \n",
       "\u001b[1;33m   \u001b[0m데이터를 생성합니다.                                                                                            \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m예: \"기후 변화의 영향은 무엇인가요?\"와 같은 프롬프트로 시작해 관련 답변을 확장합니다.                        \n",
       "\u001b[1;33m • \u001b[0m\u001b[1m피드백 루프 유지\u001b[0m: 데이터 품질을 지속적으로 개선하기 위해 생성 단계와 품질 검증 단계를 순환적으로 연결합니다.    \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m생성된 데이터는 즉시 평가 모델과 필터로 전달되어 품질이 검증되며, 이를 통해 후속 생성 단계의 프롬프트가      \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0m개선됩니다.                                                                                                  \n",
       "\u001b[1;33m • \u001b[0m\u001b[1m품질 보증\u001b[0m: 여러 LLM, \u001b[1m보상 모델\u001b[0m(Reward Model), 에이전트를 활용해 데이터의 정확성, 일관성, 유용성을 검증합니다.   \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m보상 모델은 사전 정의된 기준(예: 논리적 일관성, 사실 정확성)에 따라 데이터에 점수를 부여합니다.              \n",
       "\u001b[1;33m • \u001b[0m\u001b[1m필터링\u001b[0m: 품질 검증을 통과하지 못한 데이터는 제거되고, 고품질 데이터만 선별되어 \u001b[1m미세 조정(Fine-Tuning)\u001b[0m 또는 **사전\n",
       "\u001b[1;33m   \u001b[0m훈련(Pre-Training)**에 사용됩니다.                                                                              \n",
       "\u001b[1;33m • \u001b[0m\u001b[1m반복적 실행\u001b[0m: 목표 데이터셋 크기 또는 토큰 수에 도달할 때까지 1~3단계(생성→검증→필터링)를 반복합니다.            \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "                                              \u001b[1;2m2. NVIDIA의 SDG 솔루션\u001b[0m                                               \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1m사전 구축 파이프라인\u001b[0m: NVIDIA는 \u001b[1mNemotron 4340B\u001b[0m 모델 훈련에 사용된 검증된 파이프라인을 제공합니다.                \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m이 파이프라인은 합성 데이터 생성부터 필터링까지의 전 과정을 자동화합니다.                                    \n",
       "\u001b[1;33m • \u001b[0m\u001b[1m다양한 데이터 유형 지원\u001b[0m:                                                                                        \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1m질문-답변 쌍\u001b[0m: 교육, 기술 지원 시나리오에 적합합니다.                                                         \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1m맵 프롬프트\u001b[0m: 작업 절차 또는 프로세스 매핑을 위한 데이터 생성.                                                \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1m작문 프롬프트\u001b[0m: 창의적인 텍스트(이야기, 논평) 생성.                                                           \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1m코드 프롬프트\u001b[0m: 프로그래밍 문제 및 솔루션 생성.                                                               \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1m대화 데이터\u001b[0m: 챗봇 훈련을 위한 인간-기계 상호작용 시뮬레이션.                                                 \n",
       "\u001b[1;33m • \u001b[0m\u001b[1m커스터마이징 가능한 프레임워크\u001b[0m:                                                                                 \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m사용자 정의 모델 통합: 자체 LLM을 데이터 생성기 또는 품평가로 활용 가능합니다.                               \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m확장성: 분산 컴퓨팅을 통해 수십억 토큰 규모의 데이터 생성이 가능합니다.                                      \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0mNVIDIA 필터링 기술 적용: 사전 구축된 필터 또는 사용자 정의 필터를 결합해 데이터를 정제합니다.                \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "                                                \u001b[1;2m3. 피드백 루프 관리\u001b[0m                                                \n",
       "\n",
       "NVIDIA는 피드백 루프를 통해 데이터 품질을 지속적으로 개선하는 시스템을 관리합니다.                                 \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1m구성 요소\u001b[0m:                                                                                                      \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m 1 \u001b[0m\u001b[1mLLM NIM\u001b[0m: 초기 원시 데이터 생성.                                                                              \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m 2 \u001b[0m\u001b[1m보상 모델(NIM)\u001b[0m: 데이터에 대한 품질 점수 부여(예: 1~5점).                                                     \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m 3 \u001b[0m\u001b[1mLLM Judge\u001b[0m: 보상 모델의 결과를 보완해 주관적 품질(예: 창의성, 윤리적 적합성)을 평가합니다.                    \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m 4 \u001b[0m\u001b[1m다중 필터\u001b[0m: 점수 기반 필터링, 키워드 차단, 중복 제거 등을 계층적으로 적용합니다.                              \n",
       "\u001b[1;33m • \u001b[0m\u001b[1m파이프라인 오케스트레이션\u001b[0m:                                                                                      \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mCurator 도구\u001b[0m: 사용자가 피드백 루프 단계를 유연하게 조합해 맞춤형 워크플로우를 설계할 수 있습니다.            \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m예: 생성 → [보상 모델 평가 → LLM Judge 검토 → 동적 필터링] → 재생성                                          \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "                                                   \u001b[1;2m4. 핵심 장점\u001b[0m                                                    \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1m고품질 데이터 확보\u001b[0m: 다중 검증 계층을 통해 유효하지 않거나 편향된 데이터가 제거됩니다.                           \n",
       "\u001b[1;33m • \u001b[0m\u001b[1m확장성\u001b[0m: 클라우드 네이티브 아키텍처로 대규모 데이터 생성 시 리소스를 탄력적으로 확장할 수 있습니다.              \n",
       "\u001b[1;33m • \u001b[0m\u001b[1m유연성\u001b[0m:                                                                                                         \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m사전 훈련된 모델 사용 또는 커스텀 모델/필터 추가 가능.                                                       \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m특정 산업(의료, 금융)의 규정 준수를 위한 맞춤형 필터 설계 지원.                                              \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "                                          \u001b[1;2m5. 예시: 피드백 루프 작동 방식\u001b[0m                                           \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1m질문\u001b[0m: \"NVIDIA가 피드백 루프를 관리한다고 했는데, 구체적 절차는 어떻게 되나요?\"                                  \n",
       "\u001b[1;33m • \u001b[0m\u001b[1m답변\u001b[0m:                                                                                                           \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m 1 \u001b[0m\u001b[1mLLM NIM\u001b[0m이 \"기후 변화 대응 정책\" 관련 텍스트를 생성합니다.                                                    \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m 2 \u001b[0m\u001b[1m보상 모델\u001b[0m이 생성된 텍스트의 사실 정확성(예: IPCC 보고서 기준)을 평가합니다.                                  \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m 3 \u001b[0m\u001b[1mLLM Judge\u001b[0m가 텍스트의 가독성과 논리적 구조를 추가 검토합니다.                                                 \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m 4 \u001b[0m\u001b[1m필터\u001b[0m가 80점 미만 데이터를 제거하고, 남은 데이터는 훈련용 데이터셋에 병합됩니다.                              \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m 5 \u001b[0m부족한 데이터 양을 충족할 때까지 이 과정을 반복합니다.                                                       \n",
       "\n",
       "이 프로세스를 통해 NVIDIA는 실제 환경과 유사한 고품질 합성 데이터를 효율적으로 생성하며, 사용자는 도메인 특화      \n",
       "데이터셋을 빠르게 구축할 수 있습니다.                                                                              \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "\n",
       "                                        \u001b[1;4mSynthetic Data Generation Offerings\u001b[0m                                        \n",
       "\n",
       "                                         \u001b[1m합성 데이터 생성(SDG) 솔루션 설명\u001b[0m                                         \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "                                               \u001b[1;2m1. SDG 프로세스 개요\u001b[0m                                                \n",
       "\n",
       "\u001b[1m목적\u001b[0m: LLM NIM을 시작점으로 고품질 합성 데이터를 생성하고, 반복적 피드백 루프를 통해 품질을 보장합니다.             \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1m시작 단계\u001b[0m:                                                                                                      \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mLLM NIM\u001b[0m을 사용해 초기 원시 데이터 생성.                                                                      \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m예: \"기후 변화 대책\" 관련 프롬프트 입력 → 다양한 답변 생성.                                                  \n",
       "\u001b[1;33m • \u001b[0m\u001b[1m피드백 루프 구조\u001b[0m:                                                                                               \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m 1 \u001b[0m\u001b[1m데이터 생성\u001b[0m: LLM NIM이 대량의 원시 데이터 생성.                                                              \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m 2 \u001b[0m\u001b[1m품질 평가\u001b[0m: 보상 모델(Reward Model), 에이전트, 다른 LLM을 활용해 데이터의 정확성/유용성 검증.                 \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m보상 모델: 사전 정의된 기준(예: 사실 일치도)에 따라 1~10점으로 점수화.                                    \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m 3 \u001b[0m\u001b[1m필터링\u001b[0m: 특정 점수 이상의 데이터만 선별, 미세 조정(Fine-Tuning) 등에 활용.                                    \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m 4 \u001b[0m\u001b[1m반복\u001b[0m: 목표 데이터 크기 도달 시까지 1~3단계 반복.                                                             \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "                                            \u001b[1;2m2. NVIDIA의 핵심 제공 기능\u001b[0m                                             \n",
       "\n",
       "                                             \u001b[1;4m가. 사전 구축 파이프라인\u001b[0m                                              \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1m검증된 스타터 파이프라인\u001b[0m:                                                                                       \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mNemotron 4340B\u001b[0m 모델 훈련에 실제 사용된 인프라 제공.                                                          \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m데이터 생성 → 품질 평가 → 필터링까지의 전 과정 자동화.                                                       \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m사용 예: 대화형 AI 훈련을 위해 10억 토큰의 대화 데이터 생성.                                                 \n",
       "\n",
       "                                               \u001b[1;4m나. 지원 데이터 유형\u001b[0m                                                \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1m다양한 데이터 생성 가능\u001b[0m:                                                                                        \n",
       "\u001b[1;33m   \u001b[0m                                                                                                                \n",
       "\u001b[1;33m   \u001b[0m \u001b[1m \u001b[0m\u001b[1m데이터 유형\u001b[0m\u001b[1m  \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m설명\u001b[0m\u001b[1m                         \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m사용 사례\u001b[0m\u001b[1m            \u001b[0m\u001b[1m \u001b[0m                                        \n",
       "\u001b[1;33m   \u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━                                        \n",
       "\u001b[1;33m   \u001b[0m  질문-답변 쌍    특정 주제에 대한 Q&A            교육용 AI, FAQ 시스템                                         \n",
       "\u001b[1;33m   \u001b[0m  작문 프롬프트   창의적 글쓰기 유도              콘텐츠 생성 툴                                                \n",
       "\u001b[1;33m   \u001b[0m  코드 프롬프트   프로그래밍 문제/해결안          코드 자동 완성 모델                                           \n",
       "\u001b[1;33m   \u001b[0m  대화 데이터     인간-기계 상호작용 시뮬레이션   챗봇 훈련                                                     \n",
       "\u001b[1;33m   \u001b[0m                                                                                                                \n",
       "\u001b[1;33m • \u001b[0m\u001b[1m모듈식 설계\u001b[0m: 레고 블록 조립처럼 데이터 유형을 유연하게 결합 가능.                                               \n",
       "\n",
       "                                               \u001b[1;4m다. 커스터마이징 기능\u001b[0m                                               \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1m자체 모델 통합\u001b[0m:                                                                                                 \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m데이터 생성기 또는 품평가로 사용자 소유 LLM 활용 가능.                                                       \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m예: 금융 전문 LLM을 이용해 투자 보고서 생성 데이터셋 구축.                                                   \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mOpenAI API 호환\u001b[0m: 표준 사양을 따르는 모든 LLM과 연동 가능.                                                       \n",
       "\u001b[1;33m • \u001b[0m\u001b[1m대규모 확장\u001b[0m: 분산 처리로 수십억 토큰 수준의 데이터 생성 지원.                                                   \n",
       "\u001b[1;33m • \u001b[0m\u001b[1m고급 필터링\u001b[0m:                                                                                                    \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0mNVIDIA 필터 + 사용자 정의 필터 조합 가능 (예: 업계별 규정 준수 필터).                                        \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "                                             \u001b[1;2m3. 피드백 루프 관리 체계\u001b[0m                                              \n",
       "\n",
       "                                                 \u001b[1;4m가. NVIDIA의 역할\u001b[0m                                                 \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1m3단계 품질 관리 시스템\u001b[0m:                                                                                         \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m 1 \u001b[0m\u001b[1mLLM NIM\u001b[0m: 초기 데이터 생성.                                                                                   \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m 2 \u001b[0m\u001b[1m보상 모델 NIM\u001b[0m: 객관적 품질 지표(정확도, 일관성) 평가.                                                        \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m 3 \u001b[0m\u001b[1m다중 필터\u001b[0m:                                                                                                   \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m점수 기반 필터(예: 7점 미만 데이터 제거).                                                                 \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m중복 데이터/민감 정보 제거 필터.                                                                          \n",
       "\n",
       "                                           \u001b[1;4m나. 파이프라인 오케스트레이션\u001b[0m                                           \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1mCurator 도구 활용\u001b[0m:                                                                                              \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m사용자가 드래그 앤 드롭으로 워크플로우 설계.                                                                 \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m예시 파이프라인:                                                                                             \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0m\u001b[48;2;39;40;34m                                                                                                             \u001b[0m\n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpipeline\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mCurator\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                       \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpipeline\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34madd_step\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mLLM_NIM\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m생성 모델\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                    \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpipeline\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34madd_step\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mRewardModel_NIM\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m품질 평가\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                            \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpipeline\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34madd_step\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mCustomFilter\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m금융용어 필터\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                           \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpipeline\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mrun\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34miterations\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m5\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0m\u001b[48;2;39;40;34m                                                                                                             \u001b[0m\n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m독립 실행 가능: 클라우드 또는 온프레미스 환경에서 배포.                                                      \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "                                                  \u001b[1;2m4. 핵심 경쟁력\u001b[0m                                                   \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1m품질 보장\u001b[0m:                                                                                                      \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m생성된 데이터 100만 건 중 92%가 미세 조정에 직접 활용 가능 (NVIDIA 내부 테스트 기준).                        \n",
       "\u001b[1;33m • \u001b[0m\u001b[1m초대규모 확장성\u001b[0m:                                                                                                \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m분당 500만 토큰 처리 가능한 분산 아키텍처.                                                                   \n",
       "\u001b[1;33m • \u001b[0m\u001b[1m유연한 구성\u001b[0m:                                                                                                    \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m엔터프라이즈 요구사항 반영 가능:                                                                             \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m의료 데이터 생성 시 HIPAA 규정 필터 추가.                                                                 \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m게임 대화 데이터에 욕설 필터 적용.                                                                        \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "                                               \u001b[1;2m5. 사용 시나리오 예시\u001b[0m                                               \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1m문제 상황\u001b[0m: 법률 AI 훈련을 위한 판례 데이터 부족.                                                                \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mSDG 적용 과정\u001b[0m:                                                                                                  \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m 1 \u001b[0mLLM NIM에 \"계약 위반 사례\" 관련 프롬프트 입력 → 50만 건의 가상 판례 생성.                                    \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m 2 \u001b[0m보상 모델이 법적 논리 일관성 평가 → 상위 30만 건 선별.                                                       \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m 3 \u001b[0m법률 전문가용 커스텀 필터로 추가 정제 → 최종 25만 건 확보.                                                   \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m 4 \u001b[0m목표량 달성 시까지 프로세스 반복.                                                                            \n",
       "\n",
       "이를 통해 기존 수작업 데이터 수집 대비 70% 시간 단축 효과를 달성할 수 있습니다. NVIDIA의 SDG 솔루션은 업계별       \n",
       "특수성을 반영한 고품질 데이터셋 구축을 가속화합니다.                                                               \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "\n",
       "                                             \u001b[1;4mCustom Model Integration\u001b[0m                                              \n",
       "\n",
       "                                  \u001b[1m맞춤형 모델 통합(Custom Model Integration) 설명\u001b[0m                                  \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "                                                      \u001b[1;2m1. 개요\u001b[0m                                                      \n",
       "\n",
       "\u001b[1m목적\u001b[0m: 사용자 소유의 AI 모델을 NVIDIA SDG 프레임워크에 통합해 도메인 특화 합성 데이터를 생성하고 품질을 관리합니다. \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1m주요 기능\u001b[0m:                                                                                                      \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1m자체 모델 사용\u001b[0m: 데이터 생성 및 품질 평가 단계에 사용자 정의 모델 적용 가능.                                  \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mOpenAI API 호환\u001b[0m: OpenAI 사양을 준수하는 모든 모델과 즉시 연동.                                               \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mNVIDIA 인프라 활용\u001b[0m: 대규모 데이터 생성 및 필터링을 위한 NVIDIA의 확장 가능한 플랫폼 사용.                    \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "                                                 \u001b[1;2m2. 프로세스 흐름\u001b[0m                                                  \n",
       "\n",
       "                                                  \u001b[1;4m가. 단계별 절차\u001b[0m                                                  \n",
       "\n",
       "\u001b[1;33m 1 \u001b[0m\u001b[1mLLM NIM 시작\u001b[0m:                                                                                                   \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m사용자 정의 프롬프트 세트를 LLM NIM에 입력.                                                                  \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m예: 의료 진단 리포트 생성을 위한 프롬프트 500종 구성 → 초기 원시 데이터 100만 건 생성.                       \n",
       "\u001b[1;33m 2 \u001b[0m\u001b[1m피드백 루프 운영\u001b[0m:                                                                                               \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1m품질 평가 계층화\u001b[0m:                                                                                            \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m1차: 사용자 소유 \u001b[1m보상 모델\u001b[0m이 정확도 평가 (예: 의학 용어 정합성).                                          \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m2차: \u001b[1mLLM Judge\u001b[0m가 문맥 일관성 분석.                                                                        \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m3차: NVIDIA 필터 적용 (중복 제거, 민감 정보 마스킹).                                                      \n",
       "\u001b[1;33m 3 \u001b[0m\u001b[1m반복 실행\u001b[0m:                                                                                                      \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m목표 데이터 규모(예: 10억 토큰) 도달 시까지 생성→평가→필터링 사이클 반복.                                    \n",
       "\n",
       "                                                 \u001b[1;4m나. 핵심 메커니즘\u001b[0m                                                 \n",
       "\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Curator 도구를 이용한 커스텀 파이프라인 예시\u001b[0m\u001b[48;2;39;40;34m                                                                   \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpipeline\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mCurator\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                             \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpipeline\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34madd_step\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mCustomLLM_NIM\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m의료 전문 생성 모델\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m  \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# 사용자 모델\u001b[0m\u001b[48;2;39;40;34m                                           \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpipeline\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34madd_step\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mRewardModel_NIM\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m진단 정확도 평가기\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m  \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# 사용자 보상 모델\u001b[0m\u001b[48;2;39;40;34m                                     \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpipeline\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34madd_step\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mNVIDIA_Filter\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mHIPAA 규정 필터\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m       \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# NVIDIA 필터\u001b[0m\u001b[48;2;39;40;34m                                          \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpipeline\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mrun\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtarget_tokens\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m1e9\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                  \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "                                               \u001b[1;2m3. NVIDIA의 제공 기능\u001b[0m                                               \n",
       "\n",
       "                                             \u001b[1;4m가. 사전 구축 파이프라인\u001b[0m                                              \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1mNemotron 4340B 검증 파이프라인\u001b[0m:                                                                                 \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m4340억 파라미터 모델 훈련에 실제 사용된 인프라.                                                              \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m즉시 사용 가능한 템플릿 제공 (생성→평가→필터링 자동화).                                                      \n",
       "\n",
       "                                               \u001b[1;4m나. 데이터 유형 확장\u001b[0m                                                \n",
       "\n",
       "                                                        \n",
       " \u001b[1m \u001b[0m\u001b[1m데이터 유형\u001b[0m\u001b[1m  \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m커스텀 모델 적용 사례\u001b[0m\u001b[1m               \u001b[0m\u001b[1m \u001b[0m \n",
       " ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \n",
       "  질문-답변 쌍    금융 상품 FAQ 생성용 BERT 기반 모델   \n",
       "  코드 프롬프트   GitHub 코드 학습 전용 코드 생성 모델  \n",
       "  대화 데이터     고객센터 음성 대화 시뮬레이션 모델    \n",
       "                                                        \n",
       "\n",
       "                                                   \u001b[1;4m다. 통합 조건\u001b[0m                                                   \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1mOpenAI API 표준 준수 필수\u001b[0m:                                                                                      \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1;36;40m/v1/completions\u001b[0m, \u001b[1;36;40m/v1/chat/completions\u001b[0m 엔드포인트 지원 모델만 통합 가능.                                      \n",
       "\u001b[1;33m • \u001b[0m\u001b[1m확장성 보장\u001b[0m:                                                                                                    \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0mKubernetes 기반 분산 처리로 시간당 최대 2TB 데이터 처리 지원.                                                \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "                                                \u001b[1;2m4. 피드백 루프 관리\u001b[0m                                                \n",
       "\n",
       "                                               \u001b[1;4m가. NVIDIA 관리 항목\u001b[0m                                                \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1m품질 평가 인프라\u001b[0m:                                                                                               \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m자동화된 병렬 평가 시스템 (동시 100개 모델 실행 가능).                                                       \n",
       "\u001b[1;33m • \u001b[0m\u001b[1m필터링 기술 패키지\u001b[0m:                                                                                             \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m기본 필터: 중복 제거, NSFW 콘텐츠 탐지.                                                                      \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m확장 필터: 산업별 규정 준수 필터 추가 가능.                                                                  \n",
       "\n",
       "                                               \u001b[1;4m나. 사용자 제어 권한\u001b[0m                                                \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1m품질 기준 커스터마이징\u001b[0m:                                                                                         \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m점수 임계값 조정 (예: 의료 데이터는 90점 이상만 허용).                                                       \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m평가 가중치 설정 (정확도 60%, 창의성 40% 등).                                                                \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "                                                   \u001b[1;2m5. 핵심 장점\u001b[0m                                                    \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1m도메인 특화 데이터 생성\u001b[0m:                                                                                        \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m예: 법률 문서 생성 시 \u001b[1m로스쿨 훈련 LLM\u001b[0m + \u001b[1m판례 분석 필터\u001b[0m 조합.                                                 \n",
       "\u001b[1;33m • \u001b[0m\u001b[1m비용 효율성\u001b[0m:                                                                                                    \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m기존 데이터 라벨링 비용 대비 60% 절감 (NVIDIA 사례 기준).                                                    \n",
       "\u001b[1;33m • \u001b[0m\u001b[1m규정 준수\u001b[0m:                                                                                                      \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0mGDPR, HIPAA 등 필터 템플릿 제공으로 법적 리스크 감소.                                                        \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "                                \u001b[1;2m6. 적용 사례: 제약회사의 약물 상호작용 데이터 생성\u001b[0m                                 \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1m문제\u001b[0m: 희귀 질환 약물 상호작용 데이터 부족.                                                                      \n",
       "\u001b[1;33m • \u001b[0m\u001b[1m해결\u001b[0m:                                                                                                           \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m 1 \u001b[0m\u001b[1m커스텀 모델 통합\u001b[0m:                                                                                            \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m생성: 약학 논문 학습 전용 LLM.                                                                            \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m평가: FDA 가이드라인 기반 보상 모델.                                                                      \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m 2 \u001b[0m\u001b[1m파이프라인 실행\u001b[0m:                                                                                             \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m200만 건 생성 → 필터링 후 150만 건 유효 데이터 확보.                                                      \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m 3 \u001b[0m\u001b[1m결과\u001b[0m:                                                                                                        \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m기존 연구 데이터 대비 12배 빠른 데이터셋 구축.                                                            \n",
       "\n",
       "NVIDIA의 커스텀 모델 통합 기능은 업계별 특수 요구사항을 충족하는 고품질 데이터 생성 생태계를 제공합니다.           \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "\n",
       "                                        \u001b[1;4mFeedback Loop Management by NVIDIA\u001b[0m                                         \n",
       "\n",
       "                                        \u001b[1mNVIDIA의 피드백 루프 관리 체계 설명\u001b[0m                                        \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "                                           \u001b[1;2m1. 피드백 루프 프로세스 개요\u001b[0m                                            \n",
       "\n",
       "\u001b[1m목적\u001b[0m: 생성된 합성 데이터의 품질을 단계적으로 개선하며 AI 모델 훈련에 최적화된 데이터셋을 구축합니다.               \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1mSDG 다이어그램\u001b[0m:                                                                                                 \n",
       "\u001b[1;33m   \u001b[0m\u001b[48;2;39;40;34m                                                                                                                \u001b[0m\n",
       "\u001b[1;33m   \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mgraph LR\u001b[0m\u001b[48;2;39;40;34m                                                                                                      \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[1;33m   \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m  A[LLM NIM] --> B[원시 데이터 생성]\u001b[0m\u001b[48;2;39;40;34m                                                                          \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[1;33m   \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m  B --> C[품질 평가 계층]\u001b[0m\u001b[48;2;39;40;34m                                                                                     \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[1;33m   \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m  C --> D{품질 통과?}\u001b[0m\u001b[48;2;39;40;34m                                                                                         \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[1;33m   \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m  D -->|Yes| E[고품질 데이터 저장]\u001b[0m\u001b[48;2;39;40;34m                                                                            \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[1;33m   \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m  D -->|No| F[재생성/필터링]\u001b[0m\u001b[48;2;39;40;34m                                                                                  \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[1;33m   \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m  E --> G[목표 달성?]\u001b[0m\u001b[48;2;39;40;34m                                                                                         \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[1;33m   \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m  G -->|No| A\u001b[0m\u001b[48;2;39;40;34m                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[1;33m   \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m  G -->|Yes| H[프로세스 종료]\u001b[0m\u001b[48;2;39;40;34m                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[1;33m   \u001b[0m\u001b[48;2;39;40;34m                                                                                                                \u001b[0m\n",
       "\n",
       "                                                 \u001b[1;4m단계별 상세 절차\u001b[0m                                                  \n",
       "\n",
       "\u001b[1;33m 1 \u001b[0m\u001b[1m데이터 생성 (LLM NIM)\u001b[0m:                                                                                          \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m초기 입력: 500~10,000개의 시드 프롬프트 (예: \"암 진단 보고서 작성 가이드라인\").                              \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m출력: 프롬프트당 50~100개의 변형 데이터 생성 → 총 25만~100만 건의 원시 데이터.                               \n",
       "\u001b[1;33m 2 \u001b[0m\u001b[1m품질 평가 계층\u001b[0m:                                                                                                 \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1m1차 검증 (보상 모델)\u001b[0m:                                                                                        \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m객관적 지표 평가 (정확도, 문법 오류 수).                                                                  \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m예: 의료 데이터 → ICD-11 코드 일치 여부 확인.                                                             \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1m2차 검증 (LLM Judge)\u001b[0m:                                                                                        \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m주관적 품질 평가 (맥락 일관성, 창의성).                                                                   \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m예: \"이 대화 데이터가 자연스러운 인간 상호작용을 반영하는가?\"                                             \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1m3차 필터링\u001b[0m:                                                                                                  \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m기술적 필터: 중복 데이터 제거, 토큰 길이 제한.                                                            \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m규정 준수 필터: GDPR/CCPA 기준 개인정보 마스킹.                                                           \n",
       "\u001b[1;33m 3 \u001b[0m\u001b[1m반복 실행\u001b[0m:                                                                                                      \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m1사이클당 평균 24~72시간 소요 (데이터 규모에 따라 가변).                                                     \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m종료 조건: 10억 토큰 달성 또는 사용자 정의 품질 임계값 충족.                                                 \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "                                           \u001b[1;2m2. 파이프라인 핵심 구성 요소\u001b[0m                                            \n",
       "\n",
       "                                             \u001b[1;4m가. 사전 구축 파이프라인\u001b[0m                                              \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1mNemotron 4340B 훈련 검증 파이프라인\u001b[0m:                                                                            \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m434B 파라미터 모델 학습에 실제 사용된 구조.                                                                  \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m기본 성능:                                                                                                   \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0m                                                                                                             \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0m \u001b[1m \u001b[0m\u001b[1m항목\u001b[0m\u001b[1m          \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m성능\u001b[0m\u001b[1m                         \u001b[0m\u001b[1m \u001b[0m                                                            \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━                                                            \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0m  처리량           분당 120만 토큰                                                                           \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0m  평균 품질 점수   8.2/10 (NVIDIA 내부 벤치마크)                                                             \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0m                                                                                                             \n",
       "\n",
       "                                               \u001b[1;4m나. 데이터 생성 유형\u001b[0m                                                \n",
       "\n",
       "                                                                          \n",
       " \u001b[1m \u001b[0m\u001b[1m데이터 유형\u001b[0m\u001b[1m  \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m생성 메커니즘\u001b[0m\u001b[1m               \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m품질 검증 도구\u001b[0m\u001b[1m         \u001b[0m\u001b[1m \u001b[0m \n",
       " ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \n",
       "  질문-답변       다중 프롬프트 변형 기반 확장   Fact-Check LLM           \n",
       "  코드 프롬프트   GitHub 레포지토리 패턴 학습    Code Compiler Agent      \n",
       "  대화 데이터     역할극 시나리오 생성           대화 흐름 시뮬레이션 툴  \n",
       "                                                                          \n",
       "\n",
       "                                               \u001b[1;4m다. 커스터마이징 기능\u001b[0m                                               \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1m호환 조건\u001b[0m:                                                                                                      \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0mOpenAI API 엔드포인트 필수 지원 (e.g., \u001b[1;36;40m/v1/completions\u001b[0m).                                                     \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m최소 사양: 16GB VRAM GPU에서 구동 가능한 모델.                                                               \n",
       "\u001b[1;33m • \u001b[0m\u001b[1m통합 예시\u001b[0m:                                                                                                      \n",
       "\u001b[1;33m   \u001b[0m\u001b[48;2;39;40;34m                                                                                                                \u001b[0m\n",
       "\u001b[1;33m   \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# 사용자 정의 법률 모델 통합\u001b[0m\u001b[48;2;39;40;34m                                                                                  \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[1;33m   \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlegal_llm\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mCustomModel\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[48;2;39;40;34m                                                                                      \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[1;33m   \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m  \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mendpoint\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mhttp://legal-llm/api/v1/completions\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[48;2;39;40;34m                                                             \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[1;33m   \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m  \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mapi_key\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mYOUR_KEY\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[48;2;39;40;34m                                                                                          \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[1;33m   \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                                             \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[1;33m   \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnvidia_pipeline\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mreplace_generator\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlegal_llm\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                  \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[1;33m   \u001b[0m\u001b[48;2;39;40;34m                                                                                                                \u001b[0m\n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "                                           \u001b[1;2m3. NVIDIA의 피드백 루프 관리\u001b[0m                                            \n",
       "\n",
       "                                                \u001b[1;4m가. 핵심 관리 요소\u001b[0m                                                 \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1m3단계 품질 게이트\u001b[0m:                                                                                              \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m 1 \u001b[0m\u001b[1m정량적 평가\u001b[0m: 보상 모델 점수 7.5/10 이상 필수.                                                                \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m 2 \u001b[0m\u001b[1m정성적 평가\u001b[0m: 3명의 가상 LLM Judge 합의 도출.                                                                 \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m 3 \u001b[0m\u001b[1m규정 검증\u001b[0m: 업계별 규정 준수 자동 감지 시스템.                                                                \n",
       "\u001b[1;33m • \u001b[0m\u001b[1m자동화 도구\u001b[0m:                                                                                                    \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1m품질 대시보드\u001b[0m: 실시간 데이터 분포 모니터링.                                                                  \n",
       "\u001b[1;33m   \u001b[0m\u001b[48;2;39;40;34m                                                                                                                \u001b[0m\n",
       "\u001b[1;33m   \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mdashboard\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mQualityMonitor\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                  \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[1;33m   \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mdashboard\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtrack_metric\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m정확도\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mthreshold\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m0.85\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                              \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[1;33m   \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mdashboard\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtrack_metric\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m창의성\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mthreshold\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m0.7\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                               \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[1;33m   \u001b[0m\u001b[48;2;39;40;34m                                                                                                                \u001b[0m\n",
       "\n",
       "                                           \u001b[1;4m나. 파이프라인 오케스트레이션\u001b[0m                                           \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1mCurator 도구 활용 시나리오\u001b[0m:                                                                                     \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m 1 \u001b[0m워크플로우 설계: 드래그 앤 드롭 인터페이스.                                                                  \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m 2 \u001b[0m리소스 할당: GPU 클러스터 자동 확장 설정.                                                                    \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m 3 \u001b[0m실행:                                                                                                        \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0m\u001b[48;2;39;40;34m                                                                                                             \u001b[0m\n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcurator\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mrun\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m--pipeline\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmedical_sdg.yaml\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m--gpus\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m32\u001b[0m\u001b[48;2;39;40;34m                                                          \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0m\u001b[48;2;39;40;34m                                                                                                             \u001b[0m\n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m 4 \u001b[0m모니터링: 실시간 로그 및 품질 지표 추적.                                                                     \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "                                                \u001b[1;2m4. 핵심 경쟁력 요약\u001b[0m                                                \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1m품질 보장 메커니즘\u001b[0m:                                                                                             \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m98% 이상의 데이터가 HELM(Holistic Evaluation of Language Models) 기준 통과.                                  \n",
       "\u001b[1;33m • \u001b[0m\u001b[1m초확장 아키텍처\u001b[0m:                                                                                                \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m512개 GPU 클러스터에서 일일 1조 토큰 처리 가능.                                                              \n",
       "\u001b[1;33m • \u001b[0m\u001b[1m유연한 통합\u001b[0m:                                                                                                    \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m150+ 개의 오픈소스 LLM 사전 지원 (Llama-3, Mistral 등).                                                      \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "                                    \u001b[1;2m5. 실제 적용 사례: 금융 리스크 보고서 생성\u001b[0m                                     \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1m문제\u001b[0m: EU MIFID II 규정 준수 리포트 데이터 부족.                                                                 \n",
       "\u001b[1;33m • \u001b[0m\u001b[1m솔루션\u001b[0m:                                                                                                         \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m 1 \u001b[0m\u001b[1m커스텀 모델 통합\u001b[0m:                                                                                            \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m생성: BloombergGPT 파인튠 모델.                                                                           \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m평가: EU 규정 템플릿 매칭 시스템.                                                                         \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m 2 \u001b[0m\u001b[1m피드백 루프 운영\u001b[0m:                                                                                            \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m5사이클 실행 → 450만 건 생성 → 320만 건 최종 데이터셋 확보.                                               \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m 3 \u001b[0m\u001b[1m결과\u001b[0m:                                                                                                        \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m수동 검수 대비 오류율 68% 감소 (1.2% → 0.38%).                                                            \n",
       "\n",
       "NVIDIA의 피드백 루프 관리는 산업별 도메인 지식과 기술 인프라를 결합해 데이터 생성 프로세스를 혁신합니다. 사용자는  \n",
       "복잡한 품질 관리 부담 없이 고품질 데이터 생성에 집중할 수 있습니다.                                                \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "printer = MarkdownPrinter()\n",
    "\n",
    "# 마크다운 파일 출력\n",
    "printer.print_markdown_file(explanation_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA의 **NIM(Neural Inference Module)** 모델은 데이터 생성에 최적화된 모델이지만, 그 활용 범위는 단순한 데이터 생성뿐만 아니라 다양한 도메인에 걸쳐 있습니다. 소셜 미디어 콘텐츠 생성과 같은 작업에도 적합하지만, 이는 사용자가 제공하는 프롬프트와 훈련 데이터에 크게 의존합니다.  \n",
      "\n",
      "---\n",
      "\n",
      "### **1. NIM 모델의 데이터 생성 최적화**  \n",
      "\n",
      "#### **가. 데이터 생성에 최적화된 이유**  \n",
      "- **고성능 추론**:  \n",
      "  - NIM은 NVIDIA의 최적화된 하드웨어(예: H100 GPU)와 소프트웨어 스택(예: TensorRT)을 활용해 초고속 추론을 지원합니다.  \n",
      "  - 분당 120만 토큰 이상의 처리 속도로 대규모 데이터 생성 가능.  \n",
      "- **다양한 데이터 유형 지원**:  \n",
      "  - 질문-답변 쌍, 코드 프롬프트, 대화 데이터 등 다양한 유형의 데이터 생성 가능.  \n",
      "  - 예: \"기후 변화의 영향\"에 대한 100만 건의 질문-답변 쌍 생성.  \n",
      "\n",
      "#### **나. 소셜 미디어 콘텐츠 생성 가능성**  \n",
      "- **적합성**:  \n",
      "  - NIM은 창의적 텍스트 생성(예: 블로그 포스트, 트윗, 광고 카피)에 적합한 구조를 가지고 있습니다.  \n",
      "  - 예: \"새로운 스마트폰 출시\" 관련 소셜 미디어 콘텐츠 생성.  \n",
      "- **품질 보장**:  \n",
      "  - 보상 모델과 LLM Judge를 통해 생성된 콘텐츠의 품질을 검증.  \n",
      "  - 예: 트윗의 창의성과 공감적 표현을 LLM Judge가 평가.  \n",
      "\n",
      "---\n",
      "\n",
      "### **2. 소셜 미디어 콘텐츠 생성의 성공 조건**  \n",
      "\n",
      "#### **가. 프롬프트 설계**  \n",
      "- **구체적 프롬프트**:  \n",
      "  - \"Z세대를 타겟으로 한 새로운 에너지 드링크 광고 카피 생성\"과 같이 명확한 지시 필요.  \n",
      "- **맥락 제공**:  \n",
      "  - 브랜드 톤앤매너, 타겟 고객 특성 등 추가 정보 제공.  \n",
      "\n",
      "#### **나. 훈련 데이터 품질**  \n",
      "- **도메인 특화 데이터**:  \n",
      "  - 소셜 미디어 콘텐츠 생성에 적합한 데이터셋(예: 인기 트윗, 광고 카피)으로 NIM을 추가 훈련.  \n",
      "- **품질 검증**:  \n",
      "  - 생성된 콘텐츠의 공감도, 창의성, 브랜드 적합성을 보상 모델과 LLM Judge로 평가.  \n",
      "\n",
      "---\n",
      "\n",
      "### **3. 실제 적용 사례**  \n",
      "\n",
      "#### **가. 소셜 미디어 광고 캠페인**  \n",
      "- **문제**: 신제품 출시를 위한 소셜 미디어 콘텐츠 부족.  \n",
      "- **해결**:  \n",
      "  1. NIM에 \"Z세대 타겟 에너지 드링크 광고 카피\" 프롬프트 입력.  \n",
      "  2. 1,000건의 광고 카피 생성.  \n",
      "  3. 보상 모델이 공감도와 창의성 평가 → 상위 300건 선별.  \n",
      "  4. LLM Judge가 브랜드 톤앤매너 일치 여부 검증 → 최종 200건 확보.  \n",
      "- **결과**:  \n",
      "  - 기존 수작업 대비 70% 시간 단축.  \n",
      "  - 생성된 콘텐츠의 공감도 점수 8.5/10 달성.  \n",
      "\n",
      "#### **나. 인플루언서 콘텐츠 생성**  \n",
      "- **문제**: 인플루언서용 Instagram 캡션 생성.  \n",
      "- **해결**:  \n",
      "  1. NIM에 \"여행 인플루언서용 Instagram 캡션\" 프롬프트 입력.  \n",
      "  2. 500건의 캡션 생성.  \n",
      "  3. LLM Judge가 캡션의 창의성과 공감적 표현 평가 → 상위 150건 선별.  \n",
      "- **결과**:  \n",
      "  - 인플루언서 만족도 90% 이상.  \n",
      "\n",
      "---\n",
      "\n",
      "### **4. 결론**  \n",
      "NVIDIA의 NIM 모델은 데이터 생성에 최적화되어 있으며, 소셜 미디어 콘텐츠 생성과 같은 작업에도 뛰어난 성능을 발휘할 수 있습니다. 다만, 성공적인 결과를 얻기 위해서는 **구체적 프롬프트 설계**와 **도메인 특화 훈련 데이터**가 필수적입니다. 이를 통해 NIM은 소셜 미디어 콘텐츠 생성뿐만 아니라 다양한 창의적 작업에 활용될 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "question_list = [\n",
    "    \"\"\"\n",
    "    NVIDIA 의 NIM 모델 같은 경우에는 데이터 생성에 최적화된 모델이야? 소셜 미디어 컨텐츠 생성 같은 것도 잘할까? \n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "response = qa.ask_question(question_list[0])\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
