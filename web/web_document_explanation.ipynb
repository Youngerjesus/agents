{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "api_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "model_name = \"deepseek-chat\"\n",
    "\n",
    "def get_completion(messages, model_name=model_name):\n",
    "    client = OpenAI(api_key=api_key, base_url=\"https://api.deepseek.com\")\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=messages,\n",
    "        temperature=0.3, \n",
    "        max_tokens=8192, \n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def get_completion_json_output(messages, model_name=model_name):\n",
    "    client = OpenAI(api_key=api_key, base_url=\"https://api.deepseek.com\")\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=messages,\n",
    "        temperature=0.3, \n",
    "        max_tokens=8192, \n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "    return json.loads(response.choices[0].message.content)\n",
    "\n",
    "def get_completion_openai(messages):\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=messages,\n",
    "        temperature=0.3, \n",
    "        max_tokens=8192, \n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import os \n",
    "\n",
    "class MarkdownParser:\n",
    "    def __init__(self, max_header_level: int = 2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            max_header_level: ë³„ë„ ì„¹ì…˜ìœ¼ë¡œ ë¶„ë¦¬í•  ìµœëŒ€ í—¤ë” ë ˆë²¨ (ê¸°ë³¸ê°’: 2, ## ê¹Œì§€ë§Œ ë¶„ë¦¬)\n",
    "        \"\"\"\n",
    "        self.max_header_level = max_header_level\n",
    "    \n",
    "\n",
    "    def parse_markdown(self, markdown_text: str) -> Dict[str, str]:\n",
    "        \"\"\"ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ë¥¼ íŒŒì‹±í•˜ì—¬ ì„¹ì…˜ë³„ë¡œ ë¶„ë¦¬\"\"\"\n",
    "        sections = {}\n",
    "        current_content = []\n",
    "        current_title = None\n",
    "        \n",
    "        for line in markdown_text.split('\\n'):\n",
    "            if line.startswith('#') and ' ' in line:\n",
    "                # í—¤ë” ë ˆë²¨ í™•ì¸\n",
    "                level = len(line) - len(line.lstrip('#'))\n",
    "                \n",
    "                # max_header_level ì´í•˜ì˜ í—¤ë”ë§Œ ìƒˆë¡œìš´ ì„¹ì…˜ìœ¼ë¡œ ì²˜ë¦¬\n",
    "                if level <= self.max_header_level:\n",
    "                    # ì´ì „ ì„¹ì…˜ ì €ì¥\n",
    "                    if current_title and current_content:\n",
    "                        sections[current_title] = '\\n'.join(current_content).strip()\n",
    "                    \n",
    "                    # ìƒˆë¡œìš´ ì„¹ì…˜ ì‹œì‘\n",
    "                    current_title = line.lstrip('#').strip()\n",
    "                    current_content = []\n",
    "                else:\n",
    "                    # ìƒìœ„ ë ˆë²¨ í—¤ë”ëŠ” ë‚´ìš©ì— í¬í•¨\n",
    "                    current_content.append(line)\n",
    "            else:\n",
    "                if current_title is None:\n",
    "                    continue  # ì²« í—¤ë” ì´ì „ì˜ ë‚´ìš©ì€ ë¬´ì‹œ\n",
    "                current_content.append(line)\n",
    "        \n",
    "        # ë§ˆì§€ë§‰ ì„¹ì…˜ ì €ì¥\n",
    "        if current_title and current_content:\n",
    "            sections[current_title] = '\\n'.join(current_content).strip()\n",
    "        \n",
    "        return sections\n",
    "    \n",
    "    def save_markdown(self, file_name: str, markdown_text: str, output_dir: str = \"markdown_input\") -> None:\n",
    "        \"\"\"\n",
    "        ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ë¥¼ íŒŒì¼ë¡œ ì €ì¥\n",
    "        \"\"\"\n",
    "        with open(os.path.join(output_dir, file_name), 'w', encoding='utf-8') as f:\n",
    "            f.write(markdown_text)\n",
    "        print(f\"Markdown saved to: {os.path.join(output_dir, file_name)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Curating Custom Datasets for LLM Training with NVIDIA NeMo Curator\n",
      "\n",
      "Content: Data curation is the first, and arguably the most important, step in the pretraining and continuous training of [large language models (LLMs)](https://www.nvidia.com/en-us/glossary/large-language-models/) and small language models (SLMs). NVIDIA recently announced the open-source release of [NVIDIA NeMo Curator](https://developer.nvidia.com/blog/scale-and-curate-high-quality-datasets-for-llm-training-with-nemo-curator/), a data curation framework that prepares large-scale, high-quality datasets for pretraining generative AI models.Â \n",
      "\n",
      "NeMo Curator, which is part of [NVIDIA NeMo](https://www.nvidia.com/en-us/ai-data-science/products/nemo/), offers workflows to download and curate data from various public sources out of the box such as Common Crawl, Wikipedia, and arXiv. It also provides flexibility for developers to customize data curation pipelines to address their unique requirements and create custom datasets.Â \n",
      "\n",
      "This post walks you through creating a custom data curation pipeline using NeMo Curator. Doing so enables you to:\n",
      "\n",
      "+   Tailor data curation and customize the pipeline to fit the specific needs of your generative AI project.\n",
      "+   Ensure data quality by applying rigorous filters and deduplication to train your model with the best possible dataset.\n",
      "+   Protect privacy by identifying and removing personally identifiable information (PII) and adhere to data protection regulations.\n",
      "+   Streamline the development by automating the curation process, saving time and resources to allow you to focus on solving your business-specific problems.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Title: Overview[](#overview)\n",
      "\n",
      "Content: This tutorial focuses on creating a simple data curation pipeline that can download, process, and filter the TinyStories dataset. TinyStories is a dataset of around 2.2 million short stories generated by GPT-3.5 and GPT-4, featuring English words that are understood by 3- to 4-year olds. It is [publicly available on Hugging Face](https://huggingface.co/datasets/roneneldan/TinyStories). To learn more about the dataset, see [TinyStories: How Small Can Language Models Be and Still Speak Coherent English?](https://arxiv.org/abs/2305.07759)\n",
      "\n",
      "The small size of this dataset makes it ideal for creating and validating data curation pipelines on a local machine. The dataset is split into training and validation files. This tutorial primarily uses the validation file, which contains about 22,000 records.\n",
      "\n",
      "Defining the data curation pipeline involves the following high-level steps:\n",
      "\n",
      "1.  Defining custom document builders that can:\n",
      "    +   Download the dataset from the web and convert to the JSONL format.\n",
      "    +   Iterate through the dataset and extract each document.\n",
      "2.  Define custom modifiers to clean and unify the text data.\n",
      "3.  Filter the dataset using predefined, as well as user-defined heuristics.\n",
      "4.  Deduplicate the dataset and remove identical records.\n",
      "5.  Redact all personally identifiable information (PII) from the dataset.\n",
      "6.  Output the results into the JSONL format.\n",
      "\n",
      "The execution of this curation pipeline should take less than 5 minutes on consumer-grade hardware, and the curated dataset should have about 21,500 records after curation. To access the complete code for this tutorial, visit [NVIDIA/NeMo-Curator](https://github.com/NVIDIA/NeMo-Curator/tree/fc167a6edffd38a55c333742972a5a25b901cb26/tutorials/tinystories) on GitHub.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Title: Prerequisite[](#prerequisite)\n",
      "\n",
      "Content: Before starting, the NeMo Curator framework must be installed. Follow the instructions in the projectâ€™s [NeMo Curator GitHub README file](https://github.com/NVIDIA/NeMo-Curator/blob/main/README.md) to install the framework. After that, run the following commands from the terminal to verify the installation. Also install additional dependencies needed for following along.\n",
      "\n",
      "`$ python -c` `\"import nemo_curator; print(nemo_curator);\"`\n",
      "\n",
      "`$ pip3` `install` `requests`\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Title: Defining custom document builders[](#defining_custom_document_builders)\n",
      "\n",
      "Content: To support working with arbitrary datasets, NeMo Curator provides a set of [document builders](https://github.com/NVIDIA/NeMo-Curator/blob/fc167a6edffd38a55c333742972a5a25b901cb26/nemo_curator/download/doc_builder.py) that abstract away the representation of the underlying dataset, including:\n",
      "\n",
      "+   `DocumentDownloader`: an abstract class for downloading remote data to disk.\n",
      "+   `DocumentIterator`: an abstract class for reading dataset raw records from the disk.\n",
      "+   `DocumentExtractor`: an abstract class for extracting text records, as well as any relevant metadata from the records on the disk.\n",
      "\n",
      "Several implementations for these to work with datasets such as CommonCrawl, Wikipedia, and arXiv are available on the [NVIDIA/NeMo-Curator](https://github.com/NVIDIA/NeMo-Curator/tree/9f42c49923e314572da53dc7c0384052420ca657/nemo_curator/download) GitHub repo. The following sections show how to implement each of these abstract classes to customize the work with the TinyStories dataset.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Title: Downloading the TinyStories dataset[](#downloading_the_tinystories_dataset)\n",
      "\n",
      "Content: First, implement the `DocumentDownloader` class, which takes the URL of the datasetâ€™s validation split and downloads it using the `requests` library.Â \n",
      "\n",
      "`import` `requests`\n",
      "\n",
      "`from` `nemo_curator.download.doc_builder` `import` `DocumentDownloader`\n",
      "\n",
      "`class` `TinyStoriesDownloader(DocumentDownloader):`\n",
      "\n",
      "Â Â Â Â `def` `__init__(``self``, download_dir:` `str``):`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `super``().__init__()`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `if` `not` `os.path.isdir(download_dir):`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â `os.makedirs(download_dir)`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `self``._download_dir` `=` `download_dir`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `print``(``\"Download directory: \"``,` `self``._download_dir)`\n",
      "\n",
      "Â Â Â Â `def` `download(``self``, url:` `str``)` `-``>` `str``:`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `filename` `=` `os.path.basename(url)`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `output_file` `=` `os.path.join(``self``._download_dir, filename)`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `if` `os.path.exists(output_file):`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â `print``(f``\"File '{output_file}' already exists, skipping download.\"``)`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â `return` `output_file`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `print``(f``\"Downloading TinyStories dataset from '{url}'...\"``)`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `response` `=` `requests.get(url)`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `with` `open``(output_file,` `\"wb\"``) as` `file``:`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â `file``.write(response.content)`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `return` `output_file`\n",
      "\n",
      "Next, download the actual dataset using the following code:\n",
      "\n",
      "`# Download the TinyStories dataset.`\n",
      "\n",
      "`downloader` `=` `TinyStoriesDownloader(``\"/path/to/download/\"``)`\n",
      "\n",
      "`tinystories_fp` `=` `downloader.download(TINY_STORIES_URL)`\n",
      "\n",
      "`write_jsonl(tinystories_fp, jsonl_dir)`\n",
      "\n",
      "The dataset will download as a plain text file. To parse this dataset, implement the `DocumentIterator` and `DocumentExtractor` classes. This will enable you to convert it to the JSONL format (one of the formats that NeMo Curator supports).Â \n",
      "\n",
      "### Iterating and extracting text from the dataset[](#iterating_and_extracting_text_from_the_dataset)\n",
      "\n",
      "In the downloaded file, each record (or story) spans several lines, and records are separated by the `<|endoftext|>` token. The `DocumentIterator` class defines an `iterate` function that takes the path to the file that is to be iterated and yields each record for that file, in the form of the raw text from the record and (optionally) any relevant metadata for that record. Although adding metadata to each record is not mandatory, some data processing algorithms (such as deduplication) rely on such data to uniquely identify each document and correctly perform their intended function.\n",
      "\n",
      "Next, implement the iterator for the TinyStories dataset. Given that each story can span several lines, define the iterator function such that it would keep reading (and storing) each line in the file, until it reaches the separator token.Â \n",
      "\n",
      "Once a separator is reached, concatenate all the lines seen so far, tack on some metadata to the record, and yield the result. To ensure records are uniquely identifiable, use the datasetâ€™s filename, as well as an internal counter to create the unique `id` and (optionally) `filename` metadata included with each record:\n",
      "\n",
      "`from` `nemo_curator.download.doc_builder` `import` `DocumentIterator`\n",
      "\n",
      "`class` `TinyStoriesIterator(DocumentIterator):`\n",
      "\n",
      "Â Â Â Â `SEPARATOR_TOKEN` `=` `\"<|endoftext|>\"`\n",
      "\n",
      "Â Â Â Â `def` `__init__(``self``):`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `super``().__init__()`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `self``._counter` `=` `-``1`\n",
      "\n",
      "Â Â Â Â `def` `iterate(``self``, file_path):`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `self``._counter` `=` `-``1`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `file_name` `=` `os.path.basename(file_path)`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `with` `open``(file_path,` `\"r\"``) as` `file``:`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â `example` `=` `[]`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â `def` `split_meta(example):`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â `if` `example:`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â `self``._counter` `+``=` `1`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â `content` `=` `\" \"``.join(example)`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â `meta` `=` `{`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â `\"filename\"``: file_name,`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â `\"id\"``: f``\"{file_name}-{self._counter}\"``,`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â `}`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â `return` `meta, content`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â `for` `line` `in` `file``:`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â `if` `line.strip()` `=``=` `TinyStoriesIterator.SEPARATOR_TOKEN:`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â `if` `example:`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â `yield` `split_meta(example)`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â `example` `=` `[]`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â `else``:`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â `example.append(line.strip())`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â `if` `example:`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â `yield` `split_meta(example)`\n",
      "\n",
      "The last remaining document builder to implement is the `DocumentExtractor` class, which simply returns the text for each record. Note that you may optionally associate some metadata for the extracted text, but the usage of this metadata is beyond the scope of this tutorial.\n",
      "\n",
      "`from` `nemo_curator.download.doc_builder` `import` `DocumentExtractor`\n",
      "\n",
      "`class` `TinyStoriesExtractor(DocumentExtractor):`\n",
      "\n",
      "Â Â Â Â `def` `extract(``self``, content:` `str``)` `-``>` `Tuple``[``Set``,` `str``]:`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `# No metadata for the text, just the content.`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `return` `{}, content`\n",
      "\n",
      "### Writing the dataset to the JSONL format[](#writing_the_dataset_to_the_jsonl_format)\n",
      "\n",
      "NeMo Curator provides helpers that can load datasets from the disk in JSONL, Parquet, or Pickle formats. Given the popularity of the JSONL format, this section demonstrates the conversion of the raw text dataset to this format using the iterator and extractor classes previously implemented.\n",
      "\n",
      "To convert the dataset to JSONL, simply point the `TinyStoriesIterator` instance to the downloaded plain text file, iterate through each record, and extract entries using the `TinyStoriesExtractor` instance. Create a JSON object from each record (story) and write it to a single line in an output file. This procedure is straightforward:\n",
      "\n",
      "`import` `os`\n",
      "\n",
      "`import` `json`\n",
      "\n",
      "`def` `write_jsonl(input_filename:` `str``, output_dir:` `str``, dump_every_n:` `int` `=` `10000``):`\n",
      "\n",
      "Â Â Â Â `basename` `=` `os.path.basename(input_filename)`\n",
      "\n",
      "Â Â Â Â `iterator` `=` `TinyStoriesIterator()`\n",
      "\n",
      "Â Â Â Â `extractor` `=` `TinyStoriesExtractor()`\n",
      "\n",
      "Â Â Â Â `to_dump` `=` `[]`\n",
      "\n",
      "Â Â Â Â `dump_ctr` `=` `0`\n",
      "\n",
      "Â Â Â Â `def` `dump_to_file(to_dump, dump_ctr):`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `\"\"\"Helper function to facilitate dumping to file.\"\"\"`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `output_filename` `=` `f``\"{basename}-{dump_ctr}.jsonl\"`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `with` `open``(os.path.join(output_dir, output_filename),` `\"w\"``) as output_file:`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â `output_file.writelines(to_dump)`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `# Empty out the list and increment the counter.`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `return` `[], dump_ctr` `+` `1`\n",
      "\n",
      "Â Â Â Â `for` `item` `in` `iterator.iterate(input_filename):`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `record_meta, content` `=` `item`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `extracted` `=` `extractor.extract(content)`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `if` `extracted` `is` `None``:`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â `continue`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `text_meta, text` `=` `extracted`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `if` `text` `is` `None``:`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â `continue`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `line` `=` `{`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â `\"text\"``: text,`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â `*``*``text_meta,`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â `*``*``record_meta,`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `}`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `json_out` `=` `json.dumps(line, ensure_ascii``=``False``)`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `to_dump.append(json_out` `+` `\"\\n\"``)`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `# Should we dump what we have so far?`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `if` `len``(to_dump)` `=``=` `dump_every_n:`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â `to_dump, dump_ctr` `=` `dump_to_file(to_dump, dump_ctr)`\n",
      "\n",
      "Â Â Â Â `# Dump the remaining records.`\n",
      "\n",
      "Â Â Â Â `if` `to_dump:`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `dump_to_file(to_dump, dump_ctr)`\n",
      "\n",
      "Note that by default, this function creates one JSONL file for every 10,000 records. While entirely optional, this is to ensure that each output file remains small enough for easy manual inspection using a text editor, without consuming too much memory.\n",
      "\n",
      "Also note that the content of each story is written into the `text` field of each JSON object. Many data curation operations throughout NeMo Curator need to know which field inside each record contains the text data for that record. If not explicitly specified, these operations assume the existence of a `text` field in the dataset. As such, it is often good practice to always populate the `text` field for each record with the text data of interest.\n",
      "\n",
      "### Loading the dataset using the document builders[](#loading_the_dataset_using_the_document_builders)\n",
      "\n",
      "In NeMo Curator, datasets are represented as objects of type `DocumentDataset`. This provides helpers to load the datasets from disk in various formats. Having created the dataset in the JSONL format, you can use the following code to load it and start working with it:\n",
      "\n",
      "`from nemo_curator.datasets import DocumentDataset`\n",
      "\n",
      "``# define `files` to be a list of all the JSONL files to load``\n",
      "\n",
      "`dataset = DocumentDataset.read_json(files, add_filename=True)`\n",
      "\n",
      "You now have everything needed to define a custom dataset curation pipeline and prepare your data for training (or validation) use cases.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Title: Text cleaning and unification[](#text_cleaning_and_unification)\n",
      "\n",
      "Content: A fundamental operation in data curation pipelines involving text data is text unification and cleaning, as text scraped from online sources may contain inconsistencies or unicode issues. To modify documents, NeMo Curator provides a `DocumentModifier` interface, which defines how a given text from each document should be modified. The actual modification is done through theÂ `Modify` helper, which takes a `DocumentDataset` object along with a `DocumentModifier` object and applies the modifier to the dataset.\n",
      "\n",
      "The TinyStories dataset has inconsistent quotation marks, where some quotation marks are curly, while others are straight. Such inconsistencies (poor quality tokens, for example) may cause problems for models that are trained on this data.Â \n",
      "\n",
      "To resolve these, create a `DocumentModifier` that unifies all single- and double-quotation marks in the documents by replacing all the curly quotation marks with their straight variants:\n",
      "\n",
      "`from` `nemo_curator.modifiers` `import` `DocumentModifier`\n",
      "\n",
      "`class` `QuotationUnifier(DocumentModifier):`\n",
      "\n",
      "Â Â Â Â `def` `modify_document(``self``, text:` `str``)` `-``>` `str``:`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `text` `=` `text.replace(``\"â€˜\"``,` `\"'\").replace(\"â€™\", \"'\"``)`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `text` `=` `text.replace(``\"â€œ\"``,` `'\"').replace(\"â€\", '\"'``)`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `return` `text`\n",
      "\n",
      "NeMo Curator provides various `DocumentModifier` implementations out of the box. One such modifier is `UnicodeReformatter`, which uses [ftfy](https://github.com/rspeer/python-ftfy) to resolve all unicode issues in the dataset. Next, chain these modifiers together and clean the downloaded dataset. The chaining operation is done through the `Sequential` class, which takes a list of operations that are to be sequentially performed and applies them to a given `DocumentDataset` instance:\n",
      "\n",
      "`from` `nemo_curator` `import` `Sequential`\n",
      "\n",
      "`from` `nemo_curator.modules.modify` `import` `Modify`\n",
      "\n",
      "`from` `nemo_curator.modifiers.unicode_reformatter` `import` `UnicodeReformatter`\n",
      "\n",
      "`def` `clean_and_unify(dataset: DocumentDataset)` `-``> DocumentDataset:`\n",
      "\n",
      "Â Â Â Â `cleaners` `=` `Sequential(`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `[`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â `# Unify all the quotation marks`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â `Modify(QuotationUnifier()),`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â `# Unify all unicode`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â `Modify(UnicodeReformatter()),`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `]`\n",
      "\n",
      "Â Â Â Â `)`\n",
      "\n",
      "Â Â Â Â `return` `cleaners(dataset)`\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Title: Dataset filtering[](#dataset_filtering)\n",
      "\n",
      "Content: Another important step in the dataset curation process is data filtering, where some documents that do not fit certain criteria are discarded. For instance, you might want to discard documents that are too short, too long, or incomplete. At the time of writing, NeMo Curator provides 24 heuristics for natural languages, as well as eight heuristics for coding languages.Â \n",
      "\n",
      "NeMo Curator provides a `DocumentFilter` interface, which defines a way to score documents based on various criteria, along with a `ScoreFilter` helper to filter the documents. The `ScoreFilter` helper takes a `DocumentDataset` along with a `DocumentFilter` and determines whether each document in the dataset passes the filtering criteria.\n",
      "\n",
      "Create a simple `DocumentFilter` that determines whether a story ends with an end of sentence character. The goal is to discard all stories that do not end with an end of sentence character:\n",
      "\n",
      "`from` `nemo_curator.filters` `import` `DocumentFilter`\n",
      "\n",
      "`class` `IncompleteStoryFilter(DocumentFilter):`\n",
      "\n",
      "Â Â Â Â `def` `__init__(``self``):`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `super``().__init__()`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `self``._story_terminators` `=` `{``\".\"``,` `\"!\"``,` `\"?\"``,` `'\"'``, \"â€\"}`\n",
      "\n",
      "Â Â Â Â `def` `score_document(``self``, text:` `str``)` `-``>` `bool``:`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `ret` `=` `text.strip()[``-``1``]` `in` `self``._story_terminators`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `return` `ret`\n",
      "\n",
      "Â Â Â Â `def` `keep_document(``self``, score)` `-``>` `bool``:`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `return` `score`\n",
      "\n",
      "The main functionality is implemented in `score_document` and `keep_document` functions, where `False` (that is, donâ€™t keep this document) is returned if the document does not end with an end of sentence character.\n",
      "\n",
      "To apply this filter to the dataset, pass an instance of `IncompleteStoryFilter` to a `ScoreFilter` object. NeMo Curator provides many `DocumentFilter` implementations out of the box. These filters can be chained together through the `Sequential` class. The following code shows how to apply various filters to the dataset:\n",
      "\n",
      "`def` `filter_dataset(dataset: DocumentDataset)` `-``> DocumentDataset:`\n",
      "\n",
      "Â Â Â Â `filters` `=` `Sequential(`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `[`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â `ScoreFilter(`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â `WordCountFilter(min_words``=``80``),`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â `text_field``=``\"text\"``,`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â `score_field``=``\"word_count\"``,`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â `),`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â `ScoreFilter(IncompleteStoryFilter(), text_field``=``\"text\"``),`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â `ScoreFilter(`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â `RepeatingTopNGramsFilter(n``=``2``, max_repeating_ngram_ratio``=``0.2``),`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â `text_field``=``\"text\"``,`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â `),`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â `ScoreFilter(`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â `RepeatingTopNGramsFilter(n``=``3``, max_repeating_ngram_ratio``=``0.18``),`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â `text_field``=``\"text\"``,`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â `),`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â `ScoreFilter(`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â `RepeatingTopNGramsFilter(n``=``4``, max_repeating_ngram_ratio``=``0.16``),`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â `text_field``=``\"text\"``,`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â `),`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `]`\n",
      "\n",
      "Â Â Â Â `)`\n",
      "\n",
      "Â Â Â Â `return` `filters(dataset)`\n",
      "\n",
      "This code filters all short (less than 80 words) or incomplete stories, along with any other stories that have certain ratios of repeating n-grams. Note the usage of `text_field=â€textâ€`, which tells the `ScoreFilter` to pass the contents of the dataset `text` column to each filtering criteria.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Title: Deduplication[](#deduplication)\n",
      "\n",
      "Content: When working with large amounts of text data, there may be records that are identical (or near-identical) to each other. Training on such data may incur additional compute and storage overhead. NeMo Curator provides functionality to find and discard such duplicates. For simplicity, focus on finding exact duplicate records in the dataset. This can be accomplished using the `ExactDuplicates` class, as shown below.Â \n",
      "\n",
      "This module will automatically leverage existing CUDA devices and the GPU-accelerated implementations from the [RAPIDS cuDF library](https://rapids.ai/) to identify duplicate documents, resulting in much faster processing times. This is because the deduplication stage involves calculating a hash for every document, which is compute-intensive. Each document can be hashed independently, which makes this workload ideal to run in parallel on the GPU.\n",
      "\n",
      "`from` `nemo_curator.modules` `import` `ExactDuplicates`\n",
      "\n",
      "`def` `dedupe(dataset: DocumentDataset)` `-``> DocumentDataset:`\n",
      "\n",
      "Â Â Â Â `deduplicator` `=` `ExactDuplicates(id_field``=``\"id\"``, text_field``=``\"text\"``, hash_method``=``\"md5\"``)`\n",
      "\n",
      "Â Â Â Â `# Find the duplicates`\n",
      "\n",
      "Â Â Â Â `duplicates` `=` `deduplicator(dataset)`\n",
      "\n",
      "Â Â Â Â `docs_to_remove` `=` `duplicates.df.map_partitions(`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `lambda` `x: x[x._hashes.duplicated(keep``=``\"first\"``)]`\n",
      "\n",
      "Â Â Â Â `)`\n",
      "\n",
      "Â Â Â Â `# Remove the duplicates using their IDs.`\n",
      "\n",
      "Â Â Â Â `duplicate_ids` `=` `list``(docs_to_remove.compute().``id``)`\n",
      "\n",
      "Â Â Â Â `dataset_df` `=` `dataset.df`\n",
      "\n",
      "Â Â Â Â `deduped` `=` `dataset_df[~dataset_df.``id``.isin(duplicate_ids)]`\n",
      "\n",
      "Â Â Â Â `return` `DocumentDataset(deduped)`\n",
      "\n",
      "This specifies that each recordâ€™s unique identifier and content are in the `id` and `text` columns, respectively. Recall that a unique identifier was assigned to each document during the download and extraction phase. This enables the deduplicator to uniquely identify documents from one another. The deduplicator object returns a set of IDs that it has determined to be duplicates. Simply remove these documents from the dataset.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Title: PII redaction[](#pii_redaction)\n",
      "\n",
      "Content: The last processing step discussed in this tutorial is the redaction of personally identifiable information (PII). NeMo Curator facilitates the detection and removal of PII using the [`PiiModifier` class](https://github.com/NVIDIA/NeMo-Curator/blob/9f42c49923e314572da53dc7c0384052420ca657/nemo_curator/modifiers/pii_modifier.py), which is an implementation of the `DocumentModifier` class previously discussed. This modifier leverages the [Presidio framework](https://github.com/microsoft/presidio) and enables you to specify which PII to detect, what action to take for each detection, and process the data in batches to accelerate the operation.\n",
      "\n",
      "The stories in the TinyStories dataset contain many instances of first names. This example intends to detect all such names and replace them with an anonymized token. This can be accomplished using a few lines of code:\n",
      "\n",
      "`from` `nemo_curator.modifiers.pii_modifier` `import` `PiiModifier`\n",
      "\n",
      "`def` `redact_pii(dataset: DocumentDataset)` `-``> DocumentDataset:`\n",
      "\n",
      "Â Â Â Â `redactor` `=` `Modify(`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `PiiModifier(`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â `supported_entities``=``[``\"PERSON\"``],`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â `anonymize_action``=``\"replace\"``,`\n",
      "\n",
      "Â Â Â Â Â Â Â Â Â Â Â Â `device``=``\"cpu\"``,`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `),`\n",
      "\n",
      "Â Â Â Â `)`\n",
      "\n",
      "Â Â Â Â `return` `redactor(dataset)`\n",
      "\n",
      "The operation takes the entire dataset and returns the modified dataset.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Title: Putting the curation pipeline together[](#putting_the_curation_pipeline_together)\n",
      "\n",
      "Content: Having implemented each step of the curation pipeline, itâ€™s time to put everything together and sequentially apply each operation on the dataset. You can use the `Sequential` class to chain curation operations together:\n",
      "\n",
      "`curation_steps` `=` `Sequential(`\n",
      "\n",
      "Â Â Â Â `[`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `clean_and_unify,`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `filter_dataset,`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `dedupe,`\n",
      "\n",
      "Â Â Â Â Â Â Â Â `redact_pii,`\n",
      "\n",
      "Â Â Â Â `]`\n",
      "\n",
      "`)`\n",
      "\n",
      "`dataset` `=` `curation_steps(dataset)`\n",
      "\n",
      "`print``(``\"Executing the pipeline...\"``)`\n",
      "\n",
      "`dataset` `=` `dataset.persist()`\n",
      "\n",
      "`dataset.to_json(``\"/output/path\"``, write_to_filename``=``True``)`\n",
      "\n",
      "Under the hood, NeMo Curator uses Dask to work with the dataset in a distributed manner. Since Dask operations are lazy-evaluated, itâ€™s necessary to call the `.persist` function to instruct Dask to apply the operations. Once processing finishes, you can write the dataset to disk in the JSONL format by calling the `.to_json` function and providing an output path.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Title: Next steps[](#next_steps)\n",
      "\n",
      "Content: NeMo Curator supports many advanced data processing and filtering techniques, such as fuzzy or task-based deduplication, task identification and decontamination, domain classification (and much more) that are not covered in this tutorial. Check out the [collection of data curation examples on GitHub](https://github.com/NVIDIA/NeMo-Curator/tree/main/examples) to learn more.Â \n",
      "\n",
      "You can also [request access to the NVIDIA NeMo Curator microservice](https://developer.nvidia.com/nemo-microservices), which provides the easiest path for enterprises to get started with data curation from anywhere. It offers streamlined performance and scalability to shorten the time to market.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "with open(\"input_files/Curating Custom Datasets for LLM Training with NVIDIA NeMo Curator.md\", \"r\", encoding=\"utf-8\") as file:\n",
    "    markdown_text = file.read()\n",
    "    markdown_parser = MarkdownParser()\n",
    "    sections = markdown_parser.parse_markdown(markdown_text)\n",
    "\n",
    "    for title, content in sections.items():\n",
    "        print(f\"Title: {title}\\n\")\n",
    "        print(f\"Content: {content}\\n\")\n",
    "        print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "from typing import Dict\n",
    "\n",
    "class TextExplainer:\n",
    "    def __init__(self):\n",
    "        self.conversation_history = []\n",
    "        self.delay = 1\n",
    "        \n",
    "    def explain_section(self, section_title: str, section_content: str, is_first: bool = False) -> str:\n",
    "        \"\"\"\n",
    "        ì„¹ì…˜ì˜ ë‚´ìš©ì„ ì„¤ëª…í•˜ëŠ” í•¨ìˆ˜. Deepseek API ì‹¤íŒ¨ì‹œ OpenAI APIë¡œ fallback\n",
    "        \n",
    "        Args:\n",
    "            section_title: ì„¹ì…˜ ì œëª©\n",
    "            section_content: ì„¹ì…˜ ë‚´ìš©\n",
    "            is_first: ì²« ë²ˆì§¸ ì„¹ì…˜ì¸ì§€ ì—¬ë¶€\n",
    "            \n",
    "        Returns:\n",
    "            str: ì„¹ì…˜ì— ëŒ€í•œ ì„¤ëª…\n",
    "        \"\"\"\n",
    "        if is_first:\n",
    "            prompt = f\"\"\"ë‹¤ìŒ ì„¹ì…˜ '{section_title}'ì˜ ë‚´ìš©ì„ ëª…í™•í•˜ê³  ìì„¸í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.\n",
    "            \n",
    "            ì„¹ì…˜ ë‚´ìš©:\n",
    "            {section_content}\"\"\"\n",
    "        else:\n",
    "            prompt = f\"\"\"ì´ì „ ì„¹ì…˜ì— í–ˆë˜ ì„¤ëª…ë“¤ì„ ì¶”ê°€ ë§¥ë½ìœ¼ë¡œ ì°¸ê³ í•´ì„œ ë‹¤ìŒ ì„¹ì…˜ '{section_title}'ì˜ ë‚´ìš©ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.\n",
    "            \n",
    "            ì„¹ì…˜ ë‚´ìš©:\n",
    "            {section_content}\"\"\"\n",
    "        \n",
    "        # ì´ì „ ëŒ€í™” ë‚´ìš©ì„ í¬í•¨í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ ìœ ì§€\n",
    "        self.conversation_history.append({\"role\": \"user\", \"content\": prompt})\n",
    "        \n",
    "        # Deepseek API ì²« ì‹œë„\n",
    "        try:\n",
    "            print(\"Attempting Deepseek API...\")\n",
    "            response = get_completion(self.conversation_history, model_name=\"deepseek-reasoner\")\n",
    "            \n",
    "            if not response:\n",
    "                raise ValueError(\"Empty response from Deepseek API\")\n",
    "                \n",
    "            # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸\n",
    "            self.conversation_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Deepseek API attempt failed: {str(e)}\")\n",
    "            print(f\"Retrying Deepseek API in {self.delay} seconds...\")\n",
    "            time.sleep(self.delay)\n",
    "            \n",
    "            # Deepseek API ì¬ì‹œë„\n",
    "            try:\n",
    "                print(\"Retrying Deepseek API...\")\n",
    "                response = get_completion(self.conversation_history, model_name=\"deepseek-reasoner\")\n",
    "                \n",
    "                if not response:\n",
    "                    raise ValueError(\"Empty response from Deepseek API\")\n",
    "                    \n",
    "                # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸\n",
    "                self.conversation_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "                return response\n",
    "                \n",
    "            except Exception as retry_error:\n",
    "                print(f\"Deepseek API retry also failed: {str(retry_error)}\")\n",
    "                print(\"Falling back to OpenAI API...\")\n",
    "                \n",
    "                # OpenAI APIë¡œ fallback\n",
    "                try:\n",
    "                    response = get_completion_openai(self.conversation_history)\n",
    "                    if not response:\n",
    "                        raise ValueError(\"Empty response from OpenAI API\")\n",
    "                        \n",
    "                    # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸\n",
    "                    self.conversation_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "                    return response\n",
    "                    \n",
    "                except Exception as openai_error:\n",
    "                    print(f\"OpenAI API fallback also failed: {str(openai_error)}\")\n",
    "                    return f\"Error: Failed to explain section {section_title} with both APIs\"\n",
    "                \n",
    "    def explain_text(self, sections: Dict[str, str]) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        í…ìŠ¤íŠ¸ì˜ ê° ì„¹ì…˜ì„ ìˆœì°¨ì ìœ¼ë¡œ ì„¤ëª…\n",
    "        \n",
    "        Args:\n",
    "            sections: ì„¹ì…˜ ì œëª©ê³¼ ë‚´ìš©ì„ ë§¤í•‘í•œ ë”•ì…”ë„ˆë¦¬\n",
    "            \n",
    "        Returns:\n",
    "            ì„¹ì…˜ ì œëª©ê³¼ ì„¤ëª…ì„ ë§¤í•‘í•œ ë”•ì…”ë„ˆë¦¬\n",
    "        \"\"\"\n",
    "        explanations = {}\n",
    "        \n",
    "        print(\"\\nProcessing sections:\")\n",
    "        for i, (title, content) in tqdm(enumerate(sections.items()), desc=\"Explaining sections\"):\n",
    "            print(f\"\\nProcessing: {title}\")\n",
    "            explanation = self.explain_section(title, content, is_first=(i==0))\n",
    "            explanations[title] = explanation\n",
    "            \n",
    "        return explanations\n",
    "    \n",
    "    def get_conversation_history(self):\n",
    "        \"\"\"ëŒ€í™” íˆìŠ¤í† ë¦¬ ë°˜í™˜\"\"\"\n",
    "        return self.conversation_history\n",
    "    \n",
    "    def save_explanations(self, explanations: Dict[str, str], file_name: str, output_dir: str = \"output_files\") -> str:\n",
    "        \"\"\"ì„¤ëª…ì„ íŒŒì¼ë¡œ ì €ì¥\"\"\"\n",
    "        with open(os.path.join(output_dir, file_name), 'w', encoding='utf-8') as f:\n",
    "            for title, explanation in explanations.items():\n",
    "                f.write(f\"## {title}\\n\\n{explanation}\")\n",
    "                f.write(\"\\n\\n---\\n\")\n",
    "        print(f\"Explanations saved to: {os.path.join(output_dir, file_name)}\")\n",
    "        \n",
    "        return os.path.join(output_dir, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Dict\n",
    "import time\n",
    "\n",
    "class TextQA:\n",
    "    def __init__(self, context: Optional[List[Dict[str, str]]] = None):\n",
    "        self.conversation_history = context or []\n",
    "        self.delay = 1\n",
    "        \n",
    "    def ask_question(self, question: str) -> str:\n",
    "        \"\"\"í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì§ˆë¬¸ì— ë‹µë³€\"\"\"\n",
    "        try:\n",
    "            prompt = f\"\"\"Based on the text we discussed, please answer the following question in Korean. \n",
    "            Be specific and cite relevant sections when possible.\n",
    "\n",
    "            Question: {question}\"\"\"\n",
    "            \n",
    "            self.conversation_history.append({\"role\": \"user\", \"content\": prompt})\n",
    "            response = get_completion(self.conversation_history, model_name=\"deepseek-reasoner\")\n",
    "            self.conversation_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "            \n",
    "            time.sleep(self.delay)\n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question: {str(e)}\")\n",
    "            return f\"Error: Failed to process question\"\n",
    "    \n",
    "    def view_conversation_history(self, start_idx: int = 0, end_idx: Optional[int] = None) -> None:\n",
    "        \"\"\"ëŒ€í™” ë‚´ì—­ì„ ì¶œë ¥\n",
    "        \n",
    "        Args:\n",
    "            start_idx: ì‹œì‘ ì¸ë±ìŠ¤ (ê¸°ë³¸ê°’: 0)\n",
    "            end_idx: ì¢…ë£Œ ì¸ë±ìŠ¤ (ê¸°ë³¸ê°’: None, Noneì¼ ê²½ìš° ëê¹Œì§€ ì¶œë ¥)\n",
    "        \"\"\"\n",
    "        conversations = [\n",
    "            msg for msg in self.conversation_history \n",
    "            if not msg[\"content\"].startswith(\"Here's the text summary:\")\n",
    "        ]\n",
    "        \n",
    "        end_idx = end_idx if end_idx is not None else len(conversations)\n",
    "        \n",
    "        print(\"\\n=== ëŒ€í™” ë‚´ì—­ ===\\n\")\n",
    "        for i, msg in enumerate(conversations[start_idx:end_idx], start=start_idx):\n",
    "            role = msg[\"role\"].upper()\n",
    "            if role == \"ASSISTANT\":\n",
    "                print(f\"\\nğŸ¤– Assistant ({i}):\\n{msg['content']}\\n\")\n",
    "                print(\"-\" * 80)\n",
    "            elif role == \"USER\":\n",
    "                print(f\"\\nğŸ‘¤ User ({i}):\\n{msg['content']}\\n\")\n",
    "                print(\"-\" * 80)\n",
    "    \n",
    "    def get_last_n_conversations(self, n: int = 1) -> None:\n",
    "        \"\"\"ìµœê·¼ nê°œì˜ ëŒ€í™” ë‚´ì—­ì„ ì¶œë ¥\n",
    "        \n",
    "        Args:\n",
    "            n: ì¶œë ¥í•  ìµœê·¼ ëŒ€í™” ê°œìˆ˜ (ê¸°ë³¸ê°’: 1)\n",
    "        \"\"\"\n",
    "        conversations = [\n",
    "            msg for msg in self.conversation_history \n",
    "            if not msg[\"content\"].startswith(\"Here's the text summary:\")\n",
    "        ]\n",
    "        start_idx = max(0, len(conversations) - n)\n",
    "        self.view_conversation_history(start_idx)\n",
    "        \n",
    "    def get_conversation_history(self) -> List[Dict[str, str]]:\n",
    "        \"\"\"ì „ì²´ ëŒ€í™” ê¸°ë¡ ë°˜í™˜\"\"\"\n",
    "        return self.conversation_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.console import Console\n",
    "from rich.markdown import Markdown\n",
    "from rich.panel import Panel\n",
    "from rich.syntax import Syntax\n",
    "from rich.table import Table\n",
    "from typing import Dict\n",
    "import os\n",
    "\n",
    "class MarkdownPrinter:\n",
    "    def __init__(self):\n",
    "        self.console = Console()\n",
    "        \n",
    "    def print_markdown_file(self, file_path: str):\n",
    "        \"\"\"ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì„ ì´ì˜ê²Œ ì¶œë ¥\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                markdown_content = f.read()\n",
    "            \n",
    "            # ë§ˆí¬ë‹¤ìš´ ë Œë”ë§\n",
    "            md = Markdown(markdown_content)\n",
    "            \n",
    "            # ë§ˆí¬ë‹¤ìš´ ë‚´ìš© ì¶œë ¥\n",
    "            self.console.print(md)\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.console.print(f\"[bold red]Error reading file: {str(e)}[/]\")\n",
    "            \n",
    "    def print_sections(self, sections: Dict[str, str]):\n",
    "        \"\"\"ì„¹ì…˜ë³„ë¡œ êµ¬ë¶„í•˜ì—¬ ì¶œë ¥\"\"\"\n",
    "        for section, content in sections.items():\n",
    "            # ì„¹ì…˜ ì œëª©\n",
    "            self.console.print(\"\\n\")\n",
    "            self.console.print(Panel(\n",
    "                f\"[bold cyan]{section}[/]\",\n",
    "                border_style=\"cyan\"\n",
    "            ))\n",
    "            \n",
    "            # ì„¹ì…˜ ë‚´ìš©\n",
    "            md = Markdown(content)\n",
    "            self.console.print(md)\n",
    "            \n",
    "            # êµ¬ë¶„ì„ \n",
    "            self.console.print(\"[dim]\" + \"=\"*80 + \"[/]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_text(file_path: str) -> str:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        file_name = os.path.basename(file_path)\n",
    "        markdown_text = file.read()\n",
    "        markdown_parser = MarkdownParser()\n",
    "        sections = markdown_parser.parse_markdown(markdown_text)\n",
    "        \n",
    "        explainer = TextExplainer()\n",
    "        explanations = explainer.explain_text(sections)\n",
    "        explanation_path = explainer.save_explanations(explanations, file_name)\n",
    "        \n",
    "        qa = TextQA(context=explainer.get_conversation_history())\n",
    "    \n",
    "    return explanation_path, qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing sections:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: Curating Custom Datasets for LLM Training with NVIDIA NeMo Curator\n",
      "Attempting Deepseek API...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections: 1it [01:35, 95.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: Overview[](#overview)\n",
      "Attempting Deepseek API...\n",
      "Deepseek API attempt failed: Expecting value: line 1 column 1 (char 0)\n",
      "Retrying Deepseek API in 1 seconds...\n",
      "Retrying Deepseek API...\n",
      "Deepseek API retry also failed: Expecting value: line 1 column 1 (char 0)\n",
      "Falling back to OpenAI API...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections: 2it [03:37, 111.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API fallback also failed: Error code: 400 - {'error': {'message': 'Model Not Exist', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "\n",
      "Processing: Prerequisite[](#prerequisite)\n",
      "Attempting Deepseek API...\n",
      "Deepseek API attempt failed: Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support successive user or assistant messages (messages[2] and messages[3] in your input). You should interleave the user/assistant messages in the message sequence.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "Retrying Deepseek API in 1 seconds...\n",
      "Retrying Deepseek API...\n",
      "Deepseek API retry also failed: Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support successive user or assistant messages (messages[2] and messages[3] in your input). You should interleave the user/assistant messages in the message sequence.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "Falling back to OpenAI API...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections: 3it [03:38, 61.05s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API fallback also failed: Error code: 400 - {'error': {'message': 'Model Not Exist', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "\n",
      "Processing: Defining custom document builders[](#defining_custom_document_builders)\n",
      "Attempting Deepseek API...\n",
      "Deepseek API attempt failed: Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support successive user or assistant messages (messages[2] and messages[3] in your input). You should interleave the user/assistant messages in the message sequence.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "Retrying Deepseek API in 1 seconds...\n",
      "Retrying Deepseek API...\n",
      "Deepseek API retry also failed: Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support successive user or assistant messages (messages[2] and messages[3] in your input). You should interleave the user/assistant messages in the message sequence.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "Falling back to OpenAI API...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections: 4it [03:40, 37.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API fallback also failed: Error code: 400 - {'error': {'message': 'Model Not Exist', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "\n",
      "Processing: Downloading the TinyStories dataset[](#downloading_the_tinystories_dataset)\n",
      "Attempting Deepseek API...\n",
      "Deepseek API attempt failed: Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support successive user or assistant messages (messages[2] and messages[3] in your input). You should interleave the user/assistant messages in the message sequence.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "Retrying Deepseek API in 1 seconds...\n",
      "Retrying Deepseek API...\n",
      "Deepseek API retry also failed: Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support successive user or assistant messages (messages[2] and messages[3] in your input). You should interleave the user/assistant messages in the message sequence.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "Falling back to OpenAI API...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections: 5it [03:42, 24.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API fallback also failed: Error code: 400 - {'error': {'message': 'Model Not Exist', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "\n",
      "Processing: Text cleaning and unification[](#text_cleaning_and_unification)\n",
      "Attempting Deepseek API...\n",
      "Deepseek API attempt failed: Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support successive user or assistant messages (messages[2] and messages[3] in your input). You should interleave the user/assistant messages in the message sequence.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "Retrying Deepseek API in 1 seconds...\n",
      "Retrying Deepseek API...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections: 6it [03:43, 16.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deepseek API retry also failed: Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support successive user or assistant messages (messages[2] and messages[3] in your input). You should interleave the user/assistant messages in the message sequence.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "Falling back to OpenAI API...\n",
      "OpenAI API fallback also failed: Error code: 400 - {'error': {'message': 'Model Not Exist', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "\n",
      "Processing: Dataset filtering[](#dataset_filtering)\n",
      "Attempting Deepseek API...\n",
      "Deepseek API attempt failed: Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support successive user or assistant messages (messages[2] and messages[3] in your input). You should interleave the user/assistant messages in the message sequence.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "Retrying Deepseek API in 1 seconds...\n",
      "Retrying Deepseek API...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections: 7it [03:45, 11.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deepseek API retry also failed: Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support successive user or assistant messages (messages[2] and messages[3] in your input). You should interleave the user/assistant messages in the message sequence.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "Falling back to OpenAI API...\n",
      "OpenAI API fallback also failed: Error code: 400 - {'error': {'message': 'Model Not Exist', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "\n",
      "Processing: Deduplication[](#deduplication)\n",
      "Attempting Deepseek API...\n",
      "Deepseek API attempt failed: Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support successive user or assistant messages (messages[2] and messages[3] in your input). You should interleave the user/assistant messages in the message sequence.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "Retrying Deepseek API in 1 seconds...\n",
      "Retrying Deepseek API...\n",
      "Deepseek API retry also failed: Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support successive user or assistant messages (messages[2] and messages[3] in your input). You should interleave the user/assistant messages in the message sequence.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "Falling back to OpenAI API...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections: 8it [03:47,  8.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API fallback also failed: Error code: 400 - {'error': {'message': 'Model Not Exist', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "\n",
      "Processing: PII redaction[](#pii_redaction)\n",
      "Attempting Deepseek API...\n",
      "Deepseek API attempt failed: Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support successive user or assistant messages (messages[2] and messages[3] in your input). You should interleave the user/assistant messages in the message sequence.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "Retrying Deepseek API in 1 seconds...\n",
      "Retrying Deepseek API...\n",
      "Deepseek API retry also failed: Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support successive user or assistant messages (messages[2] and messages[3] in your input). You should interleave the user/assistant messages in the message sequence.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "Falling back to OpenAI API...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections: 9it [03:48,  6.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API fallback also failed: Error code: 400 - {'error': {'message': 'Model Not Exist', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "\n",
      "Processing: Putting the curation pipeline together[](#putting_the_curation_pipeline_together)\n",
      "Attempting Deepseek API...\n",
      "Deepseek API attempt failed: Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support successive user or assistant messages (messages[2] and messages[3] in your input). You should interleave the user/assistant messages in the message sequence.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "Retrying Deepseek API in 1 seconds...\n",
      "Retrying Deepseek API...\n",
      "Deepseek API retry also failed: Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support successive user or assistant messages (messages[2] and messages[3] in your input). You should interleave the user/assistant messages in the message sequence.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "Falling back to OpenAI API...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections: 10it [03:50,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API fallback also failed: Error code: 400 - {'error': {'message': 'Model Not Exist', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "\n",
      "Processing: Next steps[](#next_steps)\n",
      "Attempting Deepseek API...\n",
      "Deepseek API attempt failed: Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support successive user or assistant messages (messages[2] and messages[3] in your input). You should interleave the user/assistant messages in the message sequence.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "Retrying Deepseek API in 1 seconds...\n",
      "Retrying Deepseek API...\n",
      "Deepseek API retry also failed: Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support successive user or assistant messages (messages[2] and messages[3] in your input). You should interleave the user/assistant messages in the message sequence.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "Falling back to OpenAI API...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections: 11it [03:52, 21.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API fallback also failed: Error code: 400 - {'error': {'message': 'Model Not Exist', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "Explanations saved to: output_files/Curating Custom Datasets for LLM Training with NVIDIA NeMo Curator.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "                        <span style=\"font-weight: bold; text-decoration: underline\">Curating Custom Datasets for LLM Training with NVIDIA NeMo Curator</span>                         \n",
       "\n",
       "                  <span style=\"font-weight: bold\">ì„¹ì…˜ ì„¤ëª…: \"Curating Custom Datasets for LLM Training with NVIDIA NeMo Curator\"</span>                  \n",
       "\n",
       "ì´ ì„¹ì…˜ì€ <span style=\"font-weight: bold\">ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)</span> ë° ì†Œí˜• ì–¸ì–´ ëª¨ë¸(SLM) í›ˆë ¨ì„ ìœ„í•œ ë°ì´í„° íë ˆì´ì…˜ì˜ ì¤‘ìš”ì„±ê³¼ ì´ë¥¼ ì§€ì›í•˜ëŠ” <span style=\"font-weight: bold\">NVIDIA </span>\n",
       "<span style=\"font-weight: bold\">NeMo Curator</span>ì˜ ê¸°ëŠ¥ì„ ìƒì„¸íˆ ì†Œê°œí•©ë‹ˆë‹¤. ë°ì´í„° íë ˆì´ì…˜ì€ ëª¨ë¸ ì„±ëŠ¥ì„ ê²°ì •í•˜ëŠ” í•µì‹¬ ë‹¨ê³„ë¡œ, NeMo CuratorëŠ” ê³ í’ˆì§ˆ \n",
       "ë°ì´í„°ì…‹ êµ¬ì¶•ì„ ìœ„í•œ íš¨ìœ¨ì ì´ê³  ìœ ì—°í•œ í”„ë ˆì„ì›Œí¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì•„ë˜ëŠ” ì£¼ìš” ë‚´ìš©ì„ êµ¬ì¡°í™”í•œ ì„¤ëª…ì…ë‹ˆë‹¤.            \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>\n",
       "                                            <span style=\"font-weight: bold\">1. ë°ì´í„° íë ˆì´ì…˜ì˜ ì¤‘ìš”ì„±</span>                                            \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span><span style=\"font-weight: bold\">LLM/SLM í›ˆë ¨ì˜ ì²« ë‹¨ê³„ì´ì ê°€ì¥ ì¤‘ìš”í•œ ë‹¨ê³„</span>                                                                     \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>ëª¨ë¸ì˜ ì„±ëŠ¥ì€ í•™ìŠµ ë°ì´í„°ì˜ í’ˆì§ˆì— ì§ì ‘ì ìœ¼ë¡œ ì˜í–¥ì„ ë°›ìŠµë‹ˆë‹¤. ë…¸ì´ì¦ˆê°€ ë§ê±°ë‚˜ ì¤‘ë³µëœ ë°ì´í„°ëŠ” ëª¨ë¸ì˜ ì •í™•ë„ì™€  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>ì¼ë°˜í™” ëŠ¥ë ¥ì„ ì €í•´í•˜ë¯€ë¡œ, ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬ ê³¼ì •ì´ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤.                                         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span><span style=\"font-weight: bold\">ê³ í’ˆì§ˆ ë°ì´í„°ì˜ í•µì‹¬ ìš”ì†Œ</span>                                                                                       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>ë°ì´í„°ì˜ ë‹¤ì–‘ì„±, ì •í™•ì„±, ì¤‘ë³µ ì œê±°, ê°œì¸ì •ë³´ ë³´í˜¸(PII ì²˜ë¦¬) ë“±ì´ í•„ìˆ˜ì ì…ë‹ˆë‹¤.                                  \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>\n",
       "                                            <span style=\"font-weight: bold\">2. NVIDIA NeMo Curator ì†Œê°œ</span>                                            \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span><span style=\"font-weight: bold\">ëª©ì </span>: ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì„ ìë™í™”ëœ ì›Œí¬í”Œë¡œìš°ë¡œ ì²˜ë¦¬í•˜ì—¬ ê³ í’ˆì§ˆ í›ˆë ¨ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.                         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span><span style=\"font-weight: bold\">íŠ¹ì§•</span>:                                                                                                           \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    â€¢ </span><span style=\"font-weight: bold\">ì˜¤í”ˆì†ŒìŠ¤ í”„ë ˆì„ì›Œí¬</span>: ê°œë°œìê°€ ììœ ë¡­ê²Œ ì»¤ìŠ¤í„°ë§ˆì´ì§• ê°€ëŠ¥.                                                    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    â€¢ </span><span style=\"font-weight: bold\">NVIDIA NeMo ìƒíƒœê³„ í†µí•©</span>: ì—”ë“œíˆ¬ì—”ë“œ ëª¨ë¸ ê°œë°œ íŒŒì´í”„ë¼ì¸(í›ˆë ¨-ìµœì í™”-ë°°í¬)ê³¼ ì—°ë™ë©ë‹ˆë‹¤.                     \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    â€¢ </span><span style=\"font-weight: bold\">í™•ì¥ì„±</span>: ë¶„ì‚° ì»´í“¨íŒ…ì„ ì§€ì›í•˜ì—¬ í˜íƒ€ë°”ì´íŠ¸ ê·œëª¨ì˜ ë°ì´í„°(ì˜ˆ: Common Crawl)ë„ ì²˜ë¦¬ ê°€ëŠ¥.                       \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>\n",
       "                                            <span style=\"font-weight: bold\">3. NeMo Curatorì˜ ì£¼ìš” ê¸°ëŠ¥</span>                                            \n",
       "\n",
       "                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">ê°€. ê¸°ë³¸ ì œê³µ ë°ì´í„° ì†ŒìŠ¤ ë° ì›Œí¬í”Œë¡œìš°</span>                                      \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span><span style=\"font-weight: bold\">ê³µê°œ ë°ì´í„°ì…‹ ì§€ì›</span>: Common Crawl, Wikipedia, arXiv ë“± ëŒ€í‘œì ì¸ ì†ŒìŠ¤ì—ì„œ ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬í•˜ëŠ” ì‚¬ì „   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>êµ¬ì¶• íŒŒì´í”„ë¼ì¸ì„ ì œê³µí•©ë‹ˆë‹¤.                                                                                   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span><span style=\"font-weight: bold\">ì „ì²˜ë¦¬ ë‹¨ê³„ ì˜ˆì‹œ</span>:                                                                                               \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    â€¢ </span><span style=\"font-weight: bold\">í•„í„°ë§</span>: í’ˆì§ˆ ê¸°ì¤€(ì˜ˆ: ë¬¸ë²• ì˜¤ë¥˜, ì˜ë¯¸ ì—†ëŠ” í…ìŠ¤íŠ¸)ì— ë§ì§€ ì•ŠëŠ” ë°ì´í„° ì œê±°.                                  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    â€¢ </span><span style=\"font-weight: bold\">ì¤‘ë³µ ì œê±°</span>: ë¬¸ì„œ/ë¬¸ì¥ ìˆ˜ì¤€ì˜ ì¤‘ë³µ ë°ì´í„° ì‹ë³„ ë° ì‚­ì œ.                                                        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    â€¢ </span><span style=\"font-weight: bold\">ì–¸ì–´ ì‹ë³„</span>: íŠ¹ì • ì–¸ì–´(ì˜ˆ: ì˜ì–´) ë°ì´í„°ë§Œ ì„ ë³„.                                                                \n",
       "\n",
       "                                         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">ë‚˜. ì»¤ìŠ¤í…€ ë°ì´í„° íŒŒì´í”„ë¼ì¸ êµ¬ì¶•</span>                                         \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span><span style=\"font-weight: bold\">ìœ ì—°í•œ í™•ì¥ì„±</span>: ê°œë°œìëŠ” ìì²´ ë°ì´í„° ì†ŒìŠ¤ë¥¼ ì¶”ê°€í•˜ê±°ë‚˜ ì „ì²˜ë¦¬ ë‹¨ê³„ë¥¼ ìˆ˜ì •í•˜ì—¬ íŠ¹ì • ë„ë©”ì¸(ì˜ë£Œ, ë²•ë¥  ë“±)ì— ë§ëŠ”  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>íŒŒì´í”„ë¼ì¸ì„ ì„¤ê³„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.                                                                                \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span><span style=\"font-weight: bold\">ì‚¬ìš©ì ì •ì˜ ì˜ˆì‹œ</span>:                                                                                               \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    â€¢ </span><span style=\"font-weight: bold\">ë„ë©”ì¸ íŠ¹í™” í•„í„°</span>: ê¸ˆìœµ ë°ì´í„°ì—ì„œ ìˆ«ì/í†µê³„ ê´€ë ¨ í…ìŠ¤íŠ¸ ê°•ì¡° ì¶”ì¶œ.                                           \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    â€¢ </span><span style=\"font-weight: bold\">ë§ì¶¤í˜• ì¤‘ë³µ ê²€ì¶œ</span>: ì‚¬ìš©ì ì •ì˜ í•´ì‹œ í•¨ìˆ˜ë¡œ ì¤‘ë³µ ë¬¸ì„œ ì‹ë³„.                                                    \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>\n",
       "                                          <span style=\"font-weight: bold\">4. NeMo Curatorë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ </span>                                          \n",
       "\n",
       "                                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">ê°€. í”„ë¡œì íŠ¸ ë§ì¶¤í˜• ë°ì´í„° íë ˆì´ì…˜</span>                                        \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span><span style=\"font-weight: bold\">ëª©ì ì— ë”°ë¥¸ ìµœì í™”</span>: ìƒì„±í˜• AI ëª¨ë¸ì˜ ìš©ë„(ì±—ë´‡, ì½”ë“œ ìƒì„± ë“±)ì— ë”°ë¼ ë°ì´í„° íŠ¹ì„±ì„ ì¡°ì ˆí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.          \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>(ì˜ˆ: ì½”ë“œ ìƒì„± ëª¨ë¸ì„ ìœ„í•´ GitHub ë°ì´í„° ê°•ì¡° ìˆ˜ì§‘)                                                             \n",
       "\n",
       "                                               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">ë‚˜. ë°ì´í„° í’ˆì§ˆ ë³´ì¥</span>                                                \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span><span style=\"font-weight: bold\">ë‹¤ë‹¨ê³„ í•„í„°ë§</span>:                                                                                                  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    â€¢ </span><span style=\"font-weight: bold\">í’ˆì§ˆ ì ìˆ˜ ê¸°ë°˜ í•„í„°</span>: íœ´ë¦¬ìŠ¤í‹± ë˜ëŠ” ML ëª¨ë¸ì„ í™œìš©í•´ ì €í’ˆì§ˆ í…ìŠ¤íŠ¸(ìŠ¤íŒ¸, ë¬´ì˜ë¯¸í•œ ë‚´ìš©) ì œê±°.                 \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    â€¢ </span><span style=\"font-weight: bold\">ì •êµí•œ ì¤‘ë³µ ì œê±°</span>: SimHash, MinHash ë“±ì˜ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ë¬¸ì„œ/í† í° ìˆ˜ì¤€ ì¤‘ë³µ ê°ì†Œ.                               \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span><span style=\"font-weight: bold\">ê²°ê³¼</span>: ëª¨ë¸ í›ˆë ¨ ì‹œ ë…¸ì´ì¦ˆ ìµœì†Œí™” â†’ í•™ìŠµ íš¨ìœ¨ì„± ë° ì •í™•ë„ í–¥ìƒ.                                                  \n",
       "\n",
       "                                          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">ë‹¤. ê°œì¸ì •ë³´ ë³´í˜¸ ë° ê·œì • ì¤€ìˆ˜</span>                                           \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span><span style=\"font-weight: bold\">PII(ê°œì¸ ì‹ë³„ ì •ë³´) íƒì§€</span>:                                                                                       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    â€¢ </span>ì •ê·œí‘œí˜„ì‹, NLP ëª¨ë¸ì„ í™œìš©í•´ ì´ë¦„, ì „í™”ë²ˆí˜¸, ì´ë©”ì¼ ë“±ì„ ì‹ë³„í•˜ê³  ë§ˆìŠ¤í‚¹ ë˜ëŠ” ì‚­ì œ.                         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    â€¢ </span>GDPR, CCPA ë“± ë°ì´í„° ë³´í˜¸ ê·œì • ì¤€ìˆ˜ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.                                                            \n",
       "\n",
       "                                             <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">ë¼. ìë™í™”ë¥¼ í†µí•œ íš¨ìœ¨ì„±</span>                                              \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span><span style=\"font-weight: bold\">ë¶„ì‚° ì²˜ë¦¬</span>: NVIDIA GPU ë° ë‹¤ì¤‘ ë…¸ë“œ í´ëŸ¬ìŠ¤í„°ë¥¼ í™œìš©í•´ ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¥¼ ë¹ ë¥´ê²Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.                         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span><span style=\"font-weight: bold\">ì¬í˜„ì„±</span>: íŒŒì´í”„ë¼ì¸ ì„¤ì •ì„ ì½”ë“œë¡œ ê´€ë¦¬í•˜ì—¬ ì‹¤í—˜ ì¬í˜„ ë° í˜‘ì—…ì´ ìš©ì´í•©ë‹ˆë‹¤.                                       \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>\n",
       "                                           <span style=\"font-weight: bold\">5. ê²°ë¡ : NeMo Curatorì˜ ì¥ì </span>                                            \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span><span style=\"font-weight: bold\">ì‹œê°„ ë° ë¹„ìš© ì ˆê°</span>: ìˆ˜ë™ íë ˆì´ì…˜ì— ë“œëŠ” ë¦¬ì†ŒìŠ¤ë¥¼ 90% ì´ìƒ ê°ì†Œì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.                                 \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span><span style=\"font-weight: bold\">ê³ í’ˆì§ˆ ë°ì´í„° í™•ë³´</span>: ì—„ê²©í•œ í•„í„°ë§ê³¼ ì»¤ìŠ¤í…€ ì „ì²˜ë¦¬ë¡œ ëª¨ë¸ì˜ ì‹ ë¢°ì„± í–¥ìƒ.                                         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span><span style=\"font-weight: bold\">ê·œëª¨ í™•ì¥</span>: í´ë¼ìš°ë“œ ë˜ëŠ” ì˜¨í”„ë ˆë¯¸ìŠ¤ ì¸í”„ë¼ì—ì„œ í˜íƒ€ë°”ì´íŠ¸ ë°ì´í„° ì²˜ë¦¬ ê°€ëŠ¥.                                     \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>\n",
       "ì´ ì„¹ì…˜ì€ NeMo Curatorê°€ LLM ê°œë°œìì—ê²Œ <span style=\"font-weight: bold\">ë°ì´í„° íë ˆì´ì…˜ì˜ ë³µì¡ì„±ì„ í•´ê²°</span>í•˜ê³ , <span style=\"font-weight: bold\">ë¹„ì¦ˆë‹ˆìŠ¤ ëª©ì ì— ë§ëŠ” ë°ì´í„°ì…‹ì„ êµ¬ì¶•</span>í• \n",
       "ìˆ˜ ìˆëŠ” ì‹¤ìš©ì  ë„êµ¬ì„ì„ ê°•ì¡°í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ê°œë°œìëŠ” ëª¨ë¸ ì•„í‚¤í…ì²˜ ìµœì í™”ì™€ ê°™ì€ í•µì‹¬ ê³¼ì œì— ì§‘ì¤‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>\n",
       "\n",
       "                                                     <span style=\"font-weight: bold; text-decoration: underline\">Overview</span>                                                      \n",
       "\n",
       "Error: Failed to explain section Overview with both APIs                                                           \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>\n",
       "\n",
       "                                                   <span style=\"font-weight: bold; text-decoration: underline\">Prerequisite</span>                                                    \n",
       "\n",
       "Error: Failed to explain section Prerequisite with both APIs                                                       \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>\n",
       "\n",
       "                                         <span style=\"font-weight: bold; text-decoration: underline\">Defining custom document builders</span>                                         \n",
       "\n",
       "Error: Failed to explain section Defining custom document builders with both APIs                                  \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>\n",
       "\n",
       "                                        <span style=\"font-weight: bold; text-decoration: underline\">Downloading the TinyStories dataset</span>                                        \n",
       "\n",
       "Error: Failed to explain section Downloading the TinyStories dataset with both APIs                                \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>\n",
       "\n",
       "                                           <span style=\"font-weight: bold; text-decoration: underline\">Text cleaning and unification</span>                                           \n",
       "\n",
       "Error: Failed to explain section Text cleaning and unification with both APIs                                      \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>\n",
       "\n",
       "                                                 <span style=\"font-weight: bold; text-decoration: underline\">Dataset filtering</span>                                                 \n",
       "\n",
       "Error: Failed to explain section Dataset filtering with both APIs                                                  \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>\n",
       "\n",
       "                                                   <span style=\"font-weight: bold; text-decoration: underline\">Deduplication</span>                                                   \n",
       "\n",
       "Error: Failed to explain section Deduplication with both APIs                                                      \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>\n",
       "\n",
       "                                                   <span style=\"font-weight: bold; text-decoration: underline\">PII redaction</span>                                                   \n",
       "\n",
       "Error: Failed to explain section PII redaction with both APIs                                                      \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>\n",
       "\n",
       "                                      <span style=\"font-weight: bold; text-decoration: underline\">Putting the curation pipeline together</span>                                       \n",
       "\n",
       "Error: Failed to explain section Putting the curation pipeline together with both APIs                             \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>\n",
       "\n",
       "                                                    <span style=\"font-weight: bold; text-decoration: underline\">Next steps</span>                                                     \n",
       "\n",
       "Error: Failed to explain section Next steps with both APIs                                                         \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "                        \u001b[1;4mCurating Custom Datasets for LLM Training with NVIDIA NeMo Curator\u001b[0m                         \n",
       "\n",
       "                  \u001b[1mì„¹ì…˜ ì„¤ëª…: \"Curating Custom Datasets for LLM Training with NVIDIA NeMo Curator\"\u001b[0m                  \n",
       "\n",
       "ì´ ì„¹ì…˜ì€ \u001b[1mëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)\u001b[0m ë° ì†Œí˜• ì–¸ì–´ ëª¨ë¸(SLM) í›ˆë ¨ì„ ìœ„í•œ ë°ì´í„° íë ˆì´ì…˜ì˜ ì¤‘ìš”ì„±ê³¼ ì´ë¥¼ ì§€ì›í•˜ëŠ” \u001b[1mNVIDIA \u001b[0m\n",
       "\u001b[1mNeMo Curator\u001b[0mì˜ ê¸°ëŠ¥ì„ ìƒì„¸íˆ ì†Œê°œí•©ë‹ˆë‹¤. ë°ì´í„° íë ˆì´ì…˜ì€ ëª¨ë¸ ì„±ëŠ¥ì„ ê²°ì •í•˜ëŠ” í•µì‹¬ ë‹¨ê³„ë¡œ, NeMo CuratorëŠ” ê³ í’ˆì§ˆ \n",
       "ë°ì´í„°ì…‹ êµ¬ì¶•ì„ ìœ„í•œ íš¨ìœ¨ì ì´ê³  ìœ ì—°í•œ í”„ë ˆì„ì›Œí¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì•„ë˜ëŠ” ì£¼ìš” ë‚´ìš©ì„ êµ¬ì¡°í™”í•œ ì„¤ëª…ì…ë‹ˆë‹¤.            \n",
       "\n",
       "\u001b[33mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
       "                                            \u001b[1m1. \u001b[0m\u001b[1më°ì´í„° íë ˆì´ì…˜ì˜ ì¤‘ìš”ì„±\u001b[0m                                            \n",
       "\n",
       "\u001b[1;33m â€¢ \u001b[0m\u001b[1mLLM/SLM í›ˆë ¨ì˜ ì²« ë‹¨ê³„ì´ì ê°€ì¥ ì¤‘ìš”í•œ ë‹¨ê³„\u001b[0m                                                                     \n",
       "\u001b[1;33m   \u001b[0mëª¨ë¸ì˜ ì„±ëŠ¥ì€ í•™ìŠµ ë°ì´í„°ì˜ í’ˆì§ˆì— ì§ì ‘ì ìœ¼ë¡œ ì˜í–¥ì„ ë°›ìŠµë‹ˆë‹¤. ë…¸ì´ì¦ˆê°€ ë§ê±°ë‚˜ ì¤‘ë³µëœ ë°ì´í„°ëŠ” ëª¨ë¸ì˜ ì •í™•ë„ì™€  \n",
       "\u001b[1;33m   \u001b[0mì¼ë°˜í™” ëŠ¥ë ¥ì„ ì €í•´í•˜ë¯€ë¡œ, ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬ ê³¼ì •ì´ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤.                                         \n",
       "\u001b[1;33m â€¢ \u001b[0m\u001b[1mê³ í’ˆì§ˆ ë°ì´í„°ì˜ í•µì‹¬ ìš”ì†Œ\u001b[0m                                                                                       \n",
       "\u001b[1;33m   \u001b[0më°ì´í„°ì˜ ë‹¤ì–‘ì„±, ì •í™•ì„±, ì¤‘ë³µ ì œê±°, ê°œì¸ì •ë³´ ë³´í˜¸(PII ì²˜ë¦¬) ë“±ì´ í•„ìˆ˜ì ì…ë‹ˆë‹¤.                                  \n",
       "\n",
       "\u001b[33mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
       "                                            \u001b[1m2. \u001b[0m\u001b[1mNVIDIA NeMo Curator ì†Œê°œ\u001b[0m                                            \n",
       "\n",
       "\u001b[1;33m â€¢ \u001b[0m\u001b[1mëª©ì \u001b[0m: ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì„ ìë™í™”ëœ ì›Œí¬í”Œë¡œìš°ë¡œ ì²˜ë¦¬í•˜ì—¬ ê³ í’ˆì§ˆ í›ˆë ¨ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.                         \n",
       "\u001b[1;33m â€¢ \u001b[0m\u001b[1míŠ¹ì§•\u001b[0m:                                                                                                           \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m â€¢ \u001b[0m\u001b[1mì˜¤í”ˆì†ŒìŠ¤ í”„ë ˆì„ì›Œí¬\u001b[0m: ê°œë°œìê°€ ììœ ë¡­ê²Œ ì»¤ìŠ¤í„°ë§ˆì´ì§• ê°€ëŠ¥.                                                    \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m â€¢ \u001b[0m\u001b[1mNVIDIA NeMo ìƒíƒœê³„ í†µí•©\u001b[0m: ì—”ë“œíˆ¬ì—”ë“œ ëª¨ë¸ ê°œë°œ íŒŒì´í”„ë¼ì¸(í›ˆë ¨-ìµœì í™”-ë°°í¬)ê³¼ ì—°ë™ë©ë‹ˆë‹¤.                     \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m â€¢ \u001b[0m\u001b[1mí™•ì¥ì„±\u001b[0m: ë¶„ì‚° ì»´í“¨íŒ…ì„ ì§€ì›í•˜ì—¬ í˜íƒ€ë°”ì´íŠ¸ ê·œëª¨ì˜ ë°ì´í„°(ì˜ˆ: Common Crawl)ë„ ì²˜ë¦¬ ê°€ëŠ¥.                       \n",
       "\n",
       "\u001b[33mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
       "                                            \u001b[1m3. \u001b[0m\u001b[1mNeMo Curatorì˜ ì£¼ìš” ê¸°ëŠ¥\u001b[0m                                            \n",
       "\n",
       "                                      \u001b[1;2mê°€. \u001b[0m\u001b[1;2mê¸°ë³¸ ì œê³µ ë°ì´í„° ì†ŒìŠ¤ ë° ì›Œí¬í”Œë¡œìš°\u001b[0m                                      \n",
       "\n",
       "\u001b[1;33m â€¢ \u001b[0m\u001b[1mê³µê°œ ë°ì´í„°ì…‹ ì§€ì›\u001b[0m: Common Crawl, Wikipedia, arXiv ë“± ëŒ€í‘œì ì¸ ì†ŒìŠ¤ì—ì„œ ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬í•˜ëŠ” ì‚¬ì „   \n",
       "\u001b[1;33m   \u001b[0mêµ¬ì¶• íŒŒì´í”„ë¼ì¸ì„ ì œê³µí•©ë‹ˆë‹¤.                                                                                   \n",
       "\u001b[1;33m â€¢ \u001b[0m\u001b[1mì „ì²˜ë¦¬ ë‹¨ê³„ ì˜ˆì‹œ\u001b[0m:                                                                                               \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m â€¢ \u001b[0m\u001b[1mí•„í„°ë§\u001b[0m: í’ˆì§ˆ ê¸°ì¤€(ì˜ˆ: ë¬¸ë²• ì˜¤ë¥˜, ì˜ë¯¸ ì—†ëŠ” í…ìŠ¤íŠ¸)ì— ë§ì§€ ì•ŠëŠ” ë°ì´í„° ì œê±°.                                  \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m â€¢ \u001b[0m\u001b[1mì¤‘ë³µ ì œê±°\u001b[0m: ë¬¸ì„œ/ë¬¸ì¥ ìˆ˜ì¤€ì˜ ì¤‘ë³µ ë°ì´í„° ì‹ë³„ ë° ì‚­ì œ.                                                        \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m â€¢ \u001b[0m\u001b[1mì–¸ì–´ ì‹ë³„\u001b[0m: íŠ¹ì • ì–¸ì–´(ì˜ˆ: ì˜ì–´) ë°ì´í„°ë§Œ ì„ ë³„.                                                                \n",
       "\n",
       "                                         \u001b[1;2më‚˜. \u001b[0m\u001b[1;2mì»¤ìŠ¤í…€ ë°ì´í„° íŒŒì´í”„ë¼ì¸ êµ¬ì¶•\u001b[0m                                         \n",
       "\n",
       "\u001b[1;33m â€¢ \u001b[0m\u001b[1mìœ ì—°í•œ í™•ì¥ì„±\u001b[0m: ê°œë°œìëŠ” ìì²´ ë°ì´í„° ì†ŒìŠ¤ë¥¼ ì¶”ê°€í•˜ê±°ë‚˜ ì „ì²˜ë¦¬ ë‹¨ê³„ë¥¼ ìˆ˜ì •í•˜ì—¬ íŠ¹ì • ë„ë©”ì¸(ì˜ë£Œ, ë²•ë¥  ë“±)ì— ë§ëŠ”  \n",
       "\u001b[1;33m   \u001b[0míŒŒì´í”„ë¼ì¸ì„ ì„¤ê³„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.                                                                                \n",
       "\u001b[1;33m â€¢ \u001b[0m\u001b[1mì‚¬ìš©ì ì •ì˜ ì˜ˆì‹œ\u001b[0m:                                                                                               \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m â€¢ \u001b[0m\u001b[1më„ë©”ì¸ íŠ¹í™” í•„í„°\u001b[0m: ê¸ˆìœµ ë°ì´í„°ì—ì„œ ìˆ«ì/í†µê³„ ê´€ë ¨ í…ìŠ¤íŠ¸ ê°•ì¡° ì¶”ì¶œ.                                           \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m â€¢ \u001b[0m\u001b[1më§ì¶¤í˜• ì¤‘ë³µ ê²€ì¶œ\u001b[0m: ì‚¬ìš©ì ì •ì˜ í•´ì‹œ í•¨ìˆ˜ë¡œ ì¤‘ë³µ ë¬¸ì„œ ì‹ë³„.                                                    \n",
       "\n",
       "\u001b[33mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
       "                                          \u001b[1m4. \u001b[0m\u001b[1mNeMo Curatorë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ \u001b[0m                                          \n",
       "\n",
       "                                        \u001b[1;2mê°€. \u001b[0m\u001b[1;2mí”„ë¡œì íŠ¸ ë§ì¶¤í˜• ë°ì´í„° íë ˆì´ì…˜\u001b[0m                                        \n",
       "\n",
       "\u001b[1;33m â€¢ \u001b[0m\u001b[1mëª©ì ì— ë”°ë¥¸ ìµœì í™”\u001b[0m: ìƒì„±í˜• AI ëª¨ë¸ì˜ ìš©ë„(ì±—ë´‡, ì½”ë“œ ìƒì„± ë“±)ì— ë”°ë¼ ë°ì´í„° íŠ¹ì„±ì„ ì¡°ì ˆí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.          \n",
       "\u001b[1;33m   \u001b[0m(ì˜ˆ: ì½”ë“œ ìƒì„± ëª¨ë¸ì„ ìœ„í•´ GitHub ë°ì´í„° ê°•ì¡° ìˆ˜ì§‘)                                                             \n",
       "\n",
       "                                               \u001b[1;2më‚˜. \u001b[0m\u001b[1;2më°ì´í„° í’ˆì§ˆ ë³´ì¥\u001b[0m                                                \n",
       "\n",
       "\u001b[1;33m â€¢ \u001b[0m\u001b[1më‹¤ë‹¨ê³„ í•„í„°ë§\u001b[0m:                                                                                                  \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m â€¢ \u001b[0m\u001b[1mí’ˆì§ˆ ì ìˆ˜ ê¸°ë°˜ í•„í„°\u001b[0m: íœ´ë¦¬ìŠ¤í‹± ë˜ëŠ” ML ëª¨ë¸ì„ í™œìš©í•´ ì €í’ˆì§ˆ í…ìŠ¤íŠ¸(ìŠ¤íŒ¸, ë¬´ì˜ë¯¸í•œ ë‚´ìš©) ì œê±°.                 \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m â€¢ \u001b[0m\u001b[1mì •êµí•œ ì¤‘ë³µ ì œê±°\u001b[0m: SimHash, MinHash ë“±ì˜ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ë¬¸ì„œ/í† í° ìˆ˜ì¤€ ì¤‘ë³µ ê°ì†Œ.                               \n",
       "\u001b[1;33m â€¢ \u001b[0m\u001b[1mê²°ê³¼\u001b[0m: ëª¨ë¸ í›ˆë ¨ ì‹œ ë…¸ì´ì¦ˆ ìµœì†Œí™” â†’ í•™ìŠµ íš¨ìœ¨ì„± ë° ì •í™•ë„ í–¥ìƒ.                                                  \n",
       "\n",
       "                                          \u001b[1;2më‹¤. \u001b[0m\u001b[1;2mê°œì¸ì •ë³´ ë³´í˜¸ ë° ê·œì • ì¤€ìˆ˜\u001b[0m                                           \n",
       "\n",
       "\u001b[1;33m â€¢ \u001b[0m\u001b[1mPII(ê°œì¸ ì‹ë³„ ì •ë³´) íƒì§€\u001b[0m:                                                                                       \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m â€¢ \u001b[0mì •ê·œí‘œí˜„ì‹, NLP ëª¨ë¸ì„ í™œìš©í•´ ì´ë¦„, ì „í™”ë²ˆí˜¸, ì´ë©”ì¼ ë“±ì„ ì‹ë³„í•˜ê³  ë§ˆìŠ¤í‚¹ ë˜ëŠ” ì‚­ì œ.                         \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m â€¢ \u001b[0mGDPR, CCPA ë“± ë°ì´í„° ë³´í˜¸ ê·œì • ì¤€ìˆ˜ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.                                                            \n",
       "\n",
       "                                             \u001b[1;2më¼. \u001b[0m\u001b[1;2mìë™í™”ë¥¼ í†µí•œ íš¨ìœ¨ì„±\u001b[0m                                              \n",
       "\n",
       "\u001b[1;33m â€¢ \u001b[0m\u001b[1më¶„ì‚° ì²˜ë¦¬\u001b[0m: NVIDIA GPU ë° ë‹¤ì¤‘ ë…¸ë“œ í´ëŸ¬ìŠ¤í„°ë¥¼ í™œìš©í•´ ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¥¼ ë¹ ë¥´ê²Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.                         \n",
       "\u001b[1;33m â€¢ \u001b[0m\u001b[1mì¬í˜„ì„±\u001b[0m: íŒŒì´í”„ë¼ì¸ ì„¤ì •ì„ ì½”ë“œë¡œ ê´€ë¦¬í•˜ì—¬ ì‹¤í—˜ ì¬í˜„ ë° í˜‘ì—…ì´ ìš©ì´í•©ë‹ˆë‹¤.                                       \n",
       "\n",
       "\u001b[33mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
       "                                           \u001b[1m5. \u001b[0m\u001b[1mê²°ë¡ : NeMo Curatorì˜ ì¥ì \u001b[0m                                            \n",
       "\n",
       "\u001b[1;33m â€¢ \u001b[0m\u001b[1mì‹œê°„ ë° ë¹„ìš© ì ˆê°\u001b[0m: ìˆ˜ë™ íë ˆì´ì…˜ì— ë“œëŠ” ë¦¬ì†ŒìŠ¤ë¥¼ 90% ì´ìƒ ê°ì†Œì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.                                 \n",
       "\u001b[1;33m â€¢ \u001b[0m\u001b[1mê³ í’ˆì§ˆ ë°ì´í„° í™•ë³´\u001b[0m: ì—„ê²©í•œ í•„í„°ë§ê³¼ ì»¤ìŠ¤í…€ ì „ì²˜ë¦¬ë¡œ ëª¨ë¸ì˜ ì‹ ë¢°ì„± í–¥ìƒ.                                         \n",
       "\u001b[1;33m â€¢ \u001b[0m\u001b[1mê·œëª¨ í™•ì¥\u001b[0m: í´ë¼ìš°ë“œ ë˜ëŠ” ì˜¨í”„ë ˆë¯¸ìŠ¤ ì¸í”„ë¼ì—ì„œ í˜íƒ€ë°”ì´íŠ¸ ë°ì´í„° ì²˜ë¦¬ ê°€ëŠ¥.                                     \n",
       "\n",
       "\u001b[33mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
       "ì´ ì„¹ì…˜ì€ NeMo Curatorê°€ LLM ê°œë°œìì—ê²Œ \u001b[1më°ì´í„° íë ˆì´ì…˜ì˜ ë³µì¡ì„±ì„ í•´ê²°\u001b[0mí•˜ê³ , \u001b[1më¹„ì¦ˆë‹ˆìŠ¤ ëª©ì ì— ë§ëŠ” ë°ì´í„°ì…‹ì„ êµ¬ì¶•\u001b[0mí• \n",
       "ìˆ˜ ìˆëŠ” ì‹¤ìš©ì  ë„êµ¬ì„ì„ ê°•ì¡°í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ê°œë°œìëŠ” ëª¨ë¸ ì•„í‚¤í…ì²˜ ìµœì í™”ì™€ ê°™ì€ í•µì‹¬ ê³¼ì œì— ì§‘ì¤‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n",
       "\n",
       "\u001b[33mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
       "\n",
       "                                                     \u001b[1;4mOverview\u001b[0m                                                      \n",
       "\n",
       "Error: Failed to explain section Overview with both APIs                                                           \n",
       "\n",
       "\u001b[33mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
       "\n",
       "                                                   \u001b[1;4mPrerequisite\u001b[0m                                                    \n",
       "\n",
       "Error: Failed to explain section Prerequisite with both APIs                                                       \n",
       "\n",
       "\u001b[33mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
       "\n",
       "                                         \u001b[1;4mDefining custom document builders\u001b[0m                                         \n",
       "\n",
       "Error: Failed to explain section Defining custom document builders with both APIs                                  \n",
       "\n",
       "\u001b[33mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
       "\n",
       "                                        \u001b[1;4mDownloading the TinyStories dataset\u001b[0m                                        \n",
       "\n",
       "Error: Failed to explain section Downloading the TinyStories dataset with both APIs                                \n",
       "\n",
       "\u001b[33mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
       "\n",
       "                                           \u001b[1;4mText cleaning and unification\u001b[0m                                           \n",
       "\n",
       "Error: Failed to explain section Text cleaning and unification with both APIs                                      \n",
       "\n",
       "\u001b[33mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
       "\n",
       "                                                 \u001b[1;4mDataset filtering\u001b[0m                                                 \n",
       "\n",
       "Error: Failed to explain section Dataset filtering with both APIs                                                  \n",
       "\n",
       "\u001b[33mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
       "\n",
       "                                                   \u001b[1;4mDeduplication\u001b[0m                                                   \n",
       "\n",
       "Error: Failed to explain section Deduplication with both APIs                                                      \n",
       "\n",
       "\u001b[33mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
       "\n",
       "                                                   \u001b[1;4mPII redaction\u001b[0m                                                   \n",
       "\n",
       "Error: Failed to explain section PII redaction with both APIs                                                      \n",
       "\n",
       "\u001b[33mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
       "\n",
       "                                      \u001b[1;4mPutting the curation pipeline together\u001b[0m                                       \n",
       "\n",
       "Error: Failed to explain section Putting the curation pipeline together with both APIs                             \n",
       "\n",
       "\u001b[33mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
       "\n",
       "                                                    \u001b[1;4mNext steps\u001b[0m                                                     \n",
       "\n",
       "Error: Failed to explain section Next steps with both APIs                                                         \n",
       "\n",
       "\u001b[33mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "explanation_path, qa = process_text(\"input_files/Curating Custom Datasets for LLM Training with NVIDIA NeMo Curator.md\")\n",
    "\n",
    "markdown_printer = MarkdownPrinter()\n",
    "markdown_printer.print_markdown_file(explanation_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\"Scaling Language Models: Methods, Analysis & Insights from Training Gopher\"** ë…¼ë¬¸ì—ì„œ ì„¤ëª…ëœ **ê³ í’ˆì§ˆ ë°ì´í„° í•„í„°ë§ ë©”ì»¤ë‹ˆì¦˜**ì— ëŒ€í•´ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ì´ ë…¼ë¬¸ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ í›ˆë ¨ì„ ìœ„í•œ ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬ ë°©ë²•ì„ ë‹¤ë£¨ë©°, NeMo Data Curatorì—ì„œë„ ìœ ì‚¬í•œ ì ‘ê·¼ ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤([ê´€ë ¨ ì„¹ì…˜](#document-level_quality_filtering)).\n",
      "\n",
      "---\n",
      "\n",
      "### **1. ê³ í’ˆì§ˆ ë°ì´í„° í•„í„°ë§ì˜ í•„ìš”ì„±**\n",
      "- **ë¬¸ì œì **:  \n",
      "  ì›¹ í¬ë¡¤ë§ ë°ì´í„°(ì˜ˆ: Common Crawl)ì—ëŠ” **ì €í’ˆì§ˆ ì½˜í…ì¸ **(ì˜ˆ: ë°˜ë³µ ë¬¸ìì—´, ë¬´ì˜ë¯¸í•œ í…ìŠ¤íŠ¸, ìƒìš©êµ¬)ê°€ ë‹¤ëŸ‰ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.  \n",
      "  - ì´ëŸ¬í•œ ë°ì´í„°ëŠ” ëª¨ë¸ì˜ **ì¼ë°˜í™” ëŠ¥ë ¥**ê³¼ **ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ íƒœìŠ¤í¬ ì„±ëŠ¥**ì„ ì €í•˜ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
      "\n",
      "- **í•´ê²°ì±…**:  \n",
      "  - **ë¶„ë¥˜ê¸°(Classifier)**ì™€ **íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ í•„í„°**ë¥¼ ì‚¬ìš©í•´ ê³ í’ˆì§ˆ í…ìŠ¤íŠ¸ë§Œ ì„ ë³„í•©ë‹ˆë‹¤.  \n",
      "\n",
      "---\n",
      "\n",
      "### **2. í•„í„°ë§ ë©”ì»¤ë‹ˆì¦˜**\n",
      "#### **(1) íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ í•„í„°**\n",
      "- **ëª©ì **:  \n",
      "  ê°„ë‹¨í•œ ê·œì¹™ì„ ì‚¬ìš©í•´ ì €í’ˆì§ˆ í…ìŠ¤íŠ¸ë¥¼ ë¹ ë¥´ê²Œ ì‹ë³„í•©ë‹ˆë‹¤.  \n",
      "- **ì£¼ìš” ê·œì¹™**:  \n",
      "  - **ë¬¸ì¥ ê¸¸ì´**: ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ ë¬¸ì¥ì„ ì œê±°í•©ë‹ˆë‹¤.  \n",
      "  - **íŠ¹ìˆ˜ ë¬¸ì ë¹„ìœ¨**: íŠ¹ìˆ˜ ë¬¸ì(ì˜ˆ: URL, ê¸°í˜¸)ê°€ ê³¼ë„í•˜ê²Œ í¬í•¨ëœ í…ìŠ¤íŠ¸ë¥¼ ì œê±°í•©ë‹ˆë‹¤.  \n",
      "  - **ë°˜ë³µ ë¬¸ìì—´**: ë™ì¼í•œ ë‹¨ì–´ë‚˜ êµ¬ê°€ ë°˜ë³µë˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ ì œê±°í•©ë‹ˆë‹¤.  \n",
      "  - **ì–¸ì–´ ê°ì§€**: ëª©í‘œ ì–¸ì–´ì™€ ì¼ì¹˜í•˜ì§€ ì•ŠëŠ” í…ìŠ¤íŠ¸ë¥¼ ì œê±°í•©ë‹ˆë‹¤.  \n",
      "\n",
      "- **ì¥ì **:  \n",
      "  - ê³„ì‚° ë¹„ìš©ì´ ë‚®ê³ , ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì—ì„œ ë¹ ë¥´ê²Œ ì ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.  \n",
      "\n",
      "#### **(2) ë¶„ë¥˜ê¸°(Classifier) ê¸°ë°˜ í•„í„°**\n",
      "- **ëª©ì **:  \n",
      "  ë” ì •êµí•œ ë°©ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ì˜ í’ˆì§ˆì„ í‰ê°€í•©ë‹ˆë‹¤.  \n",
      "- **í•™ìŠµ ë°ì´í„°**:  \n",
      "  - ê³ í’ˆì§ˆ í…ìŠ¤íŠ¸(ì˜ˆ: ìœ„í‚¤í”¼ë””ì•„, ì „ë¬¸ ë„ë©”ì¸ ë¬¸ì„œ)ì™€ ì €í’ˆì§ˆ í…ìŠ¤íŠ¸(ì˜ˆ: ìŠ¤íŒ¸, ë¬´ì˜ë¯¸í•œ ì½˜í…ì¸ )ë¥¼ ë ˆì´ë¸”ë§í•´ ë¶„ë¥˜ê¸°ë¥¼ í›ˆë ¨í•©ë‹ˆë‹¤.  \n",
      "- **ëª¨ë¸ êµ¬ì¡°**:  \n",
      "  - **BERT** ë˜ëŠ” **RoBERTa**ì™€ ê°™ì€ ì‚¬ì „ í›ˆë ¨ëœ ì–¸ì–´ ëª¨ë¸ì„ ì‚¬ìš©í•´ í…ìŠ¤íŠ¸ì˜ í’ˆì§ˆì„ ë¶„ë¥˜í•©ë‹ˆë‹¤.  \n",
      "- **í‰ê°€ ì§€í‘œ**:  \n",
      "  - í…ìŠ¤íŠ¸ì˜ **ê°€ë…ì„±**, **ì •ë³´ì„±**, **ê´€ë ¨ì„±** ë“±ì„ ì ìˆ˜í™”í•©ë‹ˆë‹¤.  \n",
      "\n",
      "- **ì¥ì **:  \n",
      "  - íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ í•„í„°ë³´ë‹¤ ë” ì •í™•í•˜ê²Œ ì €í’ˆì§ˆ í…ìŠ¤íŠ¸ë¥¼ ì‹ë³„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
      "\n",
      "---\n",
      "\n",
      "### **3. NeMo Data Curatorì—ì„œì˜ ì ìš©**\n",
      "- **êµ¬ì„± ê°€ëŠ¥í•œ í•„í„°**:  \n",
      "  - ì‚¬ìš©ìê°€ ì •ì˜í•œ íœ´ë¦¬ìŠ¤í‹± ê·œì¹™ì„ ì ìš©í•´ ì €í’ˆì§ˆ í…ìŠ¤íŠ¸ë¥¼ ì œê±°í•©ë‹ˆë‹¤.  \n",
      "  - ì˜ˆ: íŠ¹ìˆ˜ ë¬¸ì ë¹„ìœ¨, ë¬¸ì¥ ê¸¸ì´, ì–¸ì–´ ê°ì§€ ë“±ì„ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
      "\n",
      "- **ë¶„ë¥˜ê¸° ê¸°ë°˜ í•„í„°**:  \n",
      "  - ì‚¬ì „ í›ˆë ¨ëœ ë¶„ë¥˜ê¸°ë¥¼ ì‚¬ìš©í•´ í…ìŠ¤íŠ¸ì˜ í’ˆì§ˆì„ í‰ê°€í•©ë‹ˆë‹¤.  \n",
      "  - ì˜ˆ: [ê´€ë ¨ ì—°êµ¬](https://arxiv.org/abs/2112.11446)ì—ì„œ ì œì•ˆëœ í•„í„°ë¥¼ ì ìš©í•©ë‹ˆë‹¤.  \n",
      "\n",
      "- **íš¨ê³¼**:  \n",
      "  - í•„í„°ë§ì„ í†µí•´ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ íƒœìŠ¤í¬ ì„±ëŠ¥ì´ ê°œì„ ë©ë‹ˆë‹¤([ê´€ë ¨ ì„¹ì…˜](#curated_pretraining_data_results_in_improved_model_downstream_performance)).  \n",
      "\n",
      "---\n",
      "\n",
      "### **4. ê²°ë¡ **\n",
      "- **íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ í•„í„°**ëŠ” ê°„ë‹¨í•œ ê·œì¹™ì„ ì‚¬ìš©í•´ ì €í’ˆì§ˆ í…ìŠ¤íŠ¸ë¥¼ ë¹ ë¥´ê²Œ ì œê±°í•©ë‹ˆë‹¤.  \n",
      "- **ë¶„ë¥˜ê¸° ê¸°ë°˜ í•„í„°**ëŠ” ë” ì •êµí•œ ë°©ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ì˜ í’ˆì§ˆì„ í‰ê°€í•©ë‹ˆë‹¤.  \n",
      "- NeMo Data CuratorëŠ” ë‘ ë°©ì‹ì„ ëª¨ë‘ í™œìš©í•´ **ê³ í’ˆì§ˆ ë°ì´í„°ì…‹**ì„ êµ¬ì¶•í•˜ë©°, ì´ë¥¼ í†µí•´ **LLMì˜ ì„±ëŠ¥ì„ í–¥ìƒ**ì‹œí‚µë‹ˆë‹¤.  \n",
      "\n",
      "ì´ëŸ¬í•œ í•„í„°ë§ ë©”ì»¤ë‹ˆì¦˜ì€ **Scaling Language Models: Methods, Analysis & Insights from Training Gopher** ë…¼ë¬¸ì—ì„œë„ ìœ ì‚¬í•˜ê²Œ ì„¤ëª…ë˜ë©°, ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ í›ˆë ¨ì— í•„ìˆ˜ì ì¸ ìš”ì†Œë¡œ ê°•ì¡°ë©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\" \n",
    "ì´ì œ ë‹¤ë¥¸ ì§ˆë¬¸ì´ì•¼. \n",
    "\n",
    "ê¸€ì˜ ë‚´ìš©ì„ ë³´ë©´ ë¶„ë¥˜ê¸°(Classifier) ë˜ëŠ” íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ í•„í„°ë¥¼ ì‚¬ìš©í•´ ê³ í’ˆì§ˆ í…ìŠ¤íŠ¸ë§Œ ì„ ë³„í•œë‹¤ê³  í•˜ê³  ì°¸ê³  ë¬¸ì„œëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤ê³  ë§í–ˆì–´. \n",
    "\n",
    "Scaling Language Models: Methods, Analysis & Insights from Training Gopher\n",
    "\n",
    "ì´ ë…¼ë¬¸ì— ëŒ€í•´ ì•ˆë‹¤ë©´ ì–´ë–¤ ë§¤ì»¤ë‹ˆì¦˜ìœ¼ë¡œ ê³ í’ˆì§ˆ ë°ì´í„°ë¥¼ í•„í„°ë§í•˜ëŠ”ê±´ì§€ ì„¤ëª…í•´ì¤˜.\n",
    "\"\"\"\n",
    "\n",
    "response = qa.ask_question(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ëŒ€í™” ë‚´ì—­ ===\n",
      "\n",
      "\n",
      "ğŸ‘¤ User (14):\n",
      "Based on the text we discussed, please answer the following question in Korean. \n",
      "            Be specific and cite relevant sections when possible.\n",
      "\n",
      "            Question:  \n",
      "MinHashLSM ì•Œê³ ë¦¬ì¦˜ì˜ ì‘ë™ ì›ë¦¬ì— ëŒ€í•´ êµ¬ì²´ì ìœ¼ë¡œ ì•Œë ¤ì¤˜. \n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ğŸ¤– Assistant (15):\n",
      "**MinHashLSH(Locality-Sensitive Hashing) ì•Œê³ ë¦¬ì¦˜ì˜ ì‘ë™ ì›ë¦¬**ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ì´ ì•Œê³ ë¦¬ì¦˜ì€ ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì—ì„œ **ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì°¾ê¸° ìœ„í•´ ì„¤ê³„**ë˜ì—ˆìœ¼ë©°, NeMo Data Curatorì—ì„œë„ ìœ ì‚¬ ì¤‘ë³µ ì œê±°ì— í™œìš©ë˜ì—ˆìŠµë‹ˆë‹¤([ê´€ë ¨ ì„¹ì…˜](#document-level_deduplication)).\n",
      "\n",
      "---\n",
      "\n",
      "### **1. MinHashLSHì˜ ê¸°ë³¸ ê°œë…**\n",
      "- **ëª©ì **:  \n",
      "  ë‘ ë¬¸ì„œ ê°„ì˜ **Jaccard ìœ ì‚¬ë„**ë¥¼ ë¹ ë¥´ê²Œ ì¶”ì •í•˜ê³ , ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ê·¸ë£¹í™”í•©ë‹ˆë‹¤.  \n",
      "  - **Jaccard ìœ ì‚¬ë„**: ë‘ ì§‘í•©ì˜ êµì§‘í•© í¬ê¸°ë¥¼ í•©ì§‘í•© í¬ê¸°ë¡œ ë‚˜ëˆˆ ê°’.  \n",
      "    ì˜ˆ: ë¬¸ì„œ Aì™€ Bì˜ ë‹¨ì–´ ì§‘í•©ì´ ê°ê° {a, b, c}ì™€ {a, b, d}ë¼ë©´, Jaccard ìœ ì‚¬ë„ëŠ” 2/4 = 0.5ì…ë‹ˆë‹¤.  \n",
      "\n",
      "- **í•µì‹¬ ì•„ì´ë””ì–´**:  \n",
      "  - ë¬¸ì„œë¥¼ **í•´ì‹œ ê°’ìœ¼ë¡œ ë³€í™˜**í•´ ìœ ì‚¬ì„±ì„ ë¹ ë¥´ê²Œ ê³„ì‚°í•©ë‹ˆë‹¤.  \n",
      "  - **LSH(Locality-Sensitive Hashing)**: ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ê°™ì€ ë²„í‚·ì— ê·¸ë£¹í™”í•´, ì „ì²´ ë¬¸ì„œ ìŒì„ ë¹„êµí•˜ì§€ ì•Šê³ ë„ ìœ ì‚¬ì„±ì„ íŒë‹¨í•©ë‹ˆë‹¤.  \n",
      "\n",
      "---\n",
      "\n",
      "### **2. MinHashLSHì˜ ì‘ë™ ì›ë¦¬**\n",
      "#### **(1) MinHash ê³„ì‚°**\n",
      "1. **ë¬¸ì„œë¥¼ ì§‘í•©ìœ¼ë¡œ í‘œí˜„**:  \n",
      "   - ê° ë¬¸ì„œë¥¼ ë‹¨ì–´ ë˜ëŠ” n-gramì˜ ì§‘í•©ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.  \n",
      "   - ì˜ˆ: ë¬¸ì„œ A = {a, b, c}, ë¬¸ì„œ B = {a, b, d}.  \n",
      "\n",
      "2. **í•´ì‹œ í•¨ìˆ˜ ì ìš©**:  \n",
      "   - ì—¬ëŸ¬ ê°œì˜ í•´ì‹œ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ ê° ë‹¨ì–´ë¥¼ í•´ì‹œ ê°’ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.  \n",
      "   - ì˜ˆ: í•´ì‹œ í•¨ìˆ˜ hâ‚(x), hâ‚‚(x), ..., hâ‚–(x)ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.  \n",
      "\n",
      "3. **MinHash ê°’ ê³„ì‚°**:  \n",
      "   - ê° í•´ì‹œ í•¨ìˆ˜ì— ëŒ€í•´, ë¬¸ì„œì˜ ë‹¨ì–´ ì§‘í•© ì¤‘ **ê°€ì¥ ì‘ì€ í•´ì‹œ ê°’**ì„ ì„ íƒí•©ë‹ˆë‹¤.  \n",
      "   - ì˜ˆ: ë¬¸ì„œ Aì˜ ë‹¨ì–´ {a, b, c}ì— ëŒ€í•´ hâ‚(a), hâ‚(b), hâ‚(c) ì¤‘ ìµœì†Œê°’ì„ MinHashë¡œ ì„ íƒí•©ë‹ˆë‹¤.  \n",
      "   - ì´ ê³¼ì •ì„ kê°œì˜ í•´ì‹œ í•¨ìˆ˜ì— ëŒ€í•´ ë°˜ë³µí•´, ë¬¸ì„œë§ˆë‹¤ kê°œì˜ MinHash ê°’ì„ ìƒì„±í•©ë‹ˆë‹¤.  \n",
      "\n",
      "#### **(2) LSHë¥¼ í†µí•œ ë²„í‚· ê·¸ë£¹í™”**\n",
      "1. **ë²„í‚· ìƒì„±**:  \n",
      "   - MinHash ê°’ì„ ì‚¬ìš©í•´ ë¬¸ì„œë¥¼ ë²„í‚·ì— í• ë‹¹í•©ë‹ˆë‹¤.  \n",
      "   - ì˜ˆ: MinHash ê°’ì„ íŠ¹ì • ë²”ìœ„ë¡œ ë‚˜ëˆ„ì–´ ë²„í‚·ì„ ìƒì„±í•©ë‹ˆë‹¤.  \n",
      "\n",
      "2. **ìœ ì‚¬ ë¬¸ì„œ ê·¸ë£¹í™”**:  \n",
      "   - ê°™ì€ ë²„í‚·ì— ì†í•œ ë¬¸ì„œëŠ” ìœ ì‚¬í•œ ë¬¸ì„œë¡œ ê°„ì£¼í•©ë‹ˆë‹¤.  \n",
      "   - ì´ë•Œ, **ê±°ì§“ ì–‘ì„±(false positives)**ì´ ë°œìƒí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ë²„í‚· ë‚´ ë¬¸ì„œ ê°„ ìœ ì‚¬ë„ë¥¼ ì¶”ê°€ë¡œ ê³„ì‚°í•´ ê²€ì¦í•©ë‹ˆë‹¤.  \n",
      "\n",
      "#### **(3) ìœ ì‚¬ë„ ê³„ì‚° ë° ì¤‘ë³µ ì œê±°**\n",
      "1. **ë²„í‚· ë‚´ ë¬¸ì„œ ë¹„êµ**:  \n",
      "   - ê°™ì€ ë²„í‚·ì— ì†í•œ ë¬¸ì„œ ìŒì— ëŒ€í•´ **Jaccard ìœ ì‚¬ë„**ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.  \n",
      "   - ì˜ˆ: ë¬¸ì„œ Aì™€ Bì˜ Jaccard ìœ ì‚¬ë„ê°€ ì„ê³„ê°’(ì˜ˆ: 0.8) ì´ìƒì´ë©´ ì¤‘ë³µìœ¼ë¡œ íŒë‹¨í•©ë‹ˆë‹¤.  \n",
      "\n",
      "2. **ì¤‘ë³µ ë¬¸ì„œ ì œê±°**:  \n",
      "   - ìœ ì‚¬ë„ê°€ ë†’ì€ ë¬¸ì„œ ì¤‘ í•˜ë‚˜ë¥¼ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ëŠ” ì œê±°í•©ë‹ˆë‹¤.  \n",
      "\n",
      "---\n",
      "\n",
      "### **3. NeMo Data Curatorì—ì„œì˜ ì ìš©**\n",
      "- **GPU ê°€ì†**:  \n",
      "  - MinHashLSHì˜ ê³„ì‚° ë¶€í•˜ë¥¼ ì¤„ì´ê¸° ìœ„í•´ **RAPIDS í”„ë ˆì„ì›Œí¬**ë¥¼ ì‚¬ìš©í•´ GPUì—ì„œ ë³‘ë ¬ ì²˜ë¦¬í•©ë‹ˆë‹¤.  \n",
      "  - ì˜ˆ: 4.5TB RedPajama ë°ì´í„°ì…‹ì˜ ì¤‘ë³µ ì œê±° ì‹œê°„ì„ CPU ëŒ€ë¹„ **12ë°° ë‹¨ì¶•**í–ˆìŠµë‹ˆë‹¤([ê´€ë ¨ ì„¹ì…˜](#document-level_deduplication)).  \n",
      "\n",
      "- **í™•ì¥ì„±**:  \n",
      "  - ìˆ˜ì²œ ê°œì˜ CPU ì½”ì–´ì—ì„œë„ ì„ í˜• í™•ì¥ì„±ì„ ë³´ì´ë©°, ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ ì²˜ë¦¬ì— ì í•©í•©ë‹ˆë‹¤([ê´€ë ¨ ì„¹ì…˜](#scaling_to_many_compute_cores)).  \n",
      "\n",
      "---\n",
      "\n",
      "### **4. ìš”ì•½**\n",
      "MinHashLSH ì•Œê³ ë¦¬ì¦˜ì€ ë‹¤ìŒê³¼ ê°™ì€ ë‹¨ê³„ë¡œ ì‘ë™í•©ë‹ˆë‹¤:  \n",
      "1. ë¬¸ì„œë¥¼ ë‹¨ì–´ ì§‘í•©ìœ¼ë¡œ í‘œí˜„í•˜ê³ , í•´ì‹œ í•¨ìˆ˜ë¥¼ ì ìš©í•´ MinHash ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤.  \n",
      "2. LSHë¥¼ ì‚¬ìš©í•´ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ê°™ì€ ë²„í‚·ì— ê·¸ë£¹í™”í•©ë‹ˆë‹¤.  \n",
      "3. ë²„í‚· ë‚´ ë¬¸ì„œ ê°„ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•´ ì¤‘ë³µ ë¬¸ì„œë¥¼ ì œê±°í•©ë‹ˆë‹¤.  \n",
      "\n",
      "ì´ ì•Œê³ ë¦¬ì¦˜ì€ **ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì—ì„œ ìœ ì‚¬ ì¤‘ë³µ ë¬¸ì„œë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì œê±°**í•˜ëŠ” ë° ì í•©í•˜ë©°, NeMo Data Curatorì—ì„œë„ ì„±ê³µì ìœ¼ë¡œ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ğŸ‘¤ User (16):\n",
      "Based on the text we discussed, please answer the following question in Korean. \n",
      "            Be specific and cite relevant sections when possible.\n",
      "\n",
      "            Question:  \n",
      "MinHashLSM ì•Œê³ ë¦¬ì¦˜ì˜ ì‘ë™ ì›ë¦¬ëŠ”ë¡œ K ê°œì˜ MinHash ë¥¼ ë§Œë“œëŠ” ê±´, ë‘ ë¬¸ì„œ ì‚¬ì´ì— K ê°œì˜ ê²¹ì¹˜ëŠ” ë‹¨ì–´ê°€ ìˆë‹¤ëŠ” ê²ƒì´ê³ , ì´ê²ƒìœ¼ë¡œ ì¤‘ë³µì´ë¼ê³  ê°€ì •ì„ í•˜ëŠ”ê±´ê°€? \n",
      "\n",
      "ê·¼ë° ê±°ì§“ ì–‘ì„±ì¼ ìˆ˜ ìˆìœ¼ë‹ˆê¹Œ í•œë²ˆ ë” Jaccard ìœ ì‚¬ë„ ë¹„êµë¥¼ í•˜ëŠ”ê±°ê³ ? ë‚´ê°€ ì˜ëª» ì´í•´í•œê±°ì•¼?\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ğŸ¤– Assistant (17):\n",
      "ì§ˆë¬¸ ì£¼ì‹  ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ **MinHashLSH ì•Œê³ ë¦¬ì¦˜ì˜ ì‘ë™ ì›ë¦¬**ë¥¼ ë‹¤ì‹œ í•œë²ˆ ëª…í™•íˆ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ì§ˆë¬¸ì—ì„œ ì–¸ê¸‰í•˜ì‹  ë¶€ë¶„ ì¤‘ ì¼ë¶€ëŠ” ì˜¤í•´ê°€ ìˆì„ ìˆ˜ ìˆìœ¼ë‹ˆ, ì´ë¥¼ ë°”ë¡œì¡ê³ ì í•©ë‹ˆë‹¤.\n",
      "\n",
      "---\n",
      "\n",
      "### **1. MinHashLSHì˜ í•µì‹¬ ê°œë…**\n",
      "- **MinHash ê°’ ìƒì„±**:  \n",
      "  - MinHashëŠ” ë¬¸ì„œì˜ ë‹¨ì–´ ì§‘í•©ì„ í•´ì‹œ ê°’ìœ¼ë¡œ ë³€í™˜í•œ í›„, ê° í•´ì‹œ í•¨ìˆ˜ì— ëŒ€í•´ **ê°€ì¥ ì‘ì€ í•´ì‹œ ê°’**ì„ ì„ íƒí•©ë‹ˆë‹¤.  \n",
      "  - ì´ë•Œ, **Kê°œì˜ í•´ì‹œ í•¨ìˆ˜**ë¥¼ ì‚¬ìš©í•´ Kê°œì˜ MinHash ê°’ì„ ìƒì„±í•©ë‹ˆë‹¤.  \n",
      "  - **Kê°œì˜ MinHash ê°’**ì€ ë¬¸ì„œì˜ ë‹¨ì–´ ì§‘í•©ì„ ëŒ€í‘œí•˜ëŠ” \"ì§€ë¬¸(fingerprint)\" ì—­í• ì„ í•©ë‹ˆë‹¤.  \n",
      "\n",
      "- **ì¤‘ë³µ ë¬¸ì„œ ê°€ì •**:  \n",
      "  - ë‘ ë¬¸ì„œê°€ **Kê°œì˜ MinHash ê°’ ì¤‘ ì¼ì • ë¹„ìœ¨ ì´ìƒ ì¼ì¹˜**í•˜ë©´, ë‘ ë¬¸ì„œëŠ” ìœ ì‚¬í•œ ê²ƒìœ¼ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤.  \n",
      "  - í•˜ì§€ë§Œ ì´ëŠ” **í™•ë¥ ì  ì¶”ì •**ì´ë¯€ë¡œ, ì™„ë²½í•œ ì •í™•ë„ë¥¼ ë³´ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.  \n",
      "\n",
      "---\n",
      "\n",
      "### **2. ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€**\n",
      "#### **(1) Kê°œì˜ MinHash ê°’ê³¼ ì¤‘ë³µ ë¬¸ì„œ ê°€ì •**\n",
      "- **Kê°œì˜ MinHash ê°’**ì€ ë‘ ë¬¸ì„œê°€ **Kê°œì˜ ê²¹ì¹˜ëŠ” ë‹¨ì–´**ë¥¼ ê°€ì§„ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.  \n",
      "  - MinHash ê°’ì€ ë¬¸ì„œì˜ ì „ì²´ ë‹¨ì–´ ì§‘í•©ì„ ëŒ€í‘œí•˜ëŠ” í•´ì‹œ ê°’ì˜ ìµœì†Œê°’ì…ë‹ˆë‹¤.  \n",
      "  - ë”°ë¼ì„œ, Kê°œì˜ MinHash ê°’ì´ ì¼ì¹˜í•œë‹¤ëŠ” ê²ƒì€ ë‘ ë¬¸ì„œì˜ **Jaccard ìœ ì‚¬ë„ê°€ ë†’ì„ í™•ë¥ **ì´ ë†’ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.  \n",
      "\n",
      "#### **(2) ê±°ì§“ ì–‘ì„±(False Positive) ë¬¸ì œ**\n",
      "- **ê±°ì§“ ì–‘ì„±**:  \n",
      "  - MinHashLSHëŠ” í™•ë¥ ì  ì•Œê³ ë¦¬ì¦˜ì´ë¯€ë¡œ, **ìœ ì‚¬í•˜ì§€ ì•Šì€ ë¬¸ì„œê°€ ê°™ì€ ë²„í‚·ì— ê·¸ë£¹í™”**ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
      "  - ì´ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´, ê°™ì€ ë²„í‚·ì— ì†í•œ ë¬¸ì„œ ìŒì— ëŒ€í•´ **Jaccard ìœ ì‚¬ë„**ë¥¼ ì¶”ê°€ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.  \n",
      "  - Jaccard ìœ ì‚¬ë„ê°€ ì„ê³„ê°’(ì˜ˆ: 0.8) ì´ìƒì¸ ê²½ìš°ì—ë§Œ ì¤‘ë³µ ë¬¸ì„œë¡œ íŒë‹¨í•©ë‹ˆë‹¤.  \n",
      "\n",
      "#### **(3) ì •ë¦¬**\n",
      "- **MinHashLSHì˜ ë‹¨ê³„**:  \n",
      "  1. Kê°œì˜ í•´ì‹œ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ ê° ë¬¸ì„œì˜ MinHash ê°’ì„ ìƒì„±í•©ë‹ˆë‹¤.  \n",
      "  2. ê°™ì€ ë²„í‚·ì— ì†í•œ ë¬¸ì„œëŠ” ìœ ì‚¬í•œ ë¬¸ì„œë¡œ ê°„ì£¼í•©ë‹ˆë‹¤.  \n",
      "  3. ë²„í‚· ë‚´ ë¬¸ì„œ ìŒì— ëŒ€í•´ Jaccard ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•´, ê±°ì§“ ì–‘ì„±ì„ ì œê±°í•©ë‹ˆë‹¤.  \n",
      "\n",
      "- **ì§ˆë¬¸ì˜ ì˜¤í•´**:  \n",
      "  - Kê°œì˜ MinHash ê°’ì€ **Kê°œì˜ ê²¹ì¹˜ëŠ” ë‹¨ì–´**ë¥¼ ì˜ë¯¸í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.  \n",
      "  - ëŒ€ì‹ , Kê°œì˜ í•´ì‹œ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ ë¬¸ì„œì˜ ìœ ì‚¬ì„±ì„ ì¶”ì •í•˜ëŠ” ì§€í‘œë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.  \n",
      "\n",
      "---\n",
      "\n",
      "### **3. NeMo Data Curatorì—ì„œì˜ ì ìš©**\n",
      "- **GPU ê°€ì†**:  \n",
      "  - MinHashLSHì˜ ê³„ì‚° ë¶€í•˜ë¥¼ ì¤„ì´ê¸° ìœ„í•´ **RAPIDS í”„ë ˆì„ì›Œí¬**ë¥¼ ì‚¬ìš©í•´ GPUì—ì„œ ë³‘ë ¬ ì²˜ë¦¬í•©ë‹ˆë‹¤.  \n",
      "  - ì˜ˆ: 4.5TB RedPajama ë°ì´í„°ì…‹ì˜ ì¤‘ë³µ ì œê±° ì‹œê°„ì„ CPU ëŒ€ë¹„ **12ë°° ë‹¨ì¶•**í–ˆìŠµë‹ˆë‹¤([ê´€ë ¨ ì„¹ì…˜](#document-level_deduplication)).  \n",
      "\n",
      "- **í™•ì¥ì„±**:  \n",
      "  - ìˆ˜ì²œ ê°œì˜ CPU ì½”ì–´ì—ì„œë„ ì„ í˜• í™•ì¥ì„±ì„ ë³´ì´ë©°, ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ ì²˜ë¦¬ì— ì í•©í•©ë‹ˆë‹¤([ê´€ë ¨ ì„¹ì…˜](#scaling_to_many_compute_cores)).  \n",
      "\n",
      "---\n",
      "\n",
      "### **4. ê²°ë¡ **\n",
      "- **MinHashLSH**ëŠ” ë¬¸ì„œì˜ ë‹¨ì–´ ì§‘í•©ì„ í•´ì‹œ ê°’ìœ¼ë¡œ ë³€í™˜í•´ ìœ ì‚¬ì„±ì„ ì¶”ì •í•˜ëŠ” í™•ë¥ ì  ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.  \n",
      "- Kê°œì˜ MinHash ê°’ì€ ë¬¸ì„œì˜ ìœ ì‚¬ì„±ì„ ëŒ€í‘œí•˜ëŠ” ì§€í‘œë¡œ, **Kê°œì˜ ê²¹ì¹˜ëŠ” ë‹¨ì–´**ë¥¼ ì˜ë¯¸í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.  \n",
      "- ê±°ì§“ ì–‘ì„±ì„ ë°©ì§€í•˜ê¸° ìœ„í•´, ë²„í‚· ë‚´ ë¬¸ì„œ ìŒì— ëŒ€í•´ **Jaccard ìœ ì‚¬ë„**ë¥¼ ì¶”ê°€ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.  \n",
      "\n",
      "ì´ë¥¼ í†µí•´ NeMo Data CuratorëŠ” ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì—ì„œë„ **ì •í™•í•˜ê³  íš¨ìœ¨ì ìœ¼ë¡œ ì¤‘ë³µ ë¬¸ì„œë¥¼ ì œê±°**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ğŸ‘¤ User (18):\n",
      "Based on the text we discussed, please answer the following question in Korean. \n",
      "            Be specific and cite relevant sections when possible.\n",
      "\n",
      "            Question:  \n",
      "MinHashLSM ì—ì„œ ë‹¨ì–´ ì§‘í•©ì„ í•´ì‹œë¡œ ë³€í™˜í•œë‹¤ëŠ” ê±´ ë§¤ ë‹¨ì–´ë§ˆë‹¤ í•´ì‹œí•¨ìˆ˜ë¡œ ë³€í™˜í•œë‹¤ëŠ”ê±°ê³ , ê·¸ë¦¬ê³  ì´ í•´ì‹œ ê°’ì¤‘ ê°€ì¥ ë‚®ì€ ê°’ì„ ìµœì†Œê°’ìœ¼ë¡œ ì„ íƒí•œë‹¤ëŠ”ê±°ì§€? \n",
      "\n",
      "ê·¸ë¦¬ê³  ì´ê±¸ Kë²ˆ ë°˜ë³µí•œë‹¤ëŠ”ê±°ê³ . ë¬¸ì„œê°€ ìœ ì‚¬í•˜ë‹¤ê³  ê°€ì •í•˜ëŠ”ê±°ê³ . \n",
      "\n",
      "ì´ê²Œ ë§ë‹¤ë©´ K ê°œì˜ ë‹¨ì–´ê°€ ë‘ ë¬¸ì„œì— ì¡´ì¬í•œë‹¤ ì¦‰ ê²¹ì¹œë‹¤ëŠ”ê±° ì•„ë‹˜? \n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ğŸ¤– Assistant (19):\n",
      "ì§ˆë¬¸ ì£¼ì‹  ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ **MinHashLSH ì•Œê³ ë¦¬ì¦˜**ì˜ ì‘ë™ ì›ë¦¬ë¥¼ ë‹¤ì‹œ í•œë²ˆ ëª…í™•íˆ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ì§ˆë¬¸ì—ì„œ ì–¸ê¸‰í•˜ì‹  ë¶€ë¶„ ì¤‘ ì¼ë¶€ëŠ” ì˜¤í•´ê°€ ìˆì„ ìˆ˜ ìˆìœ¼ë‹ˆ, ì´ë¥¼ ë°”ë¡œì¡ê³ ì í•©ë‹ˆë‹¤.\n",
      "\n",
      "---\n",
      "\n",
      "### **1. MinHashLSHì˜ í•µì‹¬ ê°œë…**\n",
      "- **ë‹¨ì–´ ì§‘í•©ì„ í•´ì‹œë¡œ ë³€í™˜**:  \n",
      "  - ê° ë¬¸ì„œë¥¼ ë‹¨ì–´ ë˜ëŠ” n-gramì˜ ì§‘í•©ìœ¼ë¡œ í‘œí˜„í•©ë‹ˆë‹¤.  \n",
      "  - ì˜ˆ: ë¬¸ì„œ A = {a, b, c}, ë¬¸ì„œ B = {a, b, d}.  \n",
      "\n",
      "- **í•´ì‹œ í•¨ìˆ˜ ì ìš©**:  \n",
      "  - ê° ë‹¨ì–´ì— ëŒ€í•´ **í•´ì‹œ í•¨ìˆ˜**ë¥¼ ì ìš©í•´ í•´ì‹œ ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤.  \n",
      "  - ì˜ˆ: í•´ì‹œ í•¨ìˆ˜ hâ‚(x), hâ‚‚(x), ..., hâ‚–(x)ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.  \n",
      "\n",
      "- **MinHash ê°’ ê³„ì‚°**:  \n",
      "  - ê° í•´ì‹œ í•¨ìˆ˜ì— ëŒ€í•´, ë¬¸ì„œì˜ ë‹¨ì–´ ì§‘í•© ì¤‘ **ê°€ì¥ ì‘ì€ í•´ì‹œ ê°’**ì„ ì„ íƒí•©ë‹ˆë‹¤.  \n",
      "  - ì˜ˆ: ë¬¸ì„œ Aì˜ ë‹¨ì–´ {a, b, c}ì— ëŒ€í•´ hâ‚(a), hâ‚(b), hâ‚(c) ì¤‘ ìµœì†Œê°’ì„ MinHashë¡œ ì„ íƒí•©ë‹ˆë‹¤.  \n",
      "  - ì´ ê³¼ì •ì„ kê°œì˜ í•´ì‹œ í•¨ìˆ˜ì— ëŒ€í•´ ë°˜ë³µí•´, ë¬¸ì„œë§ˆë‹¤ kê°œì˜ MinHash ê°’ì„ ìƒì„±í•©ë‹ˆë‹¤.  \n",
      "\n",
      "---\n",
      "\n",
      "### **2. ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€**\n",
      "#### **(1) ë‹¨ì–´ ì§‘í•©ì„ í•´ì‹œë¡œ ë³€í™˜**\n",
      "- **ë§¤ ë‹¨ì–´ë§ˆë‹¤ í•´ì‹œ í•¨ìˆ˜ ì ìš©**:  \n",
      "  - ê° ë‹¨ì–´ì— ëŒ€í•´ í•´ì‹œ í•¨ìˆ˜ë¥¼ ì ìš©í•´ í•´ì‹œ ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤.  \n",
      "  - ì˜ˆ: ë‹¨ì–´ a, b, cì— ëŒ€í•´ hâ‚(a), hâ‚(b), hâ‚(c)ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.  \n",
      "\n",
      "- **ê°€ì¥ ë‚®ì€ í•´ì‹œ ê°’ ì„ íƒ**:  \n",
      "  - ê° í•´ì‹œ í•¨ìˆ˜ì— ëŒ€í•´, ë¬¸ì„œì˜ ë‹¨ì–´ ì§‘í•© ì¤‘ **ê°€ì¥ ì‘ì€ í•´ì‹œ ê°’**ì„ ì„ íƒí•©ë‹ˆë‹¤.  \n",
      "  - ì˜ˆ: hâ‚(a), hâ‚(b), hâ‚(c) ì¤‘ ìµœì†Œê°’ì„ MinHashë¡œ ì„ íƒí•©ë‹ˆë‹¤.  \n",
      "\n",
      "#### **(2) Kë²ˆ ë°˜ë³µ**\n",
      "- **Kê°œì˜ í•´ì‹œ í•¨ìˆ˜ ì‚¬ìš©**:  \n",
      "  - Kê°œì˜ ì„œë¡œ ë‹¤ë¥¸ í•´ì‹œ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´, ê°ê°ì— ëŒ€í•´ MinHash ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤.  \n",
      "  - ì˜ˆ: hâ‚, hâ‚‚, ..., hâ‚–ì— ëŒ€í•´ ê°ê° MinHash ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤.  \n",
      "\n",
      "#### **(3) ë¬¸ì„œ ìœ ì‚¬ì„± ê°€ì •**\n",
      "- **Kê°œì˜ MinHash ê°’ ì¼ì¹˜**:  \n",
      "  - ë‘ ë¬¸ì„œê°€ **Kê°œì˜ MinHash ê°’ ì¤‘ ì¼ì • ë¹„ìœ¨ ì´ìƒ ì¼ì¹˜**í•˜ë©´, ë‘ ë¬¸ì„œëŠ” ìœ ì‚¬í•œ ê²ƒìœ¼ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤.  \n",
      "  - í•˜ì§€ë§Œ ì´ëŠ” **í™•ë¥ ì  ì¶”ì •**ì´ë¯€ë¡œ, ì™„ë²½í•œ ì •í™•ë„ë¥¼ ë³´ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.  \n",
      "\n",
      "#### **(4) Kê°œì˜ ë‹¨ì–´ê°€ ë‘ ë¬¸ì„œì— ì¡´ì¬í•œë‹¤ëŠ” ì˜ë¯¸ ì•„ë‹˜**\n",
      "- **ì˜¤í•´ì˜ ë¶€ë¶„**:  \n",
      "  - Kê°œì˜ MinHash ê°’ì´ ì¼ì¹˜í•œë‹¤ëŠ” ê²ƒì€ **Kê°œì˜ ê²¹ì¹˜ëŠ” ë‹¨ì–´**ê°€ ìˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.  \n",
      "  - ëŒ€ì‹ , Kê°œì˜ í•´ì‹œ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ ë¬¸ì„œì˜ ìœ ì‚¬ì„±ì„ ì¶”ì •í•˜ëŠ” ì§€í‘œë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.  \n",
      "\n",
      "---\n",
      "\n",
      "### **3. NeMo Data Curatorì—ì„œì˜ ì ìš©**\n",
      "- **GPU ê°€ì†**:  \n",
      "  - MinHashLSHì˜ ê³„ì‚° ë¶€í•˜ë¥¼ ì¤„ì´ê¸° ìœ„í•´ **RAPIDS í”„ë ˆì„ì›Œí¬**ë¥¼ ì‚¬ìš©í•´ GPUì—ì„œ ë³‘ë ¬ ì²˜ë¦¬í•©ë‹ˆë‹¤.  \n",
      "  - ì˜ˆ: 4.5TB RedPajama ë°ì´í„°ì…‹ì˜ ì¤‘ë³µ ì œê±° ì‹œê°„ì„ CPU ëŒ€ë¹„ **12ë°° ë‹¨ì¶•**í–ˆìŠµë‹ˆë‹¤([ê´€ë ¨ ì„¹ì…˜](#document-level_deduplication)).  \n",
      "\n",
      "- **í™•ì¥ì„±**:  \n",
      "  - ìˆ˜ì²œ ê°œì˜ CPU ì½”ì–´ì—ì„œë„ ì„ í˜• í™•ì¥ì„±ì„ ë³´ì´ë©°, ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ ì²˜ë¦¬ì— ì í•©í•©ë‹ˆë‹¤([ê´€ë ¨ ì„¹ì…˜](#scaling_to_many_compute_cores)).  \n",
      "\n",
      "---\n",
      "\n",
      "### **4. ê²°ë¡ **\n",
      "- **MinHashLSH**ëŠ” ë¬¸ì„œì˜ ë‹¨ì–´ ì§‘í•©ì„ í•´ì‹œ ê°’ìœ¼ë¡œ ë³€í™˜í•´ ìœ ì‚¬ì„±ì„ ì¶”ì •í•˜ëŠ” í™•ë¥ ì  ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.  \n",
      "- Kê°œì˜ MinHash ê°’ì€ ë¬¸ì„œì˜ ìœ ì‚¬ì„±ì„ ëŒ€í‘œí•˜ëŠ” ì§€í‘œë¡œ, **Kê°œì˜ ê²¹ì¹˜ëŠ” ë‹¨ì–´**ë¥¼ ì˜ë¯¸í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.  \n",
      "- ê±°ì§“ ì–‘ì„±ì„ ë°©ì§€í•˜ê¸° ìœ„í•´, ë²„í‚· ë‚´ ë¬¸ì„œ ìŒì— ëŒ€í•´ **Jaccard ìœ ì‚¬ë„**ë¥¼ ì¶”ê°€ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.  \n",
      "\n",
      "ì´ë¥¼ í†µí•´ NeMo Data CuratorëŠ” ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì—ì„œë„ **ì •í™•í•˜ê³  íš¨ìœ¨ì ìœ¼ë¡œ ì¤‘ë³µ ë¬¸ì„œë¥¼ ì œê±°**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "qa.get_last_n_conversations(6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
