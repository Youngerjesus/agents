{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "api_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "model_name = \"deepseek-chat\"\n",
    "\n",
    "def get_completion(messages, model_name=model_name):\n",
    "    client = OpenAI(api_key=api_key, base_url=\"https://api.deepseek.com\")\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=messages,\n",
    "        temperature=0.3, \n",
    "        max_tokens=8192, \n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def get_completion_json_output(messages, model_name=model_name):\n",
    "    client = OpenAI(api_key=api_key, base_url=\"https://api.deepseek.com\")\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=messages,\n",
    "        temperature=0.3, \n",
    "        max_tokens=8192, \n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "    return json.loads(response.choices[0].message.content)\n",
    "\n",
    "def get_completion_openai(messages):\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=messages,\n",
    "        temperature=0.3, \n",
    "        max_tokens=8192, \n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import os \n",
    "\n",
    "class MarkdownParser:\n",
    "    def __init__(self, max_header_level: int = 2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            max_header_level: 별도 섹션으로 분리할 최대 헤더 레벨 (기본값: 2, ## 까지만 분리)\n",
    "        \"\"\"\n",
    "        self.max_header_level = max_header_level\n",
    "    \n",
    "\n",
    "    def parse_markdown(self, markdown_text: str) -> Dict[str, str]:\n",
    "        \"\"\"마크다운 텍스트를 파싱하여 섹션별로 분리\"\"\"\n",
    "        sections = {}\n",
    "        current_content = []\n",
    "        current_title = None\n",
    "        \n",
    "        for line in markdown_text.split('\\n'):\n",
    "            if line.startswith('#') and ' ' in line:\n",
    "                # 헤더 레벨 확인\n",
    "                level = len(line) - len(line.lstrip('#'))\n",
    "                \n",
    "                # max_header_level 이하의 헤더만 새로운 섹션으로 처리\n",
    "                if level <= self.max_header_level:\n",
    "                    # 이전 섹션 저장\n",
    "                    if current_title and current_content:\n",
    "                        sections[current_title] = '\\n'.join(current_content).strip()\n",
    "                    \n",
    "                    # 새로운 섹션 시작\n",
    "                    current_title = line.lstrip('#').strip()\n",
    "                    current_content = []\n",
    "                else:\n",
    "                    # 상위 레벨 헤더는 내용에 포함\n",
    "                    current_content.append(line)\n",
    "            else:\n",
    "                if current_title is None:\n",
    "                    continue  # 첫 헤더 이전의 내용은 무시\n",
    "                current_content.append(line)\n",
    "        \n",
    "        # 마지막 섹션 저장\n",
    "        if current_title and current_content:\n",
    "            sections[current_title] = '\\n'.join(current_content).strip()\n",
    "        \n",
    "        return sections\n",
    "    \n",
    "    def save_markdown(self, file_name: str, markdown_text: str, output_dir: str = \"markdown_input\") -> None:\n",
    "        \"\"\"\n",
    "        마크다운 텍스트를 파일로 저장\n",
    "        \"\"\"\n",
    "        with open(os.path.join(output_dir, file_name), 'w', encoding='utf-8') as f:\n",
    "            f.write(markdown_text)\n",
    "        print(f\"Markdown saved to: {os.path.join(output_dir, file_name)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Curating Custom Datasets for LLM Training with NVIDIA NeMo Curator\n",
      "\n",
      "Content: Data curation is the first, and arguably the most important, step in the pretraining and continuous training of [large language models (LLMs)](https://www.nvidia.com/en-us/glossary/large-language-models/) and small language models (SLMs). NVIDIA recently announced the open-source release of [NVIDIA NeMo Curator](https://developer.nvidia.com/blog/scale-and-curate-high-quality-datasets-for-llm-training-with-nemo-curator/), a data curation framework that prepares large-scale, high-quality datasets for pretraining generative AI models. \n",
      "\n",
      "NeMo Curator, which is part of [NVIDIA NeMo](https://www.nvidia.com/en-us/ai-data-science/products/nemo/), offers workflows to download and curate data from various public sources out of the box such as Common Crawl, Wikipedia, and arXiv. It also provides flexibility for developers to customize data curation pipelines to address their unique requirements and create custom datasets. \n",
      "\n",
      "This post walks you through creating a custom data curation pipeline using NeMo Curator. Doing so enables you to:\n",
      "\n",
      "+   Tailor data curation and customize the pipeline to fit the specific needs of your generative AI project.\n",
      "+   Ensure data quality by applying rigorous filters and deduplication to train your model with the best possible dataset.\n",
      "+   Protect privacy by identifying and removing personally identifiable information (PII) and adhere to data protection regulations.\n",
      "+   Streamline the development by automating the curation process, saving time and resources to allow you to focus on solving your business-specific problems.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Title: Overview[](#overview)\n",
      "\n",
      "Content: This tutorial focuses on creating a simple data curation pipeline that can download, process, and filter the TinyStories dataset. TinyStories is a dataset of around 2.2 million short stories generated by GPT-3.5 and GPT-4, featuring English words that are understood by 3- to 4-year olds. It is [publicly available on Hugging Face](https://huggingface.co/datasets/roneneldan/TinyStories). To learn more about the dataset, see [TinyStories: How Small Can Language Models Be and Still Speak Coherent English?](https://arxiv.org/abs/2305.07759)\n",
      "\n",
      "The small size of this dataset makes it ideal for creating and validating data curation pipelines on a local machine. The dataset is split into training and validation files. This tutorial primarily uses the validation file, which contains about 22,000 records.\n",
      "\n",
      "Defining the data curation pipeline involves the following high-level steps:\n",
      "\n",
      "1.  Defining custom document builders that can:\n",
      "    +   Download the dataset from the web and convert to the JSONL format.\n",
      "    +   Iterate through the dataset and extract each document.\n",
      "2.  Define custom modifiers to clean and unify the text data.\n",
      "3.  Filter the dataset using predefined, as well as user-defined heuristics.\n",
      "4.  Deduplicate the dataset and remove identical records.\n",
      "5.  Redact all personally identifiable information (PII) from the dataset.\n",
      "6.  Output the results into the JSONL format.\n",
      "\n",
      "The execution of this curation pipeline should take less than 5 minutes on consumer-grade hardware, and the curated dataset should have about 21,500 records after curation. To access the complete code for this tutorial, visit [NVIDIA/NeMo-Curator](https://github.com/NVIDIA/NeMo-Curator/tree/fc167a6edffd38a55c333742972a5a25b901cb26/tutorials/tinystories) on GitHub.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Title: Prerequisite[](#prerequisite)\n",
      "\n",
      "Content: Before starting, the NeMo Curator framework must be installed. Follow the instructions in the project’s [NeMo Curator GitHub README file](https://github.com/NVIDIA/NeMo-Curator/blob/main/README.md) to install the framework. After that, run the following commands from the terminal to verify the installation. Also install additional dependencies needed for following along.\n",
      "\n",
      "`$ python -c` `\"import nemo_curator; print(nemo_curator);\"`\n",
      "\n",
      "`$ pip3` `install` `requests`\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Title: Defining custom document builders[](#defining_custom_document_builders)\n",
      "\n",
      "Content: To support working with arbitrary datasets, NeMo Curator provides a set of [document builders](https://github.com/NVIDIA/NeMo-Curator/blob/fc167a6edffd38a55c333742972a5a25b901cb26/nemo_curator/download/doc_builder.py) that abstract away the representation of the underlying dataset, including:\n",
      "\n",
      "+   `DocumentDownloader`: an abstract class for downloading remote data to disk.\n",
      "+   `DocumentIterator`: an abstract class for reading dataset raw records from the disk.\n",
      "+   `DocumentExtractor`: an abstract class for extracting text records, as well as any relevant metadata from the records on the disk.\n",
      "\n",
      "Several implementations for these to work with datasets such as CommonCrawl, Wikipedia, and arXiv are available on the [NVIDIA/NeMo-Curator](https://github.com/NVIDIA/NeMo-Curator/tree/9f42c49923e314572da53dc7c0384052420ca657/nemo_curator/download) GitHub repo. The following sections show how to implement each of these abstract classes to customize the work with the TinyStories dataset.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Title: Downloading the TinyStories dataset[](#downloading_the_tinystories_dataset)\n",
      "\n",
      "Content: First, implement the `DocumentDownloader` class, which takes the URL of the dataset’s validation split and downloads it using the `requests` library. \n",
      "\n",
      "`import` `requests`\n",
      "\n",
      "`from` `nemo_curator.download.doc_builder` `import` `DocumentDownloader`\n",
      "\n",
      "`class` `TinyStoriesDownloader(DocumentDownloader):`\n",
      "\n",
      "    `def` `__init__(``self``, download_dir:` `str``):`\n",
      "\n",
      "        `super``().__init__()`\n",
      "\n",
      "        `if` `not` `os.path.isdir(download_dir):`\n",
      "\n",
      "            `os.makedirs(download_dir)`\n",
      "\n",
      "        `self``._download_dir` `=` `download_dir`\n",
      "\n",
      "        `print``(``\"Download directory: \"``,` `self``._download_dir)`\n",
      "\n",
      "    `def` `download(``self``, url:` `str``)` `-``>` `str``:`\n",
      "\n",
      "        `filename` `=` `os.path.basename(url)`\n",
      "\n",
      "        `output_file` `=` `os.path.join(``self``._download_dir, filename)`\n",
      "\n",
      "        `if` `os.path.exists(output_file):`\n",
      "\n",
      "            `print``(f``\"File '{output_file}' already exists, skipping download.\"``)`\n",
      "\n",
      "            `return` `output_file`\n",
      "\n",
      "        `print``(f``\"Downloading TinyStories dataset from '{url}'...\"``)`\n",
      "\n",
      "        `response` `=` `requests.get(url)`\n",
      "\n",
      "        `with` `open``(output_file,` `\"wb\"``) as` `file``:`\n",
      "\n",
      "            `file``.write(response.content)`\n",
      "\n",
      "        `return` `output_file`\n",
      "\n",
      "Next, download the actual dataset using the following code:\n",
      "\n",
      "`# Download the TinyStories dataset.`\n",
      "\n",
      "`downloader` `=` `TinyStoriesDownloader(``\"/path/to/download/\"``)`\n",
      "\n",
      "`tinystories_fp` `=` `downloader.download(TINY_STORIES_URL)`\n",
      "\n",
      "`write_jsonl(tinystories_fp, jsonl_dir)`\n",
      "\n",
      "The dataset will download as a plain text file. To parse this dataset, implement the `DocumentIterator` and `DocumentExtractor` classes. This will enable you to convert it to the JSONL format (one of the formats that NeMo Curator supports). \n",
      "\n",
      "### Iterating and extracting text from the dataset[](#iterating_and_extracting_text_from_the_dataset)\n",
      "\n",
      "In the downloaded file, each record (or story) spans several lines, and records are separated by the `<|endoftext|>` token. The `DocumentIterator` class defines an `iterate` function that takes the path to the file that is to be iterated and yields each record for that file, in the form of the raw text from the record and (optionally) any relevant metadata for that record. Although adding metadata to each record is not mandatory, some data processing algorithms (such as deduplication) rely on such data to uniquely identify each document and correctly perform their intended function.\n",
      "\n",
      "Next, implement the iterator for the TinyStories dataset. Given that each story can span several lines, define the iterator function such that it would keep reading (and storing) each line in the file, until it reaches the separator token. \n",
      "\n",
      "Once a separator is reached, concatenate all the lines seen so far, tack on some metadata to the record, and yield the result. To ensure records are uniquely identifiable, use the dataset’s filename, as well as an internal counter to create the unique `id` and (optionally) `filename` metadata included with each record:\n",
      "\n",
      "`from` `nemo_curator.download.doc_builder` `import` `DocumentIterator`\n",
      "\n",
      "`class` `TinyStoriesIterator(DocumentIterator):`\n",
      "\n",
      "    `SEPARATOR_TOKEN` `=` `\"<|endoftext|>\"`\n",
      "\n",
      "    `def` `__init__(``self``):`\n",
      "\n",
      "        `super``().__init__()`\n",
      "\n",
      "        `self``._counter` `=` `-``1`\n",
      "\n",
      "    `def` `iterate(``self``, file_path):`\n",
      "\n",
      "        `self``._counter` `=` `-``1`\n",
      "\n",
      "        `file_name` `=` `os.path.basename(file_path)`\n",
      "\n",
      "        `with` `open``(file_path,` `\"r\"``) as` `file``:`\n",
      "\n",
      "            `example` `=` `[]`\n",
      "\n",
      "            `def` `split_meta(example):`\n",
      "\n",
      "                `if` `example:`\n",
      "\n",
      "                    `self``._counter` `+``=` `1`\n",
      "\n",
      "                    `content` `=` `\" \"``.join(example)`\n",
      "\n",
      "                    `meta` `=` `{`\n",
      "\n",
      "                        `\"filename\"``: file_name,`\n",
      "\n",
      "                        `\"id\"``: f``\"{file_name}-{self._counter}\"``,`\n",
      "\n",
      "                    `}`\n",
      "\n",
      "                    `return` `meta, content`\n",
      "\n",
      "            `for` `line` `in` `file``:`\n",
      "\n",
      "                `if` `line.strip()` `=``=` `TinyStoriesIterator.SEPARATOR_TOKEN:`\n",
      "\n",
      "                    `if` `example:`\n",
      "\n",
      "                        `yield` `split_meta(example)`\n",
      "\n",
      "                        `example` `=` `[]`\n",
      "\n",
      "                `else``:`\n",
      "\n",
      "                    `example.append(line.strip())`\n",
      "\n",
      "            `if` `example:`\n",
      "\n",
      "                `yield` `split_meta(example)`\n",
      "\n",
      "The last remaining document builder to implement is the `DocumentExtractor` class, which simply returns the text for each record. Note that you may optionally associate some metadata for the extracted text, but the usage of this metadata is beyond the scope of this tutorial.\n",
      "\n",
      "`from` `nemo_curator.download.doc_builder` `import` `DocumentExtractor`\n",
      "\n",
      "`class` `TinyStoriesExtractor(DocumentExtractor):`\n",
      "\n",
      "    `def` `extract(``self``, content:` `str``)` `-``>` `Tuple``[``Set``,` `str``]:`\n",
      "\n",
      "        `# No metadata for the text, just the content.`\n",
      "\n",
      "        `return` `{}, content`\n",
      "\n",
      "### Writing the dataset to the JSONL format[](#writing_the_dataset_to_the_jsonl_format)\n",
      "\n",
      "NeMo Curator provides helpers that can load datasets from the disk in JSONL, Parquet, or Pickle formats. Given the popularity of the JSONL format, this section demonstrates the conversion of the raw text dataset to this format using the iterator and extractor classes previously implemented.\n",
      "\n",
      "To convert the dataset to JSONL, simply point the `TinyStoriesIterator` instance to the downloaded plain text file, iterate through each record, and extract entries using the `TinyStoriesExtractor` instance. Create a JSON object from each record (story) and write it to a single line in an output file. This procedure is straightforward:\n",
      "\n",
      "`import` `os`\n",
      "\n",
      "`import` `json`\n",
      "\n",
      "`def` `write_jsonl(input_filename:` `str``, output_dir:` `str``, dump_every_n:` `int` `=` `10000``):`\n",
      "\n",
      "    `basename` `=` `os.path.basename(input_filename)`\n",
      "\n",
      "    `iterator` `=` `TinyStoriesIterator()`\n",
      "\n",
      "    `extractor` `=` `TinyStoriesExtractor()`\n",
      "\n",
      "    `to_dump` `=` `[]`\n",
      "\n",
      "    `dump_ctr` `=` `0`\n",
      "\n",
      "    `def` `dump_to_file(to_dump, dump_ctr):`\n",
      "\n",
      "        `\"\"\"Helper function to facilitate dumping to file.\"\"\"`\n",
      "\n",
      "        `output_filename` `=` `f``\"{basename}-{dump_ctr}.jsonl\"`\n",
      "\n",
      "        `with` `open``(os.path.join(output_dir, output_filename),` `\"w\"``) as output_file:`\n",
      "\n",
      "            `output_file.writelines(to_dump)`\n",
      "\n",
      "        `# Empty out the list and increment the counter.`\n",
      "\n",
      "        `return` `[], dump_ctr` `+` `1`\n",
      "\n",
      "    `for` `item` `in` `iterator.iterate(input_filename):`\n",
      "\n",
      "        `record_meta, content` `=` `item`\n",
      "\n",
      "        `extracted` `=` `extractor.extract(content)`\n",
      "\n",
      "        `if` `extracted` `is` `None``:`\n",
      "\n",
      "            `continue`\n",
      "\n",
      "        `text_meta, text` `=` `extracted`\n",
      "\n",
      "        `if` `text` `is` `None``:`\n",
      "\n",
      "            `continue`\n",
      "\n",
      "        `line` `=` `{`\n",
      "\n",
      "            `\"text\"``: text,`\n",
      "\n",
      "            `*``*``text_meta,`\n",
      "\n",
      "            `*``*``record_meta,`\n",
      "\n",
      "        `}`\n",
      "\n",
      "        `json_out` `=` `json.dumps(line, ensure_ascii``=``False``)`\n",
      "\n",
      "        `to_dump.append(json_out` `+` `\"\\n\"``)`\n",
      "\n",
      "        `# Should we dump what we have so far?`\n",
      "\n",
      "        `if` `len``(to_dump)` `=``=` `dump_every_n:`\n",
      "\n",
      "            `to_dump, dump_ctr` `=` `dump_to_file(to_dump, dump_ctr)`\n",
      "\n",
      "    `# Dump the remaining records.`\n",
      "\n",
      "    `if` `to_dump:`\n",
      "\n",
      "        `dump_to_file(to_dump, dump_ctr)`\n",
      "\n",
      "Note that by default, this function creates one JSONL file for every 10,000 records. While entirely optional, this is to ensure that each output file remains small enough for easy manual inspection using a text editor, without consuming too much memory.\n",
      "\n",
      "Also note that the content of each story is written into the `text` field of each JSON object. Many data curation operations throughout NeMo Curator need to know which field inside each record contains the text data for that record. If not explicitly specified, these operations assume the existence of a `text` field in the dataset. As such, it is often good practice to always populate the `text` field for each record with the text data of interest.\n",
      "\n",
      "### Loading the dataset using the document builders[](#loading_the_dataset_using_the_document_builders)\n",
      "\n",
      "In NeMo Curator, datasets are represented as objects of type `DocumentDataset`. This provides helpers to load the datasets from disk in various formats. Having created the dataset in the JSONL format, you can use the following code to load it and start working with it:\n",
      "\n",
      "`from nemo_curator.datasets import DocumentDataset`\n",
      "\n",
      "``# define `files` to be a list of all the JSONL files to load``\n",
      "\n",
      "`dataset = DocumentDataset.read_json(files, add_filename=True)`\n",
      "\n",
      "You now have everything needed to define a custom dataset curation pipeline and prepare your data for training (or validation) use cases.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Title: Text cleaning and unification[](#text_cleaning_and_unification)\n",
      "\n",
      "Content: A fundamental operation in data curation pipelines involving text data is text unification and cleaning, as text scraped from online sources may contain inconsistencies or unicode issues. To modify documents, NeMo Curator provides a `DocumentModifier` interface, which defines how a given text from each document should be modified. The actual modification is done through the `Modify` helper, which takes a `DocumentDataset` object along with a `DocumentModifier` object and applies the modifier to the dataset.\n",
      "\n",
      "The TinyStories dataset has inconsistent quotation marks, where some quotation marks are curly, while others are straight. Such inconsistencies (poor quality tokens, for example) may cause problems for models that are trained on this data. \n",
      "\n",
      "To resolve these, create a `DocumentModifier` that unifies all single- and double-quotation marks in the documents by replacing all the curly quotation marks with their straight variants:\n",
      "\n",
      "`from` `nemo_curator.modifiers` `import` `DocumentModifier`\n",
      "\n",
      "`class` `QuotationUnifier(DocumentModifier):`\n",
      "\n",
      "    `def` `modify_document(``self``, text:` `str``)` `-``>` `str``:`\n",
      "\n",
      "        `text` `=` `text.replace(``\"‘\"``,` `\"'\").replace(\"’\", \"'\"``)`\n",
      "\n",
      "        `text` `=` `text.replace(``\"“\"``,` `'\"').replace(\"”\", '\"'``)`\n",
      "\n",
      "        `return` `text`\n",
      "\n",
      "NeMo Curator provides various `DocumentModifier` implementations out of the box. One such modifier is `UnicodeReformatter`, which uses [ftfy](https://github.com/rspeer/python-ftfy) to resolve all unicode issues in the dataset. Next, chain these modifiers together and clean the downloaded dataset. The chaining operation is done through the `Sequential` class, which takes a list of operations that are to be sequentially performed and applies them to a given `DocumentDataset` instance:\n",
      "\n",
      "`from` `nemo_curator` `import` `Sequential`\n",
      "\n",
      "`from` `nemo_curator.modules.modify` `import` `Modify`\n",
      "\n",
      "`from` `nemo_curator.modifiers.unicode_reformatter` `import` `UnicodeReformatter`\n",
      "\n",
      "`def` `clean_and_unify(dataset: DocumentDataset)` `-``> DocumentDataset:`\n",
      "\n",
      "    `cleaners` `=` `Sequential(`\n",
      "\n",
      "        `[`\n",
      "\n",
      "            `# Unify all the quotation marks`\n",
      "\n",
      "            `Modify(QuotationUnifier()),`\n",
      "\n",
      "            `# Unify all unicode`\n",
      "\n",
      "            `Modify(UnicodeReformatter()),`\n",
      "\n",
      "        `]`\n",
      "\n",
      "    `)`\n",
      "\n",
      "    `return` `cleaners(dataset)`\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Title: Dataset filtering[](#dataset_filtering)\n",
      "\n",
      "Content: Another important step in the dataset curation process is data filtering, where some documents that do not fit certain criteria are discarded. For instance, you might want to discard documents that are too short, too long, or incomplete. At the time of writing, NeMo Curator provides 24 heuristics for natural languages, as well as eight heuristics for coding languages. \n",
      "\n",
      "NeMo Curator provides a `DocumentFilter` interface, which defines a way to score documents based on various criteria, along with a `ScoreFilter` helper to filter the documents. The `ScoreFilter` helper takes a `DocumentDataset` along with a `DocumentFilter` and determines whether each document in the dataset passes the filtering criteria.\n",
      "\n",
      "Create a simple `DocumentFilter` that determines whether a story ends with an end of sentence character. The goal is to discard all stories that do not end with an end of sentence character:\n",
      "\n",
      "`from` `nemo_curator.filters` `import` `DocumentFilter`\n",
      "\n",
      "`class` `IncompleteStoryFilter(DocumentFilter):`\n",
      "\n",
      "    `def` `__init__(``self``):`\n",
      "\n",
      "        `super``().__init__()`\n",
      "\n",
      "        `self``._story_terminators` `=` `{``\".\"``,` `\"!\"``,` `\"?\"``,` `'\"'``, \"”\"}`\n",
      "\n",
      "    `def` `score_document(``self``, text:` `str``)` `-``>` `bool``:`\n",
      "\n",
      "        `ret` `=` `text.strip()[``-``1``]` `in` `self``._story_terminators`\n",
      "\n",
      "        `return` `ret`\n",
      "\n",
      "    `def` `keep_document(``self``, score)` `-``>` `bool``:`\n",
      "\n",
      "        `return` `score`\n",
      "\n",
      "The main functionality is implemented in `score_document` and `keep_document` functions, where `False` (that is, don’t keep this document) is returned if the document does not end with an end of sentence character.\n",
      "\n",
      "To apply this filter to the dataset, pass an instance of `IncompleteStoryFilter` to a `ScoreFilter` object. NeMo Curator provides many `DocumentFilter` implementations out of the box. These filters can be chained together through the `Sequential` class. The following code shows how to apply various filters to the dataset:\n",
      "\n",
      "`def` `filter_dataset(dataset: DocumentDataset)` `-``> DocumentDataset:`\n",
      "\n",
      "    `filters` `=` `Sequential(`\n",
      "\n",
      "        `[`\n",
      "\n",
      "            `ScoreFilter(`\n",
      "\n",
      "                `WordCountFilter(min_words``=``80``),`\n",
      "\n",
      "                `text_field``=``\"text\"``,`\n",
      "\n",
      "                `score_field``=``\"word_count\"``,`\n",
      "\n",
      "            `),`\n",
      "\n",
      "            `ScoreFilter(IncompleteStoryFilter(), text_field``=``\"text\"``),`\n",
      "\n",
      "            `ScoreFilter(`\n",
      "\n",
      "                `RepeatingTopNGramsFilter(n``=``2``, max_repeating_ngram_ratio``=``0.2``),`\n",
      "\n",
      "                `text_field``=``\"text\"``,`\n",
      "\n",
      "            `),`\n",
      "\n",
      "            `ScoreFilter(`\n",
      "\n",
      "                `RepeatingTopNGramsFilter(n``=``3``, max_repeating_ngram_ratio``=``0.18``),`\n",
      "\n",
      "                `text_field``=``\"text\"``,`\n",
      "\n",
      "            `),`\n",
      "\n",
      "            `ScoreFilter(`\n",
      "\n",
      "                `RepeatingTopNGramsFilter(n``=``4``, max_repeating_ngram_ratio``=``0.16``),`\n",
      "\n",
      "                `text_field``=``\"text\"``,`\n",
      "\n",
      "            `),`\n",
      "\n",
      "        `]`\n",
      "\n",
      "    `)`\n",
      "\n",
      "    `return` `filters(dataset)`\n",
      "\n",
      "This code filters all short (less than 80 words) or incomplete stories, along with any other stories that have certain ratios of repeating n-grams. Note the usage of `text_field=”text”`, which tells the `ScoreFilter` to pass the contents of the dataset `text` column to each filtering criteria.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Title: Deduplication[](#deduplication)\n",
      "\n",
      "Content: When working with large amounts of text data, there may be records that are identical (or near-identical) to each other. Training on such data may incur additional compute and storage overhead. NeMo Curator provides functionality to find and discard such duplicates. For simplicity, focus on finding exact duplicate records in the dataset. This can be accomplished using the `ExactDuplicates` class, as shown below. \n",
      "\n",
      "This module will automatically leverage existing CUDA devices and the GPU-accelerated implementations from the [RAPIDS cuDF library](https://rapids.ai/) to identify duplicate documents, resulting in much faster processing times. This is because the deduplication stage involves calculating a hash for every document, which is compute-intensive. Each document can be hashed independently, which makes this workload ideal to run in parallel on the GPU.\n",
      "\n",
      "`from` `nemo_curator.modules` `import` `ExactDuplicates`\n",
      "\n",
      "`def` `dedupe(dataset: DocumentDataset)` `-``> DocumentDataset:`\n",
      "\n",
      "    `deduplicator` `=` `ExactDuplicates(id_field``=``\"id\"``, text_field``=``\"text\"``, hash_method``=``\"md5\"``)`\n",
      "\n",
      "    `# Find the duplicates`\n",
      "\n",
      "    `duplicates` `=` `deduplicator(dataset)`\n",
      "\n",
      "    `docs_to_remove` `=` `duplicates.df.map_partitions(`\n",
      "\n",
      "        `lambda` `x: x[x._hashes.duplicated(keep``=``\"first\"``)]`\n",
      "\n",
      "    `)`\n",
      "\n",
      "    `# Remove the duplicates using their IDs.`\n",
      "\n",
      "    `duplicate_ids` `=` `list``(docs_to_remove.compute().``id``)`\n",
      "\n",
      "    `dataset_df` `=` `dataset.df`\n",
      "\n",
      "    `deduped` `=` `dataset_df[~dataset_df.``id``.isin(duplicate_ids)]`\n",
      "\n",
      "    `return` `DocumentDataset(deduped)`\n",
      "\n",
      "This specifies that each record’s unique identifier and content are in the `id` and `text` columns, respectively. Recall that a unique identifier was assigned to each document during the download and extraction phase. This enables the deduplicator to uniquely identify documents from one another. The deduplicator object returns a set of IDs that it has determined to be duplicates. Simply remove these documents from the dataset.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Title: PII redaction[](#pii_redaction)\n",
      "\n",
      "Content: The last processing step discussed in this tutorial is the redaction of personally identifiable information (PII). NeMo Curator facilitates the detection and removal of PII using the [`PiiModifier` class](https://github.com/NVIDIA/NeMo-Curator/blob/9f42c49923e314572da53dc7c0384052420ca657/nemo_curator/modifiers/pii_modifier.py), which is an implementation of the `DocumentModifier` class previously discussed. This modifier leverages the [Presidio framework](https://github.com/microsoft/presidio) and enables you to specify which PII to detect, what action to take for each detection, and process the data in batches to accelerate the operation.\n",
      "\n",
      "The stories in the TinyStories dataset contain many instances of first names. This example intends to detect all such names and replace them with an anonymized token. This can be accomplished using a few lines of code:\n",
      "\n",
      "`from` `nemo_curator.modifiers.pii_modifier` `import` `PiiModifier`\n",
      "\n",
      "`def` `redact_pii(dataset: DocumentDataset)` `-``> DocumentDataset:`\n",
      "\n",
      "    `redactor` `=` `Modify(`\n",
      "\n",
      "        `PiiModifier(`\n",
      "\n",
      "            `supported_entities``=``[``\"PERSON\"``],`\n",
      "\n",
      "            `anonymize_action``=``\"replace\"``,`\n",
      "\n",
      "            `device``=``\"cpu\"``,`\n",
      "\n",
      "        `),`\n",
      "\n",
      "    `)`\n",
      "\n",
      "    `return` `redactor(dataset)`\n",
      "\n",
      "The operation takes the entire dataset and returns the modified dataset.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Title: Putting the curation pipeline together[](#putting_the_curation_pipeline_together)\n",
      "\n",
      "Content: Having implemented each step of the curation pipeline, it’s time to put everything together and sequentially apply each operation on the dataset. You can use the `Sequential` class to chain curation operations together:\n",
      "\n",
      "`curation_steps` `=` `Sequential(`\n",
      "\n",
      "    `[`\n",
      "\n",
      "        `clean_and_unify,`\n",
      "\n",
      "        `filter_dataset,`\n",
      "\n",
      "        `dedupe,`\n",
      "\n",
      "        `redact_pii,`\n",
      "\n",
      "    `]`\n",
      "\n",
      "`)`\n",
      "\n",
      "`dataset` `=` `curation_steps(dataset)`\n",
      "\n",
      "`print``(``\"Executing the pipeline...\"``)`\n",
      "\n",
      "`dataset` `=` `dataset.persist()`\n",
      "\n",
      "`dataset.to_json(``\"/output/path\"``, write_to_filename``=``True``)`\n",
      "\n",
      "Under the hood, NeMo Curator uses Dask to work with the dataset in a distributed manner. Since Dask operations are lazy-evaluated, it’s necessary to call the `.persist` function to instruct Dask to apply the operations. Once processing finishes, you can write the dataset to disk in the JSONL format by calling the `.to_json` function and providing an output path.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Title: Next steps[](#next_steps)\n",
      "\n",
      "Content: NeMo Curator supports many advanced data processing and filtering techniques, such as fuzzy or task-based deduplication, task identification and decontamination, domain classification (and much more) that are not covered in this tutorial. Check out the [collection of data curation examples on GitHub](https://github.com/NVIDIA/NeMo-Curator/tree/main/examples) to learn more. \n",
      "\n",
      "You can also [request access to the NVIDIA NeMo Curator microservice](https://developer.nvidia.com/nemo-microservices), which provides the easiest path for enterprises to get started with data curation from anywhere. It offers streamlined performance and scalability to shorten the time to market.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "with open(\"input_files/Curating Custom Datasets for LLM Training with NVIDIA NeMo Curator.md\", \"r\", encoding=\"utf-8\") as file:\n",
    "    markdown_text = file.read()\n",
    "    markdown_parser = MarkdownParser()\n",
    "    sections = markdown_parser.parse_markdown(markdown_text)\n",
    "\n",
    "    for title, content in sections.items():\n",
    "        print(f\"Title: {title}\\n\")\n",
    "        print(f\"Content: {content}\\n\")\n",
    "        print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "from typing import Dict\n",
    "\n",
    "class TextExplainer:\n",
    "    def __init__(self):\n",
    "        self.conversation_history = []\n",
    "        self.delay = 1\n",
    "        \n",
    "    def explain_section(self, section_title: str, section_content: str, is_first: bool = False) -> str:\n",
    "        \"\"\"\n",
    "        섹션의 내용을 설명하는 함수. Deepseek API 실패시 OpenAI API로 fallback\n",
    "        \n",
    "        Args:\n",
    "            section_title: 섹션 제목\n",
    "            section_content: 섹션 내용\n",
    "            is_first: 첫 번째 섹션인지 여부\n",
    "            \n",
    "        Returns:\n",
    "            str: 섹션에 대한 설명\n",
    "        \"\"\"\n",
    "        if is_first:\n",
    "            prompt = f\"\"\"다음 섹션 '{section_title}'의 내용을 명확하고 자세하게 설명해주세요.\n",
    "            \n",
    "            섹션 내용:\n",
    "            {section_content}\"\"\"\n",
    "        else:\n",
    "            prompt = f\"\"\"이전 섹션에 했던 설명들을 추가 맥락으로 참고해서 다음 섹션 '{section_title}'의 내용을 설명해주세요.\n",
    "            \n",
    "            섹션 내용:\n",
    "            {section_content}\"\"\"\n",
    "        \n",
    "        # 이전 대화 내용을 포함하여 컨텍스트 유지\n",
    "        self.conversation_history.append({\"role\": \"user\", \"content\": prompt})\n",
    "        \n",
    "        # Deepseek API 첫 시도\n",
    "        try:\n",
    "            print(\"Attempting Deepseek API...\")\n",
    "            response = get_completion(self.conversation_history, model_name=\"deepseek-reasoner\")\n",
    "            \n",
    "            if not response:\n",
    "                raise ValueError(\"Empty response from Deepseek API\")\n",
    "                \n",
    "            # 대화 히스토리 업데이트\n",
    "            self.conversation_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Deepseek API attempt failed: {str(e)}\")\n",
    "            print(f\"Retrying Deepseek API in {self.delay} seconds...\")\n",
    "            time.sleep(self.delay)\n",
    "            \n",
    "            # Deepseek API 재시도\n",
    "            try:\n",
    "                print(\"Retrying Deepseek API...\")\n",
    "                response = get_completion(self.conversation_history, model_name=\"deepseek-reasoner\")\n",
    "                \n",
    "                if not response:\n",
    "                    raise ValueError(\"Empty response from Deepseek API\")\n",
    "                    \n",
    "                # 대화 히스토리 업데이트\n",
    "                self.conversation_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "                return response\n",
    "                \n",
    "            except Exception as retry_error:\n",
    "                print(f\"Deepseek API retry also failed: {str(retry_error)}\")\n",
    "                print(\"Falling back to OpenAI API...\")\n",
    "                \n",
    "                # OpenAI API로 fallback\n",
    "                try:\n",
    "                    response = get_completion_openai(self.conversation_history)\n",
    "                    if not response:\n",
    "                        raise ValueError(\"Empty response from OpenAI API\")\n",
    "                        \n",
    "                    # 대화 히스토리 업데이트\n",
    "                    self.conversation_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "                    return response\n",
    "                    \n",
    "                except Exception as openai_error:\n",
    "                    print(f\"OpenAI API fallback also failed: {str(openai_error)}\")\n",
    "                    return f\"Error: Failed to explain section {section_title} with both APIs\"\n",
    "                \n",
    "    def explain_text(self, sections: Dict[str, str]) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        텍스트의 각 섹션을 순차적으로 설명\n",
    "        \n",
    "        Args:\n",
    "            sections: 섹션 제목과 내용을 매핑한 딕셔너리\n",
    "            \n",
    "        Returns:\n",
    "            섹션 제목과 설명을 매핑한 딕셔너리\n",
    "        \"\"\"\n",
    "        explanations = {}\n",
    "        \n",
    "        print(\"\\nProcessing sections:\")\n",
    "        for i, (title, content) in tqdm(enumerate(sections.items()), desc=\"Explaining sections\"):\n",
    "            print(f\"\\nProcessing: {title}\")\n",
    "            explanation = self.explain_section(title, content, is_first=(i==0))\n",
    "            explanations[title] = explanation\n",
    "            \n",
    "        return explanations\n",
    "    \n",
    "    def get_conversation_history(self):\n",
    "        \"\"\"대화 히스토리 반환\"\"\"\n",
    "        return self.conversation_history\n",
    "    \n",
    "    def save_explanations(self, explanations: Dict[str, str], file_name: str, output_dir: str = \"output_files\") -> str:\n",
    "        \"\"\"설명을 파일로 저장\"\"\"\n",
    "        with open(os.path.join(output_dir, file_name), 'w', encoding='utf-8') as f:\n",
    "            for title, explanation in explanations.items():\n",
    "                f.write(f\"## {title}\\n\\n{explanation}\")\n",
    "                f.write(\"\\n\\n---\\n\")\n",
    "        print(f\"Explanations saved to: {os.path.join(output_dir, file_name)}\")\n",
    "        \n",
    "        return os.path.join(output_dir, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Dict\n",
    "import time\n",
    "\n",
    "class TextQA:\n",
    "    def __init__(self, context: Optional[List[Dict[str, str]]] = None):\n",
    "        self.conversation_history = context or []\n",
    "        self.delay = 1\n",
    "        \n",
    "    def ask_question(self, question: str) -> str:\n",
    "        \"\"\"텍스트에 대한 질문에 답변\"\"\"\n",
    "        try:\n",
    "            prompt = f\"\"\"Based on the text we discussed, please answer the following question in Korean. \n",
    "            Be specific and cite relevant sections when possible.\n",
    "\n",
    "            Question: {question}\"\"\"\n",
    "            \n",
    "            self.conversation_history.append({\"role\": \"user\", \"content\": prompt})\n",
    "            response = get_completion(self.conversation_history, model_name=\"deepseek-reasoner\")\n",
    "            self.conversation_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "            \n",
    "            time.sleep(self.delay)\n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question: {str(e)}\")\n",
    "            return f\"Error: Failed to process question\"\n",
    "    \n",
    "    def view_conversation_history(self, start_idx: int = 0, end_idx: Optional[int] = None) -> None:\n",
    "        \"\"\"대화 내역을 출력\n",
    "        \n",
    "        Args:\n",
    "            start_idx: 시작 인덱스 (기본값: 0)\n",
    "            end_idx: 종료 인덱스 (기본값: None, None일 경우 끝까지 출력)\n",
    "        \"\"\"\n",
    "        conversations = [\n",
    "            msg for msg in self.conversation_history \n",
    "            if not msg[\"content\"].startswith(\"Here's the text summary:\")\n",
    "        ]\n",
    "        \n",
    "        end_idx = end_idx if end_idx is not None else len(conversations)\n",
    "        \n",
    "        print(\"\\n=== 대화 내역 ===\\n\")\n",
    "        for i, msg in enumerate(conversations[start_idx:end_idx], start=start_idx):\n",
    "            role = msg[\"role\"].upper()\n",
    "            if role == \"ASSISTANT\":\n",
    "                print(f\"\\n🤖 Assistant ({i}):\\n{msg['content']}\\n\")\n",
    "                print(\"-\" * 80)\n",
    "            elif role == \"USER\":\n",
    "                print(f\"\\n👤 User ({i}):\\n{msg['content']}\\n\")\n",
    "                print(\"-\" * 80)\n",
    "    \n",
    "    def get_last_n_conversations(self, n: int = 1) -> None:\n",
    "        \"\"\"최근 n개의 대화 내역을 출력\n",
    "        \n",
    "        Args:\n",
    "            n: 출력할 최근 대화 개수 (기본값: 1)\n",
    "        \"\"\"\n",
    "        conversations = [\n",
    "            msg for msg in self.conversation_history \n",
    "            if not msg[\"content\"].startswith(\"Here's the text summary:\")\n",
    "        ]\n",
    "        start_idx = max(0, len(conversations) - n)\n",
    "        self.view_conversation_history(start_idx)\n",
    "        \n",
    "    def get_conversation_history(self) -> List[Dict[str, str]]:\n",
    "        \"\"\"전체 대화 기록 반환\"\"\"\n",
    "        return self.conversation_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.console import Console\n",
    "from rich.markdown import Markdown\n",
    "from rich.panel import Panel\n",
    "from rich.syntax import Syntax\n",
    "from rich.table import Table\n",
    "from typing import Dict\n",
    "import os\n",
    "\n",
    "class MarkdownPrinter:\n",
    "    def __init__(self):\n",
    "        self.console = Console()\n",
    "        \n",
    "    def print_markdown_file(self, file_path: str):\n",
    "        \"\"\"마크다운 파일을 이쁘게 출력\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                markdown_content = f.read()\n",
    "            \n",
    "            # 마크다운 렌더링\n",
    "            md = Markdown(markdown_content)\n",
    "            \n",
    "            # 마크다운 내용 출력\n",
    "            self.console.print(md)\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.console.print(f\"[bold red]Error reading file: {str(e)}[/]\")\n",
    "            \n",
    "    def print_sections(self, sections: Dict[str, str]):\n",
    "        \"\"\"섹션별로 구분하여 출력\"\"\"\n",
    "        for section, content in sections.items():\n",
    "            # 섹션 제목\n",
    "            self.console.print(\"\\n\")\n",
    "            self.console.print(Panel(\n",
    "                f\"[bold cyan]{section}[/]\",\n",
    "                border_style=\"cyan\"\n",
    "            ))\n",
    "            \n",
    "            # 섹션 내용\n",
    "            md = Markdown(content)\n",
    "            self.console.print(md)\n",
    "            \n",
    "            # 구분선\n",
    "            self.console.print(\"[dim]\" + \"=\"*80 + \"[/]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_text(file_path: str) -> str:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        file_name = os.path.basename(file_path)\n",
    "        markdown_text = file.read()\n",
    "        markdown_parser = MarkdownParser()\n",
    "        sections = markdown_parser.parse_markdown(markdown_text)\n",
    "        \n",
    "        explainer = TextExplainer()\n",
    "        explanations = explainer.explain_text(sections)\n",
    "        explanation_path = explainer.save_explanations(explanations, file_name)\n",
    "        \n",
    "        qa = TextQA(context=explainer.get_conversation_history())\n",
    "    \n",
    "    return explanation_path, qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing sections:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: Curating Custom Datasets for LLM Training with NVIDIA NeMo Curator\n",
      "Attempting Deepseek API...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections: 1it [01:35, 95.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: Overview[](#overview)\n",
      "Attempting Deepseek API...\n",
      "Deepseek API attempt failed: Expecting value: line 1 column 1 (char 0)\n",
      "Retrying Deepseek API in 1 seconds...\n",
      "Retrying Deepseek API...\n",
      "Deepseek API retry also failed: Expecting value: line 1 column 1 (char 0)\n",
      "Falling back to OpenAI API...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections: 2it [03:37, 111.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API fallback also failed: Error code: 400 - {'error': {'message': 'Model Not Exist', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "\n",
      "Processing: Prerequisite[](#prerequisite)\n",
      "Attempting Deepseek API...\n",
      "Deepseek API attempt failed: Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support successive user or assistant messages (messages[2] and messages[3] in your input). You should interleave the user/assistant messages in the message sequence.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "Retrying Deepseek API in 1 seconds...\n",
      "Retrying Deepseek API...\n",
      "Deepseek API retry also failed: Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support successive user or assistant messages (messages[2] and messages[3] in your input). You should interleave the user/assistant messages in the message sequence.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "Falling back to OpenAI API...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections: 3it [03:38, 61.05s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API fallback also failed: Error code: 400 - {'error': {'message': 'Model Not Exist', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "\n",
      "Processing: Defining custom document builders[](#defining_custom_document_builders)\n",
      "Attempting Deepseek API...\n",
      "Deepseek API attempt failed: Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support successive user or assistant messages (messages[2] and messages[3] in your input). You should interleave the user/assistant messages in the message sequence.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "Retrying Deepseek API in 1 seconds...\n",
      "Retrying Deepseek API...\n",
      "Deepseek API retry also failed: Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support successive user or assistant messages (messages[2] and messages[3] in your input). You should interleave the user/assistant messages in the message sequence.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "Falling back to OpenAI API...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections: 4it [03:40, 37.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API fallback also failed: Error code: 400 - {'error': {'message': 'Model Not Exist', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "\n",
      "Processing: Downloading the TinyStories dataset[](#downloading_the_tinystories_dataset)\n",
      "Attempting Deepseek API...\n",
      "Deepseek API attempt failed: Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support successive user or assistant messages (messages[2] and messages[3] in your input). You should interleave the user/assistant messages in the message sequence.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "Retrying Deepseek API in 1 seconds...\n",
      "Retrying Deepseek API...\n",
      "Deepseek API retry also failed: Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support successive user or assistant messages (messages[2] and messages[3] in your input). You should interleave the user/assistant messages in the message sequence.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "Falling back to OpenAI API...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections: 5it [03:42, 24.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API fallback also failed: Error code: 400 - {'error': {'message': 'Model Not Exist', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "\n",
      "Processing: Text cleaning and unification[](#text_cleaning_and_unification)\n",
      "Attempting Deepseek API...\n",
      "Deepseek API attempt failed: Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support successive user or assistant messages (messages[2] and messages[3] in your input). You should interleave the user/assistant messages in the message sequence.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "Retrying Deepseek API in 1 seconds...\n",
      "Retrying Deepseek API...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections: 6it [03:43, 16.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deepseek API retry also failed: Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support successive user or assistant messages (messages[2] and messages[3] in your input). You should interleave the user/assistant messages in the message sequence.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "Falling back to OpenAI API...\n",
      "OpenAI API fallback also failed: Error code: 400 - {'error': {'message': 'Model Not Exist', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "\n",
      "Processing: Dataset filtering[](#dataset_filtering)\n",
      "Attempting Deepseek API...\n",
      "Deepseek API attempt failed: Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support successive user or assistant messages (messages[2] and messages[3] in your input). You should interleave the user/assistant messages in the message sequence.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "Retrying Deepseek API in 1 seconds...\n",
      "Retrying Deepseek API...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections: 7it [03:45, 11.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deepseek API retry also failed: Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support successive user or assistant messages (messages[2] and messages[3] in your input). You should interleave the user/assistant messages in the message sequence.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "Falling back to OpenAI API...\n",
      "OpenAI API fallback also failed: Error code: 400 - {'error': {'message': 'Model Not Exist', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "\n",
      "Processing: Deduplication[](#deduplication)\n",
      "Attempting Deepseek API...\n",
      "Deepseek API attempt failed: Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support successive user or assistant messages (messages[2] and messages[3] in your input). You should interleave the user/assistant messages in the message sequence.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "Retrying Deepseek API in 1 seconds...\n",
      "Retrying Deepseek API...\n",
      "Deepseek API retry also failed: Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support successive user or assistant messages (messages[2] and messages[3] in your input). You should interleave the user/assistant messages in the message sequence.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "Falling back to OpenAI API...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections: 8it [03:47,  8.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API fallback also failed: Error code: 400 - {'error': {'message': 'Model Not Exist', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "\n",
      "Processing: PII redaction[](#pii_redaction)\n",
      "Attempting Deepseek API...\n",
      "Deepseek API attempt failed: Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support successive user or assistant messages (messages[2] and messages[3] in your input). You should interleave the user/assistant messages in the message sequence.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "Retrying Deepseek API in 1 seconds...\n",
      "Retrying Deepseek API...\n",
      "Deepseek API retry also failed: Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support successive user or assistant messages (messages[2] and messages[3] in your input). You should interleave the user/assistant messages in the message sequence.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "Falling back to OpenAI API...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections: 9it [03:48,  6.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API fallback also failed: Error code: 400 - {'error': {'message': 'Model Not Exist', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "\n",
      "Processing: Putting the curation pipeline together[](#putting_the_curation_pipeline_together)\n",
      "Attempting Deepseek API...\n",
      "Deepseek API attempt failed: Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support successive user or assistant messages (messages[2] and messages[3] in your input). You should interleave the user/assistant messages in the message sequence.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "Retrying Deepseek API in 1 seconds...\n",
      "Retrying Deepseek API...\n",
      "Deepseek API retry also failed: Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support successive user or assistant messages (messages[2] and messages[3] in your input). You should interleave the user/assistant messages in the message sequence.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "Falling back to OpenAI API...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections: 10it [03:50,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API fallback also failed: Error code: 400 - {'error': {'message': 'Model Not Exist', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "\n",
      "Processing: Next steps[](#next_steps)\n",
      "Attempting Deepseek API...\n",
      "Deepseek API attempt failed: Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support successive user or assistant messages (messages[2] and messages[3] in your input). You should interleave the user/assistant messages in the message sequence.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "Retrying Deepseek API in 1 seconds...\n",
      "Retrying Deepseek API...\n",
      "Deepseek API retry also failed: Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support successive user or assistant messages (messages[2] and messages[3] in your input). You should interleave the user/assistant messages in the message sequence.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "Falling back to OpenAI API...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining sections: 11it [03:52, 21.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API fallback also failed: Error code: 400 - {'error': {'message': 'Model Not Exist', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "Explanations saved to: output_files/Curating Custom Datasets for LLM Training with NVIDIA NeMo Curator.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "                        <span style=\"font-weight: bold; text-decoration: underline\">Curating Custom Datasets for LLM Training with NVIDIA NeMo Curator</span>                         \n",
       "\n",
       "                  <span style=\"font-weight: bold\">섹션 설명: \"Curating Custom Datasets for LLM Training with NVIDIA NeMo Curator\"</span>                  \n",
       "\n",
       "이 섹션은 <span style=\"font-weight: bold\">대규모 언어 모델(LLM)</span> 및 소형 언어 모델(SLM) 훈련을 위한 데이터 큐레이션의 중요성과 이를 지원하는 <span style=\"font-weight: bold\">NVIDIA </span>\n",
       "<span style=\"font-weight: bold\">NeMo Curator</span>의 기능을 상세히 소개합니다. 데이터 큐레이션은 모델 성능을 결정하는 핵심 단계로, NeMo Curator는 고품질 \n",
       "데이터셋 구축을 위한 효율적이고 유연한 프레임워크를 제공합니다. 아래는 주요 내용을 구조화한 설명입니다.            \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "                                            <span style=\"font-weight: bold\">1. 데이터 큐레이션의 중요성</span>                                            \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">LLM/SLM 훈련의 첫 단계이자 가장 중요한 단계</span>                                                                     \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>모델의 성능은 학습 데이터의 품질에 직접적으로 영향을 받습니다. 노이즈가 많거나 중복된 데이터는 모델의 정확도와  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>일반화 능력을 저해하므로, 데이터 수집 및 전처리 과정이 매우 중요합니다.                                         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">고품질 데이터의 핵심 요소</span>                                                                                       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>데이터의 다양성, 정확성, 중복 제거, 개인정보 보호(PII 처리) 등이 필수적입니다.                                  \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "                                            <span style=\"font-weight: bold\">2. NVIDIA NeMo Curator 소개</span>                                            \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">목적</span>: 대규모 데이터셋을 자동화된 워크플로우로 처리하여 고품질 훈련 데이터를 생성합니다.                         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">특징</span>:                                                                                                           \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">오픈소스 프레임워크</span>: 개발자가 자유롭게 커스터마이징 가능.                                                    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">NVIDIA NeMo 생태계 통합</span>: 엔드투엔드 모델 개발 파이프라인(훈련-최적화-배포)과 연동됩니다.                     \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">확장성</span>: 분산 컴퓨팅을 지원하여 페타바이트 규모의 데이터(예: Common Crawl)도 처리 가능.                       \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "                                            <span style=\"font-weight: bold\">3. NeMo Curator의 주요 기능</span>                                            \n",
       "\n",
       "                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">가. 기본 제공 데이터 소스 및 워크플로우</span>                                      \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">공개 데이터셋 지원</span>: Common Crawl, Wikipedia, arXiv 등 대표적인 소스에서 데이터를 다운로드하고 전처리하는 사전   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>구축 파이프라인을 제공합니다.                                                                                   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">전처리 단계 예시</span>:                                                                                               \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">필터링</span>: 품질 기준(예: 문법 오류, 의미 없는 텍스트)에 맞지 않는 데이터 제거.                                  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">중복 제거</span>: 문서/문장 수준의 중복 데이터 식별 및 삭제.                                                        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">언어 식별</span>: 특정 언어(예: 영어) 데이터만 선별.                                                                \n",
       "\n",
       "                                         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">나. 커스텀 데이터 파이프라인 구축</span>                                         \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">유연한 확장성</span>: 개발자는 자체 데이터 소스를 추가하거나 전처리 단계를 수정하여 특정 도메인(의료, 법률 등)에 맞는  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>파이프라인을 설계할 수 있습니다.                                                                                \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">사용자 정의 예시</span>:                                                                                               \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">도메인 특화 필터</span>: 금융 데이터에서 숫자/통계 관련 텍스트 강조 추출.                                           \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">맞춤형 중복 검출</span>: 사용자 정의 해시 함수로 중복 문서 식별.                                                    \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "                                          <span style=\"font-weight: bold\">4. NeMo Curator를 사용하는 이유</span>                                          \n",
       "\n",
       "                                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">가. 프로젝트 맞춤형 데이터 큐레이션</span>                                        \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">목적에 따른 최적화</span>: 생성형 AI 모델의 용도(챗봇, 코드 생성 등)에 따라 데이터 특성을 조절할 수 있습니다.          \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>(예: 코드 생성 모델을 위해 GitHub 데이터 강조 수집)                                                             \n",
       "\n",
       "                                               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">나. 데이터 품질 보장</span>                                                \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">다단계 필터링</span>:                                                                                                  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">품질 점수 기반 필터</span>: 휴리스틱 또는 ML 모델을 활용해 저품질 텍스트(스팸, 무의미한 내용) 제거.                 \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">정교한 중복 제거</span>: SimHash, MinHash 등의 알고리즘으로 문서/토큰 수준 중복 감소.                               \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">결과</span>: 모델 훈련 시 노이즈 최소화 → 학습 효율성 및 정확도 향상.                                                  \n",
       "\n",
       "                                          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">다. 개인정보 보호 및 규정 준수</span>                                           \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">PII(개인 식별 정보) 탐지</span>:                                                                                       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>정규표현식, NLP 모델을 활용해 이름, 전화번호, 이메일 등을 식별하고 마스킹 또는 삭제.                         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>GDPR, CCPA 등 데이터 보호 규정 준수를 지원합니다.                                                            \n",
       "\n",
       "                                             <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">라. 자동화를 통한 효율성</span>                                              \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">분산 처리</span>: NVIDIA GPU 및 다중 노드 클러스터를 활용해 대용량 데이터를 빠르게 처리합니다.                         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">재현성</span>: 파이프라인 설정을 코드로 관리하여 실험 재현 및 협업이 용이합니다.                                       \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "                                           <span style=\"font-weight: bold\">5. 결론: NeMo Curator의 장점</span>                                            \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">시간 및 비용 절감</span>: 수동 큐레이션에 드는 리소스를 90% 이상 감소시킬 수 있습니다.                                 \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">고품질 데이터 확보</span>: 엄격한 필터링과 커스텀 전처리로 모델의 신뢰성 향상.                                         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">규모 확장</span>: 클라우드 또는 온프레미스 인프라에서 페타바이트 데이터 처리 가능.                                     \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "이 섹션은 NeMo Curator가 LLM 개발자에게 <span style=\"font-weight: bold\">데이터 큐레이션의 복잡성을 해결</span>하고, <span style=\"font-weight: bold\">비즈니스 목적에 맞는 데이터셋을 구축</span>할\n",
       "수 있는 실용적 도구임을 강조합니다. 이를 통해 개발자는 모델 아키텍처 최적화와 같은 핵심 과제에 집중할 수 있습니다. \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "\n",
       "                                                     <span style=\"font-weight: bold; text-decoration: underline\">Overview</span>                                                      \n",
       "\n",
       "Error: Failed to explain section Overview with both APIs                                                           \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "\n",
       "                                                   <span style=\"font-weight: bold; text-decoration: underline\">Prerequisite</span>                                                    \n",
       "\n",
       "Error: Failed to explain section Prerequisite with both APIs                                                       \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "\n",
       "                                         <span style=\"font-weight: bold; text-decoration: underline\">Defining custom document builders</span>                                         \n",
       "\n",
       "Error: Failed to explain section Defining custom document builders with both APIs                                  \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "\n",
       "                                        <span style=\"font-weight: bold; text-decoration: underline\">Downloading the TinyStories dataset</span>                                        \n",
       "\n",
       "Error: Failed to explain section Downloading the TinyStories dataset with both APIs                                \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "\n",
       "                                           <span style=\"font-weight: bold; text-decoration: underline\">Text cleaning and unification</span>                                           \n",
       "\n",
       "Error: Failed to explain section Text cleaning and unification with both APIs                                      \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "\n",
       "                                                 <span style=\"font-weight: bold; text-decoration: underline\">Dataset filtering</span>                                                 \n",
       "\n",
       "Error: Failed to explain section Dataset filtering with both APIs                                                  \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "\n",
       "                                                   <span style=\"font-weight: bold; text-decoration: underline\">Deduplication</span>                                                   \n",
       "\n",
       "Error: Failed to explain section Deduplication with both APIs                                                      \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "\n",
       "                                                   <span style=\"font-weight: bold; text-decoration: underline\">PII redaction</span>                                                   \n",
       "\n",
       "Error: Failed to explain section PII redaction with both APIs                                                      \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "\n",
       "                                      <span style=\"font-weight: bold; text-decoration: underline\">Putting the curation pipeline together</span>                                       \n",
       "\n",
       "Error: Failed to explain section Putting the curation pipeline together with both APIs                             \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "\n",
       "                                                    <span style=\"font-weight: bold; text-decoration: underline\">Next steps</span>                                                     \n",
       "\n",
       "Error: Failed to explain section Next steps with both APIs                                                         \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "                        \u001b[1;4mCurating Custom Datasets for LLM Training with NVIDIA NeMo Curator\u001b[0m                         \n",
       "\n",
       "                  \u001b[1m섹션 설명: \"Curating Custom Datasets for LLM Training with NVIDIA NeMo Curator\"\u001b[0m                  \n",
       "\n",
       "이 섹션은 \u001b[1m대규모 언어 모델(LLM)\u001b[0m 및 소형 언어 모델(SLM) 훈련을 위한 데이터 큐레이션의 중요성과 이를 지원하는 \u001b[1mNVIDIA \u001b[0m\n",
       "\u001b[1mNeMo Curator\u001b[0m의 기능을 상세히 소개합니다. 데이터 큐레이션은 모델 성능을 결정하는 핵심 단계로, NeMo Curator는 고품질 \n",
       "데이터셋 구축을 위한 효율적이고 유연한 프레임워크를 제공합니다. 아래는 주요 내용을 구조화한 설명입니다.            \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "                                            \u001b[1m1. \u001b[0m\u001b[1m데이터 큐레이션의 중요성\u001b[0m                                            \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1mLLM/SLM 훈련의 첫 단계이자 가장 중요한 단계\u001b[0m                                                                     \n",
       "\u001b[1;33m   \u001b[0m모델의 성능은 학습 데이터의 품질에 직접적으로 영향을 받습니다. 노이즈가 많거나 중복된 데이터는 모델의 정확도와  \n",
       "\u001b[1;33m   \u001b[0m일반화 능력을 저해하므로, 데이터 수집 및 전처리 과정이 매우 중요합니다.                                         \n",
       "\u001b[1;33m • \u001b[0m\u001b[1m고품질 데이터의 핵심 요소\u001b[0m                                                                                       \n",
       "\u001b[1;33m   \u001b[0m데이터의 다양성, 정확성, 중복 제거, 개인정보 보호(PII 처리) 등이 필수적입니다.                                  \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "                                            \u001b[1m2. \u001b[0m\u001b[1mNVIDIA NeMo Curator 소개\u001b[0m                                            \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1m목적\u001b[0m: 대규모 데이터셋을 자동화된 워크플로우로 처리하여 고품질 훈련 데이터를 생성합니다.                         \n",
       "\u001b[1;33m • \u001b[0m\u001b[1m특징\u001b[0m:                                                                                                           \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1m오픈소스 프레임워크\u001b[0m: 개발자가 자유롭게 커스터마이징 가능.                                                    \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mNVIDIA NeMo 생태계 통합\u001b[0m: 엔드투엔드 모델 개발 파이프라인(훈련-최적화-배포)과 연동됩니다.                     \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1m확장성\u001b[0m: 분산 컴퓨팅을 지원하여 페타바이트 규모의 데이터(예: Common Crawl)도 처리 가능.                       \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "                                            \u001b[1m3. \u001b[0m\u001b[1mNeMo Curator의 주요 기능\u001b[0m                                            \n",
       "\n",
       "                                      \u001b[1;2m가. \u001b[0m\u001b[1;2m기본 제공 데이터 소스 및 워크플로우\u001b[0m                                      \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1m공개 데이터셋 지원\u001b[0m: Common Crawl, Wikipedia, arXiv 등 대표적인 소스에서 데이터를 다운로드하고 전처리하는 사전   \n",
       "\u001b[1;33m   \u001b[0m구축 파이프라인을 제공합니다.                                                                                   \n",
       "\u001b[1;33m • \u001b[0m\u001b[1m전처리 단계 예시\u001b[0m:                                                                                               \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1m필터링\u001b[0m: 품질 기준(예: 문법 오류, 의미 없는 텍스트)에 맞지 않는 데이터 제거.                                  \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1m중복 제거\u001b[0m: 문서/문장 수준의 중복 데이터 식별 및 삭제.                                                        \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1m언어 식별\u001b[0m: 특정 언어(예: 영어) 데이터만 선별.                                                                \n",
       "\n",
       "                                         \u001b[1;2m나. \u001b[0m\u001b[1;2m커스텀 데이터 파이프라인 구축\u001b[0m                                         \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1m유연한 확장성\u001b[0m: 개발자는 자체 데이터 소스를 추가하거나 전처리 단계를 수정하여 특정 도메인(의료, 법률 등)에 맞는  \n",
       "\u001b[1;33m   \u001b[0m파이프라인을 설계할 수 있습니다.                                                                                \n",
       "\u001b[1;33m • \u001b[0m\u001b[1m사용자 정의 예시\u001b[0m:                                                                                               \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1m도메인 특화 필터\u001b[0m: 금융 데이터에서 숫자/통계 관련 텍스트 강조 추출.                                           \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1m맞춤형 중복 검출\u001b[0m: 사용자 정의 해시 함수로 중복 문서 식별.                                                    \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "                                          \u001b[1m4. \u001b[0m\u001b[1mNeMo Curator를 사용하는 이유\u001b[0m                                          \n",
       "\n",
       "                                        \u001b[1;2m가. \u001b[0m\u001b[1;2m프로젝트 맞춤형 데이터 큐레이션\u001b[0m                                        \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1m목적에 따른 최적화\u001b[0m: 생성형 AI 모델의 용도(챗봇, 코드 생성 등)에 따라 데이터 특성을 조절할 수 있습니다.          \n",
       "\u001b[1;33m   \u001b[0m(예: 코드 생성 모델을 위해 GitHub 데이터 강조 수집)                                                             \n",
       "\n",
       "                                               \u001b[1;2m나. \u001b[0m\u001b[1;2m데이터 품질 보장\u001b[0m                                                \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1m다단계 필터링\u001b[0m:                                                                                                  \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1m품질 점수 기반 필터\u001b[0m: 휴리스틱 또는 ML 모델을 활용해 저품질 텍스트(스팸, 무의미한 내용) 제거.                 \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1m정교한 중복 제거\u001b[0m: SimHash, MinHash 등의 알고리즘으로 문서/토큰 수준 중복 감소.                               \n",
       "\u001b[1;33m • \u001b[0m\u001b[1m결과\u001b[0m: 모델 훈련 시 노이즈 최소화 → 학습 효율성 및 정확도 향상.                                                  \n",
       "\n",
       "                                          \u001b[1;2m다. \u001b[0m\u001b[1;2m개인정보 보호 및 규정 준수\u001b[0m                                           \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1mPII(개인 식별 정보) 탐지\u001b[0m:                                                                                       \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m정규표현식, NLP 모델을 활용해 이름, 전화번호, 이메일 등을 식별하고 마스킹 또는 삭제.                         \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0mGDPR, CCPA 등 데이터 보호 규정 준수를 지원합니다.                                                            \n",
       "\n",
       "                                             \u001b[1;2m라. \u001b[0m\u001b[1;2m자동화를 통한 효율성\u001b[0m                                              \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1m분산 처리\u001b[0m: NVIDIA GPU 및 다중 노드 클러스터를 활용해 대용량 데이터를 빠르게 처리합니다.                         \n",
       "\u001b[1;33m • \u001b[0m\u001b[1m재현성\u001b[0m: 파이프라인 설정을 코드로 관리하여 실험 재현 및 협업이 용이합니다.                                       \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "                                           \u001b[1m5. \u001b[0m\u001b[1m결론: NeMo Curator의 장점\u001b[0m                                            \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1m시간 및 비용 절감\u001b[0m: 수동 큐레이션에 드는 리소스를 90% 이상 감소시킬 수 있습니다.                                 \n",
       "\u001b[1;33m • \u001b[0m\u001b[1m고품질 데이터 확보\u001b[0m: 엄격한 필터링과 커스텀 전처리로 모델의 신뢰성 향상.                                         \n",
       "\u001b[1;33m • \u001b[0m\u001b[1m규모 확장\u001b[0m: 클라우드 또는 온프레미스 인프라에서 페타바이트 데이터 처리 가능.                                     \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "이 섹션은 NeMo Curator가 LLM 개발자에게 \u001b[1m데이터 큐레이션의 복잡성을 해결\u001b[0m하고, \u001b[1m비즈니스 목적에 맞는 데이터셋을 구축\u001b[0m할\n",
       "수 있는 실용적 도구임을 강조합니다. 이를 통해 개발자는 모델 아키텍처 최적화와 같은 핵심 과제에 집중할 수 있습니다. \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "\n",
       "                                                     \u001b[1;4mOverview\u001b[0m                                                      \n",
       "\n",
       "Error: Failed to explain section Overview with both APIs                                                           \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "\n",
       "                                                   \u001b[1;4mPrerequisite\u001b[0m                                                    \n",
       "\n",
       "Error: Failed to explain section Prerequisite with both APIs                                                       \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "\n",
       "                                         \u001b[1;4mDefining custom document builders\u001b[0m                                         \n",
       "\n",
       "Error: Failed to explain section Defining custom document builders with both APIs                                  \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "\n",
       "                                        \u001b[1;4mDownloading the TinyStories dataset\u001b[0m                                        \n",
       "\n",
       "Error: Failed to explain section Downloading the TinyStories dataset with both APIs                                \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "\n",
       "                                           \u001b[1;4mText cleaning and unification\u001b[0m                                           \n",
       "\n",
       "Error: Failed to explain section Text cleaning and unification with both APIs                                      \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "\n",
       "                                                 \u001b[1;4mDataset filtering\u001b[0m                                                 \n",
       "\n",
       "Error: Failed to explain section Dataset filtering with both APIs                                                  \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "\n",
       "                                                   \u001b[1;4mDeduplication\u001b[0m                                                   \n",
       "\n",
       "Error: Failed to explain section Deduplication with both APIs                                                      \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "\n",
       "                                                   \u001b[1;4mPII redaction\u001b[0m                                                   \n",
       "\n",
       "Error: Failed to explain section PII redaction with both APIs                                                      \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "\n",
       "                                      \u001b[1;4mPutting the curation pipeline together\u001b[0m                                       \n",
       "\n",
       "Error: Failed to explain section Putting the curation pipeline together with both APIs                             \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "\n",
       "                                                    \u001b[1;4mNext steps\u001b[0m                                                     \n",
       "\n",
       "Error: Failed to explain section Next steps with both APIs                                                         \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "explanation_path, qa = process_text(\"input_files/Curating Custom Datasets for LLM Training with NVIDIA NeMo Curator.md\")\n",
    "\n",
    "markdown_printer = MarkdownPrinter()\n",
    "markdown_printer.print_markdown_file(explanation_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\"Scaling Language Models: Methods, Analysis & Insights from Training Gopher\"** 논문에서 설명된 **고품질 데이터 필터링 메커니즘**에 대해 설명드리겠습니다. 이 논문은 대규모 언어 모델 훈련을 위한 데이터 품질 관리 방법을 다루며, NeMo Data Curator에서도 유사한 접근 방식을 사용합니다([관련 섹션](#document-level_quality_filtering)).\n",
      "\n",
      "---\n",
      "\n",
      "### **1. 고품질 데이터 필터링의 필요성**\n",
      "- **문제점**:  \n",
      "  웹 크롤링 데이터(예: Common Crawl)에는 **저품질 콘텐츠**(예: 반복 문자열, 무의미한 텍스트, 상용구)가 다량 포함되어 있습니다.  \n",
      "  - 이러한 데이터는 모델의 **일반화 능력**과 **다운스트림 태스크 성능**을 저하시킬 수 있습니다.  \n",
      "\n",
      "- **해결책**:  \n",
      "  - **분류기(Classifier)**와 **휴리스틱 기반 필터**를 사용해 고품질 텍스트만 선별합니다.  \n",
      "\n",
      "---\n",
      "\n",
      "### **2. 필터링 메커니즘**\n",
      "#### **(1) 휴리스틱 기반 필터**\n",
      "- **목적**:  \n",
      "  간단한 규칙을 사용해 저품질 텍스트를 빠르게 식별합니다.  \n",
      "- **주요 규칙**:  \n",
      "  - **문장 길이**: 너무 짧거나 긴 문장을 제거합니다.  \n",
      "  - **특수 문자 비율**: 특수 문자(예: URL, 기호)가 과도하게 포함된 텍스트를 제거합니다.  \n",
      "  - **반복 문자열**: 동일한 단어나 구가 반복되는 텍스트를 제거합니다.  \n",
      "  - **언어 감지**: 목표 언어와 일치하지 않는 텍스트를 제거합니다.  \n",
      "\n",
      "- **장점**:  \n",
      "  - 계산 비용이 낮고, 대규모 데이터셋에서 빠르게 적용 가능합니다.  \n",
      "\n",
      "#### **(2) 분류기(Classifier) 기반 필터**\n",
      "- **목적**:  \n",
      "  더 정교한 방식으로 텍스트의 품질을 평가합니다.  \n",
      "- **학습 데이터**:  \n",
      "  - 고품질 텍스트(예: 위키피디아, 전문 도메인 문서)와 저품질 텍스트(예: 스팸, 무의미한 콘텐츠)를 레이블링해 분류기를 훈련합니다.  \n",
      "- **모델 구조**:  \n",
      "  - **BERT** 또는 **RoBERTa**와 같은 사전 훈련된 언어 모델을 사용해 텍스트의 품질을 분류합니다.  \n",
      "- **평가 지표**:  \n",
      "  - 텍스트의 **가독성**, **정보성**, **관련성** 등을 점수화합니다.  \n",
      "\n",
      "- **장점**:  \n",
      "  - 휴리스틱 기반 필터보다 더 정확하게 저품질 텍스트를 식별할 수 있습니다.  \n",
      "\n",
      "---\n",
      "\n",
      "### **3. NeMo Data Curator에서의 적용**\n",
      "- **구성 가능한 필터**:  \n",
      "  - 사용자가 정의한 휴리스틱 규칙을 적용해 저품질 텍스트를 제거합니다.  \n",
      "  - 예: 특수 문자 비율, 문장 길이, 언어 감지 등을 설정할 수 있습니다.  \n",
      "\n",
      "- **분류기 기반 필터**:  \n",
      "  - 사전 훈련된 분류기를 사용해 텍스트의 품질을 평가합니다.  \n",
      "  - 예: [관련 연구](https://arxiv.org/abs/2112.11446)에서 제안된 필터를 적용합니다.  \n",
      "\n",
      "- **효과**:  \n",
      "  - 필터링을 통해 다운스트림 태스크 성능이 개선됩니다([관련 섹션](#curated_pretraining_data_results_in_improved_model_downstream_performance)).  \n",
      "\n",
      "---\n",
      "\n",
      "### **4. 결론**\n",
      "- **휴리스틱 기반 필터**는 간단한 규칙을 사용해 저품질 텍스트를 빠르게 제거합니다.  \n",
      "- **분류기 기반 필터**는 더 정교한 방식으로 텍스트의 품질을 평가합니다.  \n",
      "- NeMo Data Curator는 두 방식을 모두 활용해 **고품질 데이터셋**을 구축하며, 이를 통해 **LLM의 성능을 향상**시킵니다.  \n",
      "\n",
      "이러한 필터링 메커니즘은 **Scaling Language Models: Methods, Analysis & Insights from Training Gopher** 논문에서도 유사하게 설명되며, 대규모 언어 모델 훈련에 필수적인 요소로 강조됩니다.\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\" \n",
    "이제 다른 질문이야. \n",
    "\n",
    "글의 내용을 보면 분류기(Classifier) 또는 휴리스틱 기반 필터를 사용해 고품질 텍스트만 선별한다고 하고 참고 문서는 다음과 같다고 말했어. \n",
    "\n",
    "Scaling Language Models: Methods, Analysis & Insights from Training Gopher\n",
    "\n",
    "이 논문에 대해 안다면 어떤 매커니즘으로 고품질 데이터를 필터링하는건지 설명해줘.\n",
    "\"\"\"\n",
    "\n",
    "response = qa.ask_question(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 대화 내역 ===\n",
      "\n",
      "\n",
      "👤 User (14):\n",
      "Based on the text we discussed, please answer the following question in Korean. \n",
      "            Be specific and cite relevant sections when possible.\n",
      "\n",
      "            Question:  \n",
      "MinHashLSM 알고리즘의 작동 원리에 대해 구체적으로 알려줘. \n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🤖 Assistant (15):\n",
      "**MinHashLSH(Locality-Sensitive Hashing) 알고리즘의 작동 원리**를 구체적으로 설명드리겠습니다. 이 알고리즘은 대규모 데이터셋에서 **유사한 문서를 효율적으로 찾기 위해 설계**되었으며, NeMo Data Curator에서도 유사 중복 제거에 활용되었습니다([관련 섹션](#document-level_deduplication)).\n",
      "\n",
      "---\n",
      "\n",
      "### **1. MinHashLSH의 기본 개념**\n",
      "- **목적**:  \n",
      "  두 문서 간의 **Jaccard 유사도**를 빠르게 추정하고, 유사한 문서를 그룹화합니다.  \n",
      "  - **Jaccard 유사도**: 두 집합의 교집합 크기를 합집합 크기로 나눈 값.  \n",
      "    예: 문서 A와 B의 단어 집합이 각각 {a, b, c}와 {a, b, d}라면, Jaccard 유사도는 2/4 = 0.5입니다.  \n",
      "\n",
      "- **핵심 아이디어**:  \n",
      "  - 문서를 **해시 값으로 변환**해 유사성을 빠르게 계산합니다.  \n",
      "  - **LSH(Locality-Sensitive Hashing)**: 유사한 문서를 같은 버킷에 그룹화해, 전체 문서 쌍을 비교하지 않고도 유사성을 판단합니다.  \n",
      "\n",
      "---\n",
      "\n",
      "### **2. MinHashLSH의 작동 원리**\n",
      "#### **(1) MinHash 계산**\n",
      "1. **문서를 집합으로 표현**:  \n",
      "   - 각 문서를 단어 또는 n-gram의 집합으로 변환합니다.  \n",
      "   - 예: 문서 A = {a, b, c}, 문서 B = {a, b, d}.  \n",
      "\n",
      "2. **해시 함수 적용**:  \n",
      "   - 여러 개의 해시 함수를 사용해 각 단어를 해시 값으로 변환합니다.  \n",
      "   - 예: 해시 함수 h₁(x), h₂(x), ..., hₖ(x)를 정의합니다.  \n",
      "\n",
      "3. **MinHash 값 계산**:  \n",
      "   - 각 해시 함수에 대해, 문서의 단어 집합 중 **가장 작은 해시 값**을 선택합니다.  \n",
      "   - 예: 문서 A의 단어 {a, b, c}에 대해 h₁(a), h₁(b), h₁(c) 중 최소값을 MinHash로 선택합니다.  \n",
      "   - 이 과정을 k개의 해시 함수에 대해 반복해, 문서마다 k개의 MinHash 값을 생성합니다.  \n",
      "\n",
      "#### **(2) LSH를 통한 버킷 그룹화**\n",
      "1. **버킷 생성**:  \n",
      "   - MinHash 값을 사용해 문서를 버킷에 할당합니다.  \n",
      "   - 예: MinHash 값을 특정 범위로 나누어 버킷을 생성합니다.  \n",
      "\n",
      "2. **유사 문서 그룹화**:  \n",
      "   - 같은 버킷에 속한 문서는 유사한 문서로 간주합니다.  \n",
      "   - 이때, **거짓 양성(false positives)**이 발생할 수 있으므로, 버킷 내 문서 간 유사도를 추가로 계산해 검증합니다.  \n",
      "\n",
      "#### **(3) 유사도 계산 및 중복 제거**\n",
      "1. **버킷 내 문서 비교**:  \n",
      "   - 같은 버킷에 속한 문서 쌍에 대해 **Jaccard 유사도**를 계산합니다.  \n",
      "   - 예: 문서 A와 B의 Jaccard 유사도가 임계값(예: 0.8) 이상이면 중복으로 판단합니다.  \n",
      "\n",
      "2. **중복 문서 제거**:  \n",
      "   - 유사도가 높은 문서 중 하나를 남기고 나머지는 제거합니다.  \n",
      "\n",
      "---\n",
      "\n",
      "### **3. NeMo Data Curator에서의 적용**\n",
      "- **GPU 가속**:  \n",
      "  - MinHashLSH의 계산 부하를 줄이기 위해 **RAPIDS 프레임워크**를 사용해 GPU에서 병렬 처리합니다.  \n",
      "  - 예: 4.5TB RedPajama 데이터셋의 중복 제거 시간을 CPU 대비 **12배 단축**했습니다([관련 섹션](#document-level_deduplication)).  \n",
      "\n",
      "- **확장성**:  \n",
      "  - 수천 개의 CPU 코어에서도 선형 확장성을 보이며, 대규모 데이터셋 처리에 적합합니다([관련 섹션](#scaling_to_many_compute_cores)).  \n",
      "\n",
      "---\n",
      "\n",
      "### **4. 요약**\n",
      "MinHashLSH 알고리즘은 다음과 같은 단계로 작동합니다:  \n",
      "1. 문서를 단어 집합으로 표현하고, 해시 함수를 적용해 MinHash 값을 계산합니다.  \n",
      "2. LSH를 사용해 유사한 문서를 같은 버킷에 그룹화합니다.  \n",
      "3. 버킷 내 문서 간 유사도를 계산해 중복 문서를 제거합니다.  \n",
      "\n",
      "이 알고리즘은 **대규모 데이터셋에서 유사 중복 문서를 효율적으로 제거**하는 데 적합하며, NeMo Data Curator에서도 성공적으로 적용되었습니다.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "👤 User (16):\n",
      "Based on the text we discussed, please answer the following question in Korean. \n",
      "            Be specific and cite relevant sections when possible.\n",
      "\n",
      "            Question:  \n",
      "MinHashLSM 알고리즘의 작동 원리는로 K 개의 MinHash 를 만드는 건, 두 문서 사이에 K 개의 겹치는 단어가 있다는 것이고, 이것으로 중복이라고 가정을 하는건가? \n",
      "\n",
      "근데 거짓 양성일 수 있으니까 한번 더 Jaccard 유사도 비교를 하는거고? 내가 잘못 이해한거야?\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🤖 Assistant (17):\n",
      "질문 주신 내용을 바탕으로 **MinHashLSH 알고리즘의 작동 원리**를 다시 한번 명확히 설명드리겠습니다. 질문에서 언급하신 부분 중 일부는 오해가 있을 수 있으니, 이를 바로잡고자 합니다.\n",
      "\n",
      "---\n",
      "\n",
      "### **1. MinHashLSH의 핵심 개념**\n",
      "- **MinHash 값 생성**:  \n",
      "  - MinHash는 문서의 단어 집합을 해시 값으로 변환한 후, 각 해시 함수에 대해 **가장 작은 해시 값**을 선택합니다.  \n",
      "  - 이때, **K개의 해시 함수**를 사용해 K개의 MinHash 값을 생성합니다.  \n",
      "  - **K개의 MinHash 값**은 문서의 단어 집합을 대표하는 \"지문(fingerprint)\" 역할을 합니다.  \n",
      "\n",
      "- **중복 문서 가정**:  \n",
      "  - 두 문서가 **K개의 MinHash 값 중 일정 비율 이상 일치**하면, 두 문서는 유사한 것으로 간주합니다.  \n",
      "  - 하지만 이는 **확률적 추정**이므로, 완벽한 정확도를 보장하지 않습니다.  \n",
      "\n",
      "---\n",
      "\n",
      "### **2. 질문에 대한 답변**\n",
      "#### **(1) K개의 MinHash 값과 중복 문서 가정**\n",
      "- **K개의 MinHash 값**은 두 문서가 **K개의 겹치는 단어**를 가진다는 것을 의미하지 않습니다.  \n",
      "  - MinHash 값은 문서의 전체 단어 집합을 대표하는 해시 값의 최소값입니다.  \n",
      "  - 따라서, K개의 MinHash 값이 일치한다는 것은 두 문서의 **Jaccard 유사도가 높을 확률**이 높다는 것을 의미합니다.  \n",
      "\n",
      "#### **(2) 거짓 양성(False Positive) 문제**\n",
      "- **거짓 양성**:  \n",
      "  - MinHashLSH는 확률적 알고리즘이므로, **유사하지 않은 문서가 같은 버킷에 그룹화**될 수 있습니다.  \n",
      "  - 이를 방지하기 위해, 같은 버킷에 속한 문서 쌍에 대해 **Jaccard 유사도**를 추가로 계산합니다.  \n",
      "  - Jaccard 유사도가 임계값(예: 0.8) 이상인 경우에만 중복 문서로 판단합니다.  \n",
      "\n",
      "#### **(3) 정리**\n",
      "- **MinHashLSH의 단계**:  \n",
      "  1. K개의 해시 함수를 사용해 각 문서의 MinHash 값을 생성합니다.  \n",
      "  2. 같은 버킷에 속한 문서는 유사한 문서로 간주합니다.  \n",
      "  3. 버킷 내 문서 쌍에 대해 Jaccard 유사도를 계산해, 거짓 양성을 제거합니다.  \n",
      "\n",
      "- **질문의 오해**:  \n",
      "  - K개의 MinHash 값은 **K개의 겹치는 단어**를 의미하지 않습니다.  \n",
      "  - 대신, K개의 해시 함수를 사용해 문서의 유사성을 추정하는 지표로 사용됩니다.  \n",
      "\n",
      "---\n",
      "\n",
      "### **3. NeMo Data Curator에서의 적용**\n",
      "- **GPU 가속**:  \n",
      "  - MinHashLSH의 계산 부하를 줄이기 위해 **RAPIDS 프레임워크**를 사용해 GPU에서 병렬 처리합니다.  \n",
      "  - 예: 4.5TB RedPajama 데이터셋의 중복 제거 시간을 CPU 대비 **12배 단축**했습니다([관련 섹션](#document-level_deduplication)).  \n",
      "\n",
      "- **확장성**:  \n",
      "  - 수천 개의 CPU 코어에서도 선형 확장성을 보이며, 대규모 데이터셋 처리에 적합합니다([관련 섹션](#scaling_to_many_compute_cores)).  \n",
      "\n",
      "---\n",
      "\n",
      "### **4. 결론**\n",
      "- **MinHashLSH**는 문서의 단어 집합을 해시 값으로 변환해 유사성을 추정하는 확률적 알고리즘입니다.  \n",
      "- K개의 MinHash 값은 문서의 유사성을 대표하는 지표로, **K개의 겹치는 단어**를 의미하지 않습니다.  \n",
      "- 거짓 양성을 방지하기 위해, 버킷 내 문서 쌍에 대해 **Jaccard 유사도**를 추가로 계산합니다.  \n",
      "\n",
      "이를 통해 NeMo Data Curator는 대규모 데이터셋에서도 **정확하고 효율적으로 중복 문서를 제거**할 수 있습니다.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "👤 User (18):\n",
      "Based on the text we discussed, please answer the following question in Korean. \n",
      "            Be specific and cite relevant sections when possible.\n",
      "\n",
      "            Question:  \n",
      "MinHashLSM 에서 단어 집합을 해시로 변환한다는 건 매 단어마다 해시함수로 변환한다는거고, 그리고 이 해시 값중 가장 낮은 값을 최소값으로 선택한다는거지? \n",
      "\n",
      "그리고 이걸 K번 반복한다는거고. 문서가 유사하다고 가정하는거고. \n",
      "\n",
      "이게 맞다면 K 개의 단어가 두 문서에 존재한다 즉 겹친다는거 아님? \n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🤖 Assistant (19):\n",
      "질문 주신 내용을 바탕으로 **MinHashLSH 알고리즘**의 작동 원리를 다시 한번 명확히 설명드리겠습니다. 질문에서 언급하신 부분 중 일부는 오해가 있을 수 있으니, 이를 바로잡고자 합니다.\n",
      "\n",
      "---\n",
      "\n",
      "### **1. MinHashLSH의 핵심 개념**\n",
      "- **단어 집합을 해시로 변환**:  \n",
      "  - 각 문서를 단어 또는 n-gram의 집합으로 표현합니다.  \n",
      "  - 예: 문서 A = {a, b, c}, 문서 B = {a, b, d}.  \n",
      "\n",
      "- **해시 함수 적용**:  \n",
      "  - 각 단어에 대해 **해시 함수**를 적용해 해시 값을 계산합니다.  \n",
      "  - 예: 해시 함수 h₁(x), h₂(x), ..., hₖ(x)를 정의합니다.  \n",
      "\n",
      "- **MinHash 값 계산**:  \n",
      "  - 각 해시 함수에 대해, 문서의 단어 집합 중 **가장 작은 해시 값**을 선택합니다.  \n",
      "  - 예: 문서 A의 단어 {a, b, c}에 대해 h₁(a), h₁(b), h₁(c) 중 최소값을 MinHash로 선택합니다.  \n",
      "  - 이 과정을 k개의 해시 함수에 대해 반복해, 문서마다 k개의 MinHash 값을 생성합니다.  \n",
      "\n",
      "---\n",
      "\n",
      "### **2. 질문에 대한 답변**\n",
      "#### **(1) 단어 집합을 해시로 변환**\n",
      "- **매 단어마다 해시 함수 적용**:  \n",
      "  - 각 단어에 대해 해시 함수를 적용해 해시 값을 계산합니다.  \n",
      "  - 예: 단어 a, b, c에 대해 h₁(a), h₁(b), h₁(c)를 계산합니다.  \n",
      "\n",
      "- **가장 낮은 해시 값 선택**:  \n",
      "  - 각 해시 함수에 대해, 문서의 단어 집합 중 **가장 작은 해시 값**을 선택합니다.  \n",
      "  - 예: h₁(a), h₁(b), h₁(c) 중 최소값을 MinHash로 선택합니다.  \n",
      "\n",
      "#### **(2) K번 반복**\n",
      "- **K개의 해시 함수 사용**:  \n",
      "  - K개의 서로 다른 해시 함수를 사용해, 각각에 대해 MinHash 값을 계산합니다.  \n",
      "  - 예: h₁, h₂, ..., hₖ에 대해 각각 MinHash 값을 계산합니다.  \n",
      "\n",
      "#### **(3) 문서 유사성 가정**\n",
      "- **K개의 MinHash 값 일치**:  \n",
      "  - 두 문서가 **K개의 MinHash 값 중 일정 비율 이상 일치**하면, 두 문서는 유사한 것으로 간주합니다.  \n",
      "  - 하지만 이는 **확률적 추정**이므로, 완벽한 정확도를 보장하지 않습니다.  \n",
      "\n",
      "#### **(4) K개의 단어가 두 문서에 존재한다는 의미 아님**\n",
      "- **오해의 부분**:  \n",
      "  - K개의 MinHash 값이 일치한다는 것은 **K개의 겹치는 단어**가 있다는 것을 의미하지 않습니다.  \n",
      "  - 대신, K개의 해시 함수를 사용해 문서의 유사성을 추정하는 지표로 사용됩니다.  \n",
      "\n",
      "---\n",
      "\n",
      "### **3. NeMo Data Curator에서의 적용**\n",
      "- **GPU 가속**:  \n",
      "  - MinHashLSH의 계산 부하를 줄이기 위해 **RAPIDS 프레임워크**를 사용해 GPU에서 병렬 처리합니다.  \n",
      "  - 예: 4.5TB RedPajama 데이터셋의 중복 제거 시간을 CPU 대비 **12배 단축**했습니다([관련 섹션](#document-level_deduplication)).  \n",
      "\n",
      "- **확장성**:  \n",
      "  - 수천 개의 CPU 코어에서도 선형 확장성을 보이며, 대규모 데이터셋 처리에 적합합니다([관련 섹션](#scaling_to_many_compute_cores)).  \n",
      "\n",
      "---\n",
      "\n",
      "### **4. 결론**\n",
      "- **MinHashLSH**는 문서의 단어 집합을 해시 값으로 변환해 유사성을 추정하는 확률적 알고리즘입니다.  \n",
      "- K개의 MinHash 값은 문서의 유사성을 대표하는 지표로, **K개의 겹치는 단어**를 의미하지 않습니다.  \n",
      "- 거짓 양성을 방지하기 위해, 버킷 내 문서 쌍에 대해 **Jaccard 유사도**를 추가로 계산합니다.  \n",
      "\n",
      "이를 통해 NeMo Data Curator는 대규모 데이터셋에서도 **정확하고 효율적으로 중복 문서를 제거**할 수 있습니다.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "qa.get_last_n_conversations(6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
